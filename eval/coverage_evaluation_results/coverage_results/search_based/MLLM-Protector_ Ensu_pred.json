{
  "paper_id": "MLLM-Protector_ Ensu",
  "meta": {
    "title": "MLLM-Protector: Ensuring MLLM\u2019s Safety without Hurting Performance",
    "section_ids": [],
    "paragraph_ids": []
  },
  "paper_pred": [
    [
      "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
      "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
      "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking",
      "Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak",
      "Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models",
      "ASTRAL: Automated Safety Testing of Large Language Models",
      "Do as I say not as I do: A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs",
      "Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision",
      "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
      "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
      "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
      "Investigating the Catastrophic Forgetting in Multimodal Large Language Models"
    ]
  ],
  "paper_gt": [
    [
      {
        "tag": "Hudson and Manning (2019)",
        "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering.",
        "authors": "Drew\u00a0A. Hudson and Christopher\u00a0D. Manning. 2019.",
        "journal": ""
      },
      {
        "tag": "Schulman et\u00a0al. (2017)",
        "title": "Proximal policy optimization algorithms.",
        "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.",
        "journal": "arXiv preprint arXiv:1707.06347."
      },
      {
        "tag": "Bai et\u00a0al. (2022b)",
        "title": "Constitutional ai: Harmlessness from ai feedback.",
        "authors": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et\u00a0al. 2022b.",
        "journal": "arXiv preprint arXiv:2212.08073."
      },
      {
        "tag": "Chowdhery et\u00a0al. (2022)",
        "title": "Palm: Scaling language modeling with pathways.",
        "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung\u00a0Won Chung, Charles Sutton, Sebastian Gehrmann, et\u00a0al. 2022.",
        "journal": "arXiv preprint arXiv:2204.02311."
      },
      {
        "tag": "Kotha et\u00a0al. (2023)",
        "title": "Understanding catastrophic forgetting in language models via implicit inference.",
        "authors": "Suhas Kotha, Jacob\u00a0Mitchell Springer, and Aditi Raghunathan. 2023.",
        "journal": "arXiv preprint arXiv:2309.10105."
      },
      {
        "tag": "Hoffmann et\u00a0al. (2022)",
        "title": "Training compute-optimal large language models.",
        "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de\u00a0Las Casas, Lisa\u00a0Anne Hendricks, Johannes Welbl, Aidan Clark, et\u00a0al. 2022.",
        "journal": "arXiv preprint arXiv:2203.15556."
      },
      {
        "tag": "Xie et\u00a0al. (2023)",
        "title": "Defending chatgpt against jailbreak attack via self-reminders.",
        "authors": "Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. 2023.",
        "journal": "Nature Machine Intelligence, pages 1\u201311."
      },
      {
        "tag": "Shayegani et\u00a0al. (2023)",
        "title": "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
        "authors": "Erfan Shayegani, Yue Dong, and Nael\u00a0B. Abu-Ghazaleh. 2023.",
        "journal": ""
      },
      {
        "tag": "Stiennon et\u00a0al. (2020)",
        "title": "Learning to summarize with human feedback.",
        "authors": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul\u00a0F Christiano. 2020.",
        "journal": "Advances in Neural Information Processing Systems, 33:3008\u20133021."
      },
      {
        "tag": "OpenAI (2023)",
        "title": "Gpt-4 technical report.",
        "authors": "OpenAI. 2023.",
        "journal": ""
      },
      {
        "tag": "Yi et\u00a0al. (2023)",
        "title": "Benchmarking and defending against indirect prompt injection attacks on large language models.",
        "authors": "Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023.",
        "journal": "arXiv preprint arXiv:2312.14197."
      },
      {
        "tag": "Bai et\u00a0al. (2022a)",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
        "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et\u00a0al. 2022a.",
        "journal": "arXiv preprint arXiv:2204.05862."
      },
      {
        "tag": "Touvron et\u00a0al. (2023)",
        "title": "Llama: Open and efficient foundation language models.",
        "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.",
        "journal": "arXiv preprint arXiv:2302.13971."
      },
      {
        "tag": "Brown et\u00a0al. (2020)",
        "title": "Language models are few-shot learners.",
        "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al. 2020.",
        "journal": "Advances in neural information processing systems, 33:1877\u20131901."
      },
      {
        "tag": "Lee et\u00a0al. (2023)",
        "title": "Aligning text-to-image models using human feedback.",
        "authors": "Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang\u00a0Shane Gu. 2023.",
        "journal": "arXiv preprint arXiv:2302.12192."
      },
      {
        "tag": "Leike et\u00a0al. (2018)",
        "title": "Scalable agent alignment via reward modeling: a research direction.",
        "authors": "Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018.",
        "journal": "arXiv preprint arXiv:1811.07871."
      },
      {
        "tag": "Smith et\u00a0al. (2022)",
        "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.",
        "authors": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et\u00a0al. 2022.",
        "journal": "arXiv preprint arXiv:2201.11990."
      },
      {
        "tag": "Lin et\u00a0al. (2024)",
        "title": "Mitigating the alignment tax of rlhf.",
        "authors": "Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. 2024.",
        "journal": ""
      },
      {
        "tag": "Taori et\u00a0al. (2023)",
        "title": "Stanford alpaca: An instruction-following llama model.",
        "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori\u00a0B. Hashimoto. 2023.",
        "journal": "https://github.com/tatsu-lab/stanford_alpaca."
      },
      {
        "tag": "Wei et\u00a0al. (2023)",
        "title": "Jailbreak and guard aligned language models with only few in-context demonstrations.",
        "authors": "Zeming Wei, Yifei Wang, and Yisen Wang. 2023.",
        "journal": "arXiv preprint arXiv:2310.06387."
      },
      {
        "tag": "Ziegler et\u00a0al. (2019)",
        "title": "Fine-tuning language models from human preferences.",
        "authors": "Daniel\u00a0M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom\u00a0B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.",
        "journal": "arXiv preprint arXiv:1909.08593."
      },
      {
        "tag": "Yao et\u00a0al. (2024)",
        "title": "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models.",
        "authors": "Dongyu Yao, Jianshu Zhang, Ian\u00a0G Harris, and Marcel Carlsson. 2024.",
        "journal": "InICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4485\u20134489. IEEE."
      },
      {
        "tag": "Hao et\u00a0al. (2022)",
        "title": "Optimizing prompts for text-to-image generation.",
        "authors": "Yaru Hao, Zewen Chi, Li\u00a0Dong, and Furu Wei. 2022.",
        "journal": "arXiv preprint arXiv:2212.09611."
      },
      {
        "tag": "Ouyang et\u00a0al. (2022)",
        "title": "Training language models to follow instructions with human feedback.",
        "authors": "Long Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al. 2022.",
        "journal": "Advances in Neural Information Processing Systems, 35:27730\u201327744."
      },
      {
        "tag": "Biderman et\u00a0al. (2023)",
        "title": "Pythia: A suite for analyzing large language models across training and scaling.",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van\u00a0der Wal. 2023.",
        "journal": ""
      },
      {
        "tag": "Pi et\u00a0al. (2023)",
        "title": "Detgpt: Detect what you need via reasoning.",
        "authors": "Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, and Tong Zhang. 2023.",
        "journal": ""
      },
      {
        "tag": "Li et\u00a0al. (2023)",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.",
        "journal": ""
      },
      {
        "tag": "Liu et\u00a0al. (2023c)",
        "title": "Prompt injection attack against llm-integrated applications.",
        "authors": "Yi\u00a0Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023c.",
        "journal": "arXiv preprint arXiv:2306.05499."
      },
      {
        "tag": "Dai et\u00a0al. (2023a)",
        "title": "Safe rlhf: Safe reinforcement learning from human feedback.",
        "authors": "Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023a.",
        "journal": ""
      },
      {
        "tag": "Radford et\u00a0al. (2019)",
        "title": "Language models are unsupervised multitask learners.",
        "authors": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
        "journal": ""
      },
      {
        "tag": "Yuan et\u00a0al. (2023)",
        "title": "Rrhf: Rank responses to align language models with human feedback without tears.",
        "authors": "Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023.",
        "journal": "arXiv preprint arXiv:2304.05302."
      },
      {
        "tag": "Yu et\u00a0al. (2023)",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities.",
        "authors": "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023.",
        "journal": ""
      },
      {
        "tag": "Gao et\u00a0al. (2023)",
        "title": "Llama-adapter v2: Parameter-efficient visual instruction model.",
        "authors": "Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu\u00a0Qiao. 2023.",
        "journal": ""
      },
      {
        "tag": "Nakano et\u00a0al. (2021)",
        "title": "Webgpt: Browser-assisted question-answering with human feedback.",
        "authors": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et\u00a0al. 2021.",
        "journal": "arXiv preprint arXiv:2112.09332."
      },
      {
        "tag": "Zhu et\u00a0al. (2023)",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
        "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",
        "journal": ""
      },
      {
        "tag": "Dong et\u00a0al. (2023)",
        "title": "Raft: Reward ranked finetuning for generative foundation model alignment.",
        "authors": "Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023.",
        "journal": ""
      },
      {
        "tag": "Robey et\u00a0al. (2023)",
        "title": "Smoothllm: Defending large language models against jailbreaking attacks.",
        "authors": "Alexander Robey, Eric Wong, Hamed Hassani, and George\u00a0J Pappas. 2023.",
        "journal": "arXiv preprint arXiv:2310.03684."
      },
      {
        "tag": "Chiang et\u00a0al. (2023)",
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
        "authors": "Wei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E. Gonzalez, Ion Stoica, and Eric\u00a0P. Xing. 2023.",
        "journal": ""
      },
      {
        "tag": "Kang et\u00a0al. (2023)",
        "title": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
        "authors": "Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023.",
        "journal": "arXiv preprint arXiv:2302.05733."
      },
      {
        "tag": "Liu et\u00a0al. (2023b)",
        "title": "Query-relevant images jailbreak large multi-modal models.",
        "authors": "Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu\u00a0Qiao. 2023b.",
        "journal": ""
      },
      {
        "tag": "Wu et\u00a0al. (2021)",
        "title": "Recursively summarizing books with human feedback.",
        "authors": "Jeff Wu, Long Ouyang, Daniel\u00a0M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021.",
        "journal": "arXiv preprint arXiv:2109.10862."
      },
      {
        "tag": "Diao et\u00a0al. (2023)",
        "title": "Lmflow: An extensible toolkit for finetuning and inference of large foundation models.",
        "authors": "Shizhe Diao, Rui Pan, Hanze Dong, Ka\u00a0Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023.",
        "journal": "arXiv preprint arXiv:2306.12420."
      },
      {
        "tag": "Wu et\u00a0al. (2023)",
        "title": "Better aligning text-to-image models with human preference.",
        "authors": "Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023.",
        "journal": "arXiv preprint arXiv:2303.14420."
      },
      {
        "tag": "Bai et\u00a0al. (2023)",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
        "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.",
        "journal": ""
      },
      {
        "tag": "Scheurer et\u00a0al. (2023)",
        "title": "Training language models with language feedback at scale.",
        "authors": "Jeremy Scheurer, Jon\u00a0Ander Campos, Tomasz Korbak, Jun\u00a0Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2023.",
        "journal": "arXiv preprint arXiv:2303.16755."
      },
      {
        "tag": "Scao et\u00a0al. (2022)",
        "title": "Bloom: A 176b-parameter open-access multilingual language model.",
        "authors": "Teven\u00a0Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra\u00a0Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Galle, et\u00a0al. 2022.",
        "journal": "arXiv preprint arXiv:2211.05100."
      },
      {
        "tag": "Su et\u00a0al. (2023)",
        "title": "Pandagpt: One model to instruction-follow them all.",
        "authors": "Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023.",
        "journal": ""
      },
      {
        "tag": "Perez and Ribeiro (2022)",
        "title": "Ignore previous prompt: Attack techniques for language models.",
        "authors": "F\u00e1bio Perez and Ian Ribeiro. 2022.",
        "journal": "arXiv preprint arXiv:2211.09527."
      },
      {
        "tag": "Xie et\u00a0al. (2024)",
        "title": "Gradsafe: Detecting jailbreak prompts for llms via safety-critical gradient analysis.",
        "authors": "Yueqi Xie, Minghong Fang, Renjie Pi, and Neil Gong. 2024.",
        "journal": "arXiv preprint arXiv:2402.13494."
      },
      {
        "tag": "Rombach et\u00a0al. (2022)",
        "title": "High-resolution image synthesis with latent diffusion models.",
        "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022.",
        "journal": ""
      },
      {
        "tag": "Rafailov et\u00a0al. (2023)",
        "title": "Direct preference optimization: Your language model is secretly a reward model.",
        "authors": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher\u00a0D. Manning, and Chelsea Finn. 2023.",
        "journal": ""
      },
      {
        "tag": "Glaese et\u00a0al. (2022)",
        "title": "Improving alignment of dialogue agents via targeted human judgements.",
        "authors": "Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et\u00a0al. 2022.",
        "journal": "arXiv preprint arXiv:2209.14375."
      },
      {
        "tag": "Gong et\u00a0al. (2023)",
        "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
        "authors": "Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. 2023.",
        "journal": ""
      },
      {
        "tag": "Greshake et\u00a0al. (2023)",
        "title": "More than you\u2019ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models.",
        "authors": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023.",
        "journal": "arXiv preprint arXiv:2302.12173."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee. 2023a.",
        "journal": ""
      },
      {
        "tag": "Dai et\u00a0al. (2023b)",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
        "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng\u00a0Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023b.",
        "journal": ""
      },
      {
        "tag": "Geng and Liu (2023)",
        "title": "Openllama: An open reproduction of llama.",
        "authors": "Xinyang Geng and Hao Liu. 2023.",
        "journal": ""
      }
    ]
  ],
  "section_preds": [],
  "section_gts": [],
  "paragraph_preds": [],
  "paragraph_gts": []
}