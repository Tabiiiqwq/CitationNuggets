<!DOCTYPE html>
<!-- saved from url=(0035)https://arxiv.org/html/2407.19651v2 -->
<html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Bridging Compressed Image Latents and Multimodal Large Language Models</title>
<!--Generated on Mon Feb 17 15:20:53 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/bootstrap.bundle.min.js.download"></script>
<script src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/html2canvas.min.js.download"></script>
<script src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/addons_new.js.download"></script>
<script src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/feedbackOverlay.js.download"></script>
<!--<base href="/html/2407.19651v2/">--><base href="."><link rel="stylesheet" href="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style type="text/css">.lf-progress {
  -webkit-appearance: none;
  -moz-apperance: none;
  width: 100%;
  /* margin: 0 10px; */
  height: 4px;
  border-radius: 3px;
  cursor: pointer;
}
.lf-progress:focus {
  outline: none;
  border: none;
}
.lf-progress::-moz-range-track {
  cursor: pointer;
  background: none;
  border: none;
  outline: none;
}
.lf-progress::-webkit-slider-thumb {
  -webkit-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-moz-range-thumb {
  -moz-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: transparent;
  border-color: transparent;
  color: transparent;
}
.lf-progress::-ms-fill-lower {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-fill-upper {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-thumb {
  border: 0;
  height: 15px;
  width: 15px;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress:focus::-ms-fill-lower {
  background: #ccc;
}
.lf-progress:focus::-ms-fill-upper {
  background: #ccc;
}
.lf-player-container :focus {
  outline: 0;
}
.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  opacity: 1;
  visibility: visible;
  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-player-btn-container {
  display: flex;
  align-items: center;
}
.lf-player-btn {
  cursor: pointer;
  fill: #999;
  width: 14px;
}

.lf-player-btn.active {
  fill: #555;
}

.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  background-color: #ffffff;
  opacity: 1;

  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
  padding: 10px;
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-arrow {
  position: absolute;
  z-index: -1;
  content: '';
  bottom: -9px;
  border-style: solid;
  border-width: 10px 10px 0px 10px;
}

.lf-left-align,
.lf-left-align .lfarrow {
  left: 0;
  right: unset;
}

.lf-right-align,
.lf-right-align .lf-arrow {
  right: 0;
  left: unset;
}

.lf-text-input {
  border: 1px #ccc solid;
  border-radius: 5px;
  padding: 3px;
  width: 60px;
  margin: 0;
}

.lf-color-picker {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  height: 90px;
}

.lf-color-selectors {
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}

.lf-color-component {
  display: flex;
  flex-direction: row;
  font-size: 12px;
  align-items: center;
  justify-content: center;
}

.lf-color-component strong {
  width: 40px;
}

.lf-color-component input[type='range'] {
  margin: 0 0 0 10px;
}

.lf-color-component input[type='number'] {
  width: 50px;
  margin: 0 0 0 10px;
}

.lf-color-preview {
  font-size: 12px;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding-left: 5px;
}

.lf-preview {
  height: 60px;
  width: 60px;
}

.lf-popover-snapshot {
  width: 150px;
}
.lf-popover-snapshot h5 {
  margin: 5px 0 10px 0;
  font-size: 0.75rem;
}
.lf-popover-snapshot a {
  display: block;
  text-decoration: none;
}
.lf-popover-snapshot a:before {
  content: '⥼';
  margin-right: 5px;
}
.lf-popover-snapshot .lf-note {
  display: block;
  margin-top: 10px;
  color: #999;
}
.lf-player-controls > div {
  margin-right: 5px;
  margin-left: 5px;
}
.lf-player-controls > div:first-child {
  margin-left: 0px;
}
.lf-player-controls > div:last-child {
  margin-right: 0px;
}
</style><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-toast {
  display: flex;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  right: 0;
  top: 1%;
  width: fit-content;
  padding: 12px 20px;
  margin: auto;
  overflow: auto;
  background: #fef6f9;
  box-shadow: 0px 4px 10px 0px rgba(0, 10, 30, 0.06);
  font-size: 15px;
  border-radius: 8px;
  color: #333;
}

.immersive-translate-toast-content {
  display: flex;
  flex-direction: row;
  align-items: center;
}

.immersive-translate-toast-hidden {
  margin: 0 20px 0 72px;
  text-decoration: underline;
  cursor: pointer;
}

.immersive-translate-toast-close {
  color: #666666;
  font-size: 20px;
  font-weight: bold;
  padding: 0 10px;
  cursor: pointer;
}

@media screen and (max-width: 768px) {
  .immersive-translate-toast {
    top: 0;
    padding: 12px 0px 0 10px;
  }
  .immersive-translate-toast-content {
    flex-direction: column;
    text-align: center;
  }
  .immersive-translate-toast-hidden {
    margin: 10px auto;
  }
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border: 1px solid #888;
  border-radius: 10px;
  width: 80%;
  max-width: 270px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 50% auto !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  word-break: break-all;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  color: black;
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 16px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #007bff;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}
</style></head>
<body data-new-gr-c-s-check-loaded="14.1104.0" data-gr-ext-installed="" data-new-gr-c-s-loaded="14.1104.0"><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2407.19651v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2407.19651v2/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2407.19651v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2407.19651v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S2" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S2.SS1" title="In 2 Related Works ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Multimodal Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S2.SS2" title="In 2 Related Works ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Image Coding for Machines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS1" title="In 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminaries: Neural Image Codecs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS2" title="In 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Overall Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS3" title="In 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Transform-neck</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS4" title="In 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Surrogate Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS5" title="In 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Training Procedure</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS5.SSS0.Px1" title="In 3.5 Training Procedure ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Phase 1: Transform-neck Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS5.SSS0.Px2" title="In 3.5 Training Procedure ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Phase 2: Joint Optimization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS1" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setting</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Training Details and Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Targeted MLLM-based Vision Tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Baselines.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS2" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS3" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Visualization of the Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS4" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Complexity Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5.SSS0.Px1" title="In 4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Training Objective.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5.SSS0.Px2" title="In 4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Partial CLIP Visual Encoder.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5.SSS0.Px3" title="In 4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Different Image Codecs.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS6" title="In 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S5" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1" title="In Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Supplementary Material</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS1" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS1.SSS0.Px1" title="In A.1 Implementation Details ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS1.SSS0.Px2" title="In A.1 Implementation Details ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Evaluation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS2" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Comparison with VVC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS3" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>More Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS4" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>License of Assets Used</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS5" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Additional Performance Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS6" title="In Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Comparison with Token Reduction Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2407.19651v2 [cs.CV] 17 Feb 2025</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Bridging Compressed Image Latents and Multimodal Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chia-Hao Kao<sup class="ltx_sup" id="id1.1.id1">1</sup> Cheng Chien<sup class="ltx_sup" id="id2.2.id2">2</sup> Yu-Jen Tseng<sup class="ltx_sup" id="id3.3.id3">2</sup> Yi-Hsin Chen<sup class="ltx_sup" id="id4.4.id4">2</sup> Alessandro Gnutti<sup class="ltx_sup" id="id5.5.id5">1</sup>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id6.6.id6">Shao-Yuan Lo<sup class="ltx_sup" id="id6.6.id6.1">3</sup> Wen-Hsiao Peng<sup class="ltx_sup" id="id6.6.id6.2">2</sup> Riccardo Leonardi<sup class="ltx_sup" id="id6.6.id6.3">1</sup></span>
<br class="ltx_break"><sup class="ltx_sup" id="id7.7.id7">1</sup>University of Brescia, Italy  <sup class="ltx_sup" id="id8.8.id8">2</sup>National Yang Ming Chiao Tung University, Taiwan 
<br class="ltx_break"><sup class="ltx_sup" id="id9.9.id9">3</sup>Honda Research Institute USA
<br class="ltx_break">
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id10.id1">This paper presents the first-ever study of adapting compressed image latents to suit the needs of downstream vision tasks that adopt Multimodal Large Language Models (MLLMs). MLLMs have extended the success of large language models to modalities (e.g.&nbsp;images) beyond text, but their billion scale hinders deployment on resource-constrained end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed images captured by end devices to the cloud requires an efficient image compression system. To address this, we focus on emerging neural image compression and propose a novel framework with a lightweight transform-neck and a surrogate loss to adapt compressed image latents for MLLM-based vision tasks.
Given the huge scale of MLLMs, our framework <span class="ltx_text" id="id10.id1.1" style="color:#000000;">excludes the entire downstream MLLM except part of its visual encoder</span> from training our system. This stands out from most existing coding for machine approaches that involve downstream networks in training and thus could be impractical when the networks are MLLMs. <span class="ltx_text" id="id10.id1.2" style="color:#000000;">The proposed framework is general in that it is applicable to various MLLMs, neural image codecs, and multiple application scenarios,</span> where the neural image codec can be (1) pre-trained for human perception without updating, (2) fully updated for joint human and machine perception, or (3) fully updated for only machine perception.
Extensive experiments on different neural image codecs and various MLLMs show that our method achieves great rate-accuracy performance with much less complexity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib42" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib41" title="">a</a>)</cite> have demonstrated impressive abilities in various Natural Language Processing (NLP) tasks. Building upon their success, the recent surge of research on Multimodal Large Language Models (MLLMs) extends LLM’s abilities to modalities beyond languages, particularly images, opening up promising opportunities in various applications&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Achiam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib1" title="">2023</a>; Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib24" title="">2024</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite>. MLLMs have shown surprising capability for many vision tasks such as classification&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite>, image captioning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite>, Visual Question Answering (VQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib24" title="">2024</a>)</cite>, and meme interpretation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Achiam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib1" title="">2023</a>)</cite>. These models excel in unseen tasks through instruction following or in-context learning, which is impossible for traditional vision networks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, the massive scale of MLLMs, often comprising billions of parameters, poses significant challenges for deployment on resource-constrained end devices. While computation can be offloaded to the cloud, transmitting images to cloud-hosted MLLMs becomes necessary. <span class="ltx_text" id="S1.p2.1.1" style="color:#000000;">In this case, efficient image compression techniques are crucial to reducing the required transmission bit-rate. Without compression, transmitting raw images incurs significant costs, particularly at scale with numerous users.</span> Our study shows that simply feeding the decoded image, generated by a fixed image codec trained for human perception, into an MLLM (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (a)) substantially degrades task performance, particularly when the image is coded at low rates. This highlights the critical need for efficient image compression that considers the requirements of downstream MLLM-based vision tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To the best of our knowledge, there have been no attempts to tackle image compression specifically for MLLMs. While many prior works address image compression for machine vision, commonly referred to as coding for machines&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Le et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib19" title="">2021b</a>; Chamain et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib6" title="">2021</a>; Matsubara et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib33" title="">2022</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib29" title="">2022b</a>)</cite>, these approaches cannot be directly applied to MLLMs. Two common approaches to coding for machines are image coding and feature coding. The image coding approaches&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Le et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib19" title="">2021b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib18" title="">a</a>)</cite> optimize the image codec for specific downstream tasks and/or networks (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (b)), while the feature coding approaches&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib11" title="">2024</a>)</cite> divide the task network into two parts and focus on compressing the intermediate features (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (c)). However, both approaches face the same issue: the training process becomes challenging when one needs to back-propagate a training objective through a massive MLLM to train the neural image codec. In practice, the billion-scale parameters of MLLMs make the existing coding for machine methods inapplicable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S1.F1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/teaser_fin.png" width="537">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>On the left is inadequate frameworks for image compression for MLLMs, where the image codec is trained for (a) human perception, (b) the downstream task network, or (c) compressing the intermediate features of the task network. On the right is the proposed transform-neck and surrogate loss under three distinct scenarios, with the image codec (d1) pre-trained for human perception, (d2) updated for joint human and machine perception, or (d3) updated for machine perception.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we propose the first neural image compression system for MLLM-based vision tasks that enables compressed latents to suit the needs of downstream MLLMs. <span class="ltx_text" id="S1.p4.1.1" style="color:#000000;">Notably, it is not our objective to develop a new image codec specifically for MLLM-based tasks. Instead, our method involves a lightweight transform-neck and a novel surrogate loss. The transform-neck adapts the compressed image latents of an off-the-shelf neural image codec to match the intermediate features of the visual encoder–i.e. a component of MLLMs–bypassing the needs for full image reconstruction and reducing computational complexity.</span> Our proposed surrogate loss function, which combines the cross-entropy and distillation terms, enables our system to be trained by back-propagating solely through the visual encoder, thus eliminating the need for back-propagation through the entire MLLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text" id="S1.p5.1.1" style="color:#000000;">The proposed method is general in that it is applicable to different neural image codecs under various application scenarios.</span> First, if the downstream applications prioritize the image reconstruction quality for human interaction, our method can work with an off-the-shelf image codec trained for human perception (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (d1)). Without any modification or re-training of the codec, our method adapts the compressed image latents while maintaining the same image reconstruction quality. Second, when allowing the image codec to be updated, we propose a multi-task training strategy that optimizes the codec for both human and machine perception (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (d2)). This significantly improves MLLM performance at the cost of a marginal drop in the image’s reconstruction quality. Finally, we consider an extreme setting in which the applications prioritize machine perception over image reconstruction. In this case, the encoder and the transform-neck are jointly optimized for the MLLM systems exclusively (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> (d3)). The main contributions of this work are summarized as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">It marks the first exploration into the field of neural image coding for MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The proposed transform-neck adapts the compressed image latents to downstream MLLMs, avoiding the need for image reconstruction and thus saving computational complexity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The proposed surrogate loss leverages the visual encoder to update the system, avoiding back-propagating the task loss through the heavy MLLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">The proposed framework is broadly applicable to a wide range of neural image codecs and MLLMs, regardless of their architectures.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">It is able to accommodate various application scenarios that involve human perception, machine perception, or both.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text" id="S1.p6.1.1" style="color:#000000;">Last but not least, the transform-neck trained with our surrogate loss exhibits a degree of universality, since it is readily applicable to multiple MLLMs that share the same visual encoder, without the need for retraining. Our method achieves (1) up to 60-80% bit-rate reductions under the same recognition accuracy over existing image codecs (e.g. ELIC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>)</cite> and VVC intra coding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bross et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib4" title="">2021</a>)</cite>) (Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS2" title="4.2 Performance Comparison ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.2</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS2" title="A.2 Comparison with VVC ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>) and (2) a nearly 95% reduction in decoding kMAC/pixel as compared to performing full image reconstruction followed by enhancing the reconstructed image for MLLM-based tasks (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS4" title="4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Our system can be successfully trained under various application scenarios on one RTX 4090 with 24GB of memory. This is not possible when the entire MLLM is involved in the training process.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multimodal Large Language Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In recent years, there has been a surge of interest in MLLMs following the impressive demonstration of LLM’s ability in the NLP field&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib42" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib41" title="">a</a>; Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib14" title="">2023</a>)</cite>. Many have sought to extend the success of these models from text to other modalities, particularly images, and several works have shown their effectiveness on various tasks, such as image captioning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib21" title="">2023b</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib24" title="">2024</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib26" title="">2023a</a>)</cite>, VQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite>, Referring Expression Comprehension (REC)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>; Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib36" title="">2024</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib48" title="">2024b</a>)</cite>, few-shot classification&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib45" title="">2024</a>; Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite>, action anticipation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mittal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib35" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Most existing MLLM approaches use a visual encoder to process the input image data, and then introduce a connector to bridge the image features to the tokens understandable by the LLM. Earlier works adopt simpler connector designs, such as linear projectors&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib26" title="">2023a</a>)</cite>, while subsequent works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib21" title="">2023b</a>; Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite> have refined upon the design for both performance and complexity. Furthermore, the entire MLLM can be further fine-tuned to enhance its capabilities through instruction tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib26" title="">2023a</a>; Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib49" title="">2024a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">A notable aspect of the MLLMs is their reliance on existing pre-trained visual encoders in their systems, with CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib37" title="">2021</a>)</cite> visual encoder being a very common choice for a large number of methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>; Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>; Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib24" title="">2024</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib21" title="">2023b</a>)</cite>. Trained on large image-text pair data, the CLIP visual encoder offers the feature space that combines language and image modalities in a sense, making it a desirable feature for MLLMs. Notably, all the existing works on MLLMs do not consider the scenarios where image compression is present, which is a significant departure from our work. <span class="ltx_text" id="S2.SS1.p3.1.1" style="color:#000000;">We note that some approaches&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib39" title="">2024</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib22" title="">2024</a>)</cite> perform token reduction to minimize the inference cost of the downstream MLLMs. These techniques are orthogonal to and can be combined with our method (see Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS6" title="A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">A.6</span></a> for more discussions).</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image Coding for Machines</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Neural image compression systems have made significant progress in the past few years. As a matter of fact, several works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib30" title="">2023b</a>)</cite> have even outperformed the traditional codecs such as intra coding in VVC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bross et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib4" title="">2021</a>)</cite>. However, these methods primarily focus on the quality of reconstructed images for human perception. Coding for machines, in contrast, targets downstream machine vision over human perception, and it has attracted increasing attention recently.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">A common approach simply involves training the compression system for a predefined target downstream computer vision task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Le et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib19" title="">2021b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib18" title="">a</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib43" title="">2022</a>)</cite>, enabling the reconstructed image to be suitable for machine vision, albeit potentially sacrificing perceptual quality. Conversely, <cite class="ltx_cite ltx_citemacro_cite">Chamain et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib6" title="">2021</a>)</cite> tune the task network to better process the compressed images, while <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib8" title="">2023b</a>)</cite> leverage prompt-tuning method on Transformer-based codecs to boost performance on multiple tasks. Also, with the trend of the new JPEG AI learning-based image coding standard&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ascenso et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib2" title="">2023</a>)</cite>, some methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib28" title="">2022a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib27" title="">2021</a>; Mei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib34" title="">2021</a>; Singh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib40" title="">2020</a>)</cite> utilize the compressed image latents instead of the reconstructed image for recognition through bridging the latents to task network. On the other hand, <cite class="ltx_cite ltx_citemacro_citep">(Ding et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib11" title="">2024</a>)</cite> directly compress the intermediate features of recognition networks, while <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib12" title="">2022</a>)</cite> learn the omnipotent features suitable for various tasks in a self-supervised manner and fine-tune each task network tail on such features.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">It is crucial to note that none of the coding for machine methods considers MLLMs at the receiver side. All the above-mentioned methods leverage back-propagation through recognition models to update the system or even re-train the recognition network itself, both of which are prohibitively expensive for MLLMs due to their huge scale. Therefore, the direct application of the same methods on MLLMs is almost infeasible. <span class="ltx_text" id="S2.SS2.p3.1.1" style="color:#000000;">In addition, mainstream image coding for machines methods (e.g.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib8" title="">2023b</a>); Ascenso et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib2" title="">2023</a>)</cite>) remain mostly task-specific.</span> They typically adopt a task-based loss, which restricts the resulting models to be optimized for a single task and recognition model, thus requiring re-training for each new task and incurring additional costs. We aim to be the first to propose a neural image compression system designed for MLLMs, achieved through a lightweight transform-neck and a surrogate loss, which bypasses the necessity of involving the entire billion-scale MLLM in the training process. Moreover, our surrogate loss incorporates a cross-entropy loss to bridge visual features with the text domain for MLLMs, complementing the feature-constraining distillation loss. This combination further differentiates our approach from existing methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries: Neural Image Codecs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.26">The high-level architecture of a neural image codec is depicted in the top central green box in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.F2" title="Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. In a typical hyperprior-based neural image codec&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ballé et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib3" title="">2018</a>)</cite>, the key components include the main encoder <math alttext="g_{a}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑔</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">g_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>, the main decoder <math alttext="g_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑔</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">g_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, as well as the hyperprior encoder <math alttext="h_{a}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ℎ</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">h_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and decoder <math alttext="h_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ℎ</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">h_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. Given an RGB image <math alttext="x\in\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml"><mn id="S3.SS1.p1.5.m5.1.1.3.3.2" xref="S3.SS1.p1.5.m5.1.1.3.3.2.cmml">3</mn><mo id="S3.SS1.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.3.cmml">H</mi><mo id="S3.SS1.p1.5.m5.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.4" xref="S3.SS1.p1.5.m5.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><in id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></in><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑥</ci><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3"><times id="S3.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.1"></times><cn id="S3.SS1.p1.5.m5.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.3.3.2">3</cn><ci id="S3.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.3">𝐻</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.4.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">x\in\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_H</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_W</annotation></semantics></math> represent the height and width of the image, respectively, <math alttext="g_{a}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">g</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝑔</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">g_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> performs the analysis transform of <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_x</annotation></semantics></math> and generates the image latent representation <math alttext="y\in\mathbb{R}^{N\times\frac{H}{16}\times\frac{W}{16}}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><mrow id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.10.m10.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.3.2" xref="S3.SS1.p1.10.m10.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.10.m10.1.1.3.3" xref="S3.SS1.p1.10.m10.1.1.3.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.3.3.2" xref="S3.SS1.p1.10.m10.1.1.3.3.2.cmml">N</mi><mo id="S3.SS1.p1.10.m10.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.10.m10.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p1.10.m10.1.1.3.3.3" xref="S3.SS1.p1.10.m10.1.1.3.3.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.3.3.3.2" xref="S3.SS1.p1.10.m10.1.1.3.3.3.2.cmml">H</mi><mn id="S3.SS1.p1.10.m10.1.1.3.3.3.3" xref="S3.SS1.p1.10.m10.1.1.3.3.3.3.cmml">16</mn></mfrac><mo id="S3.SS1.p1.10.m10.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.10.m10.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p1.10.m10.1.1.3.3.4" xref="S3.SS1.p1.10.m10.1.1.3.3.4.cmml"><mi id="S3.SS1.p1.10.m10.1.1.3.3.4.2" xref="S3.SS1.p1.10.m10.1.1.3.3.4.2.cmml">W</mi><mn id="S3.SS1.p1.10.m10.1.1.3.3.4.3" xref="S3.SS1.p1.10.m10.1.1.3.3.4.3.cmml">16</mn></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><in id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1"></in><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">𝑦</ci><apply id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.10.m10.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3"><times id="S3.SS1.p1.10.m10.1.1.3.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.1"></times><ci id="S3.SS1.p1.10.m10.1.1.3.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.2">𝑁</ci><apply id="S3.SS1.p1.10.m10.1.1.3.3.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.3"><divide id="S3.SS1.p1.10.m10.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.3"></divide><ci id="S3.SS1.p1.10.m10.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.3.2">𝐻</ci><cn id="S3.SS1.p1.10.m10.1.1.3.3.3.3.cmml" type="integer" xref="S3.SS1.p1.10.m10.1.1.3.3.3.3">16</cn></apply><apply id="S3.SS1.p1.10.m10.1.1.3.3.4.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.4"><divide id="S3.SS1.p1.10.m10.1.1.3.3.4.1.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.4"></divide><ci id="S3.SS1.p1.10.m10.1.1.3.3.4.2.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3.4.2">𝑊</ci><cn id="S3.SS1.p1.10.m10.1.1.3.3.4.3.cmml" type="integer" xref="S3.SS1.p1.10.m10.1.1.3.3.4.3">16</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">y\in\mathbb{R}^{N\times\frac{H}{16}\times\frac{W}{16}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × divide start_ARG italic_H end_ARG start_ARG 16 end_ARG × divide start_ARG italic_W end_ARG start_ARG 16 end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><mi id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><ci id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">italic_N</annotation></semantics></math> indicating the channel size. To transmit <math alttext="y" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">italic_y</annotation></semantics></math> more efficiently, it is first uniformly quantized into <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13.1"><semantics id="S3.SS1.p1.13.m13.1a"><mover accent="true" id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.13.m13.1.1.1" xref="S3.SS1.p1.13.m13.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><ci id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1.1">^</ci><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.13.m13.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> and then entropy coded considering a learned prior distribution <math alttext="p(\hat{y})" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m14.1"><semantics id="S3.SS1.p1.14.m14.1a"><mrow id="S3.SS1.p1.14.m14.1.2" xref="S3.SS1.p1.14.m14.1.2.cmml"><mi id="S3.SS1.p1.14.m14.1.2.2" xref="S3.SS1.p1.14.m14.1.2.2.cmml">p</mi><mo id="S3.SS1.p1.14.m14.1.2.1" xref="S3.SS1.p1.14.m14.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.p1.14.m14.1.2.3.2" xref="S3.SS1.p1.14.m14.1.1.cmml"><mo id="S3.SS1.p1.14.m14.1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.14.m14.1.1.cmml">(</mo><mover accent="true" id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml"><mi id="S3.SS1.p1.14.m14.1.1.2" xref="S3.SS1.p1.14.m14.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.14.m14.1.1.1" xref="S3.SS1.p1.14.m14.1.1.1.cmml">^</mo></mover><mo id="S3.SS1.p1.14.m14.1.2.3.2.2" stretchy="false" xref="S3.SS1.p1.14.m14.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.2.cmml" xref="S3.SS1.p1.14.m14.1.2"><times id="S3.SS1.p1.14.m14.1.2.1.cmml" xref="S3.SS1.p1.14.m14.1.2.1"></times><ci id="S3.SS1.p1.14.m14.1.2.2.cmml" xref="S3.SS1.p1.14.m14.1.2.2">𝑝</ci><apply id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.2.3.2"><ci id="S3.SS1.p1.14.m14.1.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1.1">^</ci><ci id="S3.SS1.p1.14.m14.1.1.2.cmml" xref="S3.SS1.p1.14.m14.1.1.2">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">p(\hat{y})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.14.m14.1d">italic_p ( over^ start_ARG italic_y end_ARG )</annotation></semantics></math>. This learned distribution is content dependent, thanks to the hyperprior encoder <math alttext="h_{a}" class="ltx_Math" display="inline" id="S3.SS1.p1.15.m15.1"><semantics id="S3.SS1.p1.15.m15.1a"><msub id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">ℎ</ci><ci id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">h_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.15.m15.1d">italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and decoder <math alttext="h_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.16.m16.1"><semantics id="S3.SS1.p1.16.m16.1a"><msub id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml"><mi id="S3.SS1.p1.16.m16.1.1.2" xref="S3.SS1.p1.16.m16.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.16.m16.1.1.3" xref="S3.SS1.p1.16.m16.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><apply id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2">ℎ</ci><ci id="S3.SS1.p1.16.m16.1.1.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">h_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.16.m16.1d">italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. In particular, <math alttext="h_{a}" class="ltx_Math" display="inline" id="S3.SS1.p1.17.m17.1"><semantics id="S3.SS1.p1.17.m17.1a"><msub id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml"><mi id="S3.SS1.p1.17.m17.1.1.2" xref="S3.SS1.p1.17.m17.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.17.m17.1.1.3" xref="S3.SS1.p1.17.m17.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><apply id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.17.m17.1.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p1.17.m17.1.1.2.cmml" xref="S3.SS1.p1.17.m17.1.1.2">ℎ</ci><ci id="S3.SS1.p1.17.m17.1.1.3.cmml" xref="S3.SS1.p1.17.m17.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">h_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.17.m17.1d">italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> takes <math alttext="y" class="ltx_Math" display="inline" id="S3.SS1.p1.18.m18.1"><semantics id="S3.SS1.p1.18.m18.1a"><mi id="S3.SS1.p1.18.m18.1.1" xref="S3.SS1.p1.18.m18.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m18.1b"><ci id="S3.SS1.p1.18.m18.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m18.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.18.m18.1d">italic_y</annotation></semantics></math> as input and produces the side information <math alttext="z\in\mathbb{R}^{N_{h}\times\frac{H}{64}\times\frac{W}{64}}" class="ltx_Math" display="inline" id="S3.SS1.p1.19.m19.1"><semantics id="S3.SS1.p1.19.m19.1a"><mrow id="S3.SS1.p1.19.m19.1.1" xref="S3.SS1.p1.19.m19.1.1.cmml"><mi id="S3.SS1.p1.19.m19.1.1.2" xref="S3.SS1.p1.19.m19.1.1.2.cmml">z</mi><mo id="S3.SS1.p1.19.m19.1.1.1" xref="S3.SS1.p1.19.m19.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.19.m19.1.1.3" xref="S3.SS1.p1.19.m19.1.1.3.cmml"><mi id="S3.SS1.p1.19.m19.1.1.3.2" xref="S3.SS1.p1.19.m19.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.19.m19.1.1.3.3" xref="S3.SS1.p1.19.m19.1.1.3.3.cmml"><msub id="S3.SS1.p1.19.m19.1.1.3.3.2" xref="S3.SS1.p1.19.m19.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.19.m19.1.1.3.3.2.2" xref="S3.SS1.p1.19.m19.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS1.p1.19.m19.1.1.3.3.2.3" xref="S3.SS1.p1.19.m19.1.1.3.3.2.3.cmml">h</mi></msub><mo id="S3.SS1.p1.19.m19.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.19.m19.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p1.19.m19.1.1.3.3.3" xref="S3.SS1.p1.19.m19.1.1.3.3.3.cmml"><mi id="S3.SS1.p1.19.m19.1.1.3.3.3.2" xref="S3.SS1.p1.19.m19.1.1.3.3.3.2.cmml">H</mi><mn id="S3.SS1.p1.19.m19.1.1.3.3.3.3" xref="S3.SS1.p1.19.m19.1.1.3.3.3.3.cmml">64</mn></mfrac><mo id="S3.SS1.p1.19.m19.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.19.m19.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS1.p1.19.m19.1.1.3.3.4" xref="S3.SS1.p1.19.m19.1.1.3.3.4.cmml"><mi id="S3.SS1.p1.19.m19.1.1.3.3.4.2" xref="S3.SS1.p1.19.m19.1.1.3.3.4.2.cmml">W</mi><mn id="S3.SS1.p1.19.m19.1.1.3.3.4.3" xref="S3.SS1.p1.19.m19.1.1.3.3.4.3.cmml">64</mn></mfrac></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m19.1b"><apply id="S3.SS1.p1.19.m19.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1"><in id="S3.SS1.p1.19.m19.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1.1"></in><ci id="S3.SS1.p1.19.m19.1.1.2.cmml" xref="S3.SS1.p1.19.m19.1.1.2">𝑧</ci><apply id="S3.SS1.p1.19.m19.1.1.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.3.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.19.m19.1.1.3.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.19.m19.1.1.3.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3"><times id="S3.SS1.p1.19.m19.1.1.3.3.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.1"></times><apply id="S3.SS1.p1.19.m19.1.1.3.3.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p1.19.m19.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS1.p1.19.m19.1.1.3.3.2.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.2.3">ℎ</ci></apply><apply id="S3.SS1.p1.19.m19.1.1.3.3.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.3"><divide id="S3.SS1.p1.19.m19.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.3"></divide><ci id="S3.SS1.p1.19.m19.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.3.2">𝐻</ci><cn id="S3.SS1.p1.19.m19.1.1.3.3.3.3.cmml" type="integer" xref="S3.SS1.p1.19.m19.1.1.3.3.3.3">64</cn></apply><apply id="S3.SS1.p1.19.m19.1.1.3.3.4.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.4"><divide id="S3.SS1.p1.19.m19.1.1.3.3.4.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.4"></divide><ci id="S3.SS1.p1.19.m19.1.1.3.3.4.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3.4.2">𝑊</ci><cn id="S3.SS1.p1.19.m19.1.1.3.3.4.3.cmml" type="integer" xref="S3.SS1.p1.19.m19.1.1.3.3.4.3">64</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m19.1c">z\in\mathbb{R}^{N_{h}\times\frac{H}{64}\times\frac{W}{64}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.19.m19.1d">italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT × divide start_ARG italic_H end_ARG start_ARG 64 end_ARG × divide start_ARG italic_W end_ARG start_ARG 64 end_ARG end_POSTSUPERSCRIPT</annotation></semantics></math>, that is used to generate the learned distribution for entropy coding, where <math alttext="N_{h}" class="ltx_Math" display="inline" id="S3.SS1.p1.20.m20.1"><semantics id="S3.SS1.p1.20.m20.1a"><msub id="S3.SS1.p1.20.m20.1.1" xref="S3.SS1.p1.20.m20.1.1.cmml"><mi id="S3.SS1.p1.20.m20.1.1.2" xref="S3.SS1.p1.20.m20.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.20.m20.1.1.3" xref="S3.SS1.p1.20.m20.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.20.m20.1b"><apply id="S3.SS1.p1.20.m20.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.20.m20.1.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1">subscript</csymbol><ci id="S3.SS1.p1.20.m20.1.1.2.cmml" xref="S3.SS1.p1.20.m20.1.1.2">𝑁</ci><ci id="S3.SS1.p1.20.m20.1.1.3.cmml" xref="S3.SS1.p1.20.m20.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.20.m20.1c">N_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.20.m20.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> is the channel size of the side information. The quantized version of <math alttext="z" class="ltx_Math" display="inline" id="S3.SS1.p1.21.m21.1"><semantics id="S3.SS1.p1.21.m21.1a"><mi id="S3.SS1.p1.21.m21.1.1" xref="S3.SS1.p1.21.m21.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.21.m21.1b"><ci id="S3.SS1.p1.21.m21.1.1.cmml" xref="S3.SS1.p1.21.m21.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.21.m21.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.21.m21.1d">italic_z</annotation></semantics></math>, denoted as <math alttext="\hat{z}" class="ltx_Math" display="inline" id="S3.SS1.p1.22.m22.1"><semantics id="S3.SS1.p1.22.m22.1a"><mover accent="true" id="S3.SS1.p1.22.m22.1.1" xref="S3.SS1.p1.22.m22.1.1.cmml"><mi id="S3.SS1.p1.22.m22.1.1.2" xref="S3.SS1.p1.22.m22.1.1.2.cmml">z</mi><mo id="S3.SS1.p1.22.m22.1.1.1" xref="S3.SS1.p1.22.m22.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.22.m22.1b"><apply id="S3.SS1.p1.22.m22.1.1.cmml" xref="S3.SS1.p1.22.m22.1.1"><ci id="S3.SS1.p1.22.m22.1.1.1.cmml" xref="S3.SS1.p1.22.m22.1.1.1">^</ci><ci id="S3.SS1.p1.22.m22.1.1.2.cmml" xref="S3.SS1.p1.22.m22.1.1.2">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.22.m22.1c">\hat{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.22.m22.1d">over^ start_ARG italic_z end_ARG</annotation></semantics></math>, is transmitted into the bitstream, in order to recover <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.23.m23.1"><semantics id="S3.SS1.p1.23.m23.1a"><mover accent="true" id="S3.SS1.p1.23.m23.1.1" xref="S3.SS1.p1.23.m23.1.1.cmml"><mi id="S3.SS1.p1.23.m23.1.1.2" xref="S3.SS1.p1.23.m23.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.23.m23.1.1.1" xref="S3.SS1.p1.23.m23.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.23.m23.1b"><apply id="S3.SS1.p1.23.m23.1.1.cmml" xref="S3.SS1.p1.23.m23.1.1"><ci id="S3.SS1.p1.23.m23.1.1.1.cmml" xref="S3.SS1.p1.23.m23.1.1.1">^</ci><ci id="S3.SS1.p1.23.m23.1.1.2.cmml" xref="S3.SS1.p1.23.m23.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.23.m23.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.23.m23.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math>. Lastly, <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.24.m24.1"><semantics id="S3.SS1.p1.24.m24.1a"><mover accent="true" id="S3.SS1.p1.24.m24.1.1" xref="S3.SS1.p1.24.m24.1.1.cmml"><mi id="S3.SS1.p1.24.m24.1.1.2" xref="S3.SS1.p1.24.m24.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.24.m24.1.1.1" xref="S3.SS1.p1.24.m24.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.24.m24.1b"><apply id="S3.SS1.p1.24.m24.1.1.cmml" xref="S3.SS1.p1.24.m24.1.1"><ci id="S3.SS1.p1.24.m24.1.1.1.cmml" xref="S3.SS1.p1.24.m24.1.1.1">^</ci><ci id="S3.SS1.p1.24.m24.1.1.2.cmml" xref="S3.SS1.p1.24.m24.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.24.m24.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.24.m24.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> undergoes the synthesis transform with <math alttext="g_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.25.m25.1"><semantics id="S3.SS1.p1.25.m25.1a"><msub id="S3.SS1.p1.25.m25.1.1" xref="S3.SS1.p1.25.m25.1.1.cmml"><mi id="S3.SS1.p1.25.m25.1.1.2" xref="S3.SS1.p1.25.m25.1.1.2.cmml">g</mi><mi id="S3.SS1.p1.25.m25.1.1.3" xref="S3.SS1.p1.25.m25.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.25.m25.1b"><apply id="S3.SS1.p1.25.m25.1.1.cmml" xref="S3.SS1.p1.25.m25.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.25.m25.1.1.1.cmml" xref="S3.SS1.p1.25.m25.1.1">subscript</csymbol><ci id="S3.SS1.p1.25.m25.1.1.2.cmml" xref="S3.SS1.p1.25.m25.1.1.2">𝑔</ci><ci id="S3.SS1.p1.25.m25.1.1.3.cmml" xref="S3.SS1.p1.25.m25.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.25.m25.1c">g_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.25.m25.1d">italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, which reconstructs the image <math alttext="\hat{x}\in\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS1.p1.26.m26.1"><semantics id="S3.SS1.p1.26.m26.1a"><mrow id="S3.SS1.p1.26.m26.1.1" xref="S3.SS1.p1.26.m26.1.1.cmml"><mover accent="true" id="S3.SS1.p1.26.m26.1.1.2" xref="S3.SS1.p1.26.m26.1.1.2.cmml"><mi id="S3.SS1.p1.26.m26.1.1.2.2" xref="S3.SS1.p1.26.m26.1.1.2.2.cmml">x</mi><mo id="S3.SS1.p1.26.m26.1.1.2.1" xref="S3.SS1.p1.26.m26.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS1.p1.26.m26.1.1.1" xref="S3.SS1.p1.26.m26.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.26.m26.1.1.3" xref="S3.SS1.p1.26.m26.1.1.3.cmml"><mi id="S3.SS1.p1.26.m26.1.1.3.2" xref="S3.SS1.p1.26.m26.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.26.m26.1.1.3.3" xref="S3.SS1.p1.26.m26.1.1.3.3.cmml"><mn id="S3.SS1.p1.26.m26.1.1.3.3.2" xref="S3.SS1.p1.26.m26.1.1.3.3.2.cmml">3</mn><mo id="S3.SS1.p1.26.m26.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.26.m26.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.26.m26.1.1.3.3.3" xref="S3.SS1.p1.26.m26.1.1.3.3.3.cmml">H</mi><mo id="S3.SS1.p1.26.m26.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.26.m26.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.26.m26.1.1.3.3.4" xref="S3.SS1.p1.26.m26.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.26.m26.1b"><apply id="S3.SS1.p1.26.m26.1.1.cmml" xref="S3.SS1.p1.26.m26.1.1"><in id="S3.SS1.p1.26.m26.1.1.1.cmml" xref="S3.SS1.p1.26.m26.1.1.1"></in><apply id="S3.SS1.p1.26.m26.1.1.2.cmml" xref="S3.SS1.p1.26.m26.1.1.2"><ci id="S3.SS1.p1.26.m26.1.1.2.1.cmml" xref="S3.SS1.p1.26.m26.1.1.2.1">^</ci><ci id="S3.SS1.p1.26.m26.1.1.2.2.cmml" xref="S3.SS1.p1.26.m26.1.1.2.2">𝑥</ci></apply><apply id="S3.SS1.p1.26.m26.1.1.3.cmml" xref="S3.SS1.p1.26.m26.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.26.m26.1.1.3.1.cmml" xref="S3.SS1.p1.26.m26.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.26.m26.1.1.3.2.cmml" xref="S3.SS1.p1.26.m26.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.26.m26.1.1.3.3.cmml" xref="S3.SS1.p1.26.m26.1.1.3.3"><times id="S3.SS1.p1.26.m26.1.1.3.3.1.cmml" xref="S3.SS1.p1.26.m26.1.1.3.3.1"></times><cn id="S3.SS1.p1.26.m26.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p1.26.m26.1.1.3.3.2">3</cn><ci id="S3.SS1.p1.26.m26.1.1.3.3.3.cmml" xref="S3.SS1.p1.26.m26.1.1.3.3.3">𝐻</ci><ci id="S3.SS1.p1.26.m26.1.1.3.3.4.cmml" xref="S3.SS1.p1.26.m26.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.26.m26.1c">\hat{x}\in\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.26.m26.1d">over^ start_ARG italic_x end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F2.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/arch_fin.png" width="521">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall architecture of the proposed method.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Overall Framework</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this work, we focus on the scenario where MLLMs are hosted on the server side, while users on end devices need to perform inference on the remote model using both text and images as inputs. Given the necessity of incorporating image compression to ensure efficient transmission, we propose the first compression framework with the consideration of MLLMs as downstream application networks, aiming to mitigate the potential task performance drop caused by image compression. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.F2" title="Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates our overall framework, which includes three major components: the neural image codec, our proposed transform-neck, and the MLLM. The depicted MLLM system adheres to a typical structure, consisting of a visual encoder, an LLM, and a connector component facilitating the transformation of features from the visual encoder to the LLM. Note that all the MLLMs are adopted off-the-shelf and without any update.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">During inference, the input image at the end device is passed through an encoder <math alttext="g_{a}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑔</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">g_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> to generate the quantized latents <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mover accent="true" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><ci id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1">^</ci><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> for transmission. Next, <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mover accent="true" id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><ci id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1">^</ci><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> is directly passed through a lightweight transform-neck <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_T</annotation></semantics></math> for transformation into a middle layer of the visual encoder of an MLLM. We opt to adapt the image latents rather than the reconstructed images because the image latents inherently contain the information needed for reconstructing the image, and potentially the semantic information for the downstream tasks (when the image encoder is guided properly). By skipping the image decoding process, our method offers reduced computational complexity while maintaining the task performance. The rest of the MLLM system operates without any changes to generate the desired output response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">In training, to address the challenge of back-propagating the task loss through the entire MLLM, we propose a novel surrogate loss that updates the system by back-propagating solely from the visual encoder (which is not re-trained), bypassing the billion-parameter LLM.
In our work, we examine three distinct settings denoted as (d1), (d2) and (d3), as illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>. Firstly, in (d1), we consider the practical scenario where a fixed off-the-shelf image codec pre-trained for human perception is directly used alongside our transform-neck. In this setting, our framework offers the option for users to decode the image latents <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mover accent="true" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">y</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><ci id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1">^</ci><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> for reconstruction by using the decoder <math alttext="g_{s}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">g</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝑔</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">g_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> instead of the transform-neck. In this way, the quality of the decoded image is not affected, as the image codec is not updated in the present case. Then, we extend the analysis to scenarios (d2) and (d3) to examine the impact of jointly training the image codec and transform-neck. In (d2), the entire image codec undergoes re-training to accommodate both human and machine perception, while in (d3), the encoder is re-trained specifically for machine perception.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Transform-neck</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our transform-neck is designed to be a lightweight module, consisting only of a linear projection, a self-attention mechanism, a feed-forward layer, and two layer norms, as shown in the central red box in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.F2" title="Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. Its purpose is to adapt the compressed image latents <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mover accent="true" id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">y</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><ci id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1">^</ci><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> into an efficient representation for consumption by the downstream MLLMs. In fact, rather than reconstructing the image and using it as input to the MLLM, we propose leveraging the latent representation directly.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6">Since the image encoder <math alttext="g_{a}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">g</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑔</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">g_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> already functions as a feature extractor, similar to the early layers of the visual encoder <math alttext="C" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_C</annotation></semantics></math>, we bridge the output of our transform-neck directly into the intermediate features of <math alttext="C" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_C</annotation></semantics></math>, effectively integrating the image codec with the MLLM system. The decision on which initial layers to bypass depends on the specific type of visual encoder used in the MLLM. For instance, when using the CLIP visual encoder as <math alttext="C" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_C</annotation></semantics></math>, we found that connecting the transform-neck to the third Transformer layer, bypassing the first two, yields optimal results (see Section <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5" title="4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.5</span></a> for ablation experiments justifying this design). Note that skipping the initial layers of <math alttext="C" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_C</annotation></semantics></math> further reduces computational complexity of our framework. We denote the partial visual encoder as <math alttext="C^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><msup id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">C</mi><mo id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝐶</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">C^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Surrogate Loss</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.4">To avoid involving huge MLLMs in the training process, and thus bypassing back-propagation through their entire structure, we propose a surrogate loss <math alttext="\mathcal{L}_{S}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{L}_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> which is back-propagated through only the partial visual encoder <math alttext="C^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><msup id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">C</mi><mo id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝐶</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">C^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>. Specifically, we design our surrogate loss to consist of two terms: distillation loss <math alttext="\mathcal{L}_{dist}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">ℒ</mi><mrow id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.p1.3.m3.1.1.3.2" xref="S3.SS4.p1.3.m3.1.1.3.2.cmml">d</mi><mo id="S3.SS4.p1.3.m3.1.1.3.1" xref="S3.SS4.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.m3.1.1.3.3" xref="S3.SS4.p1.3.m3.1.1.3.3.cmml">i</mi><mo id="S3.SS4.p1.3.m3.1.1.3.1a" xref="S3.SS4.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.m3.1.1.3.4" xref="S3.SS4.p1.3.m3.1.1.3.4.cmml">s</mi><mo id="S3.SS4.p1.3.m3.1.1.3.1b" xref="S3.SS4.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.3.m3.1.1.3.5" xref="S3.SS4.p1.3.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">ℒ</ci><apply id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3"><times id="S3.SS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.3.1"></times><ci id="S3.SS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.3.2">𝑑</ci><ci id="S3.SS4.p1.3.m3.1.1.3.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3.3">𝑖</ci><ci id="S3.SS4.p1.3.m3.1.1.3.4.cmml" xref="S3.SS4.p1.3.m3.1.1.3.4">𝑠</ci><ci id="S3.SS4.p1.3.m3.1.1.3.5.cmml" xref="S3.SS4.p1.3.m3.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\mathcal{L}_{dist}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and cross-entropy loss <math alttext="\mathcal{L}_{CE}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS4.p1.4.m4.1.1.3.2" xref="S3.SS4.p1.4.m4.1.1.3.2.cmml">C</mi><mo id="S3.SS4.p1.4.m4.1.1.3.1" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p1.4.m4.1.1.3.3" xref="S3.SS4.p1.4.m4.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">ℒ</ci><apply id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3"><times id="S3.SS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS4.p1.4.m4.1.1.3.1"></times><ci id="S3.SS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS4.p1.4.m4.1.1.3.2">𝐶</ci><ci id="S3.SS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\mathcal{L}_{CE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.4">To retain the downstream MLLM performance, the resulting features <math alttext="C^{\prime}(T(\hat{y}))" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.2"><semantics id="S3.SS4.p2.1.m1.2a"><mrow id="S3.SS4.p2.1.m1.2.2" xref="S3.SS4.p2.1.m1.2.2.cmml"><msup id="S3.SS4.p2.1.m1.2.2.3" xref="S3.SS4.p2.1.m1.2.2.3.cmml"><mi id="S3.SS4.p2.1.m1.2.2.3.2" xref="S3.SS4.p2.1.m1.2.2.3.2.cmml">C</mi><mo id="S3.SS4.p2.1.m1.2.2.3.3" xref="S3.SS4.p2.1.m1.2.2.3.3.cmml">′</mo></msup><mo id="S3.SS4.p2.1.m1.2.2.2" xref="S3.SS4.p2.1.m1.2.2.2.cmml">⁢</mo><mrow id="S3.SS4.p2.1.m1.2.2.1.1" xref="S3.SS4.p2.1.m1.2.2.1.1.1.cmml"><mo id="S3.SS4.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS4.p2.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS4.p2.1.m1.2.2.1.1.1" xref="S3.SS4.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.2.2.1.1.1.2" xref="S3.SS4.p2.1.m1.2.2.1.1.1.2.cmml">T</mi><mo id="S3.SS4.p2.1.m1.2.2.1.1.1.1" xref="S3.SS4.p2.1.m1.2.2.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS4.p2.1.m1.2.2.1.1.1.3.2" xref="S3.SS4.p2.1.m1.1.1.cmml"><mo id="S3.SS4.p2.1.m1.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS4.p2.1.m1.1.1.cmml">(</mo><mover accent="true" id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">y</mi><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.SS4.p2.1.m1.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS4.p2.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p2.1.m1.2.2.1.1.3" stretchy="false" xref="S3.SS4.p2.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.2b"><apply id="S3.SS4.p2.1.m1.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2"><times id="S3.SS4.p2.1.m1.2.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2"></times><apply id="S3.SS4.p2.1.m1.2.2.3.cmml" xref="S3.SS4.p2.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.3.1.cmml" xref="S3.SS4.p2.1.m1.2.2.3">superscript</csymbol><ci id="S3.SS4.p2.1.m1.2.2.3.2.cmml" xref="S3.SS4.p2.1.m1.2.2.3.2">𝐶</ci><ci id="S3.SS4.p2.1.m1.2.2.3.3.cmml" xref="S3.SS4.p2.1.m1.2.2.3.3">′</ci></apply><apply id="S3.SS4.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1"><times id="S3.SS4.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1.1.1"></times><ci id="S3.SS4.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1.1.2">𝑇</ci><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.2.2.1.1.1.3.2"><ci id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1">^</ci><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.2c">C^{\prime}(T(\hat{y}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.2d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) )</annotation></semantics></math> when using our transformed latents should resemble closely those obtained when inputting the uncompressed image into <math alttext="C" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">italic_C</annotation></semantics></math>, that is <math alttext="C(x)" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m3.1"><semantics id="S3.SS4.p2.3.m3.1a"><mrow id="S3.SS4.p2.3.m3.1.2" xref="S3.SS4.p2.3.m3.1.2.cmml"><mi id="S3.SS4.p2.3.m3.1.2.2" xref="S3.SS4.p2.3.m3.1.2.2.cmml">C</mi><mo id="S3.SS4.p2.3.m3.1.2.1" xref="S3.SS4.p2.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.3.m3.1.2.3.2" xref="S3.SS4.p2.3.m3.1.2.cmml"><mo id="S3.SS4.p2.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS4.p2.3.m3.1.2.cmml">(</mo><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">x</mi><mo id="S3.SS4.p2.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS4.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.2.cmml" xref="S3.SS4.p2.3.m3.1.2"><times id="S3.SS4.p2.3.m3.1.2.1.cmml" xref="S3.SS4.p2.3.m3.1.2.1"></times><ci id="S3.SS4.p2.3.m3.1.2.2.cmml" xref="S3.SS4.p2.3.m3.1.2.2">𝐶</ci><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">C(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.3.m3.1d">italic_C ( italic_x )</annotation></semantics></math>. To this end, we introduce the following distillation loss <math alttext="\mathcal{L}_{dist}" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m4.1"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.2" xref="S3.SS4.p2.4.m4.1.1.3.2.cmml">d</mi><mo id="S3.SS4.p2.4.m4.1.1.3.1" xref="S3.SS4.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.4.m4.1.1.3.3" xref="S3.SS4.p2.4.m4.1.1.3.3.cmml">i</mi><mo id="S3.SS4.p2.4.m4.1.1.3.1a" xref="S3.SS4.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.4.m4.1.1.3.4" xref="S3.SS4.p2.4.m4.1.1.3.4.cmml">s</mi><mo id="S3.SS4.p2.4.m4.1.1.3.1b" xref="S3.SS4.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.4.m4.1.1.3.5" xref="S3.SS4.p2.4.m4.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ℒ</ci><apply id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3"><times id="S3.SS4.p2.4.m4.1.1.3.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.1"></times><ci id="S3.SS4.p2.4.m4.1.1.3.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2">𝑑</ci><ci id="S3.SS4.p2.4.m4.1.1.3.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3">𝑖</ci><ci id="S3.SS4.p2.4.m4.1.1.3.4.cmml" xref="S3.SS4.p2.4.m4.1.1.3.4">𝑠</ci><ci id="S3.SS4.p2.4.m4.1.1.3.5.cmml" xref="S3.SS4.p2.4.m4.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\mathcal{L}_{dist}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, aiming to minimize the Mean Squared Error (MSE) between the two output features:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{dist}=\text{MSE}(C^{\prime}(T(\hat{y})),C(x)).%
\vspace{-2mm}" class="ltx_Math" display="inline" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.4.2" xref="S3.E1.m1.3.3.1.1.4.2.cmml">ℒ</mi><mrow id="S3.E1.m1.3.3.1.1.4.3" xref="S3.E1.m1.3.3.1.1.4.3.cmml"><mi id="S3.E1.m1.3.3.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.4.3.2.cmml">d</mi><mo id="S3.E1.m1.3.3.1.1.4.3.1" xref="S3.E1.m1.3.3.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.4.3.3" xref="S3.E1.m1.3.3.1.1.4.3.3.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.4.3.1a" xref="S3.E1.m1.3.3.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.4.3.4" xref="S3.E1.m1.3.3.1.1.4.3.4.cmml">s</mi><mo id="S3.E1.m1.3.3.1.1.4.3.1b" xref="S3.E1.m1.3.3.1.1.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.4.3.5" xref="S3.E1.m1.3.3.1.1.4.3.5.cmml">t</mi></mrow></msub><mo id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mtext id="S3.E1.m1.3.3.1.1.2.4" xref="S3.E1.m1.3.3.1.1.2.4a.cmml">MSE</mtext><mo id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">′</mo></msup><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">y</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.cmml">C</mi><mo id="S3.E1.m1.3.3.1.1.2.2.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.2.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.2.2.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">x</mi><mo id="S3.E1.m1.3.3.1.1.2.2.2.2.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" lspace="0em" xref="S3.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ℒ</ci><apply id="S3.E1.m1.3.3.1.1.4.3.cmml" xref="S3.E1.m1.3.3.1.1.4.3"><times id="S3.E1.m1.3.3.1.1.4.3.1.cmml" xref="S3.E1.m1.3.3.1.1.4.3.1"></times><ci id="S3.E1.m1.3.3.1.1.4.3.2.cmml" xref="S3.E1.m1.3.3.1.1.4.3.2">𝑑</ci><ci id="S3.E1.m1.3.3.1.1.4.3.3.cmml" xref="S3.E1.m1.3.3.1.1.4.3.3">𝑖</ci><ci id="S3.E1.m1.3.3.1.1.4.3.4.cmml" xref="S3.E1.m1.3.3.1.1.4.3.4">𝑠</ci><ci id="S3.E1.m1.3.3.1.1.4.3.5.cmml" xref="S3.E1.m1.3.3.1.1.4.3.5">𝑡</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><times id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"></times><ci id="S3.E1.m1.3.3.1.1.2.4a.cmml" xref="S3.E1.m1.3.3.1.1.2.4"><mtext id="S3.E1.m1.3.3.1.1.2.4.cmml" xref="S3.E1.m1.3.3.1.1.2.4">MSE</mtext></ci><interval closure="open" id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.3">′</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑇</ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1">^</ci><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑦</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2"><times id="S3.E1.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.1"></times><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2">𝐶</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑥</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\mathcal{L}_{dist}=\text{MSE}(C^{\prime}(T(\hat{y})),C(x)).%
\vspace{-2mm}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT = MSE ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C ( italic_x ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.19">In addition to the distillation loss, we propose incorporating a second term, which is the cross-entropy loss <math alttext="\mathcal{L}_{CE}" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">C</mi><mo id="S3.SS4.p3.1.m1.1.1.3.1" xref="S3.SS4.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">ℒ</ci><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><times id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.1"></times><ci id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">𝐶</ci><ci id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mathcal{L}_{CE}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT</annotation></semantics></math> calculated using the following procedure.
We take a classification dataset consisting of <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_n</annotation></semantics></math> images and <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_m</annotation></semantics></math> text labels <math alttext="\{l_{1},l_{2},\dots,l_{m}\}" class="ltx_Math" display="inline" id="S3.SS4.p3.4.m4.4"><semantics id="S3.SS4.p3.4.m4.4a"><mrow id="S3.SS4.p3.4.m4.4.4.3" xref="S3.SS4.p3.4.m4.4.4.4.cmml"><mo id="S3.SS4.p3.4.m4.4.4.3.4" stretchy="false" xref="S3.SS4.p3.4.m4.4.4.4.cmml">{</mo><msub id="S3.SS4.p3.4.m4.2.2.1.1" xref="S3.SS4.p3.4.m4.2.2.1.1.cmml"><mi id="S3.SS4.p3.4.m4.2.2.1.1.2" xref="S3.SS4.p3.4.m4.2.2.1.1.2.cmml">l</mi><mn id="S3.SS4.p3.4.m4.2.2.1.1.3" xref="S3.SS4.p3.4.m4.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.p3.4.m4.4.4.3.5" xref="S3.SS4.p3.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS4.p3.4.m4.3.3.2.2" xref="S3.SS4.p3.4.m4.3.3.2.2.cmml"><mi id="S3.SS4.p3.4.m4.3.3.2.2.2" xref="S3.SS4.p3.4.m4.3.3.2.2.2.cmml">l</mi><mn id="S3.SS4.p3.4.m4.3.3.2.2.3" xref="S3.SS4.p3.4.m4.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.p3.4.m4.4.4.3.6" xref="S3.SS4.p3.4.m4.4.4.4.cmml">,</mo><mi id="S3.SS4.p3.4.m4.1.1" mathvariant="normal" xref="S3.SS4.p3.4.m4.1.1.cmml">…</mi><mo id="S3.SS4.p3.4.m4.4.4.3.7" xref="S3.SS4.p3.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS4.p3.4.m4.4.4.3.3" xref="S3.SS4.p3.4.m4.4.4.3.3.cmml"><mi id="S3.SS4.p3.4.m4.4.4.3.3.2" xref="S3.SS4.p3.4.m4.4.4.3.3.2.cmml">l</mi><mi id="S3.SS4.p3.4.m4.4.4.3.3.3" xref="S3.SS4.p3.4.m4.4.4.3.3.3.cmml">m</mi></msub><mo id="S3.SS4.p3.4.m4.4.4.3.8" stretchy="false" xref="S3.SS4.p3.4.m4.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.4b"><set id="S3.SS4.p3.4.m4.4.4.4.cmml" xref="S3.SS4.p3.4.m4.4.4.3"><apply id="S3.SS4.p3.4.m4.2.2.1.1.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.2.2.1.1.1.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1">subscript</csymbol><ci id="S3.SS4.p3.4.m4.2.2.1.1.2.cmml" xref="S3.SS4.p3.4.m4.2.2.1.1.2">𝑙</ci><cn id="S3.SS4.p3.4.m4.2.2.1.1.3.cmml" type="integer" xref="S3.SS4.p3.4.m4.2.2.1.1.3">1</cn></apply><apply id="S3.SS4.p3.4.m4.3.3.2.2.cmml" xref="S3.SS4.p3.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.3.3.2.2.1.cmml" xref="S3.SS4.p3.4.m4.3.3.2.2">subscript</csymbol><ci id="S3.SS4.p3.4.m4.3.3.2.2.2.cmml" xref="S3.SS4.p3.4.m4.3.3.2.2.2">𝑙</ci><cn id="S3.SS4.p3.4.m4.3.3.2.2.3.cmml" type="integer" xref="S3.SS4.p3.4.m4.3.3.2.2.3">2</cn></apply><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">…</ci><apply id="S3.SS4.p3.4.m4.4.4.3.3.cmml" xref="S3.SS4.p3.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.4.4.3.3.1.cmml" xref="S3.SS4.p3.4.m4.4.4.3.3">subscript</csymbol><ci id="S3.SS4.p3.4.m4.4.4.3.3.2.cmml" xref="S3.SS4.p3.4.m4.4.4.3.3.2">𝑙</ci><ci id="S3.SS4.p3.4.m4.4.4.3.3.3.cmml" xref="S3.SS4.p3.4.m4.4.4.3.3.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.4c">\{l_{1},l_{2},\dots,l_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.4.m4.4d">{ italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math>, which correspond to <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.5.m5.1"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.5.m5.1d">italic_m</annotation></semantics></math> distinct classes. Each image belongs to one of these <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.6.m6.1"><semantics id="S3.SS4.p3.6.m6.1a"><mi id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><ci id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.6.m6.1d">italic_m</annotation></semantics></math> classes. Next, we compute: (1) the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p3.7.m7.1"><semantics id="S3.SS4.p3.7.m7.1a"><mi id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><ci id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.7.m7.1d">italic_n</annotation></semantics></math> image embedding <math alttext="C^{\prime}(T(\hat{y}))" class="ltx_Math" display="inline" id="S3.SS4.p3.8.m8.2"><semantics id="S3.SS4.p3.8.m8.2a"><mrow id="S3.SS4.p3.8.m8.2.2" xref="S3.SS4.p3.8.m8.2.2.cmml"><msup id="S3.SS4.p3.8.m8.2.2.3" xref="S3.SS4.p3.8.m8.2.2.3.cmml"><mi id="S3.SS4.p3.8.m8.2.2.3.2" xref="S3.SS4.p3.8.m8.2.2.3.2.cmml">C</mi><mo id="S3.SS4.p3.8.m8.2.2.3.3" xref="S3.SS4.p3.8.m8.2.2.3.3.cmml">′</mo></msup><mo id="S3.SS4.p3.8.m8.2.2.2" xref="S3.SS4.p3.8.m8.2.2.2.cmml">⁢</mo><mrow id="S3.SS4.p3.8.m8.2.2.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.1.cmml"><mo id="S3.SS4.p3.8.m8.2.2.1.1.2" stretchy="false" xref="S3.SS4.p3.8.m8.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS4.p3.8.m8.2.2.1.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.1.cmml"><mi id="S3.SS4.p3.8.m8.2.2.1.1.1.2" xref="S3.SS4.p3.8.m8.2.2.1.1.1.2.cmml">T</mi><mo id="S3.SS4.p3.8.m8.2.2.1.1.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS4.p3.8.m8.2.2.1.1.1.3.2" xref="S3.SS4.p3.8.m8.1.1.cmml"><mo id="S3.SS4.p3.8.m8.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS4.p3.8.m8.1.1.cmml">(</mo><mover accent="true" id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml"><mi id="S3.SS4.p3.8.m8.1.1.2" xref="S3.SS4.p3.8.m8.1.1.2.cmml">y</mi><mo id="S3.SS4.p3.8.m8.1.1.1" xref="S3.SS4.p3.8.m8.1.1.1.cmml">^</mo></mover><mo id="S3.SS4.p3.8.m8.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS4.p3.8.m8.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.8.m8.2.2.1.1.3" stretchy="false" xref="S3.SS4.p3.8.m8.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.2b"><apply id="S3.SS4.p3.8.m8.2.2.cmml" xref="S3.SS4.p3.8.m8.2.2"><times id="S3.SS4.p3.8.m8.2.2.2.cmml" xref="S3.SS4.p3.8.m8.2.2.2"></times><apply id="S3.SS4.p3.8.m8.2.2.3.cmml" xref="S3.SS4.p3.8.m8.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m8.2.2.3.1.cmml" xref="S3.SS4.p3.8.m8.2.2.3">superscript</csymbol><ci id="S3.SS4.p3.8.m8.2.2.3.2.cmml" xref="S3.SS4.p3.8.m8.2.2.3.2">𝐶</ci><ci id="S3.SS4.p3.8.m8.2.2.3.3.cmml" xref="S3.SS4.p3.8.m8.2.2.3.3">′</ci></apply><apply id="S3.SS4.p3.8.m8.2.2.1.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1"><times id="S3.SS4.p3.8.m8.2.2.1.1.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1"></times><ci id="S3.SS4.p3.8.m8.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.2">𝑇</ci><apply id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.3.2"><ci id="S3.SS4.p3.8.m8.1.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1.1">^</ci><ci id="S3.SS4.p3.8.m8.1.1.2.cmml" xref="S3.SS4.p3.8.m8.1.1.2">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.2c">C^{\prime}(T(\hat{y}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.8.m8.2d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) )</annotation></semantics></math> and (2) the <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.9.m9.1"><semantics id="S3.SS4.p3.9.m9.1a"><mi id="S3.SS4.p3.9.m9.1.1" xref="S3.SS4.p3.9.m9.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m9.1b"><ci id="S3.SS4.p3.9.m9.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.9.m9.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.9.m9.1d">italic_m</annotation></semantics></math> text embeddings <math alttext="C_{t}(l_{j})" class="ltx_Math" display="inline" id="S3.SS4.p3.10.m10.1"><semantics id="S3.SS4.p3.10.m10.1a"><mrow id="S3.SS4.p3.10.m10.1.1" xref="S3.SS4.p3.10.m10.1.1.cmml"><msub id="S3.SS4.p3.10.m10.1.1.3" xref="S3.SS4.p3.10.m10.1.1.3.cmml"><mi id="S3.SS4.p3.10.m10.1.1.3.2" xref="S3.SS4.p3.10.m10.1.1.3.2.cmml">C</mi><mi id="S3.SS4.p3.10.m10.1.1.3.3" xref="S3.SS4.p3.10.m10.1.1.3.3.cmml">t</mi></msub><mo id="S3.SS4.p3.10.m10.1.1.2" xref="S3.SS4.p3.10.m10.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p3.10.m10.1.1.1.1" xref="S3.SS4.p3.10.m10.1.1.1.1.1.cmml"><mo id="S3.SS4.p3.10.m10.1.1.1.1.2" stretchy="false" xref="S3.SS4.p3.10.m10.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p3.10.m10.1.1.1.1.1" xref="S3.SS4.p3.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS4.p3.10.m10.1.1.1.1.1.2" xref="S3.SS4.p3.10.m10.1.1.1.1.1.2.cmml">l</mi><mi id="S3.SS4.p3.10.m10.1.1.1.1.1.3" xref="S3.SS4.p3.10.m10.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS4.p3.10.m10.1.1.1.1.3" stretchy="false" xref="S3.SS4.p3.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.10.m10.1b"><apply id="S3.SS4.p3.10.m10.1.1.cmml" xref="S3.SS4.p3.10.m10.1.1"><times id="S3.SS4.p3.10.m10.1.1.2.cmml" xref="S3.SS4.p3.10.m10.1.1.2"></times><apply id="S3.SS4.p3.10.m10.1.1.3.cmml" xref="S3.SS4.p3.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.10.m10.1.1.3.1.cmml" xref="S3.SS4.p3.10.m10.1.1.3">subscript</csymbol><ci id="S3.SS4.p3.10.m10.1.1.3.2.cmml" xref="S3.SS4.p3.10.m10.1.1.3.2">𝐶</ci><ci id="S3.SS4.p3.10.m10.1.1.3.3.cmml" xref="S3.SS4.p3.10.m10.1.1.3.3">𝑡</ci></apply><apply id="S3.SS4.p3.10.m10.1.1.1.1.1.cmml" xref="S3.SS4.p3.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.10.m10.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.10.m10.1.1.1.1.1.2">𝑙</ci><ci id="S3.SS4.p3.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.10.m10.1.1.1.1.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.10.m10.1c">C_{t}(l_{j})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.10.m10.1d">italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="C_{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.11.m11.1"><semantics id="S3.SS4.p3.11.m11.1a"><msub id="S3.SS4.p3.11.m11.1.1" xref="S3.SS4.p3.11.m11.1.1.cmml"><mi id="S3.SS4.p3.11.m11.1.1.2" xref="S3.SS4.p3.11.m11.1.1.2.cmml">C</mi><mi id="S3.SS4.p3.11.m11.1.1.3" xref="S3.SS4.p3.11.m11.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.11.m11.1b"><apply id="S3.SS4.p3.11.m11.1.1.cmml" xref="S3.SS4.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.11.m11.1.1.1.cmml" xref="S3.SS4.p3.11.m11.1.1">subscript</csymbol><ci id="S3.SS4.p3.11.m11.1.1.2.cmml" xref="S3.SS4.p3.11.m11.1.1.2">𝐶</ci><ci id="S3.SS4.p3.11.m11.1.1.3.cmml" xref="S3.SS4.p3.11.m11.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.11.m11.1c">C_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.11.m11.1d">italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is a text encoder applied to each text label <math alttext="l_{j}" class="ltx_Math" display="inline" id="S3.SS4.p3.12.m12.1"><semantics id="S3.SS4.p3.12.m12.1a"><msub id="S3.SS4.p3.12.m12.1.1" xref="S3.SS4.p3.12.m12.1.1.cmml"><mi id="S3.SS4.p3.12.m12.1.1.2" xref="S3.SS4.p3.12.m12.1.1.2.cmml">l</mi><mi id="S3.SS4.p3.12.m12.1.1.3" xref="S3.SS4.p3.12.m12.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.12.m12.1b"><apply id="S3.SS4.p3.12.m12.1.1.cmml" xref="S3.SS4.p3.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.12.m12.1.1.1.cmml" xref="S3.SS4.p3.12.m12.1.1">subscript</csymbol><ci id="S3.SS4.p3.12.m12.1.1.2.cmml" xref="S3.SS4.p3.12.m12.1.1.2">𝑙</ci><ci id="S3.SS4.p3.12.m12.1.1.3.cmml" xref="S3.SS4.p3.12.m12.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.12.m12.1c">l_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.12.m12.1d">italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>.
In particular, we use the CLIP text encoder, independently of the visual encoder integrated into the MLLM under consideration.
For each image, we then calculate the cosine similarity <math alttext="\text{CS}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S3.SS4.p3.13.m13.2"><semantics id="S3.SS4.p3.13.m13.2a"><mrow id="S3.SS4.p3.13.m13.2.3" xref="S3.SS4.p3.13.m13.2.3.cmml"><mtext id="S3.SS4.p3.13.m13.2.3.2" xref="S3.SS4.p3.13.m13.2.3.2a.cmml">CS</mtext><mo id="S3.SS4.p3.13.m13.2.3.1" xref="S3.SS4.p3.13.m13.2.3.1.cmml">⁢</mo><mrow id="S3.SS4.p3.13.m13.2.3.3.2" xref="S3.SS4.p3.13.m13.2.3.3.1.cmml"><mo id="S3.SS4.p3.13.m13.2.3.3.2.1" stretchy="false" xref="S3.SS4.p3.13.m13.2.3.3.1.cmml">(</mo><mo id="S3.SS4.p3.13.m13.1.1" lspace="0em" rspace="0em" xref="S3.SS4.p3.13.m13.1.1.cmml">⋅</mo><mo id="S3.SS4.p3.13.m13.2.3.3.2.2" rspace="0em" xref="S3.SS4.p3.13.m13.2.3.3.1.cmml">,</mo><mo id="S3.SS4.p3.13.m13.2.2" lspace="0em" rspace="0em" xref="S3.SS4.p3.13.m13.2.2.cmml">⋅</mo><mo id="S3.SS4.p3.13.m13.2.3.3.2.3" stretchy="false" xref="S3.SS4.p3.13.m13.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.13.m13.2b"><apply id="S3.SS4.p3.13.m13.2.3.cmml" xref="S3.SS4.p3.13.m13.2.3"><times id="S3.SS4.p3.13.m13.2.3.1.cmml" xref="S3.SS4.p3.13.m13.2.3.1"></times><ci id="S3.SS4.p3.13.m13.2.3.2a.cmml" xref="S3.SS4.p3.13.m13.2.3.2"><mtext id="S3.SS4.p3.13.m13.2.3.2.cmml" xref="S3.SS4.p3.13.m13.2.3.2">CS</mtext></ci><interval closure="open" id="S3.SS4.p3.13.m13.2.3.3.1.cmml" xref="S3.SS4.p3.13.m13.2.3.3.2"><ci id="S3.SS4.p3.13.m13.1.1.cmml" xref="S3.SS4.p3.13.m13.1.1">⋅</ci><ci id="S3.SS4.p3.13.m13.2.2.cmml" xref="S3.SS4.p3.13.m13.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.13.m13.2c">\text{CS}(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.13.m13.2d">CS ( ⋅ , ⋅ )</annotation></semantics></math> between its image embedding <math alttext="C^{\prime}(T(\hat{y}))" class="ltx_Math" display="inline" id="S3.SS4.p3.14.m14.2"><semantics id="S3.SS4.p3.14.m14.2a"><mrow id="S3.SS4.p3.14.m14.2.2" xref="S3.SS4.p3.14.m14.2.2.cmml"><msup id="S3.SS4.p3.14.m14.2.2.3" xref="S3.SS4.p3.14.m14.2.2.3.cmml"><mi id="S3.SS4.p3.14.m14.2.2.3.2" xref="S3.SS4.p3.14.m14.2.2.3.2.cmml">C</mi><mo id="S3.SS4.p3.14.m14.2.2.3.3" xref="S3.SS4.p3.14.m14.2.2.3.3.cmml">′</mo></msup><mo id="S3.SS4.p3.14.m14.2.2.2" xref="S3.SS4.p3.14.m14.2.2.2.cmml">⁢</mo><mrow id="S3.SS4.p3.14.m14.2.2.1.1" xref="S3.SS4.p3.14.m14.2.2.1.1.1.cmml"><mo id="S3.SS4.p3.14.m14.2.2.1.1.2" stretchy="false" xref="S3.SS4.p3.14.m14.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS4.p3.14.m14.2.2.1.1.1" xref="S3.SS4.p3.14.m14.2.2.1.1.1.cmml"><mi id="S3.SS4.p3.14.m14.2.2.1.1.1.2" xref="S3.SS4.p3.14.m14.2.2.1.1.1.2.cmml">T</mi><mo id="S3.SS4.p3.14.m14.2.2.1.1.1.1" xref="S3.SS4.p3.14.m14.2.2.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS4.p3.14.m14.2.2.1.1.1.3.2" xref="S3.SS4.p3.14.m14.1.1.cmml"><mo id="S3.SS4.p3.14.m14.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS4.p3.14.m14.1.1.cmml">(</mo><mover accent="true" id="S3.SS4.p3.14.m14.1.1" xref="S3.SS4.p3.14.m14.1.1.cmml"><mi id="S3.SS4.p3.14.m14.1.1.2" xref="S3.SS4.p3.14.m14.1.1.2.cmml">y</mi><mo id="S3.SS4.p3.14.m14.1.1.1" xref="S3.SS4.p3.14.m14.1.1.1.cmml">^</mo></mover><mo id="S3.SS4.p3.14.m14.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS4.p3.14.m14.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.14.m14.2.2.1.1.3" stretchy="false" xref="S3.SS4.p3.14.m14.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.14.m14.2b"><apply id="S3.SS4.p3.14.m14.2.2.cmml" xref="S3.SS4.p3.14.m14.2.2"><times id="S3.SS4.p3.14.m14.2.2.2.cmml" xref="S3.SS4.p3.14.m14.2.2.2"></times><apply id="S3.SS4.p3.14.m14.2.2.3.cmml" xref="S3.SS4.p3.14.m14.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p3.14.m14.2.2.3.1.cmml" xref="S3.SS4.p3.14.m14.2.2.3">superscript</csymbol><ci id="S3.SS4.p3.14.m14.2.2.3.2.cmml" xref="S3.SS4.p3.14.m14.2.2.3.2">𝐶</ci><ci id="S3.SS4.p3.14.m14.2.2.3.3.cmml" xref="S3.SS4.p3.14.m14.2.2.3.3">′</ci></apply><apply id="S3.SS4.p3.14.m14.2.2.1.1.1.cmml" xref="S3.SS4.p3.14.m14.2.2.1.1"><times id="S3.SS4.p3.14.m14.2.2.1.1.1.1.cmml" xref="S3.SS4.p3.14.m14.2.2.1.1.1.1"></times><ci id="S3.SS4.p3.14.m14.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.14.m14.2.2.1.1.1.2">𝑇</ci><apply id="S3.SS4.p3.14.m14.1.1.cmml" xref="S3.SS4.p3.14.m14.2.2.1.1.1.3.2"><ci id="S3.SS4.p3.14.m14.1.1.1.cmml" xref="S3.SS4.p3.14.m14.1.1.1">^</ci><ci id="S3.SS4.p3.14.m14.1.1.2.cmml" xref="S3.SS4.p3.14.m14.1.1.2">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.14.m14.2c">C^{\prime}(T(\hat{y}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.14.m14.2d">italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) )</annotation></semantics></math> and each of the <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.15.m15.1"><semantics id="S3.SS4.p3.15.m15.1a"><mi id="S3.SS4.p3.15.m15.1.1" xref="S3.SS4.p3.15.m15.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.15.m15.1b"><ci id="S3.SS4.p3.15.m15.1.1.cmml" xref="S3.SS4.p3.15.m15.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.15.m15.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.15.m15.1d">italic_m</annotation></semantics></math> text embeddings <math alttext="C_{t}(l_{j})" class="ltx_Math" display="inline" id="S3.SS4.p3.16.m16.1"><semantics id="S3.SS4.p3.16.m16.1a"><mrow id="S3.SS4.p3.16.m16.1.1" xref="S3.SS4.p3.16.m16.1.1.cmml"><msub id="S3.SS4.p3.16.m16.1.1.3" xref="S3.SS4.p3.16.m16.1.1.3.cmml"><mi id="S3.SS4.p3.16.m16.1.1.3.2" xref="S3.SS4.p3.16.m16.1.1.3.2.cmml">C</mi><mi id="S3.SS4.p3.16.m16.1.1.3.3" xref="S3.SS4.p3.16.m16.1.1.3.3.cmml">t</mi></msub><mo id="S3.SS4.p3.16.m16.1.1.2" xref="S3.SS4.p3.16.m16.1.1.2.cmml">⁢</mo><mrow id="S3.SS4.p3.16.m16.1.1.1.1" xref="S3.SS4.p3.16.m16.1.1.1.1.1.cmml"><mo id="S3.SS4.p3.16.m16.1.1.1.1.2" stretchy="false" xref="S3.SS4.p3.16.m16.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.p3.16.m16.1.1.1.1.1" xref="S3.SS4.p3.16.m16.1.1.1.1.1.cmml"><mi id="S3.SS4.p3.16.m16.1.1.1.1.1.2" xref="S3.SS4.p3.16.m16.1.1.1.1.1.2.cmml">l</mi><mi id="S3.SS4.p3.16.m16.1.1.1.1.1.3" xref="S3.SS4.p3.16.m16.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS4.p3.16.m16.1.1.1.1.3" stretchy="false" xref="S3.SS4.p3.16.m16.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.16.m16.1b"><apply id="S3.SS4.p3.16.m16.1.1.cmml" xref="S3.SS4.p3.16.m16.1.1"><times id="S3.SS4.p3.16.m16.1.1.2.cmml" xref="S3.SS4.p3.16.m16.1.1.2"></times><apply id="S3.SS4.p3.16.m16.1.1.3.cmml" xref="S3.SS4.p3.16.m16.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.16.m16.1.1.3.1.cmml" xref="S3.SS4.p3.16.m16.1.1.3">subscript</csymbol><ci id="S3.SS4.p3.16.m16.1.1.3.2.cmml" xref="S3.SS4.p3.16.m16.1.1.3.2">𝐶</ci><ci id="S3.SS4.p3.16.m16.1.1.3.3.cmml" xref="S3.SS4.p3.16.m16.1.1.3.3">𝑡</ci></apply><apply id="S3.SS4.p3.16.m16.1.1.1.1.1.cmml" xref="S3.SS4.p3.16.m16.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.16.m16.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.16.m16.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.16.m16.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.16.m16.1.1.1.1.1.2">𝑙</ci><ci id="S3.SS4.p3.16.m16.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.16.m16.1.1.1.1.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.16.m16.1c">C_{t}(l_{j})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.16.m16.1d">italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> for <math alttext="j=1,2,\dots,m" class="ltx_Math" display="inline" id="S3.SS4.p3.17.m17.4"><semantics id="S3.SS4.p3.17.m17.4a"><mrow id="S3.SS4.p3.17.m17.4.5" xref="S3.SS4.p3.17.m17.4.5.cmml"><mi id="S3.SS4.p3.17.m17.4.5.2" xref="S3.SS4.p3.17.m17.4.5.2.cmml">j</mi><mo id="S3.SS4.p3.17.m17.4.5.1" xref="S3.SS4.p3.17.m17.4.5.1.cmml">=</mo><mrow id="S3.SS4.p3.17.m17.4.5.3.2" xref="S3.SS4.p3.17.m17.4.5.3.1.cmml"><mn id="S3.SS4.p3.17.m17.1.1" xref="S3.SS4.p3.17.m17.1.1.cmml">1</mn><mo id="S3.SS4.p3.17.m17.4.5.3.2.1" xref="S3.SS4.p3.17.m17.4.5.3.1.cmml">,</mo><mn id="S3.SS4.p3.17.m17.2.2" xref="S3.SS4.p3.17.m17.2.2.cmml">2</mn><mo id="S3.SS4.p3.17.m17.4.5.3.2.2" xref="S3.SS4.p3.17.m17.4.5.3.1.cmml">,</mo><mi id="S3.SS4.p3.17.m17.3.3" mathvariant="normal" xref="S3.SS4.p3.17.m17.3.3.cmml">…</mi><mo id="S3.SS4.p3.17.m17.4.5.3.2.3" xref="S3.SS4.p3.17.m17.4.5.3.1.cmml">,</mo><mi id="S3.SS4.p3.17.m17.4.4" xref="S3.SS4.p3.17.m17.4.4.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.17.m17.4b"><apply id="S3.SS4.p3.17.m17.4.5.cmml" xref="S3.SS4.p3.17.m17.4.5"><eq id="S3.SS4.p3.17.m17.4.5.1.cmml" xref="S3.SS4.p3.17.m17.4.5.1"></eq><ci id="S3.SS4.p3.17.m17.4.5.2.cmml" xref="S3.SS4.p3.17.m17.4.5.2">𝑗</ci><list id="S3.SS4.p3.17.m17.4.5.3.1.cmml" xref="S3.SS4.p3.17.m17.4.5.3.2"><cn id="S3.SS4.p3.17.m17.1.1.cmml" type="integer" xref="S3.SS4.p3.17.m17.1.1">1</cn><cn id="S3.SS4.p3.17.m17.2.2.cmml" type="integer" xref="S3.SS4.p3.17.m17.2.2">2</cn><ci id="S3.SS4.p3.17.m17.3.3.cmml" xref="S3.SS4.p3.17.m17.3.3">…</ci><ci id="S3.SS4.p3.17.m17.4.4.cmml" xref="S3.SS4.p3.17.m17.4.4">𝑚</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.17.m17.4c">j=1,2,\dots,m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.17.m17.4d">italic_j = 1 , 2 , … , italic_m</annotation></semantics></math>. This produces a vector <math alttext="v" class="ltx_Math" display="inline" id="S3.SS4.p3.18.m18.1"><semantics id="S3.SS4.p3.18.m18.1a"><mi id="S3.SS4.p3.18.m18.1.1" xref="S3.SS4.p3.18.m18.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.18.m18.1b"><ci id="S3.SS4.p3.18.m18.1.1.cmml" xref="S3.SS4.p3.18.m18.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.18.m18.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.18.m18.1d">italic_v</annotation></semantics></math> of similarity scores between the given image and all the <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.19.m19.1"><semantics id="S3.SS4.p3.19.m19.1a"><mi id="S3.SS4.p3.19.m19.1.1" xref="S3.SS4.p3.19.m19.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.19.m19.1b"><ci id="S3.SS4.p3.19.m19.1.1.cmml" xref="S3.SS4.p3.19.m19.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.19.m19.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.19.m19.1d">italic_m</annotation></semantics></math> classes, that is:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="v=[\text{CS}(C^{\prime}(T(\hat{y})),C_{t}(l_{1})),\ldots,\text{CS}(C^{\prime}(%
T(\hat{y})),C_{t}(l_{m}))]." class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.4" xref="S3.E2.m1.4.4.1.1.4.cmml">v</mi><mo id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.2.2" xref="S3.E2.m1.4.4.1.1.2.3.cmml"><mo id="S3.E2.m1.4.4.1.1.2.2.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.3.cmml">[</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.cmml"><mtext id="S3.E2.m1.4.4.1.1.1.1.1.4" xref="S3.E2.m1.4.4.1.1.1.1.1.4a.cmml">CS</mtext><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.3.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.2.3.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msup><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E2.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">^</mo></mover><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E2.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.4" xref="S3.E2.m1.4.4.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.cmml"><msub id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml">C</mi><mi id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.3" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.2.cmml">l</mi><mn id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.2.4" xref="S3.E2.m1.4.4.1.1.2.3.cmml">,</mo><mi id="S3.E2.m1.3.3" mathvariant="normal" xref="S3.E2.m1.3.3.cmml">…</mi><mo id="S3.E2.m1.4.4.1.1.2.2.5" xref="S3.E2.m1.4.4.1.1.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.cmml"><mtext id="S3.E2.m1.4.4.1.1.2.2.2.4" xref="S3.E2.m1.4.4.1.1.2.2.2.4a.cmml">CS</mtext><mo id="S3.E2.m1.4.4.1.1.2.2.2.3" xref="S3.E2.m1.4.4.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.3.cmml"><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.cmml"><msup id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.cmml">C</mi><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.3.cmml">′</mo></msup><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.2.cmml">T</mi><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.cmml"><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.cmml">(</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">y</mi><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.4" xref="S3.E2.m1.4.4.1.1.2.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.cmml"><msub id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.2.cmml">C</mi><mi id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.3" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.cmml"><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.2" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.2.cmml">l</mi><mi id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.3" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.3.cmml">m</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.3" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2.2.6" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" lspace="0em" xref="S3.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"></eq><ci id="S3.E2.m1.4.4.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.4">𝑣</ci><list id="S3.E2.m1.4.4.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2"><apply id="S3.E2.m1.4.4.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.3"></times><ci id="S3.E2.m1.4.4.1.1.1.1.1.4a.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.4"><mtext id="S3.E2.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.4">CS</mtext></ci><interval closure="open" id="S3.E2.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2"><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.3.3">′</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">𝑇</ci><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">^</ci><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑦</ci></apply></apply></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2"><times id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.2"></times><apply id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.2">𝐶</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.3.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.2">𝑙</ci><cn id="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.4.4.1.1.1.1.1.2.2.2.1.1.1.3">1</cn></apply></apply></interval></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">…</ci><apply id="S3.E2.m1.4.4.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2"><times id="S3.E2.m1.4.4.1.1.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.3"></times><ci id="S3.E2.m1.4.4.1.1.2.2.2.4a.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.4"><mtext id="S3.E2.m1.4.4.1.1.2.2.2.4.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.4">CS</mtext></ci><interval closure="open" id="S3.E2.m1.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2"><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1"><times id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.2"></times><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.2">𝐶</ci><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.3.3">′</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1"><times id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.2">𝑇</ci><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><ci id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2">𝑦</ci></apply></apply></apply><apply id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2"><times id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.2"></times><apply id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.2">𝐶</ci><ci id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.3.3">𝑡</ci></apply><apply id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.2">𝑙</ci><ci id="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2.2.2.2.1.1.1.3">𝑚</ci></apply></apply></interval></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">v=[\text{CS}(C^{\prime}(T(\hat{y})),C_{t}(l_{1})),\ldots,\text{CS}(C^{\prime}(%
T(\hat{y})),C_{t}(l_{m}))].</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_v = [ CS ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) , … , CS ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p3.21">The resulting similarity vector is transformed into a probability distribution over the <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.p3.20.m1.1"><semantics id="S3.SS4.p3.20.m1.1a"><mi id="S3.SS4.p3.20.m1.1.1" xref="S3.SS4.p3.20.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.20.m1.1b"><ci id="S3.SS4.p3.20.m1.1.1.cmml" xref="S3.SS4.p3.20.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.20.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.20.m1.1d">italic_m</annotation></semantics></math> classes using a softmax function. Finally, the cross-entropy loss is computed with respect to the corresponding one-hot encoded ground truth label vector <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S3.SS4.p3.21.m2.1"><semantics id="S3.SS4.p3.21.m2.1a"><mi id="S3.SS4.p3.21.m2.1.1" xref="S3.SS4.p3.21.m2.1.1.cmml">𝐭</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.21.m2.1b"><ci id="S3.SS4.p3.21.m2.1.1.cmml" xref="S3.SS4.p3.21.m2.1.1">𝐭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.21.m2.1c">\mathbf{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.21.m2.1d">bold_t</annotation></semantics></math>, thus:
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{CE}=\text{CE}(\text{Softmax}(v),\mathbf{t}).\vspace{%
-2mm}" class="ltx_Math" display="inline" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.3.3.1.1.3.2" xref="S3.E3.m1.3.3.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E3.m1.3.3.1.1.3.3" xref="S3.E3.m1.3.3.1.1.3.3.cmml"><mi id="S3.E3.m1.3.3.1.1.3.3.2" xref="S3.E3.m1.3.3.1.1.3.3.2.cmml">C</mi><mo id="S3.E3.m1.3.3.1.1.3.3.1" xref="S3.E3.m1.3.3.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E3.m1.3.3.1.1.3.3.3" xref="S3.E3.m1.3.3.1.1.3.3.3.cmml">E</mi></mrow></msub><mo id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mtext id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3a.cmml">CE</mtext><mo id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.3.3.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2a.cmml">Softmax</mtext><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">v</mi><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">𝐭</mi><mo id="S3.E3.m1.3.3.1.1.1.1.1.4" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" lspace="0em" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"></eq><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2">ℒ</ci><apply id="S3.E3.m1.3.3.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3"><times id="S3.E3.m1.3.3.1.1.3.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.3.1"></times><ci id="S3.E3.m1.3.3.1.1.3.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.3.2">𝐶</ci><ci id="S3.E3.m1.3.3.1.1.3.3.3.cmml" xref="S3.E3.m1.3.3.1.1.3.3.3">𝐸</ci></apply></apply><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2"></times><ci id="S3.E3.m1.3.3.1.1.1.3a.cmml" xref="S3.E3.m1.3.3.1.1.1.3"><mtext id="S3.E3.m1.3.3.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.3">CE</mtext></ci><interval closure="open" id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.2a.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2"><mtext id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.2">Softmax</mtext></ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑣</ci></apply><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝐭</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\displaystyle\mathcal{L}_{CE}=\text{CE}(\text{Softmax}(v),\mathbf{t}).\vspace{%
-2mm}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT = CE ( Softmax ( italic_v ) , bold_t ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p3.22">This approach aims to bridge the visual features to the text domain specifically for MLLM-based vision tasks, distinguishing our method from existing works in the field of coding for machines. We validate the effectiveness of our loss function design in the ablation study (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS5" title="4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Training Procedure</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Application scenarios for our method with corresponding training objective.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.1.1">Application Scenario</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.6.1.2"><span class="ltx_text" id="S3.T1.5.6.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.6.1.2.1.1">
<span class="ltx_tr" id="S3.T1.5.6.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.2.1.1.1.1.1">Update</span></span></span>
<span class="ltx_tr" id="S3.T1.5.6.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.2.1.1.2.1.1">Image Codec</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.6.1.3"><span class="ltx_text" id="S3.T1.5.6.1.3.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.6.1.3.1.1">
<span class="ltx_tr" id="S3.T1.5.6.1.3.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.3.1.1.1.1.1">Human</span></span></span>
<span class="ltx_tr" id="S3.T1.5.6.1.3.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.3.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.3.1.1.2.1.1">Viewing</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.6.1.4"><span class="ltx_text" id="S3.T1.5.6.1.4.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.6.1.4.1.1">
<span class="ltx_tr" id="S3.T1.5.6.1.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.4.1.1.1.1.1">Phase 1</span></span></span>
<span class="ltx_tr" id="S3.T1.5.6.1.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.4.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.4.1.1.2.1.1">Loss Function</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.5.6.1.5"><span class="ltx_text" id="S3.T1.5.6.1.5.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.6.1.5.1.1">
<span class="ltx_tr" id="S3.T1.5.6.1.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.5.1.1.1.1.1">Phase 2</span></span></span>
<span class="ltx_tr" id="S3.T1.5.6.1.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.6.1.5.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.5.6.1.5.1.1.2.1.1">Loss Function</span></span></span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.2">(d1) Human perception</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1"><math alttext="\mathcal{L}_{S}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><msub id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">ℒ</ci><ci id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\mathcal{L}_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.1.5">—</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_left" id="S3.T1.3.3.3">(d2) Multi-task</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.4">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.5">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.1"><math alttext="\mathcal{L}_{S}" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><msub id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.2.2.1.m1.1.1.2" xref="S3.T1.2.2.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.T1.2.2.1.m1.1.1.3" xref="S3.T1.2.2.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><apply id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1">subscript</csymbol><ci id="S3.T1.2.2.1.m1.1.1.2.cmml" xref="S3.T1.2.2.1.m1.1.1.2">ℒ</ci><ci id="S3.T1.2.2.1.m1.1.1.3.cmml" xref="S3.T1.2.2.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\mathcal{L}_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.2"><math alttext="R+\lambda(\gamma d_{recon}(x,\hat{x})+\delta\mathcal{L}_{dist})" class="ltx_Math" display="inline" id="S3.T1.3.3.2.m1.3"><semantics id="S3.T1.3.3.2.m1.3a"><mrow id="S3.T1.3.3.2.m1.3.3" xref="S3.T1.3.3.2.m1.3.3.cmml"><mi id="S3.T1.3.3.2.m1.3.3.3" xref="S3.T1.3.3.2.m1.3.3.3.cmml">R</mi><mo id="S3.T1.3.3.2.m1.3.3.2" xref="S3.T1.3.3.2.m1.3.3.2.cmml">+</mo><mrow id="S3.T1.3.3.2.m1.3.3.1" xref="S3.T1.3.3.2.m1.3.3.1.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.3" xref="S3.T1.3.3.2.m1.3.3.1.3.cmml">λ</mi><mo id="S3.T1.3.3.2.m1.3.3.1.2" xref="S3.T1.3.3.2.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.cmml"><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.2" stretchy="false" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.cmml"><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.2.cmml">γ</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.2.cmml">d</mi><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.2.cmml">r</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.3.cmml">e</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1a" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.4" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.4.cmml">c</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1b" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.5" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.5.cmml">o</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1c" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.6" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.6.cmml">n</mi></mrow></msub><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1a" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.1.cmml"><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.2.1" stretchy="false" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.1.cmml">(</mo><mi id="S3.T1.3.3.2.m1.1.1" xref="S3.T1.3.3.2.m1.1.1.cmml">x</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.2.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.1.cmml">,</mo><mover accent="true" id="S3.T1.3.3.2.m1.2.2" xref="S3.T1.3.3.2.m1.2.2.cmml"><mi id="S3.T1.3.3.2.m1.2.2.2" xref="S3.T1.3.3.2.m1.2.2.2.cmml">x</mi><mo id="S3.T1.3.3.2.m1.2.2.1" xref="S3.T1.3.3.2.m1.2.2.1.cmml">^</mo></mover><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.2.3" stretchy="false" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.1.cmml">+</mo><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.2.cmml">δ</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.cmml"><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.2" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.3" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1a" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.4" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1b" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.5" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.5.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.T1.3.3.2.m1.3.3.1.1.1.3" stretchy="false" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.2.m1.3b"><apply id="S3.T1.3.3.2.m1.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3"><plus id="S3.T1.3.3.2.m1.3.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.2"></plus><ci id="S3.T1.3.3.2.m1.3.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.3">𝑅</ci><apply id="S3.T1.3.3.2.m1.3.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1"><times id="S3.T1.3.3.2.m1.3.3.1.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.2"></times><ci id="S3.T1.3.3.2.m1.3.3.1.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.3">𝜆</ci><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1"><plus id="S3.T1.3.3.2.m1.3.3.1.1.1.1.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.1"></plus><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2"><times id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.1"></times><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.2">𝛾</ci><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3">subscript</csymbol><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.2">𝑑</ci><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3"><times id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.1"></times><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.2">𝑟</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.3">𝑒</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.4.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.4">𝑐</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.5.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.5">𝑜</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.6.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.3.3.6">𝑛</ci></apply></apply><interval closure="open" id="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.2.4.2"><ci id="S3.T1.3.3.2.m1.1.1.cmml" xref="S3.T1.3.3.2.m1.1.1">𝑥</ci><apply id="S3.T1.3.3.2.m1.2.2.cmml" xref="S3.T1.3.3.2.m1.2.2"><ci id="S3.T1.3.3.2.m1.2.2.1.cmml" xref="S3.T1.3.3.2.m1.2.2.1">^</ci><ci id="S3.T1.3.3.2.m1.2.2.2.cmml" xref="S3.T1.3.3.2.m1.2.2.2">𝑥</ci></apply></interval></apply><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3"><times id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.1"></times><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.2">𝛿</ci><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.2">ℒ</ci><apply id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3"><times id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.1"></times><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.2.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.2">𝑑</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.3.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.4.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.4">𝑠</ci><ci id="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.5.cmml" xref="S3.T1.3.3.2.m1.3.3.1.1.1.1.3.3.3.5">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.2.m1.3c">R+\lambda(\gamma d_{recon}(x,\hat{x})+\delta\mathcal{L}_{dist})</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.2.m1.3d">italic_R + italic_λ ( italic_γ italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT ( italic_x , over^ start_ARG italic_x end_ARG ) + italic_δ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.5.5.3">(d3) Machine perception</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.5.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.5.5">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.4.1"><math alttext="\mathcal{L}_{S}" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.1"><semantics id="S3.T1.4.4.1.m1.1a"><msub id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.4.4.1.m1.1.1.2" xref="S3.T1.4.4.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.T1.4.4.1.m1.1.1.3" xref="S3.T1.4.4.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><apply id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1">subscript</csymbol><ci id="S3.T1.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.1.m1.1.1.2">ℒ</ci><ci id="S3.T1.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">\mathcal{L}_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.5.5.2"><math alttext="R+\lambda\mathcal{L}_{dist}" class="ltx_Math" display="inline" id="S3.T1.5.5.2.m1.1"><semantics id="S3.T1.5.5.2.m1.1a"><mrow id="S3.T1.5.5.2.m1.1.1" xref="S3.T1.5.5.2.m1.1.1.cmml"><mi id="S3.T1.5.5.2.m1.1.1.2" xref="S3.T1.5.5.2.m1.1.1.2.cmml">R</mi><mo id="S3.T1.5.5.2.m1.1.1.1" xref="S3.T1.5.5.2.m1.1.1.1.cmml">+</mo><mrow id="S3.T1.5.5.2.m1.1.1.3" xref="S3.T1.5.5.2.m1.1.1.3.cmml"><mi id="S3.T1.5.5.2.m1.1.1.3.2" xref="S3.T1.5.5.2.m1.1.1.3.2.cmml">λ</mi><mo id="S3.T1.5.5.2.m1.1.1.3.1" xref="S3.T1.5.5.2.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.T1.5.5.2.m1.1.1.3.3" xref="S3.T1.5.5.2.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.T1.5.5.2.m1.1.1.3.3.2" xref="S3.T1.5.5.2.m1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.T1.5.5.2.m1.1.1.3.3.3" xref="S3.T1.5.5.2.m1.1.1.3.3.3.cmml"><mi id="S3.T1.5.5.2.m1.1.1.3.3.3.2" xref="S3.T1.5.5.2.m1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.T1.5.5.2.m1.1.1.3.3.3.1" xref="S3.T1.5.5.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.5.5.2.m1.1.1.3.3.3.3" xref="S3.T1.5.5.2.m1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.T1.5.5.2.m1.1.1.3.3.3.1a" xref="S3.T1.5.5.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.5.5.2.m1.1.1.3.3.3.4" xref="S3.T1.5.5.2.m1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.T1.5.5.2.m1.1.1.3.3.3.1b" xref="S3.T1.5.5.2.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.T1.5.5.2.m1.1.1.3.3.3.5" xref="S3.T1.5.5.2.m1.1.1.3.3.3.5.cmml">t</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.2.m1.1b"><apply id="S3.T1.5.5.2.m1.1.1.cmml" xref="S3.T1.5.5.2.m1.1.1"><plus id="S3.T1.5.5.2.m1.1.1.1.cmml" xref="S3.T1.5.5.2.m1.1.1.1"></plus><ci id="S3.T1.5.5.2.m1.1.1.2.cmml" xref="S3.T1.5.5.2.m1.1.1.2">𝑅</ci><apply id="S3.T1.5.5.2.m1.1.1.3.cmml" xref="S3.T1.5.5.2.m1.1.1.3"><times id="S3.T1.5.5.2.m1.1.1.3.1.cmml" xref="S3.T1.5.5.2.m1.1.1.3.1"></times><ci id="S3.T1.5.5.2.m1.1.1.3.2.cmml" xref="S3.T1.5.5.2.m1.1.1.3.2">𝜆</ci><apply id="S3.T1.5.5.2.m1.1.1.3.3.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.T1.5.5.2.m1.1.1.3.3.1.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3">subscript</csymbol><ci id="S3.T1.5.5.2.m1.1.1.3.3.2.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.2">ℒ</ci><apply id="S3.T1.5.5.2.m1.1.1.3.3.3.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3"><times id="S3.T1.5.5.2.m1.1.1.3.3.3.1.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3.1"></times><ci id="S3.T1.5.5.2.m1.1.1.3.3.3.2.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3.2">𝑑</ci><ci id="S3.T1.5.5.2.m1.1.1.3.3.3.3.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3.3">𝑖</ci><ci id="S3.T1.5.5.2.m1.1.1.3.3.3.4.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3.4">𝑠</ci><ci id="S3.T1.5.5.2.m1.1.1.3.3.3.5.cmml" xref="S3.T1.5.5.2.m1.1.1.3.3.3.5">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.2.m1.1c">R+\lambda\mathcal{L}_{dist}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.2.m1.1d">italic_R + italic_λ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">To explore the capabilities of our method under the application scenarios introduced in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.SS2" title="3.2 Overall Framework ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we employ a two-phase training procedure that adapts to different scenarios. The first phase is shared for all scenarios and focuses on training the transform-neck exclusively. The second phase, applicable only to scenarios where the codec can be updated, involves joint optimization of both the transform-neck and the image codec. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S3.T1" title="Table 1 ‣ 3.5 Training Procedure ‣ 3 Proposed Method ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> provides a summary of the training procedure.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Phase 1: Transform-neck Training</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">In the first phase, we train only the transform-neck using a progressive training strategy with the surrogate loss <math alttext="\mathcal{L}_{S}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS5.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS5.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS5.SSS0.Px1.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px1.p1.1.m1.1c">\mathcal{L}_{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and an off-the-shelf image codec. This phase is divided into three stages:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx3">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{S}=\begin{cases}\mathcal{L}_{CE},&amp;\text{epoch}&lt;E_{1}%
,\\
\alpha\mathcal{L}_{CE}+\beta\mathcal{L}_{dist},&amp;E_{1}\leq\text{epoch}&lt;E_{2},\\
\mathcal{L}_{dist},&amp;\text{epoch}\geq E_{2},\end{cases}" class="ltx_Math" display="inline" id="S3.E4.m1.6"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.7" xref="S3.E4.m1.6.7.cmml"><msub id="S3.E4.m1.6.7.2" xref="S3.E4.m1.6.7.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.6.7.2.2" xref="S3.E4.m1.6.7.2.2.cmml">ℒ</mi><mi id="S3.E4.m1.6.7.2.3" xref="S3.E4.m1.6.7.2.3.cmml">S</mi></msub><mo id="S3.E4.m1.6.7.1" xref="S3.E4.m1.6.7.1.cmml">=</mo><mrow id="S3.E4.m1.6.6a" xref="S3.E4.m1.6.7.3.1.cmml"><mo id="S3.E4.m1.6.6a.7" xref="S3.E4.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" id="S3.E4.m1.6.6.6a" rowspacing="0pt" xref="S3.E4.m1.6.7.3.1.cmml"><mtr id="S3.E4.m1.6.6.6aa" xref="S3.E4.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ab" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">ℒ</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml">E</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ac" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S3.E4.m1.2.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml"><mtext id="S3.E4.m1.2.2.2.2.2.1.1.1.2" xref="S3.E4.m1.2.2.2.2.2.1.1.1.2a.cmml">epoch</mtext><mo id="S3.E4.m1.2.2.2.2.2.1.1.1.1" xref="S3.E4.m1.2.2.2.2.2.1.1.1.1.cmml">&lt;</mo><msub id="S3.E4.m1.2.2.2.2.2.1.1.1.3" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.1.1.1.3.2" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.2.cmml">E</mi><mn id="S3.E4.m1.2.2.2.2.2.1.1.1.3.3" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S3.E4.m1.2.2.2.2.2.1.1.2" xref="S3.E4.m1.2.2.2.2.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.6.6.6ad" xref="S3.E4.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ae" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.2.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.2.cmml">α</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.2.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.2.cmml">ℒ</mi><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.2.cmml">C</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.3.cmml">E</mi></mrow></msub></mrow><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.1.cmml">+</mo><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.2.cmml">β</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.3.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.3" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1a" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.4" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1b" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.5" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.5.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.E4.m1.3.3.3.3.1.1.1.2" xref="S3.E4.m1.3.3.3.3.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6af" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.4.4.4.4.2.1.1" xref="S3.E4.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S3.E4.m1.4.4.4.4.2.1.1.1" xref="S3.E4.m1.4.4.4.4.2.1.1.1.cmml"><msub id="S3.E4.m1.4.4.4.4.2.1.1.1.2" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2.cmml"><mi id="S3.E4.m1.4.4.4.4.2.1.1.1.2.2" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2.2.cmml">E</mi><mn id="S3.E4.m1.4.4.4.4.2.1.1.1.2.3" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E4.m1.4.4.4.4.2.1.1.1.3" xref="S3.E4.m1.4.4.4.4.2.1.1.1.3.cmml">≤</mo><mtext id="S3.E4.m1.4.4.4.4.2.1.1.1.4" xref="S3.E4.m1.4.4.4.4.2.1.1.1.4a.cmml">epoch</mtext><mo id="S3.E4.m1.4.4.4.4.2.1.1.1.5" xref="S3.E4.m1.4.4.4.4.2.1.1.1.5.cmml">&lt;</mo><msub id="S3.E4.m1.4.4.4.4.2.1.1.1.6" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6.cmml"><mi id="S3.E4.m1.4.4.4.4.2.1.1.1.6.2" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6.2.cmml">E</mi><mn id="S3.E4.m1.4.4.4.4.2.1.1.1.6.3" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6.3.cmml">2</mn></msub></mrow><mo id="S3.E4.m1.4.4.4.4.2.1.1.2" xref="S3.E4.m1.4.4.4.4.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E4.m1.6.6.6ag" xref="S3.E4.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ah" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.5.5.5.5.1.1.1" xref="S3.E4.m1.5.5.5.5.1.1.1.1.cmml"><msub id="S3.E4.m1.5.5.5.5.1.1.1.1" xref="S3.E4.m1.5.5.5.5.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.5.5.5.5.1.1.1.1.2" xref="S3.E4.m1.5.5.5.5.1.1.1.1.2.cmml">ℒ</mi><mrow id="S3.E4.m1.5.5.5.5.1.1.1.1.3" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.cmml"><mi id="S3.E4.m1.5.5.5.5.1.1.1.1.3.2" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.2.cmml">d</mi><mo id="S3.E4.m1.5.5.5.5.1.1.1.1.3.1" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E4.m1.5.5.5.5.1.1.1.1.3.3" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.3.cmml">i</mi><mo id="S3.E4.m1.5.5.5.5.1.1.1.1.3.1a" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E4.m1.5.5.5.5.1.1.1.1.3.4" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.4.cmml">s</mi><mo id="S3.E4.m1.5.5.5.5.1.1.1.1.3.1b" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E4.m1.5.5.5.5.1.1.1.1.3.5" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.5.cmml">t</mi></mrow></msub><mo id="S3.E4.m1.5.5.5.5.1.1.1.2" xref="S3.E4.m1.5.5.5.5.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ai" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.6.6.6.6.2.1.1" xref="S3.E4.m1.6.6.6.6.2.1.1.1.cmml"><mrow id="S3.E4.m1.6.6.6.6.2.1.1.1" xref="S3.E4.m1.6.6.6.6.2.1.1.1.cmml"><mtext id="S3.E4.m1.6.6.6.6.2.1.1.1.2" xref="S3.E4.m1.6.6.6.6.2.1.1.1.2a.cmml">epoch</mtext><mo id="S3.E4.m1.6.6.6.6.2.1.1.1.1" xref="S3.E4.m1.6.6.6.6.2.1.1.1.1.cmml">≥</mo><msub id="S3.E4.m1.6.6.6.6.2.1.1.1.3" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3.cmml"><mi id="S3.E4.m1.6.6.6.6.2.1.1.1.3.2" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3.2.cmml">E</mi><mn id="S3.E4.m1.6.6.6.6.2.1.1.1.3.3" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.E4.m1.6.6.6.6.2.1.1.2" xref="S3.E4.m1.6.6.6.6.2.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.7.cmml" xref="S3.E4.m1.6.7"><eq id="S3.E4.m1.6.7.1.cmml" xref="S3.E4.m1.6.7.1"></eq><apply id="S3.E4.m1.6.7.2.cmml" xref="S3.E4.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.7.2.1.cmml" xref="S3.E4.m1.6.7.2">subscript</csymbol><ci id="S3.E4.m1.6.7.2.2.cmml" xref="S3.E4.m1.6.7.2.2">ℒ</ci><ci id="S3.E4.m1.6.7.2.3.cmml" xref="S3.E4.m1.6.7.2.3">𝑆</ci></apply><apply id="S3.E4.m1.6.7.3.1.cmml" xref="S3.E4.m1.6.6a"><csymbol cd="latexml" id="S3.E4.m1.6.7.3.1.1.cmml" xref="S3.E4.m1.6.6a.7">cases</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">ℒ</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.3">𝐸</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1"><lt id="S3.E4.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.1"></lt><ci id="S3.E4.m1.2.2.2.2.2.1.1.1.2a.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.2"><mtext id="S3.E4.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.2">epoch</mtext></ci><apply id="S3.E4.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.2">𝐸</ci><cn id="S3.E4.m1.2.2.2.2.2.1.1.1.3.3.cmml" type="integer" xref="S3.E4.m1.2.2.2.2.2.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1"><plus id="S3.E4.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.1"></plus><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2"><times id="S3.E4.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.1"></times><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.2">𝛼</ci><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.2">ℒ</ci><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3"><times id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.1"></times><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.2">𝐶</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.2.3.3.3">𝐸</ci></apply></apply></apply><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3"><times id="S3.E4.m1.3.3.3.3.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.1"></times><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.2">𝛽</ci><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.2">ℒ</ci><apply id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3"><times id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.1"></times><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.2">𝑑</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.4">𝑠</ci><ci id="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.5.cmml" xref="S3.E4.m1.3.3.3.3.1.1.1.1.3.3.3.5">𝑡</ci></apply></apply></apply></apply><apply id="S3.E4.m1.4.4.4.4.2.1.1.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"><and id="S3.E4.m1.4.4.4.4.2.1.1.1a.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"></and><apply id="S3.E4.m1.4.4.4.4.2.1.1.1b.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"><leq id="S3.E4.m1.4.4.4.4.2.1.1.1.3.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.3"></leq><apply id="S3.E4.m1.4.4.4.4.2.1.1.1.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.4.2.1.1.1.2.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.4.4.4.4.2.1.1.1.2.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2.2">𝐸</ci><cn id="S3.E4.m1.4.4.4.4.2.1.1.1.2.3.cmml" type="integer" xref="S3.E4.m1.4.4.4.4.2.1.1.1.2.3">1</cn></apply><ci id="S3.E4.m1.4.4.4.4.2.1.1.1.4a.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.4"><mtext id="S3.E4.m1.4.4.4.4.2.1.1.1.4.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.4">epoch</mtext></ci></apply><apply id="S3.E4.m1.4.4.4.4.2.1.1.1c.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"><lt id="S3.E4.m1.4.4.4.4.2.1.1.1.5.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.5"></lt><share href="https://arxiv.org/html/2407.19651v2#S3.E4.m1.4.4.4.4.2.1.1.1.4.cmml" id="S3.E4.m1.4.4.4.4.2.1.1.1d.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1"></share><apply id="S3.E4.m1.4.4.4.4.2.1.1.1.6.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.4.4.2.1.1.1.6.1.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6">subscript</csymbol><ci id="S3.E4.m1.4.4.4.4.2.1.1.1.6.2.cmml" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6.2">𝐸</ci><cn id="S3.E4.m1.4.4.4.4.2.1.1.1.6.3.cmml" type="integer" xref="S3.E4.m1.4.4.4.4.2.1.1.1.6.3">2</cn></apply></apply></apply><apply id="S3.E4.m1.5.5.5.5.1.1.1.1.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.5.5.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1">subscript</csymbol><ci id="S3.E4.m1.5.5.5.5.1.1.1.1.2.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.2">ℒ</ci><apply id="S3.E4.m1.5.5.5.5.1.1.1.1.3.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3"><times id="S3.E4.m1.5.5.5.5.1.1.1.1.3.1.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.1"></times><ci id="S3.E4.m1.5.5.5.5.1.1.1.1.3.2.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.2">𝑑</ci><ci id="S3.E4.m1.5.5.5.5.1.1.1.1.3.3.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.3">𝑖</ci><ci id="S3.E4.m1.5.5.5.5.1.1.1.1.3.4.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.4">𝑠</ci><ci id="S3.E4.m1.5.5.5.5.1.1.1.1.3.5.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1.1.3.5">𝑡</ci></apply></apply><apply id="S3.E4.m1.6.6.6.6.2.1.1.1.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1"><geq id="S3.E4.m1.6.6.6.6.2.1.1.1.1.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.1"></geq><ci id="S3.E4.m1.6.6.6.6.2.1.1.1.2a.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.2"><mtext id="S3.E4.m1.6.6.6.6.2.1.1.1.2.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.2">epoch</mtext></ci><apply id="S3.E4.m1.6.6.6.6.2.1.1.1.3.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.6.6.2.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.6.6.6.6.2.1.1.1.3.2.cmml" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3.2">𝐸</ci><cn id="S3.E4.m1.6.6.6.6.2.1.1.1.3.3.cmml" type="integer" xref="S3.E4.m1.6.6.6.6.2.1.1.1.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\displaystyle\mathcal{L}_{S}=\begin{cases}\mathcal{L}_{CE},&amp;\text{epoch}&lt;E_{1}%
,\\
\alpha\mathcal{L}_{CE}+\beta\mathcal{L}_{dist},&amp;E_{1}\leq\text{epoch}&lt;E_{2},\\
\mathcal{L}_{dist},&amp;\text{epoch}\geq E_{2},\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.6d">caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = { start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT , end_CELL start_CELL epoch &lt; italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_α caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT + italic_β caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ epoch &lt; italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL epoch ≥ italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.4">where the weighting factors <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.2.m1.1"><semantics id="S3.SS5.SSS0.Px1.p1.2.m1.1a"><mi id="S3.SS5.SSS0.Px1.p1.2.m1.1.1" xref="S3.SS5.SSS0.Px1.p1.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px1.p1.2.m1.1b"><ci id="S3.SS5.SSS0.Px1.p1.2.m1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px1.p1.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px1.p1.2.m1.1d">italic_α</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.3.m2.1"><semantics id="S3.SS5.SSS0.Px1.p1.3.m2.1a"><mi id="S3.SS5.SSS0.Px1.p1.3.m2.1.1" xref="S3.SS5.SSS0.Px1.p1.3.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px1.p1.3.m2.1b"><ci id="S3.SS5.SSS0.Px1.p1.3.m2.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.3.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px1.p1.3.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px1.p1.3.m2.1d">italic_β</annotation></semantics></math> are set with a ratio of 1:100 for the two loss terms, and <math alttext="E_{1},E_{2}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px1.p1.4.m3.2"><semantics id="S3.SS5.SSS0.Px1.p1.4.m3.2a"><mrow id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.3.cmml"><msub id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.cmml"><mi id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.2" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.2.cmml">E</mi><mn id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.3" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.3" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.3.cmml">,</mo><msub id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.cmml"><mi id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.2" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.2.cmml">E</mi><mn id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.3" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px1.p1.4.m3.2b"><list id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.3.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2"><apply id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.2.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.2">𝐸</ci><cn id="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.3.cmml" type="integer" xref="S3.SS5.SSS0.Px1.p1.4.m3.1.1.1.1.3">1</cn></apply><apply id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.1.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.2.cmml" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.2">𝐸</ci><cn id="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.3.cmml" type="integer" xref="S3.SS5.SSS0.Px1.p1.4.m3.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px1.p1.4.m3.2c">E_{1},E_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px1.p1.4.m3.2d">italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> are empirically set to 20 and 40, respectively, in our experiments. This initial phase ensures that the transform-neck learns the transformation to align with the target latent space.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Phase 2: Joint Optimization</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">For (d2) and (d3), where the image codec is allowed to be re-trained to produce latent representation more suitable for machine perception, the second phase is introduced with transform-neck and image codec jointly updated after phase 1 converges.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p2.10">The scenario (d2), referred to as multi-task, aims to accommodate both human and machine perception. As a result, it is trained jointly with the transform-neck on both the distillation loss and traditional rate-distortion loss, i.e. <math alttext="\mathcal{L}_{d2}=R+\lambda(\gamma d_{recon}(x,\hat{x})+\delta\mathcal{L}_{dist})" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.1.m1.3"><semantics id="S3.SS5.SSS0.Px2.p2.1.m1.3a"><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.cmml"><msub id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.2.cmml">d</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.1.cmml">⁢</mo><mn id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.3.cmml">2</mn></mrow></msub><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.2.cmml">=</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.3.cmml">R</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.2.cmml">+</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.3.cmml">λ</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.2.cmml">γ</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.2.cmml">d</mi><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.2.cmml">r</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.3.cmml">e</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1a" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.4" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.4.cmml">c</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1b" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.5" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.5.cmml">o</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1c" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.6" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.6.cmml">n</mi></mrow></msub><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1a" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.1.cmml"><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.2.1" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.1.cmml">(</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.1.1.cmml">x</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.2.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.1.cmml">,</mo><mover accent="true" id="S3.SS5.SSS0.Px2.p2.1.m1.2.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.2.2.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2.2.cmml">x</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.2.2.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2.1.cmml">^</mo></mover><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.2.3" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.2.cmml">δ</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.2" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.3" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1a" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.4" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1b" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.5" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.5.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.1.m1.3b"><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3"><eq id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.2"></eq><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.2">ℒ</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.1"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.2">𝑑</ci><cn id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.3.cmml" type="integer" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.3.3.3">2</cn></apply></apply><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1"><plus id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.2"></plus><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.3">𝑅</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.2"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.3">𝜆</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1"><plus id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.1"></plus><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.1"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.2">𝛾</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.2">𝑑</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.1"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.2">𝑟</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.3">𝑒</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.4.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.4">𝑐</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.5.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.5">𝑜</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.6.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.3.3.6">𝑛</ci></apply></apply><interval closure="open" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.2.4.2"><ci id="S3.SS5.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.1.1">𝑥</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2"><ci id="S3.SS5.SSS0.Px2.p2.1.m1.2.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.2.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.2.2.2">𝑥</ci></apply></interval></apply><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.2">𝛿</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.2">ℒ</ci><apply id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3"><times id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.1"></times><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.4.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.4">𝑠</ci><ci id="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.5.cmml" xref="S3.SS5.SSS0.Px2.p2.1.m1.3.3.1.1.1.1.1.3.3.3.5">𝑡</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.1.m1.3c">\mathcal{L}_{d2}=R+\lambda(\gamma d_{recon}(x,\hat{x})+\delta\mathcal{L}_{dist})</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.1.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_d 2 end_POSTSUBSCRIPT = italic_R + italic_λ ( italic_γ italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT ( italic_x , over^ start_ARG italic_x end_ARG ) + italic_δ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="R=-\log p(\hat{z})-\log p(\hat{y}|\hat{z})" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.2.m2.2"><semantics id="S3.SS5.SSS0.Px2.p2.2.m2.2a"><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.3" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.3.cmml">R</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.2.cmml">=</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.cmml"><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.cmml"><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3a" rspace="0.167em" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.cmml">−</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.cmml"><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.1.cmml">log</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2a" lspace="0.167em" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.cmml">⁡</mo><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.2.cmml">p</mi></mrow><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.1.cmml">⁢</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.3.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.cmml"><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.3.2.1" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.cmml">(</mo><mover accent="true" id="S3.SS5.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.2.cmml">z</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.1.cmml">^</mo></mover><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.3.2.2" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.2.cmml">−</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.cmml"><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.1.cmml">log</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3a" lspace="0.167em" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.cmml">⁡</mo><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.2.cmml">p</mi></mrow><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.cmml"><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.2" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.1.cmml">^</mo></mover><mo fence="false" id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.2" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.2.cmml">z</mi><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.1" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.3" stretchy="false" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.2.m2.2b"><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2"><eq id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.2"></eq><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.3.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.3">𝑅</ci><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1"><minus id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.2"></minus><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3"><minus id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3"></minus><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2"><times id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.1"></times><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2"><log id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.1"></log><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.2.2">𝑝</ci></apply><apply id="S3.SS5.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.3.2.3.2"><ci id="S3.SS5.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.1.1.2">𝑧</ci></apply></apply></apply><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1"><times id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.2"></times><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3"><log id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.1"></log><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.3.2">𝑝</ci></apply><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2"><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.2.2">𝑦</ci></apply><apply id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3"><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.2.m2.2.2.1.1.1.1.1.3.2">𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.2.m2.2c">R=-\log p(\hat{z})-\log p(\hat{y}|\hat{z})</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.2.m2.2d">italic_R = - roman_log italic_p ( over^ start_ARG italic_z end_ARG ) - roman_log italic_p ( over^ start_ARG italic_y end_ARG | over^ start_ARG italic_z end_ARG )</annotation></semantics></math> is the estimated rate of <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.3.m3.1"><semantics id="S3.SS5.SSS0.Px2.p2.3.m3.1a"><mover accent="true" id="S3.SS5.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS5.SSS0.Px2.p2.3.m3.1.1.1" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS5.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1"><ci id="S3.SS5.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.3.m3.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.3.m3.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.3.m3.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> and <math alttext="\hat{z}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.4.m4.1"><semantics id="S3.SS5.SSS0.Px2.p2.4.m4.1a"><mover accent="true" id="S3.SS5.SSS0.Px2.p2.4.m4.1.1" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.4.m4.1.1.2" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1.2.cmml">z</mi><mo id="S3.SS5.SSS0.Px2.p2.4.m4.1.1.1" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.4.m4.1b"><apply id="S3.SS5.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1"><ci id="S3.SS5.SSS0.Px2.p2.4.m4.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.4.m4.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.4.m4.1.1.2">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.4.m4.1c">\hat{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.4.m4.1d">over^ start_ARG italic_z end_ARG</annotation></semantics></math>, and <math alttext="d_{recon}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.5.m5.1"><semantics id="S3.SS5.SSS0.Px2.p2.5.m5.1a"><msub id="S3.SS5.SSS0.Px2.p2.5.m5.1.1" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.2" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.2.cmml">d</mi><mrow id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.2" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.2.cmml">r</mi><mo id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.3" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.3.cmml">e</mi><mo id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1a" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.4" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.4.cmml">c</mi><mo id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1b" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.5" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.5.cmml">o</mi><mo id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1c" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.6" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.5.m5.1b"><apply id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.2">𝑑</ci><apply id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3"><times id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.1"></times><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.2">𝑟</ci><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.3">𝑒</ci><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.4.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.4">𝑐</ci><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.5.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.5">𝑜</ci><ci id="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.6.cmml" xref="S3.SS5.SSS0.Px2.p2.5.m5.1.1.3.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.5.m5.1c">d_{recon}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.5.m5.1d">italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT</annotation></semantics></math> is the reconstruction loss calculated as the MSE between the uncompressed image <math alttext="x" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.6.m6.1"><semantics id="S3.SS5.SSS0.Px2.p2.6.m6.1a"><mi id="S3.SS5.SSS0.Px2.p2.6.m6.1.1" xref="S3.SS5.SSS0.Px2.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.6.m6.1b"><ci id="S3.SS5.SSS0.Px2.p2.6.m6.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.6.m6.1d">italic_x</annotation></semantics></math> and the reconstructed image <math alttext="\hat{x}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.7.m7.1"><semantics id="S3.SS5.SSS0.Px2.p2.7.m7.1a"><mover accent="true" id="S3.SS5.SSS0.Px2.p2.7.m7.1.1" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p2.7.m7.1.1.2" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1.2.cmml">x</mi><mo id="S3.SS5.SSS0.Px2.p2.7.m7.1.1.1" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.7.m7.1b"><apply id="S3.SS5.SSS0.Px2.p2.7.m7.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1"><ci id="S3.SS5.SSS0.Px2.p2.7.m7.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1.1">^</ci><ci id="S3.SS5.SSS0.Px2.p2.7.m7.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p2.7.m7.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.7.m7.1c">\hat{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.7.m7.1d">over^ start_ARG italic_x end_ARG</annotation></semantics></math>. The hyper-parameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.8.m8.1"><semantics id="S3.SS5.SSS0.Px2.p2.8.m8.1a"><mi id="S3.SS5.SSS0.Px2.p2.8.m8.1.1" xref="S3.SS5.SSS0.Px2.p2.8.m8.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.8.m8.1b"><ci id="S3.SS5.SSS0.Px2.p2.8.m8.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.8.m8.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.8.m8.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.8.m8.1d">italic_λ</annotation></semantics></math> controls the rate-distortion trade-off, while <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.9.m9.1"><semantics id="S3.SS5.SSS0.Px2.p2.9.m9.1a"><mi id="S3.SS5.SSS0.Px2.p2.9.m9.1.1" xref="S3.SS5.SSS0.Px2.p2.9.m9.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.9.m9.1b"><ci id="S3.SS5.SSS0.Px2.p2.9.m9.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.9.m9.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.9.m9.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.9.m9.1d">italic_γ</annotation></semantics></math> and <math alttext="\delta" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p2.10.m10.1"><semantics id="S3.SS5.SSS0.Px2.p2.10.m10.1a"><mi id="S3.SS5.SSS0.Px2.p2.10.m10.1.1" xref="S3.SS5.SSS0.Px2.p2.10.m10.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p2.10.m10.1b"><ci id="S3.SS5.SSS0.Px2.p2.10.m10.1.1.cmml" xref="S3.SS5.SSS0.Px2.p2.10.m10.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p2.10.m10.1c">\delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p2.10.m10.1d">italic_δ</annotation></semantics></math> weight the two losses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p3.3">In (d3), where the downstream applications do not require image reconstruction, the encoder and transform-neck are jointly optimized to minimize the trade-off cost between the rate <math alttext="R" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p3.1.m1.1"><semantics id="S3.SS5.SSS0.Px2.p3.1.m1.1a"><mi id="S3.SS5.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS5.SSS0.Px2.p3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p3.1.m1.1b"><ci id="S3.SS5.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p3.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p3.1.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p3.1.m1.1d">italic_R</annotation></semantics></math> and the distillation loss <math alttext="\mathcal{L}_{dist}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p3.2.m2.1"><semantics id="S3.SS5.SSS0.Px2.p3.2.m2.1a"><msub id="S3.SS5.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.2" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.2" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.2.cmml">d</mi><mo id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.3" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.3.cmml">i</mi><mo id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1a" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.4" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.4.cmml">s</mi><mo id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1b" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.5" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p3.2.m2.1b"><apply id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.2">ℒ</ci><apply id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3"><times id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.1"></times><ci id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.2">𝑑</ci><ci id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.3">𝑖</ci><ci id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.4.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.4">𝑠</ci><ci id="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.5.cmml" xref="S3.SS5.SSS0.Px2.p3.2.m2.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p3.2.m2.1c">\mathcal{L}_{dist}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p3.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, thus disregarding the reconstruction quality. The training objective is thus <math alttext="\mathcal{L}_{d3}=R+\lambda\mathcal{L}_{dist}" class="ltx_Math" display="inline" id="S3.SS5.SSS0.Px2.p3.3.m3.1"><semantics id="S3.SS5.SSS0.Px2.p3.3.m3.1a"><mrow id="S3.SS5.SSS0.Px2.p3.3.m3.1.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.cmml"><msub id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.cmml"><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.2.cmml">d</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.1.cmml">⁢</mo><mn id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.3.cmml">3</mn></mrow></msub><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.2.cmml">R</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.1.cmml">+</mo><mrow id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.2.cmml">λ</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.1.cmml">⁢</mo><msub id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.2" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.2.cmml">d</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.3" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.3.cmml">i</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1a" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.4" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.4.cmml">s</mi><mo id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1b" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.5" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.5.cmml">t</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p3.3.m3.1b"><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1"><eq id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.1"></eq><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.2">ℒ</ci><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3"><times id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.1"></times><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.2">𝑑</ci><cn id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.3.cmml" type="integer" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.2.3.3">3</cn></apply></apply><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3"><plus id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.1"></plus><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.2">𝑅</ci><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3"><times id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.2">𝜆</ci><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.2">ℒ</ci><apply id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3"><times id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.1"></times><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.2">𝑑</ci><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.3">𝑖</ci><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.4.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.4">𝑠</ci><ci id="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.5.cmml" xref="S3.SS5.SSS0.Px2.p3.3.m3.1.1.3.3.3.3.5">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p3.3.m3.1c">\mathcal{L}_{d3}=R+\lambda\mathcal{L}_{dist}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS0.Px2.p3.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_d 3 end_POSTSUBSCRIPT = italic_R + italic_λ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Training Details and Datasets.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.6">We utilize ELIC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>)</cite> as our image codec, which outputs image and hyperprior latents with <math alttext="N=320" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝑁</ci><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">N=320</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">italic_N = 320</annotation></semantics></math> and <math alttext="N_{h}=192" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">N</mi><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">h</mi></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1"><eq id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1"></eq><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.2">𝑁</ci><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.3">ℎ</ci></apply><cn id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">N_{h}=192</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.2.m2.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 192</annotation></semantics></math>, respectively. ELIC is trained for human perception and adheres to the training strategy outlined in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>)</cite>, using 8,000 images of the highest spatial resolution selected from ImageNet dataset. Four models are trained for four different rate points, corresponding to <math alttext="\lambda=[0.004,0.008,0.016,0.032]" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.3.m3.4"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.4a"><mrow id="S4.SS1.SSS0.Px1.p1.3.m3.4.5" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.cmml"><mi id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.2.cmml">λ</mi><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.1.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml"><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2.1" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml">[</mo><mn id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">0.004</mn><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px1.p1.3.m3.2.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.2.2.cmml">0.008</mn><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2.3" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px1.p1.3.m3.3.3" xref="S4.SS1.SSS0.Px1.p1.3.m3.3.3.cmml">0.016</mn><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2.4" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S4.SS1.SSS0.Px1.p1.3.m3.4.4" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.4.cmml">0.032</mn><mo id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2.5" stretchy="false" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.4b"><apply id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5"><eq id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.2.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.2">𝜆</ci><list id="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.5.3.2"><cn id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" type="float" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1">0.004</cn><cn id="S4.SS1.SSS0.Px1.p1.3.m3.2.2.cmml" type="float" xref="S4.SS1.SSS0.Px1.p1.3.m3.2.2">0.008</cn><cn id="S4.SS1.SSS0.Px1.p1.3.m3.3.3.cmml" type="float" xref="S4.SS1.SSS0.Px1.p1.3.m3.3.3">0.016</cn><cn id="S4.SS1.SSS0.Px1.p1.3.m3.4.4.cmml" type="float" xref="S4.SS1.SSS0.Px1.p1.3.m3.4.4">0.032</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.4c">\lambda=[0.004,0.008,0.016,0.032]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.3.m3.4d">italic_λ = [ 0.004 , 0.008 , 0.016 , 0.032 ]</annotation></semantics></math> in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>)</cite>.
For each of our scenarios (d1), (d2) and (d3), separate transform-necks are trained on ImageNet dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Deng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib10" title="">2009</a>)</cite> for individual <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S4.SS1.SSS0.Px1.p1.4.m4.1a"><mi id="S4.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.4.m4.1b"><ci id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.4.m4.1d">italic_λ</annotation></semantics></math> values. For the scenario (d2) specifically, we find empirically that fixing the ratio <math alttext="\gamma:\delta=60:1" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S4.SS1.SSS0.Px1.p1.5.m5.1a"><mrow id="S4.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">γ</mi><mo id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3" lspace="0.278em" rspace="0.278em" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">:</mo><mrow id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.cmml"><mi id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.2" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.2.cmml">δ</mi><mo id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.1" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.3" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.3.cmml">60</mn></mrow><mo id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.5" lspace="0.278em" rspace="0.278em" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.5.cmml">:</mo><mn id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.6" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"><and id="S4.SS1.SSS0.Px1.p1.5.m5.1.1a.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"></and><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1b.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.3">:</ci><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.2">𝛾</ci><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4"><eq id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.1.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.1"></eq><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.2.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.2">𝛿</ci><cn id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.3.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.3">60</cn></apply></apply><apply id="S4.SS1.SSS0.Px1.p1.5.m5.1.1c.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"><ci id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.5.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.5">:</ci><share href="https://arxiv.org/html/2407.19651v2#S4.SS1.SSS0.Px1.p1.5.m5.1.1.4.cmml" id="S4.SS1.SSS0.Px1.p1.5.m5.1.1d.cmml" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1"></share><cn id="S4.SS1.SSS0.Px1.p1.5.m5.1.1.6.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.5.m5.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.5.m5.1c">\gamma:\delta=60:1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.5.m5.1d">italic_γ : italic_δ = 60 : 1</annotation></semantics></math> leads to a good trade-off between human and machine perception.
Given that most MLLMs adopt the pre-trained visual encoder of CLIP ViT-L/14&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib37" title="">2021</a>)</cite> for image modality, as discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S2.SS1" title="2.1 Multimodal Large Language Models ‣ 2 Related Works ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">2.1</span></a>, we use the CLIP visual encoder as <math alttext="C" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.6.m6.1"><semantics id="S4.SS1.SSS0.Px1.p1.6.m6.1a"><mi id="S4.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.6.m6.1b"><ci id="S4.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.6.m6.1c">C</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.6.m6.1d">italic_C</annotation></semantics></math> for training and conduct our primary experiments on MLLMs that incorporate it.
It is worth noting that, since we consider MLLMs sharing the same visual encoder, we do not need to train separate systems for the different MLLMs or tasks.
Furthermore, we provide additional experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS6" title="4.6 Generalization ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.6</span></a> with MLLMs that use visual encoders other than CLIP ViT to demonstrate the generalizability of our approach.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S4.F3.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_main_RD_full.png" width="532">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Rate-accuracy comparison using various MLLMs on several tasks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.SS1.SSS0.Px1.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1" style="width:254.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluated tasks with corresponding dataset and MLLM.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.1" style="width:37.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.1.1">
<span class="ltx_p" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.2.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.1.fig1.1.1.1.3.1">MLLM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.1" style="width:37.0pt;padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.1.1">
<span class="ltx_p" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.1.1.1">Captioning</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">COCO Karpathy Test</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Karpathy &amp; Fei-Fei, <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib15" title="">2015</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-Adapter</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.2.1.3.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.1" style="width:37.0pt;padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.1.1">
<span class="ltx_p" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.1.1.1">VQA</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">SEED-Bench</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib20" title="">2023a</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Honeybee</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.3.2.3.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.1" style="width:37.0pt;padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.1.1">
<span class="ltx_p" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.1.1.1">REC</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">RefCOCO-val</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Kazemzadeh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib16" title="">2014</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3" style="padding-bottom:2.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Shikra</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.4.3.3.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1" style="width:37.0pt;padding-bottom:0.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_top" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Few-shot</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">classification</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2" style="padding-bottom:0.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">ImageNet</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Deng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib10" title="">2009</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3" style="padding-bottom:0.0pt;padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3.1">
<tbody><tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">V2L-Tokenizer</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS1.SSS0.Px1.1.fig1.1.5.4.3.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S4.F4" style="width:127.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="485" id="S4.SS1.SSS0.Px1.2.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_recon_kodak_v.png" width="494">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Reconstruction performance comparison on&nbsp;<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib17" title="">Kodak </a></cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Targeted MLLM-based Vision Tasks.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">To validate the generalization ability of our proposed method, we evaluate its performance on four different MLLM systems for four different tasks. The tasks, datasets, corresponding MLLMs, and metrics are listed in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS1.SSS0.Px1" title="Training Details and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
These configurations follow the settings outlined in their original papers and the accompanying code, except for the few-shot classification task due to the inaccessibility of the code.
We thus design a 5-way 1-shot classification scenario to evaluate the performance with in-context learning; the detailed setting is described in supplementary material. All MLLMs are used off-the-shelf without any fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Baselines.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">We introduce two baseline methods for comparison. The first one, denoted as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.1">Reconstruction</span>, involves inputting the reconstructed image generated by ELIC to the MLLM system. The second one, denoted as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.2">Post-processing</span>, adapts the reconstructed image to MLLMs through a U-Net&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib38" title="">2015</a>)</cite> post-processing network, which is trained using the same surrogate loss as that adopted by our method. We remark that these image-domain baselines incur higher complexity than our lightweight transform-neck, as they involve decoding the image and potentially processing it further with the post-processing network.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance Comparison</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F3" title="Figure 3 ‣ Training Details and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the performance of the baseline methods and our proposed scheme for the three examined scenarios with regards to two aspects: coding bit-rate, calculated as bits per pixel (bpp), and task performance. When comparing the baselines and our method in scenario (d1), where the original ELIC is trained solely for human perception, we make the following observations. (1) Straightforwardly using the reconstructed images generated by a codec trained for human perception leads to a significant performance drop across all the tasks (<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Reconstruction</span>). Such performance decline is expected because the MLLMs are not trained with compressed images, thus hindering their recognition performance. This highlights the necessity of adapting image compression and/or image latents to MLLMs. (2) In contrast, our transform-neck method successfully boosts the performance using the same latent representations for reconstructing the image in <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Reconstruction</span>, confirming the effectiveness of the proposed latent transformation without the decoding process. (3) <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">Post-processing</span> is able to reach comparable performance to <span class="ltx_text" id="S4.SS2.p1.1.4" style="color:#000000;">our (d1)</span>, offering another viable solution to the problem. However, it is worth noting that <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.5">Post-processing</span> requires relatively higher computational complexity with respect to our transform-neck method, rendering our approach preferable <span class="ltx_text" id="S4.SS2.p1.1.6" style="color:#000000;">(see Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.SS4" title="4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>)</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.SS2.2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S4.SS2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S4.SS2.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.1.1.1.2.1.1.1.1" style="width:20.9pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-46.46pt,10.42pt) rotate(-90deg) ;">
<div class="ltx_block ltx_parbox ltx_align_middle" id="S4.SS2.1.1.1.2.1.1.1.1.1" style="width:113.8pt;">
<p class="ltx_p" id="S4.SS2.1.1.1.2.1.1.1.1.1.1">Captioning</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p ltx_align_center" id="S4.SS2.1.1.1.2.1.1.1.1.1.2">LLaMA-Adapter</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</span></div>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.1.1.1.1.1">
<tbody><tr class="ltx_tr" id="S4.SS2.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="145" id="S4.SS2.1.1.1.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/cap_392212.png" width="439"></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1.1.1.1.2.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="S4.SS2.1.1.1.1.1.2.1.1.1">: A man is walking an elephant down a path.</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.1.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.1.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1.1.1.1.3.1.1" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="S4.SS2.1.1.1.1.1.3.1.1.1">: A man feeding an elephant with his hand.</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.1.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.1.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.SS2.1.1.1.1.1.4.1.1" style="color:#FF0000;">Ours (d1): A man is petting an elephant on the head.</span></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.SS2.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S4.SS2.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.1.1.1.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.1.1.1.3.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="S4.SS2.1.1.1.3.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0958</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.SS2.2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.2.2.2.2.1">
<tbody><tr class="ltx_tr" id="S4.SS2.2.2.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.2.2.2.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.2.2.2.2.1.1.1.1" style="width:18.9pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-47.43pt,8.96pt) rotate(-90deg) ;">
<div class="ltx_block ltx_parbox ltx_align_middle" id="S4.SS2.2.2.2.2.1.1.1.1.1" style="width:113.8pt;">
<p class="ltx_p" id="S4.SS2.2.2.2.2.1.1.1.1.1.1">REC</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p ltx_align_center" id="S4.SS2.2.2.2.2.1.1.1.1.1.2">Shikra</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</span></div>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.SS2.2.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.2.2.2.1.1">
<tbody><tr class="ltx_tr" id="S4.SS2.2.2.2.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.2.2.2.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">I’m trying to locate man with mask on in &lt;img&gt;. Can you determine its coordinates for me?</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.2.2.2.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.2.2.2.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="116" id="S4.SS2.2.2.2.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/shikra_vis_1_425_img.png" width="439"></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.SS2.2.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.2.2.2.3.1">
<tbody><tr class="ltx_tr" id="S4.SS2.2.2.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.SS2.2.2.2.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.2.2.2.3.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="S4.SS2.2.2.2.3.1.1.1.1.1" style="width:113.8pt;">BPP:0.061</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S4.F5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visualization examples of our proposed method in (d1), <span class="ltx_text ltx_font_italic" id="S4.F5.3.1">Reconstruction</span>, and <span class="ltx_text ltx_font_italic" id="S4.F5.4.2">Post-processing</span> on image captioning with LLaMA-Adapter and REC with Shikra.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Next, we evaluate the effects of allowing the image codec to be re-trained. First, we observe that (d2) outperforms both (d1) and <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">Post-processing</span>. This indicates that fine-tuning the encoder indeed results in a more suitable latent representation that can be better adapted to MLLMs.
When examining the extreme setting (d3), we see significant further improvement in the task performance, approaching the performance upper bound with uncompressed images.
This improvement comes at the cost of the image reconstruction quality, which, however, is not relevant in (d3).
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F4" title="Figure 4 ‣ Training Details and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the rate-visual quality curves associated with the three scenarios. Interestingly, (d2) exhibits only a marginal PSNR drop compared to (d1), while (d3) significantly compromises the quality of the decoded image.
We stress that our framework (i.e. the surrogate loss and transform-neck) is able to accommodate different application scenarios, allowing for a variable trade-off between the task performance and the image reconstruction quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Visualization of the Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We present the visualization of outcomes with downstream MLLM-based vision tasks in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F5" title="Figure 5 ‣ 4.2 Performance Comparison ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>. Our method (d1) is compared with the two baseline methods, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Reconstruction</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">Post-processing</span>, with particular focus on how these models work at low bitrates to reflect a bandwidth-limited scenario. In the second and third columns, we visualize the reconstructed and post-processed images from the two baselines, respectively, which exhibit drastically different characteristics. The former (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.3">Reconstruction</span>) produces blurry and smooth images, while the latter (<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.4">Post-processing</span>) introduces some artificial patterns into the post-processed images.
Compared with the baselines, our method yields better MLLM results. More visualization are presented in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.SS3" title="A.3 More Visualization ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">A.3</span></a> of the supplementary material.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of the kMACs/pixel and model size. The table omits the shared components of the two methods, i.e. the image encoder, the partial CLIP visual encoder, the connector, and the LLM.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Component</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T3.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">Params (M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">kMAC/pixel</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.1.1">Ours (d1, d2, or d3)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">Transform-neck</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T3.1.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">13.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T3.1.2.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">52.795</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.3.3.1" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.1.3.3.1.1">Post-processing</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">Decoder</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">7.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.3.3.4" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.4.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.3.3.4.1.1">
<span class="ltx_tr" id="S4.T3.1.3.3.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.1.3.3.4.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">64.16</span></span>
<span class="ltx_tr" id="S4.T3.1.3.3.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.1.3.3.4.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.3.4.1.1.2.1.1">(+386%)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">112.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.3.3.6" rowspan="3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.6.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.3.3.6.1.1">
<span class="ltx_tr" id="S4.T3.1.3.3.6.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.1.3.3.6.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">1017.96</span></span>
<span class="ltx_tr" id="S4.T3.1.3.3.6.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.1.3.3.6.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.3.6.1.1.2.1.1">(+1828%)</span></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">Post-processing network</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">31.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">835.72</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.1.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.5.5.1.1">
<tbody><tr class="ltx_tr" id="S4.T3.1.5.5.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.1.5.5.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">First 2 layers of CLIP visual encoder</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">25.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">70.24</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Complexity Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.T3" title="Table 3 ‣ 4.3 Visualization of the Results ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> compares the computational complexity between our proposed method and <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">Post-processing</span> baseline in terms of model size and the kilo-multiply-accumulate-operations per pixel (kMACs/pixel). Note that our method in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.T3" title="Table 3 ‣ 4.3 Visualization of the Results ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> refers to any of (d1), (d2), and (d3), since they share the same computational complexity characteristics at inference time. Our method offers a lightweight solution with only 13 million parameters, as opposed to 64 million parameters with the post-processing approach. Moreover, in terms of kMAC/pixel, the difference stands out even more, considering that the post-processing network operates at the full image resolution while our method operates in the latent domain, where the image latents have a much smaller spatial resolution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="188" id="S4.F6.sf1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_ablation_layer.png" width="180">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Partial CLIP visual encoder</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="188" id="S4.F6.sf2.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_ablation_loss.png" width="180">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Training objectives</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="188" id="S4.F6.sf3.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_ablation_tic.png" width="180">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Different image codecs</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Rate-accuracy comparison for three ablation studies evaluated with image captioning task.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="140" id="S4.F7.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/x1.png" width="521"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="74" id="S4.F7.g2" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/vis_token_001040.png" width="283"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of MSE reduction on CLIP visual encoder output tokens before and after training using different loss functions. Darker red colors indicate greater MSE reduction.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The following ablation experiments are performed based on (d1) to justify our design choices.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Training Objective.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F6" title="Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> (b) presents the performance of our method when trained exclusively with the cross-entropy loss or distillation loss. It is observed that training with only the cross-entropy loss results in a significant performance drop. Although providing a good initial update direction, this strategy is unable to learn an effective transformation for MLLMs. Instead, training solely with the distillation loss fails to update the transform-neck properly and leads to far inferior performance. This is potentially due to the stringent requirement of fitting the exact feature representations. <span class="ltx_text" id="S4.SS5.SSS0.Px1.p1.1.1" style="color:#000000;">Our proposed method, not a simple application of distillation, achieves the highest performance.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS5.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p2.1">To further support this finding, we present the following analysis in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F7" title="Figure 7 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>: we first calculate the mean squared error (MSE) between CLIP visual encoder output tokens derived from uncompressed images and from our transform-neck, both before and after the transform-neck has been trained. Then, we compute the difference between these MSEs to measure the improvement achieved by the specific training objectives all with equal training steps.
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F7" title="Figure 7 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the cross-entropy loss reduces feature matching errors primarily in foreground object regions, while the distillation loss reduces global matching errors. Our proposed progressive training strategy integrates these two losses, leading to a greater reduction in MSE and thus improved rate-accuracy performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Partial CLIP Visual Encoder.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px2.p1.1">This experiment investigates the proper number of Transformer layers to remove from the CLIP visual encoder in order to strike a good balance between complexity and performance. As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F6" title="Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> (a), removing the first one or two layers achieves similar performance, whereas removing four or eight layers results in a noticeable performance drop. We thus remove the first two layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Different Image Codecs.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px3.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F6" title="Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> (c) presents the performance comparison between our method and <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.1.1">Reconstruction</span> when they are tested with ELIC and TIC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib31" title="">2022a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib32" title="">b</a>)</cite>.
TIC is a Transformer-based codec, whereas ELIC is a convolutional neural network-based codec.
We see that our transform-neck still outperforms <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS0.Px3.p1.1.2">Reconstruction</span> by a significant margin when the image codec is changed from ELIC to TIC. This indicates that our method is still effective on a different type of image codec.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S4.F8.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_generalization.png" width="548">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Rate-accuracy comparison on two MLLMs not utilizing CLIP ViT visual encoder: mPLUG-Owl2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ye et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib44" title="">2024</a>)</cite> on MMBench and Osprey&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yuan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib46" title="">2024</a>)</cite> on POPE (popular setting).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Generalization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">While we utilize the CLIP ViT visual encoder in our main experiments due to its wide popularity, our proposed method is applicable to various downstream MLLMs regardless of the visual encoder they adopt. As illustrative examples, Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#S4.F8" title="Figure 8 ‣ Different Image Codecs. ‣ 4.5 Ablation Studies ‣ 4 Experimental Results ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a> presents the rate-accuracy performance of our re-trained scheme applied to two MLLMs that do not use the pre-trained CLIP ViT visual encoder: (1) mPlug-Owl2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ye et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib44" title="">2024</a>)</cite> with a custom-trained ViT visual encoder, and (2) Osprey&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yuan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib46" title="">2024</a>)</cite> with a CLIP ConvNeXt-based visual encoder. Our method under all three settings clearly outperforms the <span class="ltx_text ltx_font_italic" id="S4.SS6.p1.1.1">Reconstruction</span> baseline, confirming the generalizability of the proposed framework.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper proposes the first image compression system tailored to Multimodal Large Language Models (MLLMs). It introduces a transform-neck that bridges the compressed image latents and the intermediate layer of the visual encoder. By using our proposed surrogate loss, we avoid involving the entire MLLM in the training process and ensure downstream task performance. With lower computational complexity, our method has demonstrated effectiveness across a wide variety of tasks, MLLMs, and neural image codecs, outperforming other baselines in extensive experiments.
One consideration is that this paper focuses solely on the image compression aspect, leaving the exploration of MLLM-based video or audio coding for future work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S5.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSSx1.p1">
<p class="ltx_p" id="S5.SS0.SSSx1.p1.1">This work is supported by National Science and Technology Council, Taiwan under the Grant NSTC 113-2634-F-A49-007-, MediaTek,
and National Center for High-performance Computing, Taiwan.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ascenso et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joao Ascenso, Elena Alshina, and Touradj Ebrahimi.

</span>
<span class="ltx_bibblock">The jpeg ai standard: Providing efficient human and machine visual data consumption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Ieee Multimedia</em>, 30(1):100–111, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ballé et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Ballé, David Minnen, Saurabh Singh, Sung&nbsp;Jin Hwang, and Nick Johnston.

</span>
<span class="ltx_bibblock">Variational image compression with a scale hyperprior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1802.01436</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bross et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary&nbsp;J Sullivan, and Jens-Rainer Ohm.

</span>
<span class="ltx_bibblock">Overview of the versatile video coding (vvc) standard and its applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, 31(10):3736–3764, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cha et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.

</span>
<span class="ltx_bibblock">Honeybee: Locality-enhanced projector for multimodal llm.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chamain et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lahiru&nbsp;D Chamain, Fabien Racapé, Jean Bégaint, Akshay Pushparaja, and Simon Feltman.

</span>
<span class="ltx_bibblock">End-to-end optimized image compression for machines, a study.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">2021 Data Compression Conference (DCC)</em>, pp.&nbsp; 163–172. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.

</span>
<span class="ltx_bibblock">Shikra: Unleashing multimodal llm’s referential dialogue magic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2306.15195</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi-Hsin Chen, Ying-Chieh Weng, Chia-Hao Kao, Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng.

</span>
<span class="ltx_bibblock">Transtic: Transferring transformer-based image compression from human perception to machine perception.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.&nbsp; 23297–23307, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E Gonzalez, et&nbsp;al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">See https://vicuna. lmsys. org (accessed 14 April 2023)</em>, 2(3):6, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et&nbsp;al. (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li&nbsp;Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2009 IEEE conference on computer vision and pattern recognition</em>, pp.&nbsp; 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ding Ding, Zhenzhong Chen, Zizheng Liu, Xiaozhong Xu, and Shan Liu.

</span>
<span class="ltx_bibblock">Hierarchical image feature compression for machines via feature sparsity learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE Signal Processing Letters</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ruoyu Feng, Xin Jin, Zongyu Guo, Runsen Feng, Yixin Gao, Tianyu He, Zhizheng Zhang, Simeng Sun, and Zhibo Chen.

</span>
<span class="ltx_bibblock">Image coding for machines with omnipotent feature learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII</em>, pp.&nbsp; 510–528. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang.

</span>
<span class="ltx_bibblock">Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.&nbsp; 5718–5727, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy &amp; Fei-Fei (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrej Karpathy and Li&nbsp;Fei-Fei.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.&nbsp; 3128–3137, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemzadeh et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pp.&nbsp; 787–798, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Eastman Kodak.

</span>
<span class="ltx_bibblock">Kodak lossless true color image suite (PhotoCD PCD0992).

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://r0k.us/graphics/kodak" title="">http://r0k.us/graphics/kodak</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, and Esa Rahtu.

</span>
<span class="ltx_bibblock">Image coding for machines: an end-to-end learned approach.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.&nbsp; 1590–1594. IEEE, 2021a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Hamed&nbsp;Rezazadegan Tavakoli, and Esa Rahtu.

</span>
<span class="ltx_bibblock">Learned image coding for machines: A content-adaptive approach.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2021 IEEE International Conference on Multimedia and Expo (ICME)</em>, pp.&nbsp; 1–6. IEEE, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.

</span>
<span class="ltx_bibblock">Seed-bench: Benchmarking multimodal llms with generative comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2307.16125</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International conference on machine learning</em>, pp.&nbsp; 19730–19742. PMLR, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang.

</span>
<span class="ltx_bibblock">Tokenpacker: Efficient visual projector for multimodal llm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2407.02392</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=xozJw0kZXF" title="">https://openreview.net/forum?id=xozJw0kZXF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji&nbsp;Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.

</span>
<span class="ltx_bibblock">Vila: On pre-training for visual language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C&nbsp;Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pp.&nbsp; 740–755. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong&nbsp;Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinming Liu, Heming Sun, and Jiro Katto.

</span>
<span class="ltx_bibblock">Learning in compressed domain for faster machine vision tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">2021 International Conference on Visual Communications and Image Processing (VCIP)</em>, pp.&nbsp; 01–05, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/VCIP53242.2021.9675369</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinming Liu, Heming Sun, and Jiro Katto.

</span>
<span class="ltx_bibblock">Improving multiple machine vision tasks in the compressed domain.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2022 26th International Conference on Pattern Recognition (ICPR)</em>, pp.&nbsp; 331–337. IEEE, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinming Liu, Heming Sun, and Jiro Katto.

</span>
<span class="ltx_bibblock">Improving multiple machine vision tasks in the compressed domain.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2022 26th International Conference on Pattern Recognition (ICPR)</em>, pp.&nbsp; 331–337. IEEE, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinming Liu, Heming Sun, and Jiro Katto.

</span>
<span class="ltx_bibblock">Learned image compression with mixed transformer-cnn architectures.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.&nbsp; 14388–14397, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ming Lu, Fangdong Chen, Shiliang Pu, and Zhan Ma.

</span>
<span class="ltx_bibblock">High-efficiency lossy image coding through adaptive neighborhood information aggregation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2204.11448</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma.

</span>
<span class="ltx_bibblock">Transformer-based image compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Data Compression Conference</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matsubara et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoshitomo Matsubara, Ruihan Yang, Marco Levorato, and Stephan Mandt.

</span>
<span class="ltx_bibblock">Supervised compression for resource-constrained edge computing systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.&nbsp; 2685–2695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yixin Mei, Fan Li, Li&nbsp;Li, and Zhu Li.

</span>
<span class="ltx_bibblock">Learn a compression for objection detection - vae with a bridge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">2021 International Conference on Visual Communications and Image Processing (VCIP)</em>, pp.&nbsp; 1–5, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/VCIP53242.2021.9675387</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, and Kwonjoon Lee.

</span>
<span class="ltx_bibblock">Can’t make an omelette without breaking some eggs: Plausible action.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, Li&nbsp;Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding multimodal large language models to the world.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jong&nbsp;Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">International conference on machine learning</em>, pp.&nbsp; 8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>, pp.&nbsp; 234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Crossget: Cross-guided ensemble of tokens for accelerating vision-language transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International Conference on Machine Learning (ICML)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Johannes Ballé, Abhinav Shrivastava, and George Toderici.

</span>
<span class="ltx_bibblock">End-to-end learning of compressible features.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2020 IEEE International Conference on Image Processing (ICIP)</em>, pp.&nbsp; 3349–3353. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shurun Wang, Zhao Wang, Shiqi Wang, and Yan Ye.

</span>
<span class="ltx_bibblock">Deep image compression towards machine vision: A unified optimization framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi&nbsp;Qian, Ji&nbsp;Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.&nbsp; 13040–13051, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.

</span>
<span class="ltx_bibblock">Osprey: Pixel understanding with visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.&nbsp; 28202–28211, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu&nbsp;Qiao.

</span>
<span class="ltx_bibblock">Llama-adapter: Efficient fine-tuning of language models with zero-init attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International Conference on Learning Representations (ICLR)</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai.

</span>
<span class="ltx_bibblock">Groundhog: Grounding large language models to holistic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">International Conference on Learning Representations (ICLR)</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lei Zhu, Fangyun Wei, and Yanye Lu.

</span>
<span class="ltx_bibblock">Beyond text: Frozen large language models in visual signal comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024b.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Supplementary Material</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Implementation Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Training.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.4">We use the Adam optimizer, configured with <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="A1.SS1.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">β</mi><mn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝛽</ci><cn id="A1.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.1.m1.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> at 0.9, <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="A1.SS1.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">β</mi><mn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝛽</ci><cn id="A1.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.2.m2.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> at 0.999, <math alttext="\epsilon" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="A1.SS1.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="A1.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.3.m3.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.3.m3.1d">italic_ϵ</annotation></semantics></math> at <math alttext="10^{-8}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS1.SSS0.Px1.p1.4.m4.1a"><msup id="A1.SS1.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">10</mn><mrow id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mo id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3a" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">−</mo><mn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.2" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><cn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.2">10</cn><apply id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3"><minus id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3"></minus><cn id="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" type="integer" xref="A1.SS1.SSS0.Px1.p1.4.m4.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px1.p1.4.m4.1c">10^{-8}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS0.Px1.p1.4.m4.1d">10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT</annotation></semantics></math>. Weight decay is disabled. The transform-neck for each rate point undergoes training on an RTX 4090 for approximately three days during the training stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Evaluation.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">For few-shot classification with V2L-Tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite>, we design a 5-way 1-shot classification evaluation scenario. In particular, we generate 5000 groups of images from ImageNet dataset, where each group consists of five randomly sampled images from different classes, serving as the sample images, and one new image from one of the classes as the query image.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p2.1">Different MLLM is utilized for the evaluation of our proposed method on each task. In Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.T4" title="Table 4 ‣ Evaluation. ‣ A.1 Implementation Details ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we provide the detailed specifications of the MLLM used in our evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The specifications of the MLLM used in our tasks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T4.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="A1.T4.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T4.1.1.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1.1.1.1">Model</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="A1.T4.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T4.1.1.1.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1.1.1.1">LLM</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Captioning</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-Adapter v1&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib41" title="">2023a</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">VQA</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">Honeybee-C-7B-M144&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T4.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">Vicuna-7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib9" title="">2023</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T4.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">REC</th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">Shikra-7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T4.1.4.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib41" title="">2023a</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A1.T4.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">Few-shot classification</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">V2L-Tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T4.1.5.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA2-7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib42" title="">2023b</a>)</cite>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Comparison with VVC</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F9" title="Figure 9 ‣ A.2 Comparison with VVC ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">9</span></a> compares <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.1">Reconstruction</span> and our method in (d1) using ELIC, with the state-of-the-art traditional codec VVC (VTM 17.0 intra coding). We set the QPs to <math alttext="[37,40,43,46,49]" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.5"><semantics id="A1.SS2.p1.1.m1.5a"><mrow id="A1.SS2.p1.1.m1.5.6.2" xref="A1.SS2.p1.1.m1.5.6.1.cmml"><mo id="A1.SS2.p1.1.m1.5.6.2.1" stretchy="false" xref="A1.SS2.p1.1.m1.5.6.1.cmml">[</mo><mn id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml">37</mn><mo id="A1.SS2.p1.1.m1.5.6.2.2" xref="A1.SS2.p1.1.m1.5.6.1.cmml">,</mo><mn id="A1.SS2.p1.1.m1.2.2" xref="A1.SS2.p1.1.m1.2.2.cmml">40</mn><mo id="A1.SS2.p1.1.m1.5.6.2.3" xref="A1.SS2.p1.1.m1.5.6.1.cmml">,</mo><mn id="A1.SS2.p1.1.m1.3.3" xref="A1.SS2.p1.1.m1.3.3.cmml">43</mn><mo id="A1.SS2.p1.1.m1.5.6.2.4" xref="A1.SS2.p1.1.m1.5.6.1.cmml">,</mo><mn id="A1.SS2.p1.1.m1.4.4" xref="A1.SS2.p1.1.m1.4.4.cmml">46</mn><mo id="A1.SS2.p1.1.m1.5.6.2.5" xref="A1.SS2.p1.1.m1.5.6.1.cmml">,</mo><mn id="A1.SS2.p1.1.m1.5.5" xref="A1.SS2.p1.1.m1.5.5.cmml">49</mn><mo id="A1.SS2.p1.1.m1.5.6.2.6" stretchy="false" xref="A1.SS2.p1.1.m1.5.6.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.5b"><list id="A1.SS2.p1.1.m1.5.6.1.cmml" xref="A1.SS2.p1.1.m1.5.6.2"><cn id="A1.SS2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1">37</cn><cn id="A1.SS2.p1.1.m1.2.2.cmml" type="integer" xref="A1.SS2.p1.1.m1.2.2">40</cn><cn id="A1.SS2.p1.1.m1.3.3.cmml" type="integer" xref="A1.SS2.p1.1.m1.3.3">43</cn><cn id="A1.SS2.p1.1.m1.4.4.cmml" type="integer" xref="A1.SS2.p1.1.m1.4.4">46</cn><cn id="A1.SS2.p1.1.m1.5.5.cmml" type="integer" xref="A1.SS2.p1.1.m1.5.5">49</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.5c">[37,40,43,46,49]</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.1.m1.5d">[ 37 , 40 , 43 , 46 , 49 ]</annotation></semantics></math> for VVC. It is observed that VVC performs worse than <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.2">Reconstruction</span> across all the tasks, which is potentially due to (1) the small spatial resolution (256x256) of input images that is not optimal for VVC, (2) its inferior rate-distortion performance compared to ELIC as reported in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib13" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="284" id="A1.F9.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/paper_sup_vvc_v2.png" width="521">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Rate-accuracy comparison using VTM on several tasks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>More Visualization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We present additional visualization results on four different evaluation tasks, including image captioning (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F12" title="Figure 12 ‣ A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">12</span></a>), visual question answering (VQA) (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F13" title="Figure 13 ‣ A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">13</span></a>), referring expression comprehension (REC) (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F14" title="Figure 14 ‣ A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">14</span></a>), and few-shot classification (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F15" title="Figure 15 ‣ A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">15</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>License of Assets Used</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.T5" title="Table 5 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the used assets in our work along with their license terms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>List of assets used in the paper with their corresponding license.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T5.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">Assets</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="A1.T5.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1.1.1.1">Licenses</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Deng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib10" title="">2009</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T5.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">Custom license. Available at https://image-net.org/download.php</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib25" title="">2014</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">CC BY 4.0</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">SEED-Bench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib20" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">Apache 2.0</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">LLaMA-Adapter&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib47" title="">2024a</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">GPL-3.0</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.6.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">Honeybee&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cha et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.6.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T5.1.6.5.2.1">
<tbody><tr class="ltx_tr" id="A1.T5.1.6.5.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.6.5.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Source code: Apache 2.0</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.5.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.6.5.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Pretrained weights: CC BY-NC 4.0</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.7.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">Shikra&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.7.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">CC BY-NC 4.0</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T5.1.8.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">V2L-Tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib50" title="">2024b</a>)</cite>
</th>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A1.T5.1.8.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T5.1.8.7.2.1">
<tbody><tr class="ltx_tr" id="A1.T5.1.8.7.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.8.7.2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">No license provided.</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.7.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A1.T5.1.8.7.2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Code available at https://github.com/zh460045050/V2L-Tokenizer</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Additional Performance Comparison</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F10" title="Figure 10 ‣ A.5 Additional Performance Comparison ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">10</span></a> presents an additional performance comparison of our proposed method on the same task (POPE benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib23" title="">2023c</a>)</cite>) with two different MLLMs, Honeybee&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib5" title="">2024</a>)</cite> and Shikra&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib7" title="">2023a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="A1.F10.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/rebuttal_pope.png" width="419">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Performance comparison on POPE benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib23" title="">2023c</a>)</cite> with different MLLMs.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Comparison with Token Reduction Methods</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="A1.F11.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/Cost.png" width="494">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>High-level architecture of our proposed method and different cost associated with each component.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.1"><span class="ltx_text" id="A1.SS6.p1.1.1" style="color:#000000;">Token reduction methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib39" title="">2024</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#bib.bib22" title="">2024</a>)</cite> aim to reduce the number of visual tokens for lowering the inference computational cost of MLLMs. These methods differ fundamentally from our method as they do not consider the transmission of visual tokens in compressed form.
In contrast, our work encodes and transmits the image in compressed form, where the compressed image latents are adapted to suit the downstream MLLM and image reconstruction tasks.
The focus of our work is at maintaining downstream performance while reducing transmission cost and decoding inference cost. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.19651v2#A1.F11" title="Figure 11 ‣ A.6 Comparison with Token Reduction Methods ‣ Appendix A Supplementary Material ‣ Bridging Compressed Image Latents and Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">11</span></a> illustrates the different costs associated with each component in the system.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS6.p2">
<p class="ltx_p" id="A1.SS6.p2.1"><span class="ltx_text" id="A1.SS6.p2.1.1" style="color:#000000;">Ideally, one might propose generating visual tokens on the end device and using token reduction techniques as a compression method to reduce transmission bandwidth. However, this approach would impose significant computational demands on the end device, making it impractical for coding for machines scenarios, where the primary goal is to offload heavy feature extraction computations to the cloud.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS6.p3">
<p class="ltx_p" id="A1.SS6.p3.1"><span class="ltx_text" id="A1.SS6.p3.1.1" style="color:#000000;">Notably, our method and the token reduction technique could potentially complement each other to develop a more efficient system. For instance, our approach allows to save transmission bandwidth and reduce cloud-side complexity by eliminating the need for image decoding, then token reduction techniques can further optimize complexity on the cloud.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.SS6.4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="A1.SS6.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS6.4.4.5.1">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.SS6.4.4.5.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.4.4.5.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.4.4.5.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.5.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">Task: Captioning</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.4.4.5.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.5.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Model: LLaMA-Adapter</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS6.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.1.1.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.1.1.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.1.1.1.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/cap_385005.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.1.1.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.1.1.1.1.1.2.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.1.1.1.1.1.2.1.1.1">: A microwave and a computer sitting on a desk.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.1.1.1.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.1.1.1.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.1.1.1.1.1.3.1.1" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.1.1.1.1.1.3.1.1.1">: A microwave and a refrigerator sitting on top of a table.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.1.1.1.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.1.1.1.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A1.SS6.1.1.1.1.1.4.1.1" style="color:#FF0000;">Ours (d1): A microwave and a toaster oven on a counter.</span></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.1.1.1.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.1.1.1.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.1.1.1.2.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0725</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.2.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.2.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.2.2.2.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.2.2.2.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.2.2.2.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.2.2.2.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/cap_326911.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.2.2.2.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.2.2.2.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.2.2.2.1.1.2.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.2.2.2.1.1.2.1.1.1">: Two cats are standing on the ground near a bench.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.2.2.2.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.2.2.2.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.2.2.2.1.1.3.1.1" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.2.2.2.1.1.3.1.1.1">: A dog and a cat are standing on a sidewalk.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.2.2.2.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.2.2.2.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A1.SS6.2.2.2.1.1.4.1.1" style="color:#FF0000;">Ours (d1): Two dogs are standing near a bicycle on a sidewalk.</span></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.2.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.2.2.2.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.2.2.2.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.2.2.2.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.2.2.2.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.2.2.2.2.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0928</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.3.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.3.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.3.3.3.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.3.3.3.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.3.3.3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.3.3.3.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/cap_182503.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.3.3.3.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.3.3.3.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.3.3.3.1.1.2.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.3.3.3.1.1.2.1.1.1">: A blurry picture of a blender with a knife.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.3.3.3.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.3.3.3.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.3.3.3.1.1.3.1.1" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.3.3.3.1.1.3.1.1.1">: A close up of a blurry image of a bug.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.3.3.3.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.3.3.3.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A1.SS6.3.3.3.1.1.4.1.1" style="color:#FF0000;">Ours (d1): A close up of a knife cutting into a pizza.</span></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.3.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.3.3.3.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.3.3.3.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.3.3.3.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.3.3.3.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.3.3.3.2.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0910</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.4.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.4.4.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.4.4.4.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.4.4.4.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.4.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.4.4.4.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/cap_238449.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.4.4.4.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.4.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.4.4.4.1.1.2.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.4.4.4.1.1.2.1.1.1">: A young boy in a red shirt and tie posing for a picture.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.4.4.4.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.4.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="A1.SS6.4.4.4.1.1.3.1.1" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.4.4.4.1.1.3.1.1.1">: A young boy standing in front of a wall with a clock.</span></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.4.4.4.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.4.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="A1.SS6.4.4.4.1.1.4.1.1" style="color:#FF0000;">Ours (d1): A young boy in a tie and a white shirt.</span></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.4.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.4.4.4.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.4.4.4.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.4.4.4.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.4.4.4.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.4.4.4.2.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0899</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A1.F12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Visualization examples of our proposed method in (d1), <span class="ltx_text ltx_font_italic" id="A1.F12.3.1">Reconstruction</span>, and <span class="ltx_text ltx_font_italic" id="A1.F12.4.2">Post-processing</span> on image captioning with LLaMA-Adapter.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS6.8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="A1.SS6.8.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS6.8.4.5.1">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.SS6.8.4.5.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.8.4.5.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.8.4.5.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.5.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">Task: Visual question answering</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.8.4.5.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.5.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Model: Honeybee</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS6.5.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.5.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.5.1.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.5.1.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.5.1.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">What is the dog doing in the image?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.5.1.1.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.5.1.1.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">A. Standing still  B. Chasing after something  C. Lying down  D. Jumping in the air</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.5.1.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.5.1.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.5.1.1.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/seed_vis_1_1822_img.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.5.1.1.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.5.1.1.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.5.1.1.1.1.4.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.5.1.1.1.1.4.1.1.1">: B</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.5.1.1.1.1.4.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.5.1.1.1.1.4.1.2.1">: B</span></span>  <span class="ltx_text" id="A1.SS6.5.1.1.1.1.4.1.3" style="color:#FF0000;">Ours (d1): A</span>  <span class="ltx_text" id="A1.SS6.5.1.1.1.1.4.1.4" style="color:#6394ED;">GT: A</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.5.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.5.1.1.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.5.1.1.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.5.1.1.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.5.1.1.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.5.1.1.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.082</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.6.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.6.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.6.2.2.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.6.2.2.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.6.2.2.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">How many people are on the field in this image?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.6.2.2.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.6.2.2.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">A. Four  B. Nine  C. Twelve  D. Eleven</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.6.2.2.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.6.2.2.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.6.2.2.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/seed_vis_1_4535_img.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.6.2.2.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.6.2.2.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.6.2.2.1.1.4.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.6.2.2.1.1.4.1.1.1">: D</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.6.2.2.1.1.4.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.6.2.2.1.1.4.1.2.1">: B</span></span>  <span class="ltx_text" id="A1.SS6.6.2.2.1.1.4.1.3" style="color:#FF0000;">Ours (d1): A</span>  <span class="ltx_text" id="A1.SS6.6.2.2.1.1.4.1.4" style="color:#6394ED;">GT: A</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.6.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.6.2.2.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.6.2.2.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.6.2.2.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.6.2.2.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.6.2.2.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.097</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.7.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.7.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.7.3.3.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.7.3.3.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.7.3.3.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">What is the person in the blue jacket holding?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.7.3.3.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.7.3.3.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">A. A phone  B. Nothing  C. A wallet  D. A clipboard</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.7.3.3.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.7.3.3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.7.3.3.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/seed_vis_2_4821_img.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.7.3.3.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.7.3.3.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.7.3.3.1.1.4.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.7.3.3.1.1.4.1.1.1">: D</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.7.3.3.1.1.4.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.7.3.3.1.1.4.1.2.1">: D</span></span>  <span class="ltx_text" id="A1.SS6.7.3.3.1.1.4.1.3" style="color:#FF0000;">Ours (d1): B</span>  <span class="ltx_text" id="A1.SS6.7.3.3.1.1.4.1.4" style="color:#6394ED;">GT: B</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.7.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.7.3.3.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.7.3.3.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.7.3.3.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.7.3.3.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.7.3.3.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.160</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.8.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.8.4.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.8.4.4.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.8.4.4.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.4.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">How many people are lighting candles in this image?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.8.4.4.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.4.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">A. Two B. One C. Three D. Four</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.8.4.4.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.4.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="145" id="A1.SS6.8.4.4.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/seed_vis_2_2481_img.png" width="439"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.8.4.4.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.4.1.1.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.8.4.4.1.1.4.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.8.4.4.1.1.4.1.1.1">: A</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.8.4.4.1.1.4.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.8.4.4.1.1.4.1.2.1">: A</span></span>  <span class="ltx_text" id="A1.SS6.8.4.4.1.1.4.1.3" style="color:#FF0000;">Ours (d1): C</span>  <span class="ltx_text" id="A1.SS6.8.4.4.1.1.4.1.4" style="color:#6394ED;">GT: C</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.8.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.8.4.4.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.8.4.4.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.8.4.4.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.8.4.4.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.8.4.4.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.066</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A1.F13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Visualization examples of our proposed method in (d1), <span class="ltx_text ltx_font_italic" id="A1.F13.3.1">Reconstruction</span>, and <span class="ltx_text ltx_font_italic" id="A1.F13.4.2">Post-processing</span> on VQA with Honeybee.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS6.12">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="A1.SS6.12.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS6.12.4.5.1">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.SS6.12.4.5.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.12.4.5.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.12.4.5.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.5.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">Task: Referring expression comprehension</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.12.4.5.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.5.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Model: Shikra</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS6.9.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.9.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.9.1.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.9.1.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.9.1.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Guide me to the location of brown bear within the image &lt;img&gt; by providing its coordinates.</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.9.1.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.9.1.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="204" id="A1.SS6.9.1.1.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/shikra_vis_1_6137_img.png" width="466"></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.9.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.9.1.1.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.9.1.1.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.9.1.1.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.9.1.1.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.9.1.1.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.040</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.10.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.10.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.10.2.2.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.10.2.2.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.10.2.2.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Point me to the location of wine glass far left in the picture &lt;img&gt; by providing its coordinates.</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.10.2.2.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.10.2.2.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="124" id="A1.SS6.10.2.2.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/shikra_vis_2_7238_img.png" width="466"></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.10.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.10.2.2.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.10.2.2.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.10.2.2.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.10.2.2.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.10.2.2.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.115</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.11.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.11.3.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.11.3.3.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.11.3.3.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.11.3.3.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Can you assist me in locating right female cop in &lt;img&gt;, and then provide its coordinates?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.11.3.3.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.11.3.3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="124" id="A1.SS6.11.3.3.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/shikra_vis_1_8573_img.png" width="466"></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.11.3.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.11.3.3.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.11.3.3.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.11.3.3.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.11.3.3.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.11.3.3.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.098</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.12.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.12.4.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.12.4.4.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.12.4.4.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.4.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">In the photograph &lt;img&gt;, could you pinpoint the location of</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.12.4.4.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.4.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">person holding a snowboard and tell me its coordinates?</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.12.4.4.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.4.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="112" id="A1.SS6.12.4.4.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/shikra_vis_1_3960_img.png" width="466"></td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.12.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.12.4.4.2.1">
<tbody><tr class="ltx_tr" id="A1.SS6.12.4.4.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.12.4.4.2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.12.4.4.2.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.12.4.4.2.1.1.1.1.1" style="width:113.8pt;">BPP:0.088</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="A1.F14">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Visualization examples of our proposed method in (d1), <span class="ltx_text ltx_font_italic" id="A1.F14.3.1">Reconstruction</span>, and <span class="ltx_text ltx_font_italic" id="A1.F14.4.2">Post-processing</span> on REC with Shikra.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS6.18">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="A1.SS6.18.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS6.18.6.7.1">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.SS6.18.6.7.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.18.6.7.1.1.1">
<tbody><tr class="ltx_tr" id="A1.SS6.18.6.7.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.7.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">Task: Few-shot classification</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.18.6.7.1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.7.1.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Model: V2L-tokenizer</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS6.14.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.14.2.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.14.2.2.3.1">
<tbody><tr class="ltx_tr" id="A1.SS6.14.2.2.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.14.2.2.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.14.2.2.3.1.1.1.1" style="width:16.3pt;height:142.2pt;vertical-align:-67.6pt;"><span class="ltx_transformed_inner" style="width:142.3pt;transform:translate(-62.98pt,6.98pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.14.2.2.3.1.1.1.1.1" style="width:142.3pt;">Query                Examples</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.14.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.14.2.2.2.2">
<tbody><tr class="ltx_tr" id="A1.SS6.13.1.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.13.1.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="101" id="A1.SS6.13.1.1.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0011_examples.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.14.2.2.2.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.14.2.2.2.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.14.2.2.2.2.2.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0011_query.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.14.2.2.2.2.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.14.2.2.2.2.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.14.2.2.2.2.3.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.14.2.2.2.2.3.1.1.1">: ptarmigan</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.14.2.2.2.2.3.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.14.2.2.2.2.3.1.2.1">: ptarmigan</span></span>  <span class="ltx_text" id="A1.SS6.14.2.2.2.2.3.1.3" style="color:#FF0000;">Ours (d1): walking stick</span>  <span class="ltx_text" id="A1.SS6.14.2.2.2.2.3.1.4" style="color:#6394ED;">GT: walking stick</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.14.2.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.14.2.2.4.1">
<tbody><tr class="ltx_tr" id="A1.SS6.14.2.2.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.14.2.2.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.14.2.2.4.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.14.2.2.4.1.1.1.1.1" style="width:113.8pt;">BPP: 0.0786</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.16.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.16.4.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.16.4.4.3.1">
<tbody><tr class="ltx_tr" id="A1.SS6.16.4.4.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.16.4.4.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.16.4.4.3.1.1.1.1" style="width:16.3pt;height:142.2pt;vertical-align:-67.6pt;"><span class="ltx_transformed_inner" style="width:142.3pt;transform:translate(-62.98pt,6.98pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.16.4.4.3.1.1.1.1.1" style="width:142.3pt;">Query                Examples</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.16.4.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.16.4.4.2.2">
<tbody><tr class="ltx_tr" id="A1.SS6.15.3.3.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.15.3.3.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="101" id="A1.SS6.15.3.3.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0039_examples.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.16.4.4.2.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.16.4.4.2.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.16.4.4.2.2.2.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0039_query.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.16.4.4.2.2.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.16.4.4.2.2.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.16.4.4.2.2.3.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.16.4.4.2.2.3.1.1.1">: hippopotamus</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.16.4.4.2.2.3.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.16.4.4.2.2.3.1.2.1">: wall clock</span></span>  <span class="ltx_text" id="A1.SS6.16.4.4.2.2.3.1.3" style="color:#FF0000;">Ours (d1): otterhound</span>  <span class="ltx_text" id="A1.SS6.16.4.4.2.2.3.1.4" style="color:#6394ED;">GT: otterhound</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.SS6.16.4.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.16.4.4.4.1">
<tbody><tr class="ltx_tr" id="A1.SS6.16.4.4.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.16.4.4.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.16.4.4.4.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.16.4.4.4.1.1.1.1.1" style="width:113.8pt;">BPP: 0.1369</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="A1.SS6.18.6.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.18.6.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.18.6.6.3.1">
<tbody><tr class="ltx_tr" id="A1.SS6.18.6.6.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.6.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.18.6.6.3.1.1.1.1" style="width:16.3pt;height:142.2pt;vertical-align:-67.6pt;"><span class="ltx_transformed_inner" style="width:142.3pt;transform:translate(-62.98pt,6.98pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.18.6.6.3.1.1.1.1.1" style="width:142.3pt;">Query                Examples</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.18.6.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.18.6.6.2.2">
<tbody><tr class="ltx_tr" id="A1.SS6.17.5.5.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.17.5.5.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="101" id="A1.SS6.17.5.5.1.1.1.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0024_examples.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.18.6.6.2.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.6.2.2.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="154" id="A1.SS6.18.6.6.2.2.2.1.g1" src="./Bridging Compressed Image Latents and Multimodal Large Language Models_files/v2l_0024_query.png" width="466"></td>
</tr>
<tr class="ltx_tr" id="A1.SS6.18.6.6.2.2.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.6.2.2.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text ltx_font_italic" id="A1.SS6.18.6.6.2.2.3.1.1" style="color:#000000;">Reconstruction<span class="ltx_text ltx_font_upright" id="A1.SS6.18.6.6.2.2.3.1.1.1">: horned viper</span></span>  <span class="ltx_text ltx_font_italic" id="A1.SS6.18.6.6.2.2.3.1.2" style="color:#00FF00;">Post-processing<span class="ltx_text ltx_font_upright" id="A1.SS6.18.6.6.2.2.3.1.2.1">: horned viper</span></span>  <span class="ltx_text" id="A1.SS6.18.6.6.2.2.3.1.3" style="color:#FF0000;">Ours (d1): hornbill</span>  <span class="ltx_text" id="A1.SS6.18.6.6.2.2.3.1.4" style="color:#6394ED;">GT: hornbill</span>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.SS6.18.6.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.SS6.18.6.6.4.1">
<tbody><tr class="ltx_tr" id="A1.SS6.18.6.6.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.SS6.18.6.6.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS6.18.6.6.4.1.1.1.1" style="width:6.8pt;height:113.8pt;vertical-align:-53.4pt;"><span class="ltx_transformed_inner" style="width:113.8pt;transform:translate(-53.49pt,-0.13pt) rotate(-90deg) ;">
<p class="ltx_p ltx_parbox ltx_align_middle" id="A1.SS6.18.6.6.4.1.1.1.1.1" style="width:113.8pt;">BPP: 0.2329</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="A1.F15">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Visualization examples of our proposed method in (d1), <span class="ltx_text ltx_font_italic" id="A1.F15.3.1">Reconstruction</span>, and <span class="ltx_text ltx_font_italic" id="A1.F15.4.2">Post-processing</span> on few-shot classification with V2L-tokenizer.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 4);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 3px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #F5F7F9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #F6F6F6;
  --float-ball-more-button-background-color: #FFFFFF;
  --float-ball-more-button-svg-color: #6C6F73;
  color-scheme: light;
  --service-bg-hover:#F7FAFF;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --float-ball-more-button-background-color: #191919;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
    color-scheme: dark;
    --service-bg-hover:#22292F;
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --float-ball-more-button-background-color: #191919;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  color-scheme: dark;
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 1130px;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}

@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 992px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);

}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --popup-trial-pro-background-color: #222222;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
    --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
    --service-select-border-color: #2c2c2c;
    --service-select-selected-background-color: #333333;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.translate-service {
  color: var(--text-black-2);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

.widget-item {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 59px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-container {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 30px;
  width: 100%;
  margin-bottom: 4px;
}

.widget-title-container {
  display: flex;
  align-items: flex-start;
  justify-content: center;
  height: 24px;
  width: 100%;
  padding-bottom: 4px;
}

.widget-icon {
  margin-bottom: 4px;
  display: flex;
  justify-content: center;
}

.widget-title {
  color: var(--text-gray-6);
  font-size: 12px;
  text-align: center;
  width: 100%;
  font-weight: 400;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding: 0 2px 2px;
}

.widget-item svg {
  fill: var(--text-gray-2);
}

.setting svg {
  fill: var(--text-gray-6);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: 0;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: 100%;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
}
.more-container {
  position: relative;
}
.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  z-index: 2147483647;
  top: 335px;
  width: 56px;
  display: flex;
  flex-direction: column;
  display: none;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ED6D8F;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-50%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 16px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-right: 8px;
}

.imt-fb-more-button {
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}

/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 12px 0 0 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
}

.imt-manga-feedback {
  cursor: pointer;
  margin: 9px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 372px;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 16px auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " dir="ltr" style="z-index: 2147483647; pointer-events: none; top: 911px; display: flex;"><div title="关闭悬浮球" class="btn-animate" style="transform: translateX(100%); padding: 4px; cursor: pointer;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-btn  right btn-animate " dir="ltr" style="transform: translateX(15px); opacity: 0.7;"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg hidden="true" class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg></div></div></div></div><div hidden="" class="imt-manga-button imt-no-events btn-animate " id="manga-button" style="transform: translateX(8px);"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg><svg hidden="true" class="imt-manga-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg><svg class="imt-float-ball-loading" hidden="true" width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin: 9px;"><path d="M9.42859 0C9.84288 0 10.1929 0.387143 10.1929 0.847143V3.99429C10.1929 4.45429 9.84431 4.84143 9.42859 4.84143C9.01431 4.84143 8.66431 4.45571 8.66431 3.99429V0.847143C8.66431 0.387143 9.01288 0 9.42859 0Z" fill="#E9E9E9"></path><path d="M14.1301 1.38877C14.5158 1.62591 14.6301 2.12163 14.4258 2.52305L12.9515 5.19448C12.901 5.28714 12.8325 5.36876 12.75 5.43455C12.6675 5.50035 12.5727 5.54898 12.4712 5.5776C12.3696 5.60621 12.2634 5.61424 12.1586 5.60119C12.0539 5.58814 11.9529 5.55429 11.8615 5.50163C11.6787 5.38432 11.5468 5.20237 11.4923 4.9921C11.4377 4.78184 11.4645 4.55874 11.5672 4.36734L13.0415 1.69591C13.2686 1.29448 13.7443 1.15305 14.1301 1.38877Z" fill="#989697"></path><path d="M17.4685 4.75707C17.5813 4.95451 17.6123 5.18824 17.5549 5.40825C17.4975 5.62826 17.3563 5.81705 17.1614 5.93422L14.4971 7.52564C14.0971 7.76993 13.6014 7.62422 13.3657 7.20707C13.2532 7.00994 13.2222 6.77667 13.2793 6.55702C13.3365 6.33737 13.4771 6.14874 13.6714 6.03136L16.3357 4.43993C16.7371 4.21993 17.2557 4.34136 17.4685 4.7585V4.75707Z" fill="#9B999A"></path><path d="M18.8572 9.42835C18.8572 9.84263 18.47 10.1926 18.01 10.1926H14.8629C14.4029 10.1926 14.0157 9.84406 14.0157 9.42835C14.0157 9.01406 14.4029 8.66406 14.8629 8.66406H18.01C18.47 8.66406 18.8572 9.01263 18.8572 9.42835Z" fill="#A3A1A2"></path><path d="M17.4686 14.1303C17.3515 14.3134 17.1697 14.4455 16.9594 14.5003C16.7491 14.5552 16.5259 14.5286 16.3343 14.426L13.6629 12.9517C13.5702 12.9012 13.4886 12.8327 13.4228 12.7503C13.357 12.6678 13.3084 12.573 13.2798 12.4714C13.2512 12.3698 13.2431 12.2636 13.2562 12.1589C13.2692 12.0542 13.3031 11.9532 13.3558 11.8617C13.4731 11.6789 13.655 11.547 13.8653 11.4925C14.0755 11.4379 14.2986 11.4647 14.49 11.5674L17.1615 13.0417C17.5629 13.2689 17.7043 13.7446 17.4686 14.1303Z" fill="#ABA9AA"></path><path opacity="0.7" d="M14.1 17.4686C13.9026 17.5814 13.6689 17.6124 13.4489 17.555C13.2288 17.4976 13.04 17.3564 12.9229 17.1615L11.3315 14.4972C11.0872 14.0972 11.2329 13.6015 11.65 13.3658C11.8472 13.2533 12.0804 13.2224 12.3001 13.2795C12.5197 13.3366 12.7084 13.4773 12.8257 13.6715L14.4172 16.3358C14.6372 16.7372 14.5157 17.2558 14.0986 17.4686H14.1Z" fill="#B2B2B2"></path><path opacity="0.6" d="M9.42859 18.8571C9.01431 18.8571 8.66431 18.4699 8.66431 18.0099V14.8628C8.66431 14.4028 9.01288 14.0156 9.42859 14.0156C9.84288 14.0156 10.1929 14.4028 10.1929 14.8628V18.0099C10.1929 18.4699 9.84431 18.8571 9.42859 18.8571Z" fill="#BAB8B9"></path><path opacity="0.5" d="M4.72717 17.4685C4.5441 17.3514 4.41195 17.1696 4.35713 16.9593C4.30231 16.749 4.32885 16.5258 4.43145 16.3342L5.90574 13.6628C5.95622 13.5701 6.02472 13.4885 6.1072 13.4227C6.18969 13.3569 6.2845 13.3083 6.38606 13.2797C6.48762 13.251 6.59387 13.243 6.69857 13.2561C6.80327 13.2691 6.90431 13.303 6.99574 13.3556C7.38145 13.5914 7.49431 14.0885 7.29002 14.4899L5.81574 17.1614C5.5886 17.5628 5.11288 17.7042 4.72717 17.4685Z" fill="#C2C0C1"></path><path opacity="0.4" d="M1.38862 14.1002C1.27584 13.9027 1.24483 13.669 1.30223 13.449C1.35964 13.229 1.50089 13.0402 1.69576 12.923L4.36004 11.3316C4.76004 11.0873 5.25576 11.233 5.49147 11.6502C5.60393 11.8473 5.63491 12.0806 5.5778 12.3002C5.52069 12.5199 5.38 12.7085 5.18576 12.8259L2.52004 14.4173C2.12004 14.6373 1.60004 14.5159 1.38862 14.0987V14.1002Z" fill="#CBCBCB"></path><path d="M0 9.42835C0 9.01406 0.387143 8.66406 0.847143 8.66406H3.99429C4.45429 8.66406 4.84143 9.01263 4.84143 9.42835C4.84143 9.84263 4.45571 10.1926 3.99429 10.1926H0.847143C0.387143 10.1926 0 9.84406 0 9.42835Z" fill="#D2D2D2"></path><path opacity="0.2" d="M1.38852 4.72705C1.50561 4.54398 1.68746 4.41183 1.89774 4.35701C2.10803 4.30219 2.33125 4.32873 2.52281 4.43133L5.19424 5.90562C5.28689 5.9561 5.36851 6.0246 5.43431 6.10708C5.5001 6.18957 5.54874 6.28438 5.57735 6.38594C5.60597 6.48749 5.61399 6.59375 5.60094 6.69845C5.5879 6.80315 5.55405 6.90419 5.50138 6.99562C5.38407 7.17844 5.20212 7.31029 4.99186 7.36484C4.78159 7.4194 4.55849 7.39263 4.3671 7.2899L1.69567 5.81562C1.29424 5.58847 1.15281 5.11276 1.38852 4.72705Z" fill="#DADADA"></path><path d="M4.75719 1.38849C4.95463 1.27571 5.18837 1.24471 5.40838 1.30211C5.62838 1.35952 5.81718 1.50077 5.93434 1.69564L7.52577 4.35992C7.77005 4.75992 7.62434 5.25564 7.20719 5.49135C7.01006 5.60381 6.77679 5.63479 6.55714 5.57768C6.33749 5.52056 6.14886 5.37988 6.03148 5.18564L4.44005 2.51992C4.22005 2.11992 4.34148 1.59992 4.75862 1.38849H4.75719Z" fill="#E2E2E2"></path></svg><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.6861 1L15.2353 4.54635V7.11765V14.6471V15.5882C15.2353 15.9627 15.0866 16.3217 14.8218 16.5865C14.557 16.8513 14.198 17 13.8235 17H4.41176C4.03734 17 3.67825 16.8513 3.4135 16.5865C3.14874 16.3217 3 15.9627 3 15.5882V14.6471V7.11765V2.41176C3 2.03734 3.14874 1.67825 3.4135 1.4135C3.67825 1.14874 4.03734 1 4.41176 1H11.6861ZM11.8692 3.17882V4.74212H13.4334L11.8692 3.17882ZM4.41171 15.5882V14.647V2.41176H10.4574L10.4578 6.15341H13.8235V14.647V15.5882H4.41171ZM12.7739 7.51746H5.46094V8.6155H12.7739V7.51746ZM5.46094 9.98805H12.7739V11.0861H5.46094V9.98805ZM9.5127 12.36H5.46094V13.458H9.5127V12.36Z" fill="#666666"></path></svg></div></div></div><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.55741 1L10.0685 1.00329C10.8482 1.00471 11.4802 1.63624 11.4812 2.41647L11.4821 2.82588C11.9687 3.0278 12.4297 3.28671 12.8553 3.59718L13.1913 3.40329C13.516 3.21676 13.9013 3.1665 14.2629 3.26352C14.6246 3.36055 14.933 3.59695 15.1207 3.92094L16.3795 6.09365C16.5601 6.40546 16.6149 6.7744 16.5328 7.12523C16.4507 7.47606 16.2378 7.78235 15.9376 7.98165L15.8609 8.02871L15.5235 8.22353C15.5819 8.76273 15.5736 9.30708 15.4986 9.84424L15.7372 9.98259C16.0496 10.1631 16.2812 10.4561 16.3848 10.8017C16.4884 11.1472 16.456 11.5193 16.2944 11.8419L16.2553 11.9153L15.076 13.9576C14.8955 14.27 14.6025 14.5017 14.2569 14.6053C13.9113 14.7088 13.5392 14.6765 13.2167 14.5148L13.1433 14.4753L12.8172 14.2871C12.4074 14.5817 11.9651 14.8283 11.4991 15.0221L11.4995 15.5831C11.5 15.9434 11.3629 16.2904 11.1163 16.5532C10.8697 16.816 10.5321 16.9748 10.1725 16.9972L10.0831 17L7.57153 16.9967C7.19697 16.9961 6.83793 16.847 6.57312 16.5821C6.30831 16.3172 6.15932 15.9581 6.15883 15.5835L6.15788 14.9073C5.76852 14.7244 5.39771 14.5044 5.05059 14.2504L4.44918 14.5967C4.12448 14.7834 3.73902 14.8337 3.37726 14.7367C3.01551 14.6397 2.70698 14.4032 2.5193 14.0791L1.26047 11.9064C1.07996 11.5945 1.02522 11.2255 1.10742 10.8747C1.18962 10.5238 1.40257 10.2176 1.70283 10.0184L1.77906 9.97129L2.3913 9.61835C2.34424 9.17129 2.34188 8.71765 2.38706 8.26494L1.70753 7.87247C1.39506 7.69207 1.16331 7.39911 1.05965 7.05351C0.955998 6.70791 0.988275 6.33577 1.14989 6.01318L1.18941 5.93976L2.36871 3.89741C2.54919 3.58502 2.84218 3.35337 3.18777 3.2498C3.53336 3.14624 3.90547 3.17859 4.228 3.34023L4.30141 3.37976L4.89436 3.72188C5.28027 3.42082 5.69854 3.1637 6.14141 2.95529L6.14047 2.41694C6.14001 2.05657 6.27707 1.7096 6.52367 1.44681C6.77028 1.18403 7.10786 1.02523 7.46753 1.00282L7.55741 1ZM7.55553 2.41506L7.55694 3.85271L6.74377 4.23576C6.39553 4.39906 6.06706 4.60094 5.764 4.83718L5.01247 5.424L3.62941 4.62494L3.59365 4.60518L2.41483 6.64753L3.88636 7.49694L3.79506 8.40612C3.75968 8.7598 3.76078 9.11619 3.79836 9.46965L3.8953 10.3854L2.48494 11.1976L3.7433 13.3704L5.14377 12.5647L5.88636 13.1087C6.15997 13.309 6.45231 13.4823 6.7593 13.6264L7.57106 14.008L7.57388 15.5816L10.0845 15.5849L10.0831 14.0791L10.9555 13.7158C11.3216 13.5635 11.6689 13.3698 11.9908 13.1384L12.7329 12.6047L13.8506 13.2499L15.0289 11.2075L13.9654 10.592L14.0972 9.64847C14.1561 9.22659 14.1628 8.79904 14.1169 8.37553L14.0181 7.45882L15.1555 6.80235L13.8967 4.62965L12.7645 5.28235L12.0214 4.74024C11.686 4.4956 11.3229 4.29152 10.9395 4.13224L10.0689 3.77082L10.0666 2.41835L7.55553 2.41506ZM10.3715 6.47624C11.0214 6.85201 11.4955 7.47036 11.6898 8.19547C11.8841 8.92058 11.7827 9.69316 11.4078 10.3435C11.2223 10.6654 10.9752 10.9476 10.6805 11.1739C10.3859 11.4002 10.0495 11.5662 9.69068 11.6623C9.33183 11.7585 8.95754 11.7829 8.58923 11.7343C8.22092 11.6856 7.86582 11.5648 7.54424 11.3788C6.89445 11.003 6.4204 10.3846 6.2262 9.65948C6.032 8.93438 6.13352 8.16184 6.50847 7.51153C6.69395 7.18963 6.94107 6.90746 7.23571 6.68117C7.53034 6.45488 7.86671 6.28891 8.22556 6.19275C8.58441 6.09659 8.9587 6.07213 9.32701 6.12077C9.69532 6.16942 10.0504 6.29021 10.372 6.47624H10.3715ZM7.73388 8.21835C7.54638 8.54388 7.49567 8.9305 7.5929 9.29336C7.69012 9.65623 7.92733 9.96571 8.25247 10.1539C8.41305 10.2468 8.59037 10.3071 8.77429 10.3314C8.9582 10.3557 9.14511 10.3435 9.32431 10.2956C9.50351 10.2476 9.67149 10.1647 9.81864 10.0517C9.96579 9.93877 10.0892 9.7979 10.1819 9.63718C10.5588 8.98353 10.356 8.15435 9.73435 7.74494L9.66377 7.70118L9.59035 7.66165C9.26834 7.49988 8.89663 7.46742 8.55145 7.57093C8.20626 7.67444 7.91375 7.90608 7.73388 8.21835Z" fill="#666666"></path></svg></div></div></div></div><div hidden="" class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div></div></div></template></div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>