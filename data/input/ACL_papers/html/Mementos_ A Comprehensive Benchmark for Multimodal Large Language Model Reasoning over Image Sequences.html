<!DOCTYPE html>
<!-- saved from url=(0035)https://arxiv.org/html/2401.10529v2 -->
<html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences</title>
<!--Generated on Thu Jan 25 04:08:04 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/bootstrap.bundle.min.js.download"></script>
<script src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/html2canvas.min.js.download"></script>
<script src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/addons.js.download"></script>
<script src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/feedbackOverlay.js.download"></script>
<!--<base href="/html/2401.10529v2/">--><base href="."><link rel="stylesheet" href="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-toast {
  display: flex;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  right: 0;
  top: 1%;
  width: fit-content;
  padding: 12px 20px;
  margin: auto;
  overflow: auto;
  background: #fef6f9;
  box-shadow: 0px 4px 10px 0px rgba(0, 10, 30, 0.06);
  font-size: 15px;
  border-radius: 8px;
  color: #333;
}

.immersive-translate-toast-content {
  display: flex;
  flex-direction: row;
  align-items: center;
}

.immersive-translate-toast-hidden {
  margin: 0 20px 0 72px;
  text-decoration: underline;
  cursor: pointer;
}

.immersive-translate-toast-close {
  color: #666666;
  font-size: 20px;
  font-weight: bold;
  padding: 0 10px;
  cursor: pointer;
}

@media screen and (max-width: 768px) {
  .immersive-translate-toast {
    top: 0;
    padding: 12px 0px 0 10px;
  }
  .immersive-translate-toast-content {
    flex-direction: column;
    text-align: center;
  }
  .immersive-translate-toast-hidden {
    margin: 10px auto;
  }
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border: 1px solid #888;
  border-radius: 10px;
  width: 80%;
  max-width: 270px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 50% auto !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  word-break: break-all;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  color: black;
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 16px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #007bff;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}
</style><style type="text/css">.lf-progress {
  -webkit-appearance: none;
  -moz-apperance: none;
  width: 100%;
  /* margin: 0 10px; */
  height: 4px;
  border-radius: 3px;
  cursor: pointer;
}
.lf-progress:focus {
  outline: none;
  border: none;
}
.lf-progress::-moz-range-track {
  cursor: pointer;
  background: none;
  border: none;
  outline: none;
}
.lf-progress::-webkit-slider-thumb {
  -webkit-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-moz-range-thumb {
  -moz-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: transparent;
  border-color: transparent;
  color: transparent;
}
.lf-progress::-ms-fill-lower {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-fill-upper {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-thumb {
  border: 0;
  height: 15px;
  width: 15px;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress:focus::-ms-fill-lower {
  background: #ccc;
}
.lf-progress:focus::-ms-fill-upper {
  background: #ccc;
}
.lf-player-container :focus {
  outline: 0;
}
.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  opacity: 1;
  visibility: visible;
  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-player-btn-container {
  display: flex;
  align-items: center;
}
.lf-player-btn {
  cursor: pointer;
  fill: #999;
  width: 14px;
}

.lf-player-btn.active {
  fill: #555;
}

.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  background-color: #ffffff;
  opacity: 1;

  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
  padding: 10px;
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-arrow {
  position: absolute;
  z-index: -1;
  content: '';
  bottom: -9px;
  border-style: solid;
  border-width: 10px 10px 0px 10px;
}

.lf-left-align,
.lf-left-align .lfarrow {
  left: 0;
  right: unset;
}

.lf-right-align,
.lf-right-align .lf-arrow {
  right: 0;
  left: unset;
}

.lf-text-input {
  border: 1px #ccc solid;
  border-radius: 5px;
  padding: 3px;
  width: 60px;
  margin: 0;
}

.lf-color-picker {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  height: 90px;
}

.lf-color-selectors {
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}

.lf-color-component {
  display: flex;
  flex-direction: row;
  font-size: 12px;
  align-items: center;
  justify-content: center;
}

.lf-color-component strong {
  width: 40px;
}

.lf-color-component input[type='range'] {
  margin: 0 0 0 10px;
}

.lf-color-component input[type='number'] {
  width: 50px;
  margin: 0 0 0 10px;
}

.lf-color-preview {
  font-size: 12px;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding-left: 5px;
}

.lf-preview {
  height: 60px;
  width: 60px;
}

.lf-popover-snapshot {
  width: 150px;
}
.lf-popover-snapshot h5 {
  margin: 5px 0 10px 0;
  font-size: 0.75rem;
}
.lf-popover-snapshot a {
  display: block;
  text-decoration: none;
}
.lf-popover-snapshot a:before {
  content: '⥼';
  margin-right: 5px;
}
.lf-popover-snapshot .lf-note {
  display: block;
  margin-top: 10px;
  color: #999;
}
.lf-player-controls > div {
  margin-right: 5px;
  margin-left: 5px;
}
.lf-player-controls > div:first-child {
  margin-left: 0px;
}
.lf-player-controls > div:last-child {
  margin-right: 0px;
}
</style><style type="text/css">.lf-progress {
  -webkit-appearance: none;
  -moz-apperance: none;
  width: 100%;
  /* margin: 0 10px; */
  height: 4px;
  border-radius: 3px;
  cursor: pointer;
}
.lf-progress:focus {
  outline: none;
  border: none;
}
.lf-progress::-moz-range-track {
  cursor: pointer;
  background: none;
  border: none;
  outline: none;
}
.lf-progress::-webkit-slider-thumb {
  -webkit-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-moz-range-thumb {
  -moz-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: transparent;
  border-color: transparent;
  color: transparent;
}
.lf-progress::-ms-fill-lower {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-fill-upper {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-thumb {
  border: 0;
  height: 15px;
  width: 15px;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress:focus::-ms-fill-lower {
  background: #ccc;
}
.lf-progress:focus::-ms-fill-upper {
  background: #ccc;
}
.lf-player-container :focus {
  outline: 0;
}
.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  opacity: 1;
  visibility: visible;
  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-player-btn-container {
  display: flex;
  align-items: center;
}
.lf-player-btn {
  cursor: pointer;
  fill: #999;
  width: 14px;
}

.lf-player-btn.active {
  fill: #555;
}

.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  background-color: #ffffff;
  opacity: 1;

  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
  padding: 10px;
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-arrow {
  position: absolute;
  z-index: -1;
  content: '';
  bottom: -9px;
  border-style: solid;
  border-width: 10px 10px 0px 10px;
}

.lf-left-align,
.lf-left-align .lfarrow {
  left: 0;
  right: unset;
}

.lf-right-align,
.lf-right-align .lf-arrow {
  right: 0;
  left: unset;
}

.lf-text-input {
  border: 1px #ccc solid;
  border-radius: 5px;
  padding: 3px;
  width: 60px;
  margin: 0;
}

.lf-color-picker {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  height: 90px;
}

.lf-color-selectors {
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}

.lf-color-component {
  display: flex;
  flex-direction: row;
  font-size: 12px;
  align-items: center;
  justify-content: center;
}

.lf-color-component strong {
  width: 40px;
}

.lf-color-component input[type='range'] {
  margin: 0 0 0 10px;
}

.lf-color-component input[type='number'] {
  width: 50px;
  margin: 0 0 0 10px;
}

.lf-color-preview {
  font-size: 12px;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding-left: 5px;
}

.lf-preview {
  height: 60px;
  width: 60px;
}

.lf-popover-snapshot {
  width: 150px;
}
.lf-popover-snapshot h5 {
  margin: 5px 0 10px 0;
  font-size: 0.75rem;
}
.lf-popover-snapshot a {
  display: block;
  text-decoration: none;
}
.lf-popover-snapshot a:before {
  content: '⥼';
  margin-right: 5px;
}
.lf-popover-snapshot .lf-note {
  display: block;
  margin-top: 10px;
  color: #999;
}
.lf-player-controls > div {
  margin-right: 5px;
  margin-left: 5px;
}
.lf-player-controls > div:first-child {
  margin-left: 0px;
}
.lf-player-controls > div:last-child {
  margin-right: 0px;
}
</style></head>
<body data-new-gr-c-s-check-loaded="14.1104.0" data-gr-ext-installed="" data-new-gr-c-s-loaded="14.1104.0"><header class="mob_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
          <img alt="logo" class="logomark" role="presentation" width="100" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/arxiv-logomark-small-white.svg">
          <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
  
      <!--TOC, dark mode, links-->
      <div class="html-header-nav">
        <!--back to abstract-->
        
          <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.10529v2">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
              <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
          </svg>
          </a>
        <!--dark mode-->
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
          <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
          </label>
          <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
          <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
        </a>
        <!--nav-->
        <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
        </button>
      </div>
      </header><header class="desktop_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
            <img alt="logo" class="logo" role="presentation" width="100" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/arxiv-logo-one-color-white.svg">
            <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
      <div class="html-header-message" role="banner">
          <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
          </p>
      </div>
      <nav class="html-header-nav">
        <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
        <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2401.10529v2/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.10529v2">Back to Abstract</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.10529v2" target="_blank">Download PDF</a>
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

        <div id="listIcon" type="button" class="hide">
            <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
            <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
            </svg>
        </div>
        <div id="arrowIcon" type="button">
            <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
            <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
            </svg>
        </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S1" title="1 Introduction ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2" title="2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Mementos</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1" title="2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Mementos Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS1" title="2.1.1 Dataset Composition ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Dataset Composition</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS1.Px1" title="Daily-life ‣ 2.1.1 Dataset Composition ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Daily-life</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS1.Px2" title="Robotics ‣ 2.1.1 Dataset Composition ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Robotics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS1.Px3" title="Comics ‣ 2.1.1 Dataset Composition ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Comics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS2" title="2.1.2 Dataset Annotation ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Dataset Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS3" title="2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Dataset Statistics</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS3.Px1" title="Image sequence length ‣ 2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Image sequence length</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS1.SSS3.Px2" title="Episode length ‣ 2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Episode length</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS2" title="2.2 Evaluation Procedure and Metrics ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Evaluation Procedure and Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS2.SSS0.Px1" title="Procedure ‣ 2.2 Evaluation Procedure and Metrics ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS2.SSS0.Px2" title="Synonym graph ‣ 2.2 Evaluation Procedure and Metrics ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Synonym graph</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.SS2.SSS0.Px3" title="Metrics ‣ 2.2 Evaluation Procedure and Metrics ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3" title="3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1" title="3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Baseline evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1.SSS1" title="3.1.1 Models ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1.SSS2" title="3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Evaluation results</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1.SSS2.Px1" title="GPT-4V (s-input) and LLaVA-1.5 are the best-performing models among black-box and open-source MLLMs, respectively. ‣ 3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">GPT-4V (s-input) and LLaVA-1.5 are the best-performing models among black-box and open-source MLLMs, respectively.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1.SSS2.Px2" title="MLLMs possess a much stronger ability on reasoning objects in image sequences than they do on reasoning behaviors. ‣ 3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">MLLMs possess a much stronger ability on reasoning objects in image sequences than they do on reasoning behaviors.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS1.SSS2.Px3" title="Reasoning capability of MLLMs varies across different domains. ‣ 3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Reasoning capability of MLLMs varies across different domains.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS2" title="3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Analysis of Failure Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS2.SSS0.Px1" title="Interplay between object and behavioral hallucinations in MLLMs. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">Interplay between object and behavioral hallucinations in MLLMs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS2.SSS0.Px2" title="The impact of co-occurrence on behavioral hallucination. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">The impact of co-occurrence on behavioral hallucination.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS2.SSS0.Px3" title="The Snowball effect in behavioral hallucinations. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title">The Snowball effect in behavioral hallucinations.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S4" title="4 Related work ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S4.SS1" title="4.1 Benchmarking in MLLMs ‣ 4 Related work ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Benchmarking in MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S4.SS2" title="4.2 Hallucination in MLLMs ‣ 4 Related work ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Hallucination in MLLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S5" title="5 Conclusion and Future Works ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A1" title="Appendix A Details of Open X-Embodiment Data Selection ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of Open X-Embodiment Data Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A2" title="Appendix B Human Evaluation ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A3" title="Appendix C Prompt Details ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Prompt Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4" title="Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Case Study</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2401.10529v2 [cs.CV] 25 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="id1.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/x1.png" width="14"> Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiyao Wang<math alttext="{}^{1{\dagger}}" class="ltx_Math" display="inline" id="id2.1.m1.2"><semantics id="id2.1.m1.2a"><msup id="id2.1.m1.2.2" xref="id2.1.m1.2.2.cmml"><mi id="id2.1.m1.2.2a" xref="id2.1.m1.2.2.cmml"></mi><mrow id="id2.1.m1.2.2.2.4" xref="id2.1.m1.2.2.2.3.cmml"><mn id="id2.1.m1.1.1.1.1" xref="id2.1.m1.1.1.1.1.cmml">1</mn><mo id="id2.1.m1.2.2.2.4.1" lspace="0.222em" xref="id2.1.m1.2.2.2.3.cmml">⁣</mo><mo id="id2.1.m1.2.2.2.2" xref="id2.1.m1.2.2.2.2.cmml">†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.1.m1.2b"><apply id="id2.1.m1.2.2.cmml" xref="id2.1.m1.2.2"><list id="id2.1.m1.2.2.2.3.cmml" xref="id2.1.m1.2.2.2.4"><cn id="id2.1.m1.1.1.1.1.cmml" type="integer" xref="id2.1.m1.1.1.1.1">1</cn><ci id="id2.1.m1.2.2.2.2.cmml" xref="id2.1.m1.2.2.2.2">†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.2c">{}^{1{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.2d">start_FLOATSUPERSCRIPT 1 † end_FLOATSUPERSCRIPT</annotation></semantics></math> Yuhang Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.2.m2.1"><semantics id="id3.2.m2.1a"><msup id="id3.2.m2.1.1" xref="id3.2.m2.1.1.cmml"><mi id="id3.2.m2.1.1a" xref="id3.2.m2.1.1.cmml"></mi><mn id="id3.2.m2.1.1.1" xref="id3.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.2.m2.1b"><apply id="id3.2.m2.1.1.cmml" xref="id3.2.m2.1.1"><cn id="id3.2.m2.1.1.1.cmml" type="integer" xref="id3.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> Xiaoyu Liu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.3.m3.1"><semantics id="id4.3.m3.1a"><msup id="id4.3.m3.1.1" xref="id4.3.m3.1.1.cmml"><mi id="id4.3.m3.1.1a" xref="id4.3.m3.1.1.cmml"></mi><mn id="id4.3.m3.1.1.1" xref="id4.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.3.m3.1b"><apply id="id4.3.m3.1.1.cmml" xref="id4.3.m3.1.1"><cn id="id4.3.m3.1.1.1.cmml" type="integer" xref="id4.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id5.4.1">Hongjin Lu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.4.1.m1.1"><semantics id="id5.4.1.m1.1a"><msup id="id5.4.1.m1.1.1" xref="id5.4.1.m1.1.1.cmml"><mi id="id5.4.1.m1.1.1a" xref="id5.4.1.m1.1.1.cmml"></mi><mn id="id5.4.1.m1.1.1.1" mathvariant="normal" xref="id5.4.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.4.1.m1.1b"><apply id="id5.4.1.m1.1.1.cmml" xref="id5.4.1.m1.1.1"><cn id="id5.4.1.m1.1.1.1.cmml" type="integer" xref="id5.4.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.4.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.4.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id6.5.2">Yuancheng Xu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id6.5.2.m1.1"><semantics id="id6.5.2.m1.1a"><msup id="id6.5.2.m1.1.1" xref="id6.5.2.m1.1.1.cmml"><mi id="id6.5.2.m1.1.1a" xref="id6.5.2.m1.1.1.cmml"></mi><mn id="id6.5.2.m1.1.1.1" mathvariant="normal" xref="id6.5.2.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id6.5.2.m1.1b"><apply id="id6.5.2.m1.1.1.cmml" xref="id6.5.2.m1.1.1"><cn id="id6.5.2.m1.1.1.1.cmml" type="integer" xref="id6.5.2.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.5.2.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id6.5.2.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id7.6.3">Feihong He<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id7.6.3.m1.1"><semantics id="id7.6.3.m1.1a"><msup id="id7.6.3.m1.1.1" xref="id7.6.3.m1.1.1.cmml"><mi id="id7.6.3.m1.1.1a" xref="id7.6.3.m1.1.1.cmml"></mi><mn id="id7.6.3.m1.1.1.1" mathvariant="normal" xref="id7.6.3.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id7.6.3.m1.1b"><apply id="id7.6.3.m1.1.1.cmml" xref="id7.6.3.m1.1.1"><cn id="id7.6.3.m1.1.1.1.cmml" type="integer" xref="id7.6.3.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.6.3.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id7.6.3.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id8.7.4">Jaehong Yoon<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id8.7.4.m1.1"><semantics id="id8.7.4.m1.1a"><msup id="id8.7.4.m1.1.1" xref="id8.7.4.m1.1.1.cmml"><mi id="id8.7.4.m1.1.1a" xref="id8.7.4.m1.1.1.cmml"></mi><mn id="id8.7.4.m1.1.1.1" mathvariant="normal" xref="id8.7.4.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id8.7.4.m1.1b"><apply id="id8.7.4.m1.1.1.cmml" xref="id8.7.4.m1.1.1"><cn id="id8.7.4.m1.1.1.1.cmml" type="integer" xref="id8.7.4.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.7.4.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id8.7.4.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id10.9.6">Taixi Lu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id9.8.5.m1.1"><semantics id="id9.8.5.m1.1a"><msup id="id9.8.5.m1.1.1" xref="id9.8.5.m1.1.1.cmml"><mi id="id9.8.5.m1.1.1a" xref="id9.8.5.m1.1.1.cmml"></mi><mn id="id9.8.5.m1.1.1.1" mathvariant="normal" xref="id9.8.5.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id9.8.5.m1.1b"><apply id="id9.8.5.m1.1.1.cmml" xref="id9.8.5.m1.1.1"><cn id="id9.8.5.m1.1.1.1.cmml" type="integer" xref="id9.8.5.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.8.5.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id9.8.5.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Gedas Bertasius<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id10.9.6.m2.1"><semantics id="id10.9.6.m2.1a"><msup id="id10.9.6.m2.1.1" xref="id10.9.6.m2.1.1.cmml"><mi id="id10.9.6.m2.1.1a" xref="id10.9.6.m2.1.1.cmml"></mi><mn id="id10.9.6.m2.1.1.1" mathvariant="normal" xref="id10.9.6.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id10.9.6.m2.1b"><apply id="id10.9.6.m2.1.1.cmml" xref="id10.9.6.m2.1.1"><cn id="id10.9.6.m2.1.1.1.cmml" type="integer" xref="id10.9.6.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.9.6.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id10.9.6.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id11.10.7">Mohit Bansal<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id11.10.7.m1.1"><semantics id="id11.10.7.m1.1a"><msup id="id11.10.7.m1.1.1" xref="id11.10.7.m1.1.1.cmml"><mi id="id11.10.7.m1.1.1a" xref="id11.10.7.m1.1.1.cmml"></mi><mn id="id11.10.7.m1.1.1.1" mathvariant="normal" xref="id11.10.7.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id11.10.7.m1.1b"><apply id="id11.10.7.m1.1.1.cmml" xref="id11.10.7.m1.1.1"><cn id="id11.10.7.m1.1.1.1.cmml" type="integer" xref="id11.10.7.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.10.7.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id11.10.7.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id12.11.8">Huaxiu Yao<math alttext="{}^{2{\ddagger}}" class="ltx_Math" display="inline" id="id12.11.8.m1.2"><semantics id="id12.11.8.m1.2a"><msup id="id12.11.8.m1.2.2" xref="id12.11.8.m1.2.2.cmml"><mi id="id12.11.8.m1.2.2a" xref="id12.11.8.m1.2.2.cmml"></mi><mrow id="id12.11.8.m1.2.2.2.4" xref="id12.11.8.m1.2.2.2.3.cmml"><mn id="id12.11.8.m1.1.1.1.1" mathvariant="normal" xref="id12.11.8.m1.1.1.1.1.cmml">2</mn><mo id="id12.11.8.m1.2.2.2.4.1" lspace="0.222em" mathvariant="bold" xref="id12.11.8.m1.2.2.2.3.cmml">⁣</mo><mo id="id12.11.8.m1.2.2.2.2" mathvariant="normal" xref="id12.11.8.m1.2.2.2.2.cmml">‡</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id12.11.8.m1.2b"><apply id="id12.11.8.m1.2.2.cmml" xref="id12.11.8.m1.2.2"><list id="id12.11.8.m1.2.2.2.3.cmml" xref="id12.11.8.m1.2.2.2.4"><cn id="id12.11.8.m1.1.1.1.1.cmml" type="integer" xref="id12.11.8.m1.1.1.1.1">2</cn><ci id="id12.11.8.m1.2.2.2.2.cmml" xref="id12.11.8.m1.2.2.2.2">normal-‡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.11.8.m1.2c">{}^{2{\ddagger}}</annotation><annotation encoding="application/x-llamapun" id="id12.11.8.m1.2d">start_FLOATSUPERSCRIPT 2 ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="id13.12.9">Furong Huang<math alttext="{}^{1{\ddagger}}" class="ltx_Math" display="inline" id="id13.12.9.m1.2"><semantics id="id13.12.9.m1.2a"><msup id="id13.12.9.m1.2.2" xref="id13.12.9.m1.2.2.cmml"><mi id="id13.12.9.m1.2.2a" xref="id13.12.9.m1.2.2.cmml"></mi><mrow id="id13.12.9.m1.2.2.2.4" xref="id13.12.9.m1.2.2.2.3.cmml"><mn id="id13.12.9.m1.1.1.1.1" mathvariant="normal" xref="id13.12.9.m1.1.1.1.1.cmml">1</mn><mo id="id13.12.9.m1.2.2.2.4.1" lspace="0.222em" mathvariant="bold" xref="id13.12.9.m1.2.2.2.3.cmml">⁣</mo><mo id="id13.12.9.m1.2.2.2.2" mathvariant="normal" xref="id13.12.9.m1.2.2.2.2.cmml">‡</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="id13.12.9.m1.2b"><apply id="id13.12.9.m1.2.2.cmml" xref="id13.12.9.m1.2.2"><list id="id13.12.9.m1.2.2.2.3.cmml" xref="id13.12.9.m1.2.2.2.4"><cn id="id13.12.9.m1.1.1.1.1.cmml" type="integer" xref="id13.12.9.m1.1.1.1.1">1</cn><ci id="id13.12.9.m1.2.2.2.2.cmml" xref="id13.12.9.m1.2.2.2.2">normal-‡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.12.9.m1.2c">{}^{1{\ddagger}}</annotation><annotation encoding="application/x-llamapun" id="id13.12.9.m1.2d">start_FLOATSUPERSCRIPT 1 ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id14.13.m4.1"><semantics id="id14.13.m4.1a"><msup id="id14.13.m4.1.1" xref="id14.13.m4.1.1.cmml"><mi id="id14.13.m4.1.1a" xref="id14.13.m4.1.1.cmml"></mi><mn id="id14.13.m4.1.1.1" xref="id14.13.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id14.13.m4.1b"><apply id="id14.13.m4.1.1.cmml" xref="id14.13.m4.1.1"><cn id="id14.13.m4.1.1.1.cmml" type="integer" xref="id14.13.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.13.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id14.13.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Maryland, College Park 
<br class="ltx_break"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id15.14.m5.1"><semantics id="id15.14.m5.1a"><msup id="id15.14.m5.1.1" xref="id15.14.m5.1.1.cmml"><mi id="id15.14.m5.1.1a" xref="id15.14.m5.1.1.cmml"></mi><mn id="id15.14.m5.1.1.1" xref="id15.14.m5.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id15.14.m5.1b"><apply id="id15.14.m5.1.1.cmml" xref="id15.14.m5.1.1"><cn id="id15.14.m5.1.1.1.cmml" type="integer" xref="id15.14.m5.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id15.14.m5.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id15.14.m5.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>UNC-Chapel Hill, Chapel Hill 
<br class="ltx_break"><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="id16.15.m6.1"><semantics id="id16.15.m6.1a"><msup id="id16.15.m6.1.1" xref="id16.15.m6.1.1.cmml"><mi id="id16.15.m6.1.1a" xref="id16.15.m6.1.1.cmml"></mi><mo id="id16.15.m6.1.1.1" xref="id16.15.m6.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="id16.15.m6.1b"><apply id="id16.15.m6.1.1.cmml" xref="id16.15.m6.1.1"><ci id="id16.15.m6.1.1.1.cmml" xref="id16.15.m6.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id16.15.m6.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id16.15.m6.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="id18.17.id1">xywang@umd.edu</span> <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="id17.16.m7.1"><semantics id="id17.16.m7.1a"><msup id="id17.16.m7.1.1" xref="id17.16.m7.1.1.cmml"><mi id="id17.16.m7.1.1a" xref="id17.16.m7.1.1.cmml"></mi><mo id="id17.16.m7.1.1.1" xref="id17.16.m7.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="id17.16.m7.1b"><apply id="id17.16.m7.1.1.cmml" xref="id17.16.m7.1.1"><ci id="id17.16.m7.1.1.1.cmml" xref="id17.16.m7.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id17.16.m7.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="id17.16.m7.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math> Equal advising
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id19.id1">Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling
a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance.
Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations.
Our dataset is available at <a class="ltx_ref ltx_href" href="https://github.com/umd-huang-lab/Mementos" title="">https://github.com/umd-huang-lab/Mementos</a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_square" height="14" id="p1.1.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/x1.png" width="14">
<p class="ltx_p ltx_align_center" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.16"><span class="ltx_text" id="p2.16.16" style="width:433.6pt;"><span class="ltx_text" id="p2.16.16.16" style="width:0.0pt;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p2.16.16.16.16">
<span class="ltx_thead">
<span class="ltx_tr" id="p2.3.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p2.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p2.3.3.3.3.3.3.3">Xiyao Wang<math alttext="{}^{1{\dagger}}" class="ltx_Math" display="inline" id="p2.1.1.1.1.1.1.1.m1.2"><semantics id="p2.1.1.1.1.1.1.1.m1.2a"><msup id="p2.1.1.1.1.1.1.1.m1.2.2" xref="p2.1.1.1.1.1.1.1.m1.2.2.cmml"><mi id="p2.1.1.1.1.1.1.1.m1.2.2a" xref="p2.1.1.1.1.1.1.1.m1.2.2.cmml"></mi><mrow id="p2.1.1.1.1.1.1.1.m1.2.2.2.4" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml"><mn id="p2.1.1.1.1.1.1.1.m1.1.1.1.1" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.1.cmml">1</mn><mo id="p2.1.1.1.1.1.1.1.m1.2.2.2.4.1" lspace="0.222em" mathvariant="bold" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml">⁣</mo><mo id="p2.1.1.1.1.1.1.1.m1.2.2.2.2" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.2.cmml">†</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="p2.1.1.1.1.1.1.1.m1.2b"><apply id="p2.1.1.1.1.1.1.1.m1.2.2.cmml" xref="p2.1.1.1.1.1.1.1.m1.2.2"><list id="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.4"><cn id="p2.1.1.1.1.1.1.1.m1.1.1.1.1.cmml" type="integer" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.1">1</cn><ci id="p2.1.1.1.1.1.1.1.m1.2.2.2.2.cmml" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.2">normal-†</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.1.1.1.1.m1.2c">{}^{1{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.1.1.1.1.m1.2d">start_FLOATSUPERSCRIPT 1 † end_FLOATSUPERSCRIPT</annotation></semantics></math> Yuhang Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.2.2.2.2.2.2.2.m2.1"><semantics id="p2.2.2.2.2.2.2.2.m2.1a"><msup id="p2.2.2.2.2.2.2.2.m2.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p2.2.2.2.2.2.2.2.m2.1.1a" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mn id="p2.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.2.2.2.2.2.2.2.m2.1b"><apply id="p2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1"><cn id="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p2.2.2.2.2.2.2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.2.2.2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> Xiaoyu Liu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.3.3.3.3.3.3.3.m3.1"><semantics id="p2.3.3.3.3.3.3.3.m3.1a"><msup id="p2.3.3.3.3.3.3.3.m3.1.1" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"><mi id="p2.3.3.3.3.3.3.3.m3.1.1a" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"></mi><mn id="p2.3.3.3.3.3.3.3.m3.1.1.1" mathvariant="normal" xref="p2.3.3.3.3.3.3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.3.3.3.3.3.3.3.m3.1b"><apply id="p2.3.3.3.3.3.3.3.m3.1.1.cmml" xref="p2.3.3.3.3.3.3.3.m3.1.1"><cn id="p2.3.3.3.3.3.3.3.m3.1.1.1.cmml" type="integer" xref="p2.3.3.3.3.3.3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.3.3.3.3.3.3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.3.3.3.3.3.3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p2.8.8.8.8.8">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p2.8.8.8.8.8.5"><span class="ltx_text ltx_font_bold" id="p2.4.4.4.4.4.1.1">Hongjin Lu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.4.4.4.4.4.1.1.m1.1"><semantics id="p2.4.4.4.4.4.1.1.m1.1a"><msup id="p2.4.4.4.4.4.1.1.m1.1.1" xref="p2.4.4.4.4.4.1.1.m1.1.1.cmml"><mi id="p2.4.4.4.4.4.1.1.m1.1.1a" xref="p2.4.4.4.4.4.1.1.m1.1.1.cmml"></mi><mn id="p2.4.4.4.4.4.1.1.m1.1.1.1" mathvariant="normal" xref="p2.4.4.4.4.4.1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.4.4.4.4.4.1.1.m1.1b"><apply id="p2.4.4.4.4.4.1.1.m1.1.1.cmml" xref="p2.4.4.4.4.4.1.1.m1.1.1"><cn id="p2.4.4.4.4.4.1.1.m1.1.1.1.cmml" type="integer" xref="p2.4.4.4.4.4.1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.4.4.4.4.4.1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.4.4.4.4.4.1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.5.5.5.5.5.2.2">Yuancheng Xu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.5.5.5.5.5.2.2.m1.1"><semantics id="p2.5.5.5.5.5.2.2.m1.1a"><msup id="p2.5.5.5.5.5.2.2.m1.1.1" xref="p2.5.5.5.5.5.2.2.m1.1.1.cmml"><mi id="p2.5.5.5.5.5.2.2.m1.1.1a" xref="p2.5.5.5.5.5.2.2.m1.1.1.cmml"></mi><mn id="p2.5.5.5.5.5.2.2.m1.1.1.1" mathvariant="normal" xref="p2.5.5.5.5.5.2.2.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.5.5.5.5.5.2.2.m1.1b"><apply id="p2.5.5.5.5.5.2.2.m1.1.1.cmml" xref="p2.5.5.5.5.5.2.2.m1.1.1"><cn id="p2.5.5.5.5.5.2.2.m1.1.1.1.cmml" type="integer" xref="p2.5.5.5.5.5.2.2.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.5.5.5.5.5.2.2.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.5.5.5.5.5.2.2.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.6.6.6.6.6.3.3">Feihong He<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.6.6.6.6.6.3.3.m1.1"><semantics id="p2.6.6.6.6.6.3.3.m1.1a"><msup id="p2.6.6.6.6.6.3.3.m1.1.1" xref="p2.6.6.6.6.6.3.3.m1.1.1.cmml"><mi id="p2.6.6.6.6.6.3.3.m1.1.1a" xref="p2.6.6.6.6.6.3.3.m1.1.1.cmml"></mi><mn id="p2.6.6.6.6.6.3.3.m1.1.1.1" mathvariant="normal" xref="p2.6.6.6.6.6.3.3.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.6.6.6.6.6.3.3.m1.1b"><apply id="p2.6.6.6.6.6.3.3.m1.1.1.cmml" xref="p2.6.6.6.6.6.3.3.m1.1.1"><cn id="p2.6.6.6.6.6.3.3.m1.1.1.1.cmml" type="integer" xref="p2.6.6.6.6.6.3.3.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.6.6.6.6.6.3.3.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.6.6.6.6.6.3.3.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.7.7.7.7.7.4.4">Jaehong Yoon<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.7.7.7.7.7.4.4.m1.1"><semantics id="p2.7.7.7.7.7.4.4.m1.1a"><msup id="p2.7.7.7.7.7.4.4.m1.1.1" xref="p2.7.7.7.7.7.4.4.m1.1.1.cmml"><mi id="p2.7.7.7.7.7.4.4.m1.1.1a" xref="p2.7.7.7.7.7.4.4.m1.1.1.cmml"></mi><mn id="p2.7.7.7.7.7.4.4.m1.1.1.1" mathvariant="normal" xref="p2.7.7.7.7.7.4.4.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.7.7.7.7.7.4.4.m1.1b"><apply id="p2.7.7.7.7.7.4.4.m1.1.1.cmml" xref="p2.7.7.7.7.7.4.4.m1.1.1"><cn id="p2.7.7.7.7.7.4.4.m1.1.1.1.cmml" type="integer" xref="p2.7.7.7.7.7.4.4.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.7.7.7.7.7.4.4.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.7.7.7.7.7.4.4.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.8.8.8.8.8.5.5">Taixi Lu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.8.8.8.8.8.5.5.m1.1"><semantics id="p2.8.8.8.8.8.5.5.m1.1a"><msup id="p2.8.8.8.8.8.5.5.m1.1.1" xref="p2.8.8.8.8.8.5.5.m1.1.1.cmml"><mi id="p2.8.8.8.8.8.5.5.m1.1.1a" xref="p2.8.8.8.8.8.5.5.m1.1.1.cmml"></mi><mn id="p2.8.8.8.8.8.5.5.m1.1.1.1" mathvariant="normal" xref="p2.8.8.8.8.8.5.5.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.8.8.8.8.8.5.5.m1.1b"><apply id="p2.8.8.8.8.8.5.5.m1.1.1.cmml" xref="p2.8.8.8.8.8.5.5.m1.1.1"><cn id="p2.8.8.8.8.8.5.5.m1.1.1.1.cmml" type="integer" xref="p2.8.8.8.8.8.5.5.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.8.8.8.8.8.5.5.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.8.8.8.8.8.5.5.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p2.12.12.12.12.12">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p2.12.12.12.12.12.4"><span class="ltx_text ltx_font_bold" id="p2.9.9.9.9.9.1.1">Gedas Bertasius<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.9.9.9.9.9.1.1.m1.1"><semantics id="p2.9.9.9.9.9.1.1.m1.1a"><msup id="p2.9.9.9.9.9.1.1.m1.1.1" xref="p2.9.9.9.9.9.1.1.m1.1.1.cmml"><mi id="p2.9.9.9.9.9.1.1.m1.1.1a" xref="p2.9.9.9.9.9.1.1.m1.1.1.cmml"></mi><mn id="p2.9.9.9.9.9.1.1.m1.1.1.1" mathvariant="normal" xref="p2.9.9.9.9.9.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.9.9.9.9.9.1.1.m1.1b"><apply id="p2.9.9.9.9.9.1.1.m1.1.1.cmml" xref="p2.9.9.9.9.9.1.1.m1.1.1"><cn id="p2.9.9.9.9.9.1.1.m1.1.1.1.cmml" type="integer" xref="p2.9.9.9.9.9.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.9.9.9.9.9.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.9.9.9.9.9.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.10.10.10.10.10.2.2">Mohit Bansal<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.10.10.10.10.10.2.2.m1.1"><semantics id="p2.10.10.10.10.10.2.2.m1.1a"><msup id="p2.10.10.10.10.10.2.2.m1.1.1" xref="p2.10.10.10.10.10.2.2.m1.1.1.cmml"><mi id="p2.10.10.10.10.10.2.2.m1.1.1a" xref="p2.10.10.10.10.10.2.2.m1.1.1.cmml"></mi><mn id="p2.10.10.10.10.10.2.2.m1.1.1.1" mathvariant="normal" xref="p2.10.10.10.10.10.2.2.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.10.10.10.10.10.2.2.m1.1b"><apply id="p2.10.10.10.10.10.2.2.m1.1.1.cmml" xref="p2.10.10.10.10.10.2.2.m1.1.1"><cn id="p2.10.10.10.10.10.2.2.m1.1.1.1.cmml" type="integer" xref="p2.10.10.10.10.10.2.2.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.10.10.10.10.10.2.2.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.10.10.10.10.10.2.2.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.11.11.11.11.11.3.3">Huaxiu Yao<math alttext="{}^{2{\ddagger}}" class="ltx_Math" display="inline" id="p2.11.11.11.11.11.3.3.m1.2"><semantics id="p2.11.11.11.11.11.3.3.m1.2a"><msup id="p2.11.11.11.11.11.3.3.m1.2.2" xref="p2.11.11.11.11.11.3.3.m1.2.2.cmml"><mi id="p2.11.11.11.11.11.3.3.m1.2.2a" xref="p2.11.11.11.11.11.3.3.m1.2.2.cmml"></mi><mrow id="p2.11.11.11.11.11.3.3.m1.2.2.2.4" xref="p2.11.11.11.11.11.3.3.m1.2.2.2.3.cmml"><mn id="p2.11.11.11.11.11.3.3.m1.1.1.1.1" mathvariant="normal" xref="p2.11.11.11.11.11.3.3.m1.1.1.1.1.cmml">2</mn><mo id="p2.11.11.11.11.11.3.3.m1.2.2.2.4.1" lspace="0.222em" mathvariant="bold" xref="p2.11.11.11.11.11.3.3.m1.2.2.2.3.cmml">⁣</mo><mo id="p2.11.11.11.11.11.3.3.m1.2.2.2.2" mathvariant="normal" xref="p2.11.11.11.11.11.3.3.m1.2.2.2.2.cmml">‡</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="p2.11.11.11.11.11.3.3.m1.2b"><apply id="p2.11.11.11.11.11.3.3.m1.2.2.cmml" xref="p2.11.11.11.11.11.3.3.m1.2.2"><list id="p2.11.11.11.11.11.3.3.m1.2.2.2.3.cmml" xref="p2.11.11.11.11.11.3.3.m1.2.2.2.4"><cn id="p2.11.11.11.11.11.3.3.m1.1.1.1.1.cmml" type="integer" xref="p2.11.11.11.11.11.3.3.m1.1.1.1.1">2</cn><ci id="p2.11.11.11.11.11.3.3.m1.2.2.2.2.cmml" xref="p2.11.11.11.11.11.3.3.m1.2.2.2.2">normal-‡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.11.11.11.11.11.3.3.m1.2c">{}^{2{\ddagger}}</annotation><annotation encoding="application/x-llamapun" id="p2.11.11.11.11.11.3.3.m1.2d">start_FLOATSUPERSCRIPT 2 ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="p2.12.12.12.12.12.4.4">Furong Huang<math alttext="{}^{1{\ddagger}}" class="ltx_Math" display="inline" id="p2.12.12.12.12.12.4.4.m1.2"><semantics id="p2.12.12.12.12.12.4.4.m1.2a"><msup id="p2.12.12.12.12.12.4.4.m1.2.2" xref="p2.12.12.12.12.12.4.4.m1.2.2.cmml"><mi id="p2.12.12.12.12.12.4.4.m1.2.2a" xref="p2.12.12.12.12.12.4.4.m1.2.2.cmml"></mi><mrow id="p2.12.12.12.12.12.4.4.m1.2.2.2.4" xref="p2.12.12.12.12.12.4.4.m1.2.2.2.3.cmml"><mn id="p2.12.12.12.12.12.4.4.m1.1.1.1.1" mathvariant="normal" xref="p2.12.12.12.12.12.4.4.m1.1.1.1.1.cmml">1</mn><mo id="p2.12.12.12.12.12.4.4.m1.2.2.2.4.1" lspace="0.222em" mathvariant="bold" xref="p2.12.12.12.12.12.4.4.m1.2.2.2.3.cmml">⁣</mo><mo id="p2.12.12.12.12.12.4.4.m1.2.2.2.2" mathvariant="normal" xref="p2.12.12.12.12.12.4.4.m1.2.2.2.2.cmml">‡</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="p2.12.12.12.12.12.4.4.m1.2b"><apply id="p2.12.12.12.12.12.4.4.m1.2.2.cmml" xref="p2.12.12.12.12.12.4.4.m1.2.2"><list id="p2.12.12.12.12.12.4.4.m1.2.2.2.3.cmml" xref="p2.12.12.12.12.12.4.4.m1.2.2.2.4"><cn id="p2.12.12.12.12.12.4.4.m1.1.1.1.1.cmml" type="integer" xref="p2.12.12.12.12.12.4.4.m1.1.1.1.1">1</cn><ci id="p2.12.12.12.12.12.4.4.m1.2.2.2.2.cmml" xref="p2.12.12.12.12.12.4.4.m1.2.2.2.2">normal-‡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.12.12.12.12.12.4.4.m1.2c">{}^{1{\ddagger}}</annotation><annotation encoding="application/x-llamapun" id="p2.12.12.12.12.12.4.4.m1.2d">start_FLOATSUPERSCRIPT 1 ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.13.13.13.13.13">
<span class="ltx_td ltx_align_center" id="p2.13.13.13.13.13.1"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.13.13.13.13.13.1.m1.1"><semantics id="p2.13.13.13.13.13.1.m1.1a"><msup id="p2.13.13.13.13.13.1.m1.1.1" xref="p2.13.13.13.13.13.1.m1.1.1.cmml"><mi id="p2.13.13.13.13.13.1.m1.1.1a" xref="p2.13.13.13.13.13.1.m1.1.1.cmml"></mi><mn id="p2.13.13.13.13.13.1.m1.1.1.1" xref="p2.13.13.13.13.13.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.13.13.13.13.13.1.m1.1b"><apply id="p2.13.13.13.13.13.1.m1.1.1.cmml" xref="p2.13.13.13.13.13.1.m1.1.1"><cn id="p2.13.13.13.13.13.1.m1.1.1.1.cmml" type="integer" xref="p2.13.13.13.13.13.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.13.13.13.13.13.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.13.13.13.13.13.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Maryland, College Park</span></span>
<span class="ltx_tr" id="p2.14.14.14.14.14">
<span class="ltx_td ltx_align_center" id="p2.14.14.14.14.14.1"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.14.14.14.14.14.1.m1.1"><semantics id="p2.14.14.14.14.14.1.m1.1a"><msup id="p2.14.14.14.14.14.1.m1.1.1" xref="p2.14.14.14.14.14.1.m1.1.1.cmml"><mi id="p2.14.14.14.14.14.1.m1.1.1a" xref="p2.14.14.14.14.14.1.m1.1.1.cmml"></mi><mn id="p2.14.14.14.14.14.1.m1.1.1.1" xref="p2.14.14.14.14.14.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.14.14.14.14.14.1.m1.1b"><apply id="p2.14.14.14.14.14.1.m1.1.1.cmml" xref="p2.14.14.14.14.14.1.m1.1.1"><cn id="p2.14.14.14.14.14.1.m1.1.1.1.cmml" type="integer" xref="p2.14.14.14.14.14.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.14.14.14.14.14.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.14.14.14.14.14.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>UNC-Chapel Hill, Chapel Hill</span></span>
<span class="ltx_tr" id="p2.16.16.16.16.16">
<span class="ltx_td ltx_align_center" id="p2.16.16.16.16.16.2"><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="p2.15.15.15.15.15.1.m1.1"><semantics id="p2.15.15.15.15.15.1.m1.1a"><msup id="p2.15.15.15.15.15.1.m1.1.1" xref="p2.15.15.15.15.15.1.m1.1.1.cmml"><mi id="p2.15.15.15.15.15.1.m1.1.1a" xref="p2.15.15.15.15.15.1.m1.1.1.cmml"></mi><mo id="p2.15.15.15.15.15.1.m1.1.1.1" xref="p2.15.15.15.15.15.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="p2.15.15.15.15.15.1.m1.1b"><apply id="p2.15.15.15.15.15.1.m1.1.1.cmml" xref="p2.15.15.15.15.15.1.m1.1.1"><ci id="p2.15.15.15.15.15.1.m1.1.1.1.cmml" xref="p2.15.15.15.15.15.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.15.15.15.15.15.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="p2.15.15.15.15.15.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="p2.16.16.16.16.16.2.1">xywang@umd.edu</span> <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="p2.16.16.16.16.16.2.m2.1"><semantics id="p2.16.16.16.16.16.2.m2.1a"><msup id="p2.16.16.16.16.16.2.m2.1.1" xref="p2.16.16.16.16.16.2.m2.1.1.cmml"><mi id="p2.16.16.16.16.16.2.m2.1.1a" xref="p2.16.16.16.16.16.2.m2.1.1.cmml"></mi><mo id="p2.16.16.16.16.16.2.m2.1.1.1" xref="p2.16.16.16.16.16.2.m2.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="p2.16.16.16.16.16.2.m2.1b"><apply id="p2.16.16.16.16.16.2.m2.1.1.cmml" xref="p2.16.16.16.16.16.2.m2.1.1"><ci id="p2.16.16.16.16.16.2.m2.1.1.1.cmml" xref="p2.16.16.16.16.16.2.m2.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.16.16.16.16.16.2.m2.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="p2.16.16.16.16.16.2.m2.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math> Equal advising</span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The recent emergence of Multimodal Large Language Models (MLLMs) such as GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib27" title="">2023b</a>)</cite> and Gemini&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib31" title="">2023</a>)</cite> has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering.
Despite the notable performance of existing MLLMs, they often suffer from hallucination (a phenomenon where MLLMs produce inaccurate descriptions of the given images) due to insufficient reasoning capabilities, generating inaccurate responses in visual inference&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib18" title="">2023a</a>); Yue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib41" title="">2023</a>)</cite>.
Thus, monitoring the reasoning capability is of great importance in understanding the ability and the limitations of MLLMs and applying MLLMs in the real world.
Previous benchmarks, such as those in <cite class="ltx_cite ltx_citemacro_citet">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib18" title="">2023a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Yue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib41" title="">2023</a>)</cite>, have primarily addressed evaluating reasoning in each individual image, relying on static and object-centric knowledge.
However, they are insufficient to comprehensively assess the reasoning capabilities of MLLMs due to a lack of time-varying object behaviors or events.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="597" id="S1.F1.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/demo.jpeg" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of hallucinations by GPT-4V in three domains on Mementos. The red box shows the description generated by GPT-4V based on the given prompt, and the human-annotated descriptions are in the blue box. Texts highlighted in yellow are hallucination parts generated by GPT-4V. This illustrates that even GPT-4V experiences severe hallucinations when reasoning from image sequences.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To investigate the capabilities of Multi-Modal Language Models (MLLMs) in dynamic reasoning across image sequences, we present a new benchmark, <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Mementos</span>.
This benchmark focuses on the complex task of monitoring and deciphering the <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">positional changes</span> of objects within an image sequence, followed by the inference of behavioral patterns and logical connections among them.
Such an endeavor requires the interpretation of the overarching context based on time-variant visual elements, posing a greater challenge than the analysis of static scenes. Concretely, Mementos consists of 4,761 image sequences with varying episode lengths<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Here, an episode refers to a specific event or series of events depicted in the image sequence.</span></span></span>, encompassing diverse scenarios from everyday life, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated description of the primary objects and their behaviors within the sequence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To assess the reasoning capability of MLLMs on Mementos, we employ a <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">GPT-4</span>-assisted evaluation procedure:
after an MLLM produces a description for an image sequence,
we extract behavior and object keywords from both AI-generated and human-annotated descriptions using GPT-4. We then use keyword matching to assess the degree of behavioral and object hallucinations.
To refine the correctness of this evaluation, we have developed behavior and object synonym graphs for each domain. These graphs facilitate more precise keyword matching, ensuring a thorough and nuanced analysis of the MLLMs’ reasoning abilities.
Besides, we also provide the comparison with human evaluation to demonstrates that the GPT-4-assisted evaluation procedure is very reliable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We evaluated the reasoning proficiency of <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">nine leading-edge MLLMs</span> on Mementos, encompassing both black-box and open-source models.
Our findings indicate that Mementos poses a considerable challenge to these current MLLMs.
For instance, as depicted in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">1</span></a>, GPT-4V exhibits notable behavioral and object hallucinations in various domains during image sequence reasoning.
Behavioral hallucinations are defined as the MLLMs’ erroneous interpretations or predictions of entity actions, while object hallucinations pertain to the inaccurate identification or creation of objects within the image sequences.
Notably, behavioral hallucinations were more frequent than object hallucinations, highlighting a significant deficiency in MLLMs’ capability to deduce events from image sequences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Furthermore, our research pinpoints three principal factors that lead to the reasoning failures of MLLMs: (1) the interconnectedness of object and behavioral hallucinations, (2) the impact of co-occurring behaviors, and (3) the cumulative effect of behavioral hallucinations. The objective of our proposed benchmark and analyses is to shed light on innovative approaches to augment the reasoning abilities of MLLMs and to reduce hallucinations in their subsequent advancements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Mementos</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we introduce Mementos, a novel and challenging benchmark designed to test the reasoning capability of Multimodal Large Language Model (MLLM) under sequential image input.
Initially, we detail the data gathering and annotation methodology for Mementos, alongside an overview of its data distribution.
Subsequently, we outline the evaluation protocol and the metric employed to assess the reasoning prowess of MLLMs in Mementos.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Mementos Benchmark</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Dataset Composition</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Mementos comprises 4,761 image sequences of varying lengths, predominantly sourced from Daily-life, Robotics, and Comics domains. Detailed statistics are provided in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.T1" title="Table 1 ‣ 2.1.1 Dataset Composition ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">1</span></a>. This diverse collection is instrumental in evaluating the comprehensive time-varying reasoning abilities of MLLMs. Specifically, the robotics data, closely associated with embodied AI or real-world contexts, and the comic-style storyboard data, rich in stylistic and episodic diversity in image sequences, significantly enhance the benchmark’s relevance and robustness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The number of image sequences in different categories within Mementos.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.2">Total</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.3">Train Set</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4">Val set</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">Daily-life</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2">3505</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3">3055</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">450</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.3.2.1">Robotics</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.2.2">1101</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.2.3">902</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.4">199</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.1.4.3.1">Comics</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.4.3.2">155</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.4.3.3">105</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.4">50</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Daily-life</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.Px1.p1.1">The Daily-life image sequences in Mementos are derived from video clips in the Next-QA dataset, as cited in <cite class="ltx_cite ltx_citemacro_citet">Xiao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib35" title="">2021</a>)</cite>. These sequences represent a range of everyday life scenarios. We have selectively extracted videos from the Next-QA Training set, specifically those with frame counts ranging from 400 to 2,500.
To balance the challenge of testing MLLMs’ reasoning capabilities against the risk of losing critical information, our methodology involves retaining the first frame of each video. Subsequently, we sample one image every 100 frames. The collected images from this sampling process then form an image sequence that corresponds to the original video. This approach ensures a rigorous yet feasible evaluation of MLLMs’ reasoning abilities in dynamically evolving everyday scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Robotics</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS1.Px2.p1.2">For the Robotics data, we utilized videos from various sub-datasets within Open X-Embodiment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Collaboration et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib6" title="">2023</a>)</cite>.
Open X-Embodiment aggregates video datasets from multiple university laboratories, showcasing a variety of tasks performed by different robotic systems.
We meticulously selected sub-datasets from Open X-Embodiment that offer video resolutions exceeding 128x128 and exhibit a high degree of task diversity. From these chosen sub-datasets, a total of 1,101 videos were sampled. The precise number of videos sourced from each sub-dataset is detailed in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A1" title="Appendix A Details of Open X-Embodiment Data Selection ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">A</span></a>.
For video sampling, our approach varied based on the length of the videos. Videos exceeding 100 frames were processed by sampling one image every <math alttext="n/20" class="ltx_Math" display="inline" id="S2.SS1.SSS1.Px2.p1.1.m1.1"><semantics id="S2.SS1.SSS1.Px2.p1.1.m1.1a"><mrow id="S2.SS1.SSS1.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.2" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.2.cmml">n</mi><mo id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.1" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.1.cmml">/</mo><mn id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.3" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.Px2.p1.1.m1.1b"><apply id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1"><divide id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.1"></divide><ci id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.2">𝑛</ci><cn id="S2.SS1.SSS1.Px2.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.Px2.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.Px2.p1.1.m1.1c">n/20</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.Px2.p1.1.m1.1d">italic_n / 20</annotation></semantics></math> frames, where <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS1.Px2.p1.2.m2.1"><semantics id="S2.SS1.SSS1.Px2.p1.2.m2.1a"><mi id="S2.SS1.SSS1.Px2.p1.2.m2.1.1" xref="S2.SS1.SSS1.Px2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.Px2.p1.2.m2.1b"><ci id="S2.SS1.SSS1.Px2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.Px2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.Px2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.Px2.p1.2.m2.1d">italic_n</annotation></semantics></math> represents the total frame count of the video. Conversely, for videos with frame counts ranging from 20 to 100, we sampled one image every 5 frames. This ensures the formation of comprehensive and representative image sequences for each video, catering to the evaluation of MLLMs in diverse and complex robotic contexts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Comics</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS1.SSS1.Px3.p1.1">The Comics data is composed of wordless multi-panel comics of diverse styles, curated from online sources.
Unlike Daily-life and Robotics sections, where image sequences are uniformly extracted from video frames, the comics represent intentionally selected key moments within a narrative, manually illustrated by artists.
This distinction sets our dataset apart from conventional video datasets.
In addition to traditional comics, this category also incorporates 20 storyboards from movies reimagined in comic style.
We have further deconstructed these comics into individual image sequences by taking screenshots.
This approach enables a unique exploration of sequential visual reasoning, enhancing the diversity and complexity of the dataset for evaluating MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Dataset Annotation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">For each image sequence in Mementos, we have meticulously annotated a ground truth description that captures the unfolding events.
These descriptions focus on the primary objects and their respective behaviors, where <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p1.1.1">behavior</span> refers to a verb or verb phrase associated with the object in question.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">For the Daily-life data, we initially employed GPT-4V(ision)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib26" title="">2023a</a>)</cite>, to amalgamate and reformulate the questions and answers from the Next-QA videos into single paragraph descriptions.
This method significantly expedited the manual annotation process. Following this, we conducted a thorough manual review of these automated descriptions, making necessary adjustments.
This included rectifying inaccuracies, removing non-existent episodes, and adding missing details to ensure alignment with the actual image sequences.
To ensure reliability, we implemented a cross-validation step, where a separate set of annotators performed a secondary review.
For the Robotics and Comics categories, the annotation process was entirely manual, conducted by human annotators. These annotations were then subjected to a verification process by the authors which ensures the accuracy and consistency of the descriptions across all categories.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Dataset Statistics</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">In showcasing the extensive diversity of Mementos, we present a detailed overview of the data distribution within the Mementos validation set.
Our analysis focuses on two key dimensions: the length of the image sequence and the length of the episode.
The length of an image sequence is defined by the number of frames it contains, while the episode length is determined by the total number of events depicted in the sequence.
A longer image sequence necessitates the MLLM to process a larger number of images, thereby challenging the model’s capacity to manage sequences spanning broader timeframes.
A greater episode length signifies that the image sequence encompasses more intricate scenarios.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S2.F2.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/Image_length.jpeg" width="269">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of image sequence length in Mementos Val set.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S2.F3.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/Episode_length.jpeg" width="269">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of episode length in Mementos Val set.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS1.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Image sequence length</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS3.Px1.p1.1">For the image sequence length, we count the number of frames in each image sequence. As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.F2" title="Figure 2 ‣ 2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">2</span></a>,
the majority of image sequences are between 4 and 14 frames in length.
67.38% of image sequences contain 4 to 14 frames, yet 31.90% of sequences are composed of longer frames - more than 15 frames.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Episode length</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS3.Px2.p1.1">To quantify the episode length within each image sequence of Mementos, we employed GPT-4 for extracting behavior keywords, specifically verbs associated with objects, from the human-annotated descriptions. This extraction was facilitated using a pre-defined manual prompt, details of which can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A3" title="Appendix C Prompt Details ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">C</span></a>. Following the extraction, we calculated the length of the behavior list for each image sequence.
A lengthier behavior list signifies a more extended episode within the image sequence, which inherently poses a greater challenge for the MLLM in comprehending the entire image sequence. As illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.F3" title="Figure 3 ‣ 2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">3</span></a>, a significant portion of the image sequences, particularly those from the robotics data, feature episode lengths ranging between 1 and 3. This is mainly attributed to the dominance of two-action episodes like ‘pick up and place’, ‘move and pull open’, ‘locate and push’. Meanwhile, the remaining data exhibits a normal distribution for episode lengths spanning 4 to 17.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S2.F4.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/procedure.jpeg" width="299">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>GPT-4-assisted evaluation procedure. We employ "O-" and "B-" to indicate objects and behaviors, respectively.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Evaluation Procedure and Metrics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In this section, we illustrate how to evaluate the descriptions generated by MLLMs, including the evaluation procedure and metrics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Procedure</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S2.F4" title="Figure 4 ‣ Episode length ‣ 2.1.3 Dataset Statistics ‣ 2.1 Mementos Benchmark ‣ 2 Mementos ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">4</span></a>, we use an image sequence and a pre-designed prompt together as the input for MLLMs, and generate the description aligned with the corresponding image sequence.
Next,
we ask GPT-4 to extract object and behavior keywords using the AI-generated description.
We then match the obtained keywords with the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.1.1">synonym graph</span> we built, replacing the matched keywords with the root word from the synonym graph.
Finally, we obtain two lists of keywords: AI-generated object list and AI-generated behavior list.
We note that the proposed keyword extraction leveraging GPT-4 is surprisingly reliable and accurate, which is competitive with human extraction.
Please refer to Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A2" title="Appendix B Human Evaluation ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">B</span></a> for a detailed comparison.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Synonym graph</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">The synonym graph is an unilateral digraph where each edge connects two nodes representing words or phrases. For instance, given a synonym pair (pick up, lift up), an edge would be directed from ‘lift up’ to ‘pick up’.
In each synonym pair, the first word, originating from the human-annotated keyword list, is referred to as the root word, while the second word comes from the AI-generated keyword list.
To construct this synonym graph, we first use GPT-4 to extract object and behavior keywords from all human-annotated descriptions in the Val set, forming a human-annotated keyword list.
Then, we generate descriptions on the Val set using GPT-4V, LLAVA, and Gemini and use GPT-4 to extract object and behavior keywords.
After that,
we manually match these words with the human-annotated keyword list to identify all synonym pairs and add them as edges to the synonym graph.
Given a word or phrase, this synonym graph can quickly match the corresponding root word if a synonym exists in the human-annotated keyword list, completing the keyword replacement.
For convenience in evaluation, we maintain separate synonym graphs for objects and behaviors of different categories.
We make all constructed synonym graphs publicly available as open-source resources.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Metrics</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">After obtaining the AI-generated object list and AI-generated behavior list for each image sequence, we utilize the corresponding human-annotated object list and human-annotated behavior list as the ground truth to calculate ‘Recall,’ ‘Precision,’ and ‘F1 metrics’ at both the object and behavior levels.
These metrics are used to measure the understanding capabilities of different MLLMs regarding the image sequence episode.
‘Recall’ reflects the accuracy of an MLLM’s reasoning about episodes in an image sequence, while ‘precision’ focuses on assessing the severity of hallucinations that occur when understanding the image sequence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In our experimental section, we delve into two key questions:
(a) We examine the reasoning <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">capabilities</span> of current black-box and open-source MLLMs on Mementos. Specifically, we assess the <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">severity</span> of object and behavioral hallucinations in these models.
(b) We investigate the underlying <span class="ltx_text ltx_font_bold" id="S3.p1.1.3">causes</span> of reasoning failures in MLLMs when interpreting image sequences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baseline evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Models</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We establish our baseline using 9 popular black-box and open-source MLLMs.
The black-box MLLMs include GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib26" title="">2023a</a>)</cite> and Gemini&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib31" title="">2023</a>)</cite>, and the open-source MLLMs are Video-LLaMA-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib42" title="">2023a</a>)</cite>, Chat-UniVi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib12" title="">2023</a>)</cite>, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib20" title="">2023c</a>)</cite>, MiniGPT4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib50" title="">2023</a>)</cite>, MiniGPT5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib46" title="">2023</a>)</cite>, mPLUG_Owl-v2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ye et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib37" title="">2023</a>)</cite>, and InstructBLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib8" title="">2023</a>)</cite>.
Additionally, considering that only a few open-source MLLMs are designed to process sequential images or videos (Video-LLaMA-2 and Chat-UniVi), we adapt input for other open-source MLLMs by combining all frames from an image sequence into one composite image. This input is referred to as the combined-input (c-input) setting. For black-box MLLMs and Chat-UniVi, we conduct evaluations using both the c-input setting and an alternative approach where frames from the image sequence are input sequentially, termed the sequential-input (s-input) setting. For Video-LLaMA-2, we only test the performance on s-input setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="269" id="S3.F5.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/total_v2.png" width="269">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of the six metrics for different MLLMs on Mementos.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Evaluation results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">We evaluate all baseline MLLMs on Mementos and report the results in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.F5" title="Figure 5 ‣ 3.1.1 Models ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">5</span></a>.
Besides, we provide the performance comparison of each baseline method across the three different domains (Daily-life, Robotics, and Comics) in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.T2" title="Table 2 ‣ 3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">2</span></a>. We summarize our findings as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation of different MLLMs on Mementos.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Input type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Model</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.5.1">Object</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.6"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.8.1">Behavior</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.9"></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<td class="ltx_td ltx_align_center" id="S3.T2.1.2.2.1">Recall</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.2.2.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.2.2.3">F1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.2.2.4">Recall</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.2.2.5">Precision</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.2.2.6">F1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.3.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.3.3.2" rowspan="4"><span class="ltx_text" id="S3.T2.1.3.3.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.3.3.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.4.1">59.80%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.5.1">50.96%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.3.3.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.6.1">53.51%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.7.1">36.71%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.8">32.97%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.9.1">33.59%</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.4.4.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.4.4.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.3">35.92%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.4">42.06%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.4.4.5">37.10%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.6">18.80%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.7">29.42%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.8">21.64%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.5">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.5.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.5.5.2">Video-LLaMA-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.5.5.3">31.59%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.5.5.4">30.01%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.5.5.5">29.37%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.5.5.6">17.05%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.5.5.7">28.19%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.5.5.8">20.12%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.6.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.6.6.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.3">40.74%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.4">40.78%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.6.6.5">39.13%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.6">22.30%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.7">31.10%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.8">24.90%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.7">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.7.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.7.7.2" rowspan="8"><span class="ltx_text" id="S3.T2.1.7.7.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.7.7.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.7.7.4">39.45%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.7.7.5">39.64%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.7.7.6">38.04%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.7.7.7">26.43%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.7.7.8">23.59%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.7.7.9">23.98%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.8.8.1">Daily-life</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.8.8.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.3">31.17%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.4">37.39%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.8.8.5">32.38%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.6">17.71%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.7">25.65%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.8">19.74%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.9">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.9.9.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.9.9.2">Chat-UniVi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.9.9.3">36.19%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.9.9.4">38.88%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.9.9.5">36.02%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.9.9.6">21.80%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.9.9.7">28.52%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.9.9.8">23.73%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.10">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.10.10.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.10.10.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.3">37.72%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.4">47.01%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.10.10.5">40.18%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.6">22.17%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.7.1">37.33%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.8">26.65%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.11">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.11.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.11.11.2">MiniGPT4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.11.11.3">32.25%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.11.11.4">23.14%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.11.11.5">25.75%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.11.11.6">18.09%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.11.11.7">24.16%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.11.11.8">19.45%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.12.12">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.12.12.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.12.12.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.3">31.39%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.4">22.62%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.12.12.5">24.91%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.6">18.42%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.7">24.56%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.8">19.85%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.13.13">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.13.13.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.13.13.2">mPLUG_Owl-v2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.13.13.3">32.59%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.13.13.4">47.17%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.13.13.5">37.04%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.13.13.6">17.96%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.13.13.7">33.57%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.13.13.8">22.13%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.14.14">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.14.14.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.14.14.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.14.3">31.82%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.14.4">41,14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.14.14.5">34.28%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.14.6">22.40%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.14.7">30.30%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.14.8">24.55%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.15.15">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.15.15.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.15.15.2" rowspan="4"><span class="ltx_text" id="S3.T2.1.15.15.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.15.15.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.15.15.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.15.15.4.1">63.94%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.15.15.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.15.15.5.1">65.42%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.15.15.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.15.15.6.1">62.99%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.15.15.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.15.15.7.1">60.72%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.15.15.8">24.43%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.15.15.9">33.95%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.16.16">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.16.16.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.16.16.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.16.3">43.80%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.16.4">46.26%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.16.16.5">43.15%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.16.6">46.43%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.16.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.16.16.7.1">38.13%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.16.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.16.16.8.1">39.38%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.17.17">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.17.17.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.17.17.2">Video-LLaMA-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.17.17.3">13.41%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.17.17.4">10.33%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.17.17.5">11.15%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.17.17.6">17.04%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.17.17.7">8.96%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.17.17.8">11.23%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.18.18">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.18.18.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.18.18.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.18.3">35.40%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.18.4">32.57%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.18.18.5">32.39%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.18.6">32.24%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.18.7">16.69%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.18.8">21.14%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.19.19">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.19.19.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.19.19.2" rowspan="8"><span class="ltx_text" id="S3.T2.1.19.19.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.19.19.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.19.19.4">27.87%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.19.19.5">31.86%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.19.19.6">28.58%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.19.19.7">44.72%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.19.19.8">16.54%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.19.19.9">23.58%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.20.20.1">Robotics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.20.20.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.20.3">34.78%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.20.4">41.66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.20.20.5">36.16%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.20.6">47.29%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.20.7">29.59%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.20.8">34.17%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.21.21">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.21.21.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.21.21.2">Chat-UniVi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.21.21.3">17.74%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.21.21.4">18.32%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.21.21.5">17.07%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.21.21.6">19.81%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.21.21.7">10.01%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.21.21.8">12.54%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.22.22">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.22.22.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.22.22.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.22.22.3">36.88%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.22.22.4">46.62%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.22.22.5">39.31%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.22.22.6">25.27%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.22.22.7">14.80%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.22.22.8">17.95%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.23.23">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.23.23.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.23.23.2">MiniGPT4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.23.23.3">10.97%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.23.23.4">7.28%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.23.23.5">8.16%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.23.23.6">13.40%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.23.23.7">5.88%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.23.23.8">7.76%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.24.24">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.24.24.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.24.24.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.24.24.3">9.75%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.24.24.4">6.52%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.24.24.5">7.16%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.24.24.6">8.96%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.24.24.7">4.53%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.24.24.8">5.43%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.25.25">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.25.25.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.25.25.2">mPLUG_Owl-v2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.25.25.3">19.75%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.25.25.4">26.70%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.25.25.5">21.99%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.25.25.6">26.46%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.25.25.7">16.59%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.25.25.8">19.51%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.26.26">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.26.26.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.26.26.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.26.26.3">17.96%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.26.26.4">18.65%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.26.26.5">17.29%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.26.26.6">31.41%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.26.26.7">19.08%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.26.26.8">22.69%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.27.27">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.27.27.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.27.27.2" rowspan="4"><span class="ltx_text" id="S3.T2.1.27.27.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.27.27.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.27.27.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.27.27.4.1">49.53%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.27.27.5">37.57%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.27.27.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.27.27.6.1">41.71%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.27.27.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.27.27.7.1">19.97%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.27.27.8">17.29%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.27.27.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.27.27.9.1">18.11%</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.28.28">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.28.28.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.28.28.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.28.28.3">38.57%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.28.28.4">40.64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.28.28.5">38.53%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.28.28.6">15.23%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.28.28.7">19.11%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.28.28.8">16.30%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.29.29">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.29.29.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.29.29.2">Video-LLaMA-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.29.29.3">20.26%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.29.29.4">17.59%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.29.29.5">18.09%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.29.29.6">5.45%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.29.29.7">11.07%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.29.29.8">6.81%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.30.30">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.30.30.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.30.30.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.30.30.3">28.04%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.30.30.4">31.61%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.30.30.5">28.13%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.30.30.6">10.42%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.30.30.7">15.74%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.30.30.8">11.97%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.31.31">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.31.31.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.31.31.2" rowspan="8"><span class="ltx_text" id="S3.T2.1.31.31.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.31.31.3">GPT-4V</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.31.31.4">29.23%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.31.31.5">24.64%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.31.31.6">25.90%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.31.31.7">13.19%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.31.31.8">13.09%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.31.31.9">12.90%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.32.32">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.32.32.1">Comics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.32.32.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.32.32.3">41.25%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.32.32.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.32.32.4.1">45.07%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.32.32.5">41.18%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.32.32.6">15.37%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.32.32.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.32.32.7.1">20.55%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.32.32.8">16.42%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.33.33">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.33.33.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.33.33.2">Chat-UniVi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.33.33.3">25.12%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.33.33.4">28.08%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.33.33.5">25.51%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.33.33.6">8.85%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.33.33.7">10.67%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.33.33.8">9.31%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.34.34">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.34.34.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.34.34.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.34.34.3">29.44%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.34.34.4">35.61%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.34.34.5">30.97%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.34.34.6">8.63%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.34.34.7">13.56%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.34.34.8">10.27%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.35.35">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.35.35.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.35.35.2">MiniGPT4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.35.35.3">20.50%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.35.35.4">13.94%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.35.35.5">15.74%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.35.35.6">7.95%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.35.35.7">8.64%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.35.35.8">7.98%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.36.36">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.36.36.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.36.36.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.36.36.3">22.94%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.36.36.4">18.11%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.36.36.5">19.42%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.36.36.6">8.88%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.36.36.7">11.92%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.36.36.8">9.94%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.37.37">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.37.37.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T2.1.37.37.2">mPLUG_Owl-v2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.37.37.3">26.82%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.37.37.4">37.74%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T2.1.37.37.5">29.49%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.37.37.6">8.70%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.37.37.7">20.85%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.1.37.37.8">11.74%</th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.38.38">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T2.1.38.38.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T2.1.38.38.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.38.38.3">25.02%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.38.38.4">29.15%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.1.38.38.5">25.10%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.38.38.6">8.25%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.38.38.7">10.48%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.38.38.8">8.97%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">GPT-4V (s-input) and LLaVA-1.5 are the best-performing models among black-box and open-source MLLMs, respectively.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px1.p1.1">As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.F5" title="Figure 5 ‣ 3.1.1 Models ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">5</span></a>, except for being on par with Gemini (s-input) and LLaVA-1.5 in behavior precision, GPT-4V with s-input demonstrates the best reasoning capability compared with all other MLLMs in understanding image sequences.
Among open-source models, LLaVA1.5 performs the best, nearly matching or even surpassing the black-box model Gemini in object comprehension, but its ability to infer behaviors from image sequences is weaker compared to Gemini and GPT-4V.
Although Video-LLaMA-2 and Chat-UniVi are designed for video understanding, they do not show an advantage over LLaVA-1.5, especially Video-LLaMA-2, which performs notably worse compared to LLaVA-1.5.
The weakest models in understanding image sequences are MiniGPT4 and MiniGPT5, with a significant gap in every metric compared to the other baselines.
It’s noteworthy that under c-input setting, the performance of black-box MLLMs does not significantly differ from that of open-source MLLMs.
LLaVA-1.5 and mPLUG_Owl-v2 meet or even exceed the black-box MLLMs on many metrics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">MLLMs possess a much stronger ability on reasoning objects in image sequences than they do on reasoning behaviors.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px2.p1.1">We find that all MLLM methods perform significantly better on the three metrics for objects than those for behaviors.
Taking the best-performing GPT-4V as an example, it achieves over 50% on all three object metrics, with recall even reaching 60%, indicating it can effectively recognize the main objects in an image sequence.
However, for behaviors, GPT-4V scores only around 30%, with the best recall metric barely exceeding 40%.
Despite this, GPT-4V is still the best-performing MLLM in reasoning behaviors.
This suggests that current MLLMs do not possess strong abilities to autonomously infer the behaviors from given sequential images, indicating the importance of our benchmark in highlighting the limitations in the reasoning abilities of MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Reasoning capability of MLLMs varies across different domains.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS2.Px3.p1.1">From Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.T2" title="Table 2 ‣ 3.1.2 Evaluation results ‣ 3.1 Baseline evaluation ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">2</span></a>,
we find that black-box models perform best in the robotics domain across the three domains, while open-source models show relatively better performance in the daily-life domain.
Analyzing each domain specifically, it is evident that in the daily-life domain, the performance of all methods, except for GPT-4V (s-input), does not vary significantly.
The main reason for the performance gap between open-source MLLMs and black-box MLLMs is the noticeably lower metrics of open-source models compared to black-box models in the robotics and comics domains.
The recall, precision, and F1 of both object and behavior for black-box MLLMs are almost more than double those of open-source models.
We speculate that one reason for this phenomenon is the distribution shift between Mementos and the training data of open-source MLLMs.
The limitations of the training data lead to weaker reasoning capability of open-source MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysis of Failure Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we will provide reasons for failure reasoning results in current MLLMs, combining specific quantitative analyses and case studies.
Since behavioral hallucination is a unique phenomenon in image sequence reasoning, and the causes of object hallucination are not significantly different from those in single image reasoning, we only present the reasons leading to behavioral hallucination in this paper.
Due to space limitations, please refer to the Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4" title="Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">D</span></a> for specific case studies. The following are our main findings:
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Interplay between object and behavioral hallucinations in MLLMs.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">A key hypothesis underpinning behavioral hallucination is that incorrect object identification leads to subsequent inaccuracies in behavior identification. To test this, we evaluated the correlation coefficients between object and behavioral hallucinations across different domains for various MLLMs, as detailed in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.T3" title="Table 3 ‣ The impact of co-occurrence on behavioral hallucination. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">3</span></a>.
Our findings reveal that, for most MLLMs, the correlation coefficients in the three domains fluctuate between 0.1 and 0.4, suggesting a weak yet present correlation. This outcome supports the hypothesis that object hallucination contributes to behavioral hallucination to some extent.
Case studies further reveal that after an object hallucination occurs, MLLMs tend to describe behaviors related to the hallucinated object, even if these behaviors do not exist in the image sequence.
As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.F6" title="Figure 6 ‣ Interplay between object and behavioral hallucinations in MLLMs. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">6</span></a>, after recognizing a scene as a tennis court, a MLLM might describe a person playing tennis.
Interestingly, in the robotics domain, there is a negligible correlation between object and behavioral hallucinations in black-box MLLMs. This divergence is likely because behaviors in robotics are predominantly linked to robotic arms, which these MLLMs generally identify correctly.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="281" id="S3.F6.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case_main.jpeg" width="269">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A sample of failure reasoning case in Daily-life domain. The failure reason is object hallucination, correlation between object hallucination and behavioral hallucination, and co-occurrence behavior. Following the object hallucination of <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F6.4.1">tennis court</span>, the LVLM subsequently exhibits behavioral hallucinations of <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F6.5.2">holding a tennis racket</span> (correlation between object hallucination and behavioral hallucination) and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F6.6.3">appears to be playing tennis</span> (co-occurrence behavior).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">The impact of co-occurrence on behavioral hallucination.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">In line with object hallucination phenomena, as noted in <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib16" title="">2023c</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib47" title="">2023a</a>)</cite>, MLLMs demonstrate a tendency to generate behaviors that are commonly paired together when reasoning through image sequences. This proclivity exacerbates the problem of behavioral hallucination, especially in the field of robotics.
Consider the case in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">1</span></a> where a robotic arm is tasked with opening a drawer by <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.1">grabbing its side</span>.
MLLMs might erroneously depict the sequence as the arm <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.2">grabbing the handle</span> first, followed by pulling the drawer open, since <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.3">grabbing the handle</span> is a more co-occurring behavior with ‘pull open’.
Despite the final outcome being accurately described, such errors in key details are unacceptable in robotics.
This issue is of particular concern given the growing inclination to utilize MLLMs as reward functions in robotic training, as discussed in recent studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib24" title="">2023</a>); Sontakke et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib30" title="">2023</a>); Rocamonde et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib28" title="">2023</a>); Baumli et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib2" title="">2023</a>)</cite>.
Such subtle yet significant behavioral hallucinations can critically affect the quality of the reward function, leading to potential mislearning of behaviors in robotic systems.
For more detailed case studies, please refer to the Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4" title="Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">D</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Correlation coefficient between behavioral hallucination and object hallucination of different MLLMs on Mementos.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:195.1pt;height:435.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.8pt,115.4pt) scale(0.653358382706062,0.653358382706062) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.2.1">Input type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.3.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.4">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.5">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.6">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.2" rowspan="4"><span class="ltx_text" id="S3.T3.1.1.2.1.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.4">0.120</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.5">0.188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.6">0.132</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.3.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.3.2.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.3">0.165</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.4">0.179</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.5">0.146</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.4.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.4.3.2">Video-LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.3">0.197</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.4">0.067</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.5">0.125</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.5.4.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.5.4.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.3">0.138</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.4">0.178</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.5">0.137</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.6.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.2" rowspan="8"><span class="ltx_text" id="S3.T3.1.1.6.5.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.5.4">0.242</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.5.5">0.182</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6.5.6">0.199</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.7.6.1">Daily-life</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.7.6.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.3">0.158</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.4">0.179</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.5">0.152</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.8.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.8.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.8.7.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.3">0.127</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.4">0.184</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.5">0.172</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.9.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.9.8.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.9.8.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.3">0.112</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.4">0.134</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.5">0.106</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.10.9">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.10.9.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.10.9.2">MiniGPT4</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.3">0.135</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.4">0.145</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.5">0.115</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.11.10">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.11.10.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.11.10.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.3">0.126</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.4">0.188</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.5">0.146</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.12.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.12.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.12.11.2">mPLUG_Owl-v2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.3">0.106</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.4">0.113</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.5">0.069</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.13.12">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.13.12.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.13.12.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.3">0.133</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.4">0.125</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.5">0.127</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.14.13">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.14.13.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.14.13.2" rowspan="4"><span class="ltx_text" id="S3.T3.1.1.14.13.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.14.13.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.14.13.4">-0.012</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.14.13.5">0.022</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.14.13.6">0.011</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.15.14">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.15.14.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.15.14.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.3">0.027</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.4">0.144</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.5">0.101</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.16.15">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.16.15.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.16.15.2">Video-LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.16.15.3">0.107</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.16.15.4">0.107</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.16.15.5">0.109</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.17.16">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.17.16.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.17.16.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.17.16.3">0.038</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.17.16.4">0.121</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.17.16.5">0.089</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.18.17">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.18.17.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.18.17.2" rowspan="8"><span class="ltx_text" id="S3.T3.1.1.18.17.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.18.17.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.18.17.4">0.041</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.18.17.5">-0.022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.18.17.6">0.008</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.19.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.19.18.1">Robotics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.19.18.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.19.18.3">-0.049</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.19.18.4">-0.086</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.19.18.5">-0.106</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.20.19">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.20.19.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.20.19.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.20.19.3">0.189</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.20.19.4">0.242</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.20.19.5">0.207</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.21.20">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.21.20.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.21.20.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.21.20.3">0.135</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.21.20.4">0.123</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.21.20.5">0.157</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.22.21">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.22.21.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.22.21.2">MiniGPT4</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.22.21.3">0.186</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.22.21.4">0.316</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.22.21.5">0.233</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.23.22">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.23.22.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.23.22.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.23.22.3">0.056</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.23.22.4">0.027</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.23.22.5">0.045</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.24.23">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.24.23.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.24.23.2">mPLUG_Owl-v2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.24.23.3">0.244</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.24.23.4">0.163</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.24.23.5">0.231</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.25.24">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.25.24.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.25.24.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.25.24.3">0.227</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.25.24.4">0.235</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.25.24.5">0.253</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.26.25">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.26.25.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.26.25.2" rowspan="4"><span class="ltx_text" id="S3.T3.1.1.26.25.2.1">Sequential</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.26.25.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.26.25.4">0.045</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.26.25.5">0.225</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.26.25.6">0.158</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.27.26">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.27.26.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.27.26.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.27.26.3">0.176</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.27.26.4">0.081</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.27.26.5">0.144</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.28.27">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.28.27.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.28.27.2">Video-LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.28.27.3">0.261</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.28.27.4">0.280</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.28.27.5">0.299</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.29.28">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.29.28.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.29.28.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.29.28.3">0.239</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.29.28.4">0.331</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.29.28.5">0.221</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.30.29">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.30.29.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.30.29.2" rowspan="8"><span class="ltx_text" id="S3.T3.1.1.30.29.2.1">Combined</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.30.29.3">GPT-4V</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.30.29.4">0.343</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.30.29.5">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.30.29.6">0.471</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.31.30">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.31.30.1">Comics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.31.30.2">Gemini</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.31.30.3">0.187</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.31.30.4">0.121</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.31.30.5">0.167</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.32.31">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.32.31.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.32.31.2">Chat-UniVi</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.32.31.3">0.293</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.32.31.4">0.113</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.32.31.5">0.279</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.33.32">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.33.32.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.33.32.2">LLaVa-1.5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.33.32.3">0.062</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.33.32.4">0.101</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.33.32.5">0.088</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.34.33">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.34.33.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.34.33.2">MiniGPT4</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.34.33.3">0.199</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.34.33.4">0.134</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.34.33.5">0.213</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.35.34">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.35.34.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.35.34.2">MiniGPT5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.35.34.3">0.324</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.35.34.4">0.366</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.35.34.5">0.339</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.36.35">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.36.35.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.36.35.2">mPLUG_Owl-v2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.36.35.3">0.231</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.36.35.4">-0.043</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.36.35.5">0.157</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.37.36">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.1.1.37.36.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.1.1.37.36.2">InstructBLIP</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.37.36.3">0.288</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.37.36.4">0.005</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.37.36.5">0.262</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">The Snowball effect in behavioral hallucinations.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">In machine learning, the Snowball effect is a well-documented phenomenon, referring to the progressive accumulation or intensification of errors within a system, as discussed in <cite class="ltx_cite ltx_citemacro_citet">Asadi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib1" title="">2019</a>); Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib43" title="">2023b</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib34" title="">2023c</a>); Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib21" title="">2023d</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib43" title="">2023b</a>)</cite> notably highlight this phenomenon in Large Language Models.
Experiments on Mementos reveal that the snowball effect in both behavioral and object hallucinations becomes markedly pronounced when reasoning through image sequences.
The temporal nature of image sequences, consisting of a series of frames rather than a solitary image, demands that MLLMs sequentially infer the narrative, frame by frame. This process makes the models susceptible to accumulating and exacerbating hallucinations if errors occur early in the sequence.
In our analysis, we specifically examined the trend of object and behavioral hallucination in GPT-4V and LLaVA-1.5 within the daily-life domain, correlating it with the increasing episode length of image sequences. As depicted in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.F7" title="Figure 7 ‣ The Snowball effect in behavioral hallucinations. ‣ 3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">7</span></a>, there is a noticeable decrease in object and behavior recall for both MLLMs as the episode length extends. This trend suggests a heightened susceptibility to hallucinations and a pronounced snowball effect in MLLMs when processing image sequences with a greater array of objects and behaviors.
Detailed case studies can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4" title="Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">D</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="79" id="S3.F7.sf1.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/o_recall.jpeg" width="132">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Object</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="79" id="S3.F7.sf2.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/a_recall.jpeg" width="132">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Behavior</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The trend of changes in object and behavior recall for GPT-4V and LLaVA-1.5 in the Daily-life domain as the episode length of the image sequence increases.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarking in MLLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The advent of Multimodal Large Language Models (MLLMs) has prompted a reassessment of traditional benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib17" title="">2014</a>); Marino et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib25" title="">2019</a>); Hudson and Manning (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib10" title="">2019</a>)</cite>, originally conceived for Vision Language Models (VLMs). These existing benchmarks fail to sufficiently expose the robustness and hallucination issues inherent in MLLMs. Consequently, there is a growing impetus within the research community to devise more challenging benchmarks. This trend spans various domains, from question and answering (QA) reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib18" title="">2023a</a>); Yue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib41" title="">2023</a>)</cite>, to optical character recognition (OCR)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib23" title="">2023f</a>)</cite>, and extends to the study of hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib32" title="">2023a</a>)</cite>, with benchmarks like POPE <cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib16" title="">2023c</a>)</cite> and Bingo <cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib7" title="">2023</a>)</cite>. Additionally, comprehensive analyses of MLLMs, such as Mmbench <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib22" title="">2023e</a>)</cite>, Mm-vet<cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib40" title="">2023b</a>)</cite>, LVLM-eHub<cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib36" title="">2023</a>)</cite>, SEED<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib14" title="">2023a</a>)</cite>, GAVIE<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib19" title="">2023b</a>)</cite>, and LAMM <cite class="ltx_cite ltx_citemacro_cite">Yin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib38" title="">2023</a>)</cite>, are emerging.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Diverging from the focus on single images in prior studies, our paper introduces a novel benchmark that employs image sequences derived from video frames or comics, specifically examining the behavioral hallucinations in MLLMs. While <cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib3" title="">2023a</a>)</cite> have employed a method of uniformly sampling frames from videos to create vision QA tasks for MLLMs, our benchmark is distinct in its challenge, as it prompts MLLMs to formulate descriptions of image sequences without the guidance of questions. This approach allows for a more nuanced evaluation of behavioral hallucinations and, by extension, a more precise assessment of MLLMs’ reasoning capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Hallucination in MLLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Hallucinations in MLLMs, akin to those in Large Language Models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib44" title="">2023c</a>); Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib15" title="">2023b</a>); Zhou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib49" title="">2023c</a>); Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib4" title="">2023b</a>)</cite>, represent a significant challenge. In MLLMs, hallucinations are characterized by inconsistencies between the model’s output and the visual content&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Rohrbach et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib29" title="">2018</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib32" title="">2023a</a>)</cite>. Recent studies have explored various aspects of hallucination in MLLMs, covering topics such as object hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib16" title="">2023c</a>)</cite>, hallucination assessment in GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib7" title="">2023</a>)</cite>, and knowledge hallucination&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib18" title="">2023a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">While there are frameworks proposed for mitigating hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib47" title="">2023a</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib33" title="">2023b</a>); Leng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib13" title="">2023</a>); Zhou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib48" title="">2023b</a>); Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib5" title="">2023c</a>); Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib11" title="">2023</a>); Huang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib9" title="">2023</a>); Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib39" title="">2023a</a>); Zhao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#bib.bib45" title="">2023</a>)</cite>, there is a noticeable gap in the literature regarding the study of behavioral hallucination. Behavioral hallucination refers to scenarios where the generated content contains actions that conflict with what is depicted in image sequences. Moreover, the existing body of work does not offer a dedicated metric for evaluating behavioral hallucinations, an area that warrants further exploration and development.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Works</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we present Mementos, an novel and challenging benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in interpreting image sequences.
We conduct evaluations on nine most recent MLLMs using GPT-4-assisted evaluation procedure.
Our findings indicate that all tested MLLMs struggle with significant behavioral and object hallucinations in generating descriptions for image sequences.
Through a mix of quantitative analysis and case studies, we identify three primary factors contributing to these reasoning failures in MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Looking ahead, there are three potential avenues for future research:
<span class="ltx_text ltx_font_italic" id="S5.p2.1.1">(1) Dataset Diversification:</span> We suggest further enriching Mementos by including a broader variety of data types. This expansion could encompass first-person navigation experiences, sequential medical CT scans, and interactive gaming data. Such diversification would provide a more comprehensive platform for evaluating MLLMs across a wider range of contexts and scenarios.
<span class="ltx_text ltx_font_italic" id="S5.p2.1.2">(2) Evaluation Process Optimization:</span> Another key area for development is refining the evaluation process. This involves exploring more nuanced methods to assess the reasoning capabilities of MLLMs, focusing on semantic understanding rather than relying predominantly on keyword matching. Such advancements would enable a deeper and more accurate appraisal of MLLMs’ comprehension skills.
<span class="ltx_text ltx_font_italic" id="S5.p2.1.3">(3) Hallucination Mitigation and Reasoning Enhancement:</span> Lastly, informed by the three identified causes of reasoning failures, we propose developing targeted strategies to reduce both behavioral and object hallucinations in the future work. These methods would aim to bolster the reasoning faculties of MLLMs, making them more adept at accurately interpreting and describing complex image sequences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Wang, Zhou, Liu, Xu, and Huang are supported by National Science Foundation NSF-IIS FAI program, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, Capital One and JP Morgan faculty fellowships. Yao thanks Center for AI Safety and Google Cloud Research Credits program for supporting our computing needs. Bansal is supported by DARPA ECOLE Program No. HR00112390060 and ONR Grant N00014-23-1-2356.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asadi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kavosh Asadi, Dipendra Misra, Seungchan Kim, and Michel&nbsp;L. Littman. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1905.13320" title="">Combating the compounding-error problem with a multi-step model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baumli et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Vision-language models as a source of rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2312.09187</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. 2023a.

</span>
<span class="ltx_bibblock">Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2311.14906</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge&nbsp;Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. 2023b.

</span>
<span class="ltx_bibblock">Hallucination detection: Robustly discerning reliable answers in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>, pages 245–255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. 2023c.

</span>
<span class="ltx_bibblock">Mitigating hallucination in visual language models with visual supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2311.16479</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collaboration et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav&nbsp;S. Sukhatme, Gautam Salhotra, Ge&nbsp;Yan, Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, Heni&nbsp;Ben Amor, Henrik&nbsp;I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph&nbsp;J. Lim, João
Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Srinivasan, Lawrence&nbsp;Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan&nbsp;Kumar Srirama, Mohit Sharma, Moo&nbsp;Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil&nbsp;J Joshi, Niko Suenderhauf, Norman&nbsp;Di Palo, Nur Muhammad&nbsp;Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag&nbsp;R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari,
Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony&nbsp;Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi&nbsp;Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen&nbsp;Jeff Cui. 2023.

</span>
<span class="ltx_bibblock">Open X-Embodiment: Robotic learning datasets and RT-X models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.08864" title="">https://arxiv.org/abs/2310.08864</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023.

</span>
<span class="ltx_bibblock">Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2311.03287</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng&nbsp;Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.06500" title="">Instructblip: Towards general-purpose vision-language models with instruction tuning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023.

</span>
<span class="ltx_bibblock">Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2311.17911</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Drew&nbsp;A Hudson and Christopher&nbsp;D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 6700–6709.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji&nbsp;Zhang, Fei Huang, and Shikun Zhang. 2023.

</span>
<span class="ltx_bibblock">Hallucination augmented contrastive learning for multimodal large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2312.06968</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li&nbsp;Yuan. 2023.

</span>
<span class="ltx_bibblock">Chat-univi: Unified visual representation empowers large language models with image and video understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2311.08046</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Mitigating object hallucinations in large vision-language models through visual contrastive decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2311.16922</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023a.

</span>
<span class="ltx_bibblock">Seed-bench: Benchmarking multimodal llms with generative comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2307.16125</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junyi Li, Xiaoxue Cheng, Wayne&nbsp;Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock">Halueval: A large-scale hallucination evaluation benchmark for large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 6449–6464.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne&nbsp;Xin Zhao, and Ji-Rong Wen. 2023c.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2305.10355</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C&nbsp;Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pages 740–755. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a.

</span>
<span class="ltx_bibblock">Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2310.14566</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023b.

</span>
<span class="ltx_bibblock">Aligning large multi-modal model with robust instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2306.14565</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong&nbsp;Jae Lee. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.03744" title="">Improved baselines with visual instruction tuning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaoyu Liu, Jiaxin Yuan, Bang An, Yuancheng Xu, Yifan Yang, and Furong Huang. 2023d.

</span>
<span class="ltx_bibblock">C-disentanglement: Discovering causally-independent generative factors under an inductive bias of confounder.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2310.17325</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023e)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo&nbsp;Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et&nbsp;al. 2023e.

</span>
<span class="ltx_bibblock">Mmbench: Is your multi-modal model an all-around player?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2307.06281</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023f)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et&nbsp;al. 2023f.

</span>
<span class="ltx_bibblock">On the hidden mystery of ocr in large multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2305.07895</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yecheng&nbsp;Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. 2023.

</span>
<span class="ltx_bibblock">Liv: Language-image representations and rewards for robotic control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2306.00958</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</em>, pages 3195–3204.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock">Gpt-4v(ision) system card.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rocamonde et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. 2023.

</span>
<span class="ltx_bibblock">Vision-language models are zero-shot reward models for reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2310.12921</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anna Rohrbach, Lisa&nbsp;Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.

</span>
<span class="ltx_bibblock">Object hallucination in image captioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:1809.02156</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sontakke et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sumedh&nbsp;Anand Sontakke, Jesse Zhang, Séb Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. 2023.

</span>
<span class="ltx_bibblock">Roboclip: One demonstration is enough to learn robot policies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji&nbsp;Zhang, Jihua Zhu, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Evaluation and analysis of hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2308.15126</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2023b.

</span>
<span class="ltx_bibblock">Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2312.01701</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, and Furong Huang. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.07220" title="">Coplanner: Plan to roll out conservatively but to explore optimistically for model-based rl</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock">Next-qa: Next phase of question-answering to explaining temporal actions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 9777–9786.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu&nbsp;Qiao, and Ping Luo. 2023.

</span>
<span class="ltx_bibblock">Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2306.09265</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi&nbsp;Qian, Ji&nbsp;Zhang, Fei Huang, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2311.04257</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu&nbsp;Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2306.06687</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi&nbsp;Tian, and Yueting Zhuang. 2023a.

</span>
<span class="ltx_bibblock">Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2311.13614</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023b.

</span>
<span class="ltx_bibblock">Mm-vet: Evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2308.02490</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge&nbsp;Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2311.16502</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.02858" title="">Video-llama: An instruction-tuned audio-visual language model for video understanding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2306.02858</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah&nbsp;A. Smith. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.13534" title="">How language model hallucinations can snowball</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu&nbsp;Zhang, Yulong Chen, et&nbsp;al. 2023c.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.01219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023.

</span>
<span class="ltx_bibblock">Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2311.16839</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kaizhi Zheng, Xuehai He, and Xin&nbsp;Eric Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.02239" title="">Minigpt-5: Interleaved vision-and-language generation via generative vokens</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023a.

</span>
<span class="ltx_bibblock">Analyzing and mitigating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2310.00754</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhang Zhou, Suraj Maharjan, and Beiye Liu. 2023b.

</span>
<span class="ltx_bibblock">Scalable prompt generation for semi-supervised learning with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2302.09236</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, and Furong Huang. 2023c.

</span>
<span class="ltx_bibblock">Explore spurious correlations at the concept level in language models for text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2311.08648</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2304.10592</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of Open X-Embodiment Data Selection</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide the names of all subsets selected from Open X-Embodiment dataset and the corresponding sampling video numbers. For detailed information, please refer to Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A1.T4" title="Table 4 ‣ Appendix A Details of Open X-Embodiment Data Selection ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Number of videos selected from each sub-dataset of Open X-Embodiment.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.1">Sub-dataset name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.1.1.1.2">Number of videos selected</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.2.1.1">fractal20220817_data</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.2">400</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.3.2.1">kuka</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.4.3.1">bridge</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.2">300</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.5.4.1">jaco_play</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.6.5.1">berkeley_autolab_ur5</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.6.5.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.7.6.1">toto</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.7.6.2">10</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.8.7.1">columbia_cairlab_pusht_real</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.8.7.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.9.8.1">stanford_hydra_dataset_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.9.8.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.10.9.1">ucsd_kitchen_dataset_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.10.9.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.11.10.1">bc_z</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.11.10.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.12.11.1">utokyo_pr2_opening_fridge_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.12.11.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.13.12.1">utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.13.12.2">10</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.14.13.1">utokyo_xarm_pick_and_place_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.14.13.2">1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.15.14.1">utokyo_xarm_bimanual_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.15.14.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.16.15.1">dlr_sara_pour_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.16.15.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.17.16.1">dlr_edan_shared_control_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.17.16.2">100</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.18.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.18.17.1">asu_table_top_converted_externally_to_rlds</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.18.17.2">20</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T4.1.19.18.1">utaustin_mutex</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.19.18.2">30</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.20.19">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T4.1.20.19.1">berkeley_fanuc_manipulation</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.20.19.2">30</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Human Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, to verify the reliability of the GPT-4 assisted evaluation procedure, we compare the results of GPT-4 assisted evaluation with those of human evaluation.
We randomly select 200 image sequences from the entire Val set and manually extract object and behavior keyword lists for each image sequence’s AI-generated description and human-annotated description.
Then, we calculate six metrics and compare them with the metrics obtained using keyword lists extracted by GPT-4.
We choose the four MLLMs that performed best in reasoning on Mementos as representatives: GPT-4V (s-input), Gemini (s-input), Chat-UniVi (s-input), and LLaVA-1.5.
The evaluation results are shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A2.T5" title="Table 5 ‣ Appendix B Human Evaluation ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">After comparison, we find that there is not a significant gap between the results of GPT-4 assisted evaluation and human evaluation, with the absolute value of the difference mostly ranging between 1% to 4%.
For most metrics, the GPT-4 assisted evaluation tends to overestimate the performance of MLLMs, meaning the evaluation results are higher than those of human evaluation.
However, the relative ranking among different MLLMs remains essentially unchanged.
Overall, the GPT-4 assisted evaluation is quite reliable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A2.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Human evaluation.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A2.T5.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A2.T5.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.2.1">Eval type</span></th>
<td class="ltx_td ltx_border_tt" id="A2.T5.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.4.1">Object</span></td>
<td class="ltx_td ltx_border_r ltx_border_tt" id="A2.T5.1.1.1.5"></td>
<td class="ltx_td ltx_border_tt" id="A2.T5.1.1.1.6"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.7.1">Behavior</span></td>
<td class="ltx_td ltx_border_tt" id="A2.T5.1.1.1.8"></td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.2.2">
<td class="ltx_td ltx_align_center" id="A2.T5.1.2.2.1">Recall</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.2.2.2">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.2.2.3">F1</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.2.2.4">Recall</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.2.2.5">Precision</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.2.2.6">F1</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.3.3.1" rowspan="2"><span class="ltx_text" id="A2.T5.1.3.3.1.1">GPT-4V (s-input)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.3.3.2">GPT-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.3.3">60.91%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.3.4">51.04%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.3.3.5">54.13%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.3.6">38.02%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.3.7">33.05%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.3.8">34.12%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.4.4.1">Human</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.4.4.2">57.69%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.4.4.3">49.54%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.4.4.4">52.01%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.4.4.5">35.26%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.4.4.6">31.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.4.4.7">32.67%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.5.5.1" rowspan="2"><span class="ltx_text" id="A2.T5.1.5.5.1.1">Gemini (s-input)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.5.5.2">GPT-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.5.5.3">37.54%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.5.5.4">39.43%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.5.5.5">36.88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.5.5.6">23.38%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.5.5.7">34.19%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.5.5.8">24.02%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.6.6.1">Human</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.6.6.2">35.82%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.6.6.3">38.11%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.6.6.4">37.09%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.6.6.5">20.46%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.6.6.6">33.72%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.6.6.7">22.99%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.7.7.1" rowspan="2"><span class="ltx_text" id="A2.T5.1.7.7.1.1">ChatUnivi (s-input)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.7.7.2">GPT-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.7.7.3">40.32%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.7.7.4">42.04%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.7.7.5">39.52%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.7.7.6">24.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.7.7.7">28.06%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.7.7.8">27.15%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.8.8.1">Human</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.8.8.2">37.65%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.8.8.3">38.59%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.8.8.4">36.46%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.8.8.5">25.73%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.8.8.6">27.40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.8.8.7">26.64%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A2.T5.1.9.9.1" rowspan="2"><span class="ltx_text" id="A2.T5.1.9.9.1.1">LLaVA-1.5 (c-input)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.9.9.2">GPT-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.9.9.3">35.77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.9.9.4">44.18%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.9.9.5">38.09%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.9.9.6">24.47%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.9.9.7">38.79%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.9.9.8">28.59%</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A2.T5.1.10.10.1">Human</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.10.10.2">36.84%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.10.10.3">41.37%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A2.T5.1.10.10.4">39.77%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.10.10.5">22.95%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.10.10.6">39.82%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T5.1.10.10.7">29.18%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Prompt Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we provide all the prompts used in our paper, including those used to merge questions and answers from Daily-life videos into a single description, prompts for MLLMs to generate descriptions corresponding to image sequences, and prompts for extracting object and behavior keywords from both human-annotated and AI-generated descriptions. The detailed prompts are showm in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A3.T6" title="Table 6 ‣ Appendix C Prompt Details ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">6</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>All prompts used in our paper.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T6.1.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="A3.T6.1.1.1.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.1.1.1.1">Prompt</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T6.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="A3.T6.1.2.1.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.2.1.1.1">Task: Rewrite questions and answers into a single paragraph</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="A3.T6.1.3.2.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.3.2.1.1">Image: &lt;Image sequence&gt;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.4.3">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.4.3.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.4.3.1.1">Text: &lt;Write a description for this image based on the following questions and answers in one paragraph. Please remember that some objects or actions in the following questions and answers may not be included in the images. Please do not include the excluded items in your description.
Here are the questions and answers: Question: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.1">Question 1</span>} Answer: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.2">Answer 1</span>} Question: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.3">Question 2</span>} Answer: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.4">Answer 2</span>} … Question: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.5">Question n</span>} Answer: {<span class="ltx_text ltx_font_bold" id="A3.T6.1.4.3.1.1.6">Answer n</span>}&gt;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.5.4">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="A3.T6.1.5.4.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.5.4.1.1">Task: Generate description for the given image sequence</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.6.5">
<td class="ltx_td ltx_align_justify ltx_border_t" id="A3.T6.1.6.5.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.6.5.1.1">Image: &lt;Image sequence&gt;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.7.6">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.7.6.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.7.6.1.1">Text: &lt;Write a description for the given image sequence in a single paragraph, what is happening in this episode?&gt;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.8.7">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="A3.T6.1.8.7.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.8.7.1.1">Task: Extract object and behavior keywords</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.9.8">
<td class="ltx_td ltx_align_justify ltx_border_t" id="A3.T6.1.9.8.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.9.8.1.1">Text: &lt;I will provide you two paragraphs. The first paragraph is human-composed and the second paragraph is generated by AI models. I want to evaluate the hallucination in the second paragraph. Please extract the object and action words or phrases from the following text. The objects should have a tangible meaning and consist of no more than two words; non-tangible objects should not be extracted. The action words or phrases should only relate to the extracted objects. Also, you must convert the corresponding actions to their complete root form. Then, for the final answer, please examine 4 lists and must transfer the synonyms in 4 lists into the same word. Please directly output the final object and action lists in two paragraphs, respectively as in the form in the example below without any justifications or intermediate steps.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.10.9">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.10.9.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.10.9.1.1">Here is an example:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.11.10">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.11.10.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.11.10.1.1">1. The sequence of images captures a dog’s cautious interaction with a metal toy inside a house. The dog appears wary and maintains a distance from the unfamiliar object, barking to express its disapproval and possibly intimidation. As the toy moves, the dog’s reaction is to bark and lean backward, showing a clear sign of being unsettled by the toy’s motion. When the toy momentarily ceases movement, the dog also stops, remaining alert and attentive. At the end of the image, when the toy comes to a halt, the dog looks up, still processing the strange encounter with the inanimate object.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.12.11">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.12.11.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.12.11.1.1">2. The image is a collage of multiple pictures featuring two dogs playing with a toy alligator. The dogs are in various positions, with some of them standing on the toy alligator, while others are interacting with it in different ways. The collage captures the dogs’ playfulness and excitement as they engage with the toy alligator.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.13.12">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.13.12.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.13.12.1.1">The lists are</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.14.13">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.14.13.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.14.13.1.1">Object list 1: [dog, toy, house]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.15.14">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.15.14.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.15.14.1.1">Action list 1: [interaction, bark, express intimidation, move, lean backward, stop, look up]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.16.15">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.16.15.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.16.15.1.1">Object list 2: [dog, toy]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.17.16">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.17.16.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.17.16.1.1">Action list 2: [play, stand, interaction]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.18.17">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.18.17.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.18.17.1.1">Here is the paragraphs:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.19.18">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.19.18.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.19.18.1.1">1. {<span class="ltx_text ltx_font_bold" id="A3.T6.1.19.18.1.1.1">Human-annotated description</span>}</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.20.19">
<td class="ltx_td ltx_align_justify" id="A3.T6.1.20.19.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.20.19.1.1">2. {<span class="ltx_text ltx_font_bold" id="A3.T6.1.20.19.1.1.1">AI-generated description</span>}</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.21.20">
<td class="ltx_td ltx_align_justify ltx_border_bb" id="A3.T6.1.21.20.1">
<p class="ltx_p ltx_align_top" id="A3.T6.1.21.20.1.1">The lists are:&gt;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Case Study</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we present failure reasoning cases of different domains (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4.F8" title="Figure 8 ‣ Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">8</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4.F23" title="Figure 23 ‣ Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">23</span></a>), with specific reasons for failure detailed in the captions of each figure.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="A4.F8.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case1.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>A sample of failure reasoning case in Daily-life domain, we highlight the hallucination parts in yellow. Failure reason: co-occurrence behavior and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="454" id="A4.F9.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case2.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A sample of failure reasoning case in Daily-life domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, and correlation between object hallucination and behavioral hallucination.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="A4.F10.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case3.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>A sample of failure reasoning case in Daily-life domain, we highlight the hallucination parts in yellow. Failure reason: lack of common sense and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="211" id="A4.F11.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case4.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>A sample of failure reasoning case in Daily-life domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and co-occurrence behavior.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="609" id="A4.F12.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case5.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A sample of failure reasoning case in Daily-life domain, we highlight the hallucination parts in yellow. Failure reason: Snowball. In this case, we observe that in addition to the significant behavioral hallucinations caused by Snowball effect mentioned in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#S3.SS2" title="3.2 Analysis of Failure Reasoning ‣ 3 Experiments ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">3.2</span></a>, another result of Snowball is that LVLMs may not fully describe all episodes in an image sequence. That is, after a behavioral hallucination occurs, the LVLM might assume the episode has ended and stop describing. For instance, in this case, the LVLM stopped describing after mentioning the child reaching the living room and the adult leaving, without continuing to describe the child pushing the box back along the hallway.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="427" id="A4.F13.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case6.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: co-occurrence behavior.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="493" id="A4.F14.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case7.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: Snowball. This case effectively demonstrates the lack of LVLM’s reasoning ability in image sequence comprehension. In the first image, the robotic arm indeed appears to be moving towards the cube, but from the second image, the arm lowers and moves towards the disc-shaped object. The LVLM failed to infer this behavior from the first two images and based its subsequent description solely on the understanding in the first image, leading to a Snowball effect.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="543" id="A4.F15.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case8.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: co-occurrence behavior and Snowball. This case also reflects another outcome of the Snowball effect that we mentioned in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.10529v2#A4.F12" title="Figure 12 ‣ Appendix D Case Study ‣ Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences"><span class="ltx_text ltx_ref_tag">12</span></a>. After assuming that the robotic arm is cooking, the LVLM do not continue to describe the behavior of the robotic arm moving the pot from the right stove to the left.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="437" id="A4.F16.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case9.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="446" id="A4.F17.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case10.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="480" id="A4.F18.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case11.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>A sample of failure reasoning case in Robotics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="335" id="A4.F19.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case12.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>A sample of failure reasoning case in Comics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="A4.F20.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case13.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>A sample of failure reasoning case in Comics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="417" id="A4.F21.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case14.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>A sample of failure reasoning case in Comics domain, we highlight the hallucination parts in yellow. Failure reason: Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="A4.F22.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case15.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>A sample of failure reasoning case in Comics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="348" id="A4.F23.g1" src="./Mementos_ A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences_files/case16.jpeg" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>A sample of failure reasoning case in Comics domain, we highlight the hallucination parts in yellow. Failure reason: object hallucination, correlation between object hallucination and behavioral hallucination, and Snowball.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
          <div class="ltx_page_logo">
              Generated by
              <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                  <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                      L
                      <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                      T
                      <span style="position: relative; bottom: -0.4ex;">E</span>
                  </span>
                  <span class="ltx_font_smallcaps">xml</span>
                  <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
              </a>
          </div></div><footer id="footer" class="ltx_document">
          <div class="keyboard-glossary">
              <h2>Instructions for reporting errors</h2>
              <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
              <ul>
                  <li>Click the "Report Issue" button.</li>
                  <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                  <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                  <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
              </ul>
              <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
              <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
          </div>
      </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none;">Report Issue for Selection</button></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 4);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 3px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #F5F7F9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #F6F6F6;
  --float-ball-more-button-background-color: #FFFFFF;
  --float-ball-more-button-svg-color: #6C6F73;
  color-scheme: light;
  --service-bg-hover:#F7FAFF;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --float-ball-more-button-background-color: #191919;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
    color-scheme: dark;
    --service-bg-hover:#22292F;
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --float-ball-more-button-background-color: #191919;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  color-scheme: dark;
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 1130px;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}

@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 992px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);

}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --popup-trial-pro-background-color: #222222;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
    --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
    --service-select-border-color: #2c2c2c;
    --service-select-selected-background-color: #333333;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.translate-service {
  color: var(--text-black-2);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

.widget-item {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 59px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-container {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 30px;
  width: 100%;
  margin-bottom: 4px;
}

.widget-title-container {
  display: flex;
  align-items: flex-start;
  justify-content: center;
  height: 24px;
  width: 100%;
  padding-bottom: 4px;
}

.widget-icon {
  margin-bottom: 4px;
  display: flex;
  justify-content: center;
}

.widget-title {
  color: var(--text-gray-6);
  font-size: 12px;
  text-align: center;
  width: 100%;
  font-weight: 400;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding: 0 2px 2px;
}

.widget-item svg {
  fill: var(--text-gray-2);
}

.setting svg {
  fill: var(--text-gray-6);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: 0;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: 100%;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
}
.more-container {
  position: relative;
}
.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  z-index: 2147483647;
  top: 335px;
  width: 56px;
  display: flex;
  flex-direction: column;
  display: none;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ED6D8F;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-50%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 16px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-right: 8px;
}

.imt-fb-more-button {
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}

/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 12px 0 0 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
}

.imt-manga-feedback {
  cursor: pointer;
  margin: 9px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 372px;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 16px auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " dir="ltr" style="z-index: 2147483647; pointer-events: none; top: 911px; display: flex;"><div title="关闭悬浮球" class="btn-animate" style="transform: translateX(100%); padding: 4px; cursor: pointer;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-btn  right btn-animate " dir="ltr" style="transform: translateX(15px); opacity: 0.7;"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg hidden="true" class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg></div></div></div></div><div hidden="" class="imt-manga-button imt-no-events btn-animate " id="manga-button" style="transform: translateX(8px);"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg><svg hidden="true" class="imt-manga-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg><svg class="imt-float-ball-loading" hidden="true" width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin: 9px;"><path d="M9.42859 0C9.84288 0 10.1929 0.387143 10.1929 0.847143V3.99429C10.1929 4.45429 9.84431 4.84143 9.42859 4.84143C9.01431 4.84143 8.66431 4.45571 8.66431 3.99429V0.847143C8.66431 0.387143 9.01288 0 9.42859 0Z" fill="#E9E9E9"></path><path d="M14.1301 1.38877C14.5158 1.62591 14.6301 2.12163 14.4258 2.52305L12.9515 5.19448C12.901 5.28714 12.8325 5.36876 12.75 5.43455C12.6675 5.50035 12.5727 5.54898 12.4712 5.5776C12.3696 5.60621 12.2634 5.61424 12.1586 5.60119C12.0539 5.58814 11.9529 5.55429 11.8615 5.50163C11.6787 5.38432 11.5468 5.20237 11.4923 4.9921C11.4377 4.78184 11.4645 4.55874 11.5672 4.36734L13.0415 1.69591C13.2686 1.29448 13.7443 1.15305 14.1301 1.38877Z" fill="#989697"></path><path d="M17.4685 4.75707C17.5813 4.95451 17.6123 5.18824 17.5549 5.40825C17.4975 5.62826 17.3563 5.81705 17.1614 5.93422L14.4971 7.52564C14.0971 7.76993 13.6014 7.62422 13.3657 7.20707C13.2532 7.00994 13.2222 6.77667 13.2793 6.55702C13.3365 6.33737 13.4771 6.14874 13.6714 6.03136L16.3357 4.43993C16.7371 4.21993 17.2557 4.34136 17.4685 4.7585V4.75707Z" fill="#9B999A"></path><path d="M18.8572 9.42835C18.8572 9.84263 18.47 10.1926 18.01 10.1926H14.8629C14.4029 10.1926 14.0157 9.84406 14.0157 9.42835C14.0157 9.01406 14.4029 8.66406 14.8629 8.66406H18.01C18.47 8.66406 18.8572 9.01263 18.8572 9.42835Z" fill="#A3A1A2"></path><path d="M17.4686 14.1303C17.3515 14.3134 17.1697 14.4455 16.9594 14.5003C16.7491 14.5552 16.5259 14.5286 16.3343 14.426L13.6629 12.9517C13.5702 12.9012 13.4886 12.8327 13.4228 12.7503C13.357 12.6678 13.3084 12.573 13.2798 12.4714C13.2512 12.3698 13.2431 12.2636 13.2562 12.1589C13.2692 12.0542 13.3031 11.9532 13.3558 11.8617C13.4731 11.6789 13.655 11.547 13.8653 11.4925C14.0755 11.4379 14.2986 11.4647 14.49 11.5674L17.1615 13.0417C17.5629 13.2689 17.7043 13.7446 17.4686 14.1303Z" fill="#ABA9AA"></path><path opacity="0.7" d="M14.1 17.4686C13.9026 17.5814 13.6689 17.6124 13.4489 17.555C13.2288 17.4976 13.04 17.3564 12.9229 17.1615L11.3315 14.4972C11.0872 14.0972 11.2329 13.6015 11.65 13.3658C11.8472 13.2533 12.0804 13.2224 12.3001 13.2795C12.5197 13.3366 12.7084 13.4773 12.8257 13.6715L14.4172 16.3358C14.6372 16.7372 14.5157 17.2558 14.0986 17.4686H14.1Z" fill="#B2B2B2"></path><path opacity="0.6" d="M9.42859 18.8571C9.01431 18.8571 8.66431 18.4699 8.66431 18.0099V14.8628C8.66431 14.4028 9.01288 14.0156 9.42859 14.0156C9.84288 14.0156 10.1929 14.4028 10.1929 14.8628V18.0099C10.1929 18.4699 9.84431 18.8571 9.42859 18.8571Z" fill="#BAB8B9"></path><path opacity="0.5" d="M4.72717 17.4685C4.5441 17.3514 4.41195 17.1696 4.35713 16.9593C4.30231 16.749 4.32885 16.5258 4.43145 16.3342L5.90574 13.6628C5.95622 13.5701 6.02472 13.4885 6.1072 13.4227C6.18969 13.3569 6.2845 13.3083 6.38606 13.2797C6.48762 13.251 6.59387 13.243 6.69857 13.2561C6.80327 13.2691 6.90431 13.303 6.99574 13.3556C7.38145 13.5914 7.49431 14.0885 7.29002 14.4899L5.81574 17.1614C5.5886 17.5628 5.11288 17.7042 4.72717 17.4685Z" fill="#C2C0C1"></path><path opacity="0.4" d="M1.38862 14.1002C1.27584 13.9027 1.24483 13.669 1.30223 13.449C1.35964 13.229 1.50089 13.0402 1.69576 12.923L4.36004 11.3316C4.76004 11.0873 5.25576 11.233 5.49147 11.6502C5.60393 11.8473 5.63491 12.0806 5.5778 12.3002C5.52069 12.5199 5.38 12.7085 5.18576 12.8259L2.52004 14.4173C2.12004 14.6373 1.60004 14.5159 1.38862 14.0987V14.1002Z" fill="#CBCBCB"></path><path d="M0 9.42835C0 9.01406 0.387143 8.66406 0.847143 8.66406H3.99429C4.45429 8.66406 4.84143 9.01263 4.84143 9.42835C4.84143 9.84263 4.45571 10.1926 3.99429 10.1926H0.847143C0.387143 10.1926 0 9.84406 0 9.42835Z" fill="#D2D2D2"></path><path opacity="0.2" d="M1.38852 4.72705C1.50561 4.54398 1.68746 4.41183 1.89774 4.35701C2.10803 4.30219 2.33125 4.32873 2.52281 4.43133L5.19424 5.90562C5.28689 5.9561 5.36851 6.0246 5.43431 6.10708C5.5001 6.18957 5.54874 6.28438 5.57735 6.38594C5.60597 6.48749 5.61399 6.59375 5.60094 6.69845C5.5879 6.80315 5.55405 6.90419 5.50138 6.99562C5.38407 7.17844 5.20212 7.31029 4.99186 7.36484C4.78159 7.4194 4.55849 7.39263 4.3671 7.2899L1.69567 5.81562C1.29424 5.58847 1.15281 5.11276 1.38852 4.72705Z" fill="#DADADA"></path><path d="M4.75719 1.38849C4.95463 1.27571 5.18837 1.24471 5.40838 1.30211C5.62838 1.35952 5.81718 1.50077 5.93434 1.69564L7.52577 4.35992C7.77005 4.75992 7.62434 5.25564 7.20719 5.49135C7.01006 5.60381 6.77679 5.63479 6.55714 5.57768C6.33749 5.52056 6.14886 5.37988 6.03148 5.18564L4.44005 2.51992C4.22005 2.11992 4.34148 1.59992 4.75862 1.38849H4.75719Z" fill="#E2E2E2"></path></svg><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.6861 1L15.2353 4.54635V7.11765V14.6471V15.5882C15.2353 15.9627 15.0866 16.3217 14.8218 16.5865C14.557 16.8513 14.198 17 13.8235 17H4.41176C4.03734 17 3.67825 16.8513 3.4135 16.5865C3.14874 16.3217 3 15.9627 3 15.5882V14.6471V7.11765V2.41176C3 2.03734 3.14874 1.67825 3.4135 1.4135C3.67825 1.14874 4.03734 1 4.41176 1H11.6861ZM11.8692 3.17882V4.74212H13.4334L11.8692 3.17882ZM4.41171 15.5882V14.647V2.41176H10.4574L10.4578 6.15341H13.8235V14.647V15.5882H4.41171ZM12.7739 7.51746H5.46094V8.6155H12.7739V7.51746ZM5.46094 9.98805H12.7739V11.0861H5.46094V9.98805ZM9.5127 12.36H5.46094V13.458H9.5127V12.36Z" fill="#666666"></path></svg></div></div></div><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.55741 1L10.0685 1.00329C10.8482 1.00471 11.4802 1.63624 11.4812 2.41647L11.4821 2.82588C11.9687 3.0278 12.4297 3.28671 12.8553 3.59718L13.1913 3.40329C13.516 3.21676 13.9013 3.1665 14.2629 3.26352C14.6246 3.36055 14.933 3.59695 15.1207 3.92094L16.3795 6.09365C16.5601 6.40546 16.6149 6.7744 16.5328 7.12523C16.4507 7.47606 16.2378 7.78235 15.9376 7.98165L15.8609 8.02871L15.5235 8.22353C15.5819 8.76273 15.5736 9.30708 15.4986 9.84424L15.7372 9.98259C16.0496 10.1631 16.2812 10.4561 16.3848 10.8017C16.4884 11.1472 16.456 11.5193 16.2944 11.8419L16.2553 11.9153L15.076 13.9576C14.8955 14.27 14.6025 14.5017 14.2569 14.6053C13.9113 14.7088 13.5392 14.6765 13.2167 14.5148L13.1433 14.4753L12.8172 14.2871C12.4074 14.5817 11.9651 14.8283 11.4991 15.0221L11.4995 15.5831C11.5 15.9434 11.3629 16.2904 11.1163 16.5532C10.8697 16.816 10.5321 16.9748 10.1725 16.9972L10.0831 17L7.57153 16.9967C7.19697 16.9961 6.83793 16.847 6.57312 16.5821C6.30831 16.3172 6.15932 15.9581 6.15883 15.5835L6.15788 14.9073C5.76852 14.7244 5.39771 14.5044 5.05059 14.2504L4.44918 14.5967C4.12448 14.7834 3.73902 14.8337 3.37726 14.7367C3.01551 14.6397 2.70698 14.4032 2.5193 14.0791L1.26047 11.9064C1.07996 11.5945 1.02522 11.2255 1.10742 10.8747C1.18962 10.5238 1.40257 10.2176 1.70283 10.0184L1.77906 9.97129L2.3913 9.61835C2.34424 9.17129 2.34188 8.71765 2.38706 8.26494L1.70753 7.87247C1.39506 7.69207 1.16331 7.39911 1.05965 7.05351C0.955998 6.70791 0.988275 6.33577 1.14989 6.01318L1.18941 5.93976L2.36871 3.89741C2.54919 3.58502 2.84218 3.35337 3.18777 3.2498C3.53336 3.14624 3.90547 3.17859 4.228 3.34023L4.30141 3.37976L4.89436 3.72188C5.28027 3.42082 5.69854 3.1637 6.14141 2.95529L6.14047 2.41694C6.14001 2.05657 6.27707 1.7096 6.52367 1.44681C6.77028 1.18403 7.10786 1.02523 7.46753 1.00282L7.55741 1ZM7.55553 2.41506L7.55694 3.85271L6.74377 4.23576C6.39553 4.39906 6.06706 4.60094 5.764 4.83718L5.01247 5.424L3.62941 4.62494L3.59365 4.60518L2.41483 6.64753L3.88636 7.49694L3.79506 8.40612C3.75968 8.7598 3.76078 9.11619 3.79836 9.46965L3.8953 10.3854L2.48494 11.1976L3.7433 13.3704L5.14377 12.5647L5.88636 13.1087C6.15997 13.309 6.45231 13.4823 6.7593 13.6264L7.57106 14.008L7.57388 15.5816L10.0845 15.5849L10.0831 14.0791L10.9555 13.7158C11.3216 13.5635 11.6689 13.3698 11.9908 13.1384L12.7329 12.6047L13.8506 13.2499L15.0289 11.2075L13.9654 10.592L14.0972 9.64847C14.1561 9.22659 14.1628 8.79904 14.1169 8.37553L14.0181 7.45882L15.1555 6.80235L13.8967 4.62965L12.7645 5.28235L12.0214 4.74024C11.686 4.4956 11.3229 4.29152 10.9395 4.13224L10.0689 3.77082L10.0666 2.41835L7.55553 2.41506ZM10.3715 6.47624C11.0214 6.85201 11.4955 7.47036 11.6898 8.19547C11.8841 8.92058 11.7827 9.69316 11.4078 10.3435C11.2223 10.6654 10.9752 10.9476 10.6805 11.1739C10.3859 11.4002 10.0495 11.5662 9.69068 11.6623C9.33183 11.7585 8.95754 11.7829 8.58923 11.7343C8.22092 11.6856 7.86582 11.5648 7.54424 11.3788C6.89445 11.003 6.4204 10.3846 6.2262 9.65948C6.032 8.93438 6.13352 8.16184 6.50847 7.51153C6.69395 7.18963 6.94107 6.90746 7.23571 6.68117C7.53034 6.45488 7.86671 6.28891 8.22556 6.19275C8.58441 6.09659 8.9587 6.07213 9.32701 6.12077C9.69532 6.16942 10.0504 6.29021 10.372 6.47624H10.3715ZM7.73388 8.21835C7.54638 8.54388 7.49567 8.9305 7.5929 9.29336C7.69012 9.65623 7.92733 9.96571 8.25247 10.1539C8.41305 10.2468 8.59037 10.3071 8.77429 10.3314C8.9582 10.3557 9.14511 10.3435 9.32431 10.2956C9.50351 10.2476 9.67149 10.1647 9.81864 10.0517C9.96579 9.93877 10.0892 9.7979 10.1819 9.63718C10.5588 8.98353 10.356 8.15435 9.73435 7.74494L9.66377 7.70118L9.59035 7.66165C9.26834 7.49988 8.89663 7.46742 8.55145 7.57093C8.20626 7.67444 7.91375 7.90608 7.73388 8.21835Z" fill="#666666"></path></svg></div></div></div></div><div hidden="" class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div></div></div></template></div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>