<!DOCTYPE html>
<!-- saved from url=(0033)https://arxiv.org/html/2312.14233 -->
<html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>VCoder: Versatile Vision Encoders for Multimodal Large Language Models</title>
<!--Generated on Thu Dec 21 18:40:23 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/bootstrap.bundle.min.js.download"></script>
<script src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/html2canvas.min.js.download"></script>
<script src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/addons.js.download"></script>
<script src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/feedbackOverlay.js.download"></script>
<!--<base href="/html/2312.14233v1/">--><base href="."><link rel="stylesheet" href="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-toast {
  display: flex;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  right: 0;
  top: 1%;
  width: fit-content;
  padding: 12px 20px;
  margin: auto;
  overflow: auto;
  background: #fef6f9;
  box-shadow: 0px 4px 10px 0px rgba(0, 10, 30, 0.06);
  font-size: 15px;
  border-radius: 8px;
  color: #333;
}

.immersive-translate-toast-content {
  display: flex;
  flex-direction: row;
  align-items: center;
}

.immersive-translate-toast-hidden {
  margin: 0 20px 0 72px;
  text-decoration: underline;
  cursor: pointer;
}

.immersive-translate-toast-close {
  color: #666666;
  font-size: 20px;
  font-weight: bold;
  padding: 0 10px;
  cursor: pointer;
}

@media screen and (max-width: 768px) {
  .immersive-translate-toast {
    top: 0;
    padding: 12px 0px 0 10px;
  }
  .immersive-translate-toast-content {
    flex-direction: column;
    text-align: center;
  }
  .immersive-translate-toast-hidden {
    margin: 10px auto;
  }
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border: 1px solid #888;
  border-radius: 10px;
  width: 80%;
  max-width: 270px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 50% auto !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  word-break: break-all;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  color: black;
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 16px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #007bff;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}
</style><style type="text/css">.lf-progress {
  -webkit-appearance: none;
  -moz-apperance: none;
  width: 100%;
  /* margin: 0 10px; */
  height: 4px;
  border-radius: 3px;
  cursor: pointer;
}
.lf-progress:focus {
  outline: none;
  border: none;
}
.lf-progress::-moz-range-track {
  cursor: pointer;
  background: none;
  border: none;
  outline: none;
}
.lf-progress::-webkit-slider-thumb {
  -webkit-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-moz-range-thumb {
  -moz-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: transparent;
  border-color: transparent;
  color: transparent;
}
.lf-progress::-ms-fill-lower {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-fill-upper {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-thumb {
  border: 0;
  height: 15px;
  width: 15px;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress:focus::-ms-fill-lower {
  background: #ccc;
}
.lf-progress:focus::-ms-fill-upper {
  background: #ccc;
}
.lf-player-container :focus {
  outline: 0;
}
.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  opacity: 1;
  visibility: visible;
  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-player-btn-container {
  display: flex;
  align-items: center;
}
.lf-player-btn {
  cursor: pointer;
  fill: #999;
  width: 14px;
}

.lf-player-btn.active {
  fill: #555;
}

.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  background-color: #ffffff;
  opacity: 1;

  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
  padding: 10px;
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-arrow {
  position: absolute;
  z-index: -1;
  content: '';
  bottom: -9px;
  border-style: solid;
  border-width: 10px 10px 0px 10px;
}

.lf-left-align,
.lf-left-align .lfarrow {
  left: 0;
  right: unset;
}

.lf-right-align,
.lf-right-align .lf-arrow {
  right: 0;
  left: unset;
}

.lf-text-input {
  border: 1px #ccc solid;
  border-radius: 5px;
  padding: 3px;
  width: 60px;
  margin: 0;
}

.lf-color-picker {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  height: 90px;
}

.lf-color-selectors {
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}

.lf-color-component {
  display: flex;
  flex-direction: row;
  font-size: 12px;
  align-items: center;
  justify-content: center;
}

.lf-color-component strong {
  width: 40px;
}

.lf-color-component input[type='range'] {
  margin: 0 0 0 10px;
}

.lf-color-component input[type='number'] {
  width: 50px;
  margin: 0 0 0 10px;
}

.lf-color-preview {
  font-size: 12px;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding-left: 5px;
}

.lf-preview {
  height: 60px;
  width: 60px;
}

.lf-popover-snapshot {
  width: 150px;
}
.lf-popover-snapshot h5 {
  margin: 5px 0 10px 0;
  font-size: 0.75rem;
}
.lf-popover-snapshot a {
  display: block;
  text-decoration: none;
}
.lf-popover-snapshot a:before {
  content: '⥼';
  margin-right: 5px;
}
.lf-popover-snapshot .lf-note {
  display: block;
  margin-top: 10px;
  color: #999;
}
.lf-player-controls > div {
  margin-right: 5px;
  margin-left: 5px;
}
.lf-player-controls > div:first-child {
  margin-left: 0px;
}
.lf-player-controls > div:last-child {
  margin-right: 0px;
}
</style><style type="text/css">.lf-progress {
  -webkit-appearance: none;
  -moz-apperance: none;
  width: 100%;
  /* margin: 0 10px; */
  height: 4px;
  border-radius: 3px;
  cursor: pointer;
}
.lf-progress:focus {
  outline: none;
  border: none;
}
.lf-progress::-moz-range-track {
  cursor: pointer;
  background: none;
  border: none;
  outline: none;
}
.lf-progress::-webkit-slider-thumb {
  -webkit-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-moz-range-thumb {
  -moz-appearance: none !important;
  height: 13px;
  width: 13px;
  border: 0;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress::-ms-track {
  width: 100%;
  height: 3px;
  cursor: pointer;
  background: transparent;
  border-color: transparent;
  color: transparent;
}
.lf-progress::-ms-fill-lower {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-fill-upper {
  background: #ccc;
  border-radius: 3px;
}
.lf-progress::-ms-thumb {
  border: 0;
  height: 15px;
  width: 15px;
  border-radius: 50%;
  background: #0fccce;
  cursor: pointer;
}
.lf-progress:focus::-ms-fill-lower {
  background: #ccc;
}
.lf-progress:focus::-ms-fill-upper {
  background: #ccc;
}
.lf-player-container :focus {
  outline: 0;
}
.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  opacity: 1;
  visibility: visible;
  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-player-btn-container {
  display: flex;
  align-items: center;
}
.lf-player-btn {
  cursor: pointer;
  fill: #999;
  width: 14px;
}

.lf-player-btn.active {
  fill: #555;
}

.lf-popover {
  position: relative;
}

.lf-popover-content {
  display: inline-block;
  position: absolute;
  background-color: #ffffff;
  opacity: 1;

  transform: translate(0, -10px);
  box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.26);
  transition: all 0.3s cubic-bezier(0.75, -0.02, 0.2, 0.97);
  padding: 10px;
}

.lf-popover-content.hidden {
  opacity: 0;
  visibility: hidden;
  transform: translate(0, 0px);
}

.lf-arrow {
  position: absolute;
  z-index: -1;
  content: '';
  bottom: -9px;
  border-style: solid;
  border-width: 10px 10px 0px 10px;
}

.lf-left-align,
.lf-left-align .lfarrow {
  left: 0;
  right: unset;
}

.lf-right-align,
.lf-right-align .lf-arrow {
  right: 0;
  left: unset;
}

.lf-text-input {
  border: 1px #ccc solid;
  border-radius: 5px;
  padding: 3px;
  width: 60px;
  margin: 0;
}

.lf-color-picker {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  height: 90px;
}

.lf-color-selectors {
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}

.lf-color-component {
  display: flex;
  flex-direction: row;
  font-size: 12px;
  align-items: center;
  justify-content: center;
}

.lf-color-component strong {
  width: 40px;
}

.lf-color-component input[type='range'] {
  margin: 0 0 0 10px;
}

.lf-color-component input[type='number'] {
  width: 50px;
  margin: 0 0 0 10px;
}

.lf-color-preview {
  font-size: 12px;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: space-between;
  padding-left: 5px;
}

.lf-preview {
  height: 60px;
  width: 60px;
}

.lf-popover-snapshot {
  width: 150px;
}
.lf-popover-snapshot h5 {
  margin: 5px 0 10px 0;
  font-size: 0.75rem;
}
.lf-popover-snapshot a {
  display: block;
  text-decoration: none;
}
.lf-popover-snapshot a:before {
  content: '⥼';
  margin-right: 5px;
}
.lf-popover-snapshot .lf-note {
  display: block;
  margin-top: 10px;
  color: #999;
}
.lf-player-controls > div {
  margin-right: 5px;
  margin-left: 5px;
}
.lf-player-controls > div:first-child {
  margin-left: 0px;
}
.lf-player-controls > div:last-child {
  margin-right: 0px;
}
</style></head>
<body data-new-gr-c-s-check-loaded="14.1104.0" data-gr-ext-installed="" data-new-gr-c-s-loaded="14.1104.0"><header class="mob_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
          <img alt="logo" class="logomark" role="presentation" width="100" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/arxiv-logomark-small-white.svg">
          <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
  
      <!--TOC, dark mode, links-->
      <div class="html-header-nav">
        <!--back to abstract-->
        
          <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2312.14233">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
              <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
          </svg>
          </a>
        <!--dark mode-->
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
          <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
          </label>
          <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
          <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
          </label>
        </a>
        <!--nav-->
        <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
          <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
        </button>
      </div>
      </header><header class="desktop_header">
      <div class="html-header-logo">
        <a href="https://arxiv.org/">
            <img alt="logo" class="logo" role="presentation" width="100" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/arxiv-logo-one-color-white.svg">
            <span class="sr-only">Back to arXiv</span>
        </a>
      </div>
      <div class="html-header-message" role="banner">
          <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
          </p>
      </div>
      <nav class="html-header-nav">
        <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
        <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2312.14233v1/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2312.14233">Back to Abstract</a>
        <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2312.14233" target="_blank">Download PDF</a>
        <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

        <div id="listIcon" type="button" class="hide">
            <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
            <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
            </svg>
        </div>
        <div id="arrowIcon" type="button">
            <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
            <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
            </svg>
        </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S1" title="1 Introduction ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2" title="2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2.SS1" title="2.1 Visual Perception ‣ 2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Visual Perception</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2.SS2" title="2.2 Visual Understanding with LLMs ‣ 2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Visual Understanding with LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2.SS3" title="2.3 Perception Hallucination in MLLMs ‣ 2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Perception Hallucination in MLLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3" title="3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Object Identification with MLLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.SS1" title="3.1 COST to Identify Objects with MLLMs ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>COST to Identify Objects with MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.SS2" title="3.2 VCoder for Multimodal LLMs ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>VCoder for Multimodal LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.SS3" title="3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluating MLLMs for Object Identification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S4" title="4 Experiments ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S4.SS1" title="4.1 Implementation Details ‣ 4 Experiments ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S4.SS2" title="4.2 Main Results ‣ 4 Experiments ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S5" title="5 Object Order Perception with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Object Order Perception with MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S6" title="6 Limitations ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S7" title="7 Conclusion ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S7.SS0.SSS0.Px1" title="Acknowledgements. ‣ 7 Conclusion ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title">Acknowledgements.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A1" title="Appendix A Control Through Segmentation Map ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Control Through Segmentation Map</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2" title="Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Object Order Perception</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2.SS1" title="B.1 Obtaining Ground Truth ‣ Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Obtaining Ground Truth</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2.SS2" title="B.2 Depth Score ‣ Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Depth Score</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A3" title="Appendix C Object Counts in COST Dataset ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Object Counts in COST Dataset</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: scalerel</li>
<li>failed: silence</li>
<li>failed: quoting</li>
<li>failed: epic</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2312.14233v1 [cs.CV] 21 Dec 2023</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VCoder: Versatile Vision Encoders for Multimodal Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jitesh Jain<sup class="ltx_sup" id="id1.1.id1">1</sup>  Jianwei Yang<sup class="ltx_sup" id="id2.2.id2">2</sup>  Humphrey Shi<sup class="ltx_sup" id="id3.3.id3">1,3</sup>
<br class="ltx_break"><sup class="ltx_sup" id="id4.4.id4"><span class="ltx_text" id="id4.4.id4.1" style="font-size:90%;">1</span></sup><span class="ltx_text" id="id5.5.id5" style="font-size:90%;">SHI Labs @ Georgia Tech  <sup class="ltx_sup" id="id5.5.id5.1">2</sup>Microsoft Research</span>  <sup class="ltx_sup" id="id6.6.id6"><span class="ltx_text" id="id6.6.id6.1" style="font-size:90%;">3</span></sup><span class="ltx_text" id="id7.7.id7" style="font-size:90%;">Picsart AI Research (PAIR)

<br class="ltx_break"><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_bold" href="https://github.com/SHI-Labs/VCoder" title="">https://github.com/SHI-Labs/VCoder</a>
</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id8.id1">Humans possess the remarkable skill of Visual Perception, the ability to see and understand the seen, helping them make sense of the visual world and, in turn, reason. Multimodal Large Language Models (MLLM) have recently achieved impressive performance on vision-language tasks ranging from visual question-answering and image captioning to visual reasoning and image generation. However, when prompted to identify or count (perceive) the entities in a given image, existing MLLM systems fail. Working towards developing an accurate MLLM system for perception and reasoning, we propose using Versatile vision enCoders (<span class="ltx_text ltx_font_bold" id="id8.id1.1">VCoder</span>) as perception eyes for Multimodal LLMs. We feed the VCoder with perception modalities such as segmentation or depth maps, improving the MLLM’s perception abilities. Secondly, we leverage the images from COCO and outputs from off-the-shelf vision perception models to create our COCO Segmentation Text (<span class="ltx_text ltx_font_bold" id="id8.id1.2">COST</span>) dataset for training and evaluating MLLMs on the object perception task. Thirdly, we introduce metrics to assess the object perception abilities in MLLMs on our COST dataset. Lastly, we provide extensive experimental evidence proving the VCoder’s improved object-level perception skills over existing Multimodal LLMs, including GPT-4V. We open-source our dataset, code, and models to promote research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<span class="ltx_ERROR undefined" id="S1.p1.1">{quoting}</span>
<p class="ltx_p" id="S1.p1.2">[
indentfirst=true,
leftmargin=rightmargin=]<span class="ltx_text ltx_font_italic" id="S1.p1.2.1">
‘Perception is the soil; reasoning, the seed. Without fertile ground, the seed cannot flourish.’
 &nbsp;&nbsp;&nbsp;<span class="ltx_text" id="S1.p1.2.1.1"></span> <span class="ltx_text ltx_font_upright" id="S1.p1.2.1.2">(GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, 2023)</span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1125" id="S1.F1.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x1.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> (access date: Dec 16, 2023) returns impressive responses when prompted to describe complex visual scenes. However, it fails at the simple task of counting in the same scene. Our VCoder returns the correct count of people.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="700" id="S1.1.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x2.png" width="951">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.11.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F2.12.2" style="font-size:90%;">MLLMs counting and identifying objects.<span class="ltx_text ltx_font_medium" id="S1.F2.12.2.1"> As shown in the first column, GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> (access date: Dec 16, 2023) and LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> both fail at counting <span class="ltx_text ltx_font_italic" id="S1.F2.12.2.1.1">people</span>. Moreover, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> misses background entities like <span class="ltx_text ltx_font_italic" id="S1.F2.12.2.1.2">window, wall, etc.</span> and hallucinates about the presence of a handbag. VCoder can predict the <span class="ltx_text ltx_font_italic" id="S1.F2.12.2.1.3">people</span> counts and other background entities accurately except <span class="ltx_text ltx_font_italic" id="S1.F2.12.2.1.4">chairs</span>. Similarly, in the second column, GPT-4V and LLaVA-1.5 fail at counting <span class="ltx_text ltx_font_italic" id="S1.F2.12.2.1.5">chairs</span> while the VCoder matches the Oracle’s performance. Notably, all MLLMs can perceive objects accurately for a non-cluttered image in the third column, with LLaVA-1.5 failing at counting ties. Our VCoder can also accurately perform general question-answering tasks, as shown in the fourth column. We treat OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> as the Oracle for object perception.
<span class="ltx_text" id="S1.F2.12.2.1.6" style="color:#FF0000;">Red</span> text represents counting mistakes; <span class="ltx_text" id="S1.F2.12.2.1.7" style="color:#FF00FF;">pink</span> text represents hallucination; <span class="ltx_text" id="S1.F2.12.2.1.8" style="color:#0000FF;">blue</span> text represents correct object perception.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The ability to think and reason is one of the most remarkable traits that help humans function daily. Generally, understanding the environment precedes the act of thinking and reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. Following the success of ChatGPT-like instruction following AI agents&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> at language understanding and reasoning, researchers have leveraged LLMs to develop instruct frameworks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> that can understand vision and language inputs in an effort to imitate human perception and reasoning ability. We refer to such systems as Multimodal LLMs (MLLM). Although MLLMs exhibit the ability to perform complex vision-language tasks like visual captioning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, image generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, visual reasoning and grounding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, they often display sub-par performance at simple tasks like counting objects (<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>). As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, MLLMs output incorrect object counts (<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">people, chairs</span>) and hallucinate about the presence (<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">handbag, couch</span>) of certain objects when prompted to identify entities in a visual input. The perception performance is much worse when the scenes are cluttered with many entities. Consequently, a natural question arises: <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">“How to develop MLLM systems that respond to <span class="ltx_text ltx_font_bold" id="S1.p2.1.3.1">perception</span> questions accurately?”</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This work aims to improve Multimodal LLMs at the simple yet fundamental object-level perception skills, including counting. Our motivation stems from the intuition that one can only describe and reason about a visual scene with the correct understanding of the entities in the image. In our effort to develop an accurate Multimodal LLM perception system, we face three significant challenges: (i) the scarcity of a vision-language dataset focused on the object perception task; (ii) existing open-sourced Multimodal LLMs usually use the ViT from CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite> with an RGB image as input as the visual component that majorly focuses only on salient objects, and (iii) the absence of evaluation metrics to quantitatively measure Multimodal LLMs’ object perception and in particular, counting skills. We list our efforts to overcome the issues above in the following paragraphs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The contemporary vision-language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> owe their success to the availability of large-scale image-text datasets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. However, these datasets are more focused on image captioning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> and VQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> tasks, making them unfit for training Multimodal LLMs for basic perception skills like object identification and counting. To overcome the scarcity of fundamental perception-focused image-text data, we leverage images from the COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> dataset and use predictions from off-the-shelf visual perception models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> to prepare a <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">CO</span>CO <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">S</span>egmentation <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">T</span>ext (<span class="ltx_text ltx_font_bold" id="S1.p4.1.4">COST</span>) dataset comprising of question-answer pairs about the objects (background and foreground) present in each image. We provide more details in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.SS1" title="3.1 COST to Identify Objects with MLLMs ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Inspired by diffusion models that add various perception “control” or “context” images&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> as auxiliary inputs to aid image generation, we propose feeding extra perception modalities as control inputs through additional vision encoders, which we term as our Versatile vision enCoders (<span class="ltx_text ltx_font_bold" id="S1.p5.1.1">VCoder</span>). In this work, we focus on the task of object perception and leverage a segmentation map, depth map, or both as the control inputs; however, the same design can be extended to other modalities. Our VCoder projects the control inputs’ information into the LLM’s space as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.F4" title="Figure 4 ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>. We hypothesize that this added control helps the MLLM improve its object perception ability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Lastly, owing to the absence of metrics to quantify the counting ability in MLLMs, we propose computing a count score (<span class="ltx_text ltx_font_bold" id="S1.p6.1.1">CS</span>) using one-to-one matching of object words in the ground truth and MLLM’s answer. We also compute a hallucination score (<span class="ltx_text ltx_font_bold" id="S1.p6.1.2">HS</span>) based on the extra objects in the MLLM’s response that are absent from the ground truth. Similarly, we introduce a depth score (<span class="ltx_text ltx_font_bold" id="S1.p6.1.3">DS</span>) to quantify the object order prediction performance in MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Among the open-source MLLMs, we choose LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> as our base MLLM due to its impressive performance. Our extensive experimental analysis demonstrates the importance of our COST dataset and VCoder LLaVA-1.5’s improved perception ability. To summarize, our contributions are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose using extra (perception) control inputs and feeding those to a <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">V</span>ersatile en<span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.2">Coder</span> (<span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.3">VCoder</span>) for improved object perception performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce a COCO Segmentation Text (<span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">COST</span>) dataset to train and evaluate Multimodal LLM systems on the fundamental object-level perception tasks of object identification, counting, and order prediction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Furthermore, to quantify the object perception ability in MLLMs, we propose calculating a count score (<span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">CS</span>), a hallucination score (<span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.2">HS</span>) and a depth score (<span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.3">DS</span>). Our experiments show that the VCoder-adapted LLaVA-1.5 outperforms the baseline MLLMs on all metrics when validated on the COST dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Perception</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The fundamental nature of visual perception makes it a critical component in MLLM systems. The task of perception can be divided into sub-tasks, including dense prediction tasks like image segmentation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> and depth estimation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, and sparse prediction tasks like object detection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> and pose estimation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. In the deep learning era, initial methods tackled the perception task using CNN based methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite> with recent methods shifting to the use of vision transformer based architectures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>. In this work, we tackle the fundamental task of object-level perception, mainly focusing on predicting names, counts, and order of objects in an image using MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Understanding with LLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Using LLMs for vision applications is not a new concept. In a nutshell, developing Multimodal LLMs involves projecting&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> the features from a vision encoder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> to the embedding space of a language model (LLM)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, and, instruction-tuning on a vision-language dialog dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">LLaVA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> proposed a pipeline to convert existing image-text data into dialog format and then finetuned a CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite> and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> model end-to-end on their collected dataset showing one of the earliest evidence of visual-language instruction tuning. Concurrent to LLaVA, MiniGPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite> used the visual encoder from BLIP2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> and used a linear layer for projecting visual features into Vicuna’s&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> feature space. InstructBLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> open-sourced a collection of 16 different datasets covering various vision tasks like VQA, reasoning, captioning, classification, etc., and finetuned a BLIP2 model on their dataset. mPLUG-Owl&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> proposed using a vision abstractor and finetuning the vision encoder. More recently, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> proposed using an MLP as the projector and finetuned on academic instruction datasets to achieve state-of-the-art performance on various benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. Among various open-source MLLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, we chose LLaVA-1.5 as our baseline due to its superior performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="586" id="S2.F3.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x3.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F3.4.2" style="font-size:90%;">Organization of the COST dataset<span class="ltx_text ltx_font_medium" id="S2.F3.4.2.1">. We incorporate the images from COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, the questions from GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, and the segmentation outputs from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> in a question-answer format for training and evaluating MLLMs on the object identification task. We also extend COST to the object order perception task by incorporating depth map outputs from DINOv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> DPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. COST can be extended to more object-level tasks by similarly incorporating other modalities (for example, keypoint maps).</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Perception Hallucination in MLLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Since the introduction of LLMs, there has been a comprehensive study about their ability to hallucinate&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite> in the NLP community. However, the phenomenon of hallucination in Multimodal LLMs has received comparatively less attention. LRV-Instruction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> introduced a new instruction-tuning dataset containing 400k visual instructions to prevent hallucination in MLLMs and measured performance treating responses from GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> as ground truths. More recently, HallusionBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> quantitatively benchmarked various failure modes in MLLMs that lead to hallucinations based primarily on logical consistency and reasoning. Unlike these works that tried to benchmark MLLMs mainly on VQA-type tasks, this paper focuses on the object-level hallucination in MLLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The two closest works to our objective are POPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> and CHAIR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. On the one hand, POPE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> tried to measure hallucination in MLLMs using a binary “Yes”-“No” answer policy in response to questions based on the absence or presence of an object in the image. On the other hand, CHAIR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> focused on measuring hallucination in image captioning based on only words and not counts for the objects. In our work, we consider not only object words but also the corresponding count to compute an object-level count score and hallucination score.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Object Identification with MLLMs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="417" id="S3.F4.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x4.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F4.6.2" style="font-size:90%;">Adapting Multimodal LLMs for accurate object perception with VCoder.<span class="ltx_text ltx_font_medium" id="S3.F4.6.2.1"> </span>(a)<span class="ltx_text ltx_font_medium" id="S3.F4.6.2.2"> We add our VCoder as an adapter to the LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and feed perception modalities as extra control inputs for improved object perception performance. During training, we freeze the components from LLaVA-1.5 (ImCoder, MLP, and LLM) to retain the original reasoning performance. </span>(b)<span class="ltx_text ltx_font_medium" id="S3.F4.6.2.3"> Using depth map and segmentation map as the control inputs to VCoder for the object order perception task.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Suppose you are invited to a Halloween party and want to bring candies for every person at that party. You ask your friend to send you a picture (<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>) of the party room so that you can estimate the number of people and the number of candies you need to buy. In a hurry, you ask GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>: “<span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Can you count the number of people in the image?</span>”, and it responds: “<span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Yes, there are <span class="ltx_text" id="S3.p1.1.2.1" style="color:#FF0000;">ten</span> people visible in the image.</span>”. Excited, you arrive at the party with ten candies but wait, you see fourteen people! Confused, you look at the image your friend sent you, and you can count <span class="ltx_text" id="S3.p1.1.3" style="color:#0000FF;">fourteen</span> people in that image, realizing that GPT-4V fails at the simple task of counting the people in the picture. At the same time, it can accurately describe the happening of a Halloween party in the image (<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>). We refer to the phenomenon of Multimodal LLMs failing at simple visual perception tasks while succeeding at complex visual reasoning tasks as Moravec’s Paradox&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> in perception.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We hypothesize that one of the main reasons for the above phenomenon is the absence of conversations covering object identification for not only the salient objects but also the objects in the background from the instruction-tuning data for MLLMs. To overcome this issue, we prepare the COCO Segmentation Text (<span class="ltx_text ltx_font_bold" id="S3.p2.1.1">COST</span>) dataset with COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> images and create sentences using the output from an image segmentation model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> to obtain an image-text dataset to train and evaluate MLLMs for object perception MLLMs. Moreover, we also introduce a segmentation map as a control image input to the MLLM for better performance and quantify object perception performance with a count score (<span class="ltx_text ltx_font_bold" id="S3.p2.1.2">CS</span>) and a hallucination score (<span class="ltx_text ltx_font_bold" id="S3.p2.1.3">HS</span>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>COST to Identify Objects with MLLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.10">We find that image segmentation methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> can accurately identify salient (foreground objects like <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.10.10">people, cars, etc.</span>) and background objects (like <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.10.11">sky, wall, etc.</span>) in a given scene. Guided by this finding, we use images from the COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> dataset and obtain the corresponding segmentation outputs from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, a state-of-the-art image segmentation model. Next, we extract the object (class) names and counts from the segmentation outputs and convert them into a sentence form for the ground-truth answer:<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.7.7">“The objects present in the image are: [<math alttext="\text{CNT}_{\text{1}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.1.m1.1"><semantics id="S3.SS1.p1.1.1.m1.1a"><msub id="S3.SS1.p1.1.1.m1.1.1" xref="S3.SS1.p1.1.1.m1.1.1.cmml"><mtext id="S3.SS1.p1.1.1.m1.1.1.2" xref="S3.SS1.p1.1.1.m1.1.1.2a.cmml">𝐶𝑁𝑇</mtext><mtext id="S3.SS1.p1.1.1.m1.1.1.3" mathvariant="italic" xref="S3.SS1.p1.1.1.m1.1.1.3a.cmml">1</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.1.m1.1b"><apply id="S3.SS1.p1.1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.1.m1.1.1.2a.cmml" xref="S3.SS1.p1.1.1.m1.1.1.2"><mtext id="S3.SS1.p1.1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.1.m1.1.1.2">𝐶𝑁𝑇</mtext></ci><ci id="S3.SS1.p1.1.1.m1.1.1.3a.cmml" xref="S3.SS1.p1.1.1.m1.1.1.3"><mtext id="S3.SS1.p1.1.1.m1.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p1.1.1.m1.1.1.3">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.1.m1.1c">\text{CNT}_{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.1.m1.1d">CNT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>] [<math alttext="\text{OBJ}_{\text{1}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.2.m2.1"><semantics id="S3.SS1.p1.2.2.m2.1a"><msub id="S3.SS1.p1.2.2.m2.1.1" xref="S3.SS1.p1.2.2.m2.1.1.cmml"><mtext id="S3.SS1.p1.2.2.m2.1.1.2" xref="S3.SS1.p1.2.2.m2.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p1.2.2.m2.1.1.3" mathvariant="italic" xref="S3.SS1.p1.2.2.m2.1.1.3a.cmml">1</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.2.m2.1b"><apply id="S3.SS1.p1.2.2.m2.1.1.cmml" xref="S3.SS1.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.2.m2.1.1.2a.cmml" xref="S3.SS1.p1.2.2.m2.1.1.2"><mtext id="S3.SS1.p1.2.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.2.m2.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p1.2.2.m2.1.1.3a.cmml" xref="S3.SS1.p1.2.2.m2.1.1.3"><mtext id="S3.SS1.p1.2.2.m2.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p1.2.2.m2.1.1.3">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.2.m2.1c">\text{OBJ}_{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.2.m2.1d">OBJ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>], [<math alttext="\text{CNT}_{\text{2}}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.3.m3.1"><semantics id="S3.SS1.p1.3.3.m3.1a"><msub id="S3.SS1.p1.3.3.m3.1.1" xref="S3.SS1.p1.3.3.m3.1.1.cmml"><mtext id="S3.SS1.p1.3.3.m3.1.1.2" xref="S3.SS1.p1.3.3.m3.1.1.2a.cmml">𝐶𝑁𝑇</mtext><mtext id="S3.SS1.p1.3.3.m3.1.1.3" mathvariant="italic" xref="S3.SS1.p1.3.3.m3.1.1.3a.cmml">2</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.3.m3.1b"><apply id="S3.SS1.p1.3.3.m3.1.1.cmml" xref="S3.SS1.p1.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.3.m3.1.1.2a.cmml" xref="S3.SS1.p1.3.3.m3.1.1.2"><mtext id="S3.SS1.p1.3.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.3.m3.1.1.2">𝐶𝑁𝑇</mtext></ci><ci id="S3.SS1.p1.3.3.m3.1.1.3a.cmml" xref="S3.SS1.p1.3.3.m3.1.1.3"><mtext id="S3.SS1.p1.3.3.m3.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p1.3.3.m3.1.1.3">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.3.m3.1c">\text{CNT}_{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.3.m3.1d">CNT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>] [<math alttext="\text{OBJ}_{\text{2}}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.4.m4.1"><semantics id="S3.SS1.p1.4.4.m4.1a"><msub id="S3.SS1.p1.4.4.m4.1.1" xref="S3.SS1.p1.4.4.m4.1.1.cmml"><mtext id="S3.SS1.p1.4.4.m4.1.1.2" xref="S3.SS1.p1.4.4.m4.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p1.4.4.m4.1.1.3" mathvariant="italic" xref="S3.SS1.p1.4.4.m4.1.1.3a.cmml">2</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.4.m4.1b"><apply id="S3.SS1.p1.4.4.m4.1.1.cmml" xref="S3.SS1.p1.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.4.m4.1.1.2a.cmml" xref="S3.SS1.p1.4.4.m4.1.1.2"><mtext id="S3.SS1.p1.4.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.4.m4.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p1.4.4.m4.1.1.3a.cmml" xref="S3.SS1.p1.4.4.m4.1.1.3"><mtext id="S3.SS1.p1.4.4.m4.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p1.4.4.m4.1.1.3">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.4.m4.1c">\text{OBJ}_{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.4.m4.1d">OBJ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>], <math alttext="\ldots" class="ltx_Math" display="inline" id="S3.SS1.p1.5.5.m5.1"><semantics id="S3.SS1.p1.5.5.m5.1a"><mi id="S3.SS1.p1.5.5.m5.1.1" mathvariant="normal" xref="S3.SS1.p1.5.5.m5.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.5.m5.1b"><ci id="S3.SS1.p1.5.5.m5.1.1.cmml" xref="S3.SS1.p1.5.5.m5.1.1">normal-…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.5.m5.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.5.m5.1d">…</annotation></semantics></math>, [<math alttext="\text{CNT}_{\text{N}}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.6.m6.1"><semantics id="S3.SS1.p1.6.6.m6.1a"><msub id="S3.SS1.p1.6.6.m6.1.1" xref="S3.SS1.p1.6.6.m6.1.1.cmml"><mtext id="S3.SS1.p1.6.6.m6.1.1.2" xref="S3.SS1.p1.6.6.m6.1.1.2a.cmml">𝐶𝑁𝑇</mtext><mtext id="S3.SS1.p1.6.6.m6.1.1.3" xref="S3.SS1.p1.6.6.m6.1.1.3a.cmml">𝑁</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.6.m6.1b"><apply id="S3.SS1.p1.6.6.m6.1.1.cmml" xref="S3.SS1.p1.6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.6.m6.1.1.2a.cmml" xref="S3.SS1.p1.6.6.m6.1.1.2"><mtext id="S3.SS1.p1.6.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.6.m6.1.1.2">𝐶𝑁𝑇</mtext></ci><ci id="S3.SS1.p1.6.6.m6.1.1.3a.cmml" xref="S3.SS1.p1.6.6.m6.1.1.3"><mtext id="S3.SS1.p1.6.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.6.6.m6.1.1.3">𝑁</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.6.m6.1c">\text{CNT}_{\text{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.6.m6.1d">CNT start_POSTSUBSCRIPT N end_POSTSUBSCRIPT</annotation></semantics></math>] [<math alttext="\text{OBJ}_{\text{N}}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.7.m7.1"><semantics id="S3.SS1.p1.7.7.m7.1a"><msub id="S3.SS1.p1.7.7.m7.1.1" xref="S3.SS1.p1.7.7.m7.1.1.cmml"><mtext id="S3.SS1.p1.7.7.m7.1.1.2" xref="S3.SS1.p1.7.7.m7.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p1.7.7.m7.1.1.3" xref="S3.SS1.p1.7.7.m7.1.1.3a.cmml">𝑁</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.7.m7.1b"><apply id="S3.SS1.p1.7.7.m7.1.1.cmml" xref="S3.SS1.p1.7.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.7.m7.1.1.2a.cmml" xref="S3.SS1.p1.7.7.m7.1.1.2"><mtext id="S3.SS1.p1.7.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.7.m7.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p1.7.7.m7.1.1.3a.cmml" xref="S3.SS1.p1.7.7.m7.1.1.3"><mtext id="S3.SS1.p1.7.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.7.7.m7.1.1.3">𝑁</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.7.m7.1c">\text{OBJ}_{\text{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.7.m7.1d">OBJ start_POSTSUBSCRIPT N end_POSTSUBSCRIPT</annotation></semantics></math>].”</span>, with <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.8.8">[<math alttext="\text{OBJ}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.8.m1.1"><semantics id="S3.SS1.p1.8.8.m1.1a"><msub id="S3.SS1.p1.8.8.m1.1.1" xref="S3.SS1.p1.8.8.m1.1.1.cmml"><mtext id="S3.SS1.p1.8.8.m1.1.1.2" xref="S3.SS1.p1.8.8.m1.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mi id="S3.SS1.p1.8.8.m1.1.1.3" xref="S3.SS1.p1.8.8.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.8.m1.1b"><apply id="S3.SS1.p1.8.8.m1.1.1.cmml" xref="S3.SS1.p1.8.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.8.m1.1.1.1.cmml" xref="S3.SS1.p1.8.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.8.m1.1.1.2a.cmml" xref="S3.SS1.p1.8.8.m1.1.1.2"><mtext id="S3.SS1.p1.8.8.m1.1.1.2.cmml" xref="S3.SS1.p1.8.8.m1.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p1.8.8.m1.1.1.3.cmml" xref="S3.SS1.p1.8.8.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.8.m1.1c">\text{OBJ}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.8.m1.1d">OBJ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>]</span> representing the object name and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.9.9">[<math alttext="CNT_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.9.m1.1"><semantics id="S3.SS1.p1.9.9.m1.1a"><mrow id="S3.SS1.p1.9.9.m1.1.1" xref="S3.SS1.p1.9.9.m1.1.1.cmml"><mi id="S3.SS1.p1.9.9.m1.1.1.2" xref="S3.SS1.p1.9.9.m1.1.1.2.cmml">C</mi><mo id="S3.SS1.p1.9.9.m1.1.1.1" mathvariant="italic" xref="S3.SS1.p1.9.9.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.p1.9.9.m1.1.1.3" xref="S3.SS1.p1.9.9.m1.1.1.3.cmml">N</mi><mo id="S3.SS1.p1.9.9.m1.1.1.1a" mathvariant="italic" xref="S3.SS1.p1.9.9.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.9.9.m1.1.1.4" xref="S3.SS1.p1.9.9.m1.1.1.4.cmml"><mi id="S3.SS1.p1.9.9.m1.1.1.4.2" xref="S3.SS1.p1.9.9.m1.1.1.4.2.cmml">T</mi><mi id="S3.SS1.p1.9.9.m1.1.1.4.3" xref="S3.SS1.p1.9.9.m1.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.9.m1.1b"><apply id="S3.SS1.p1.9.9.m1.1.1.cmml" xref="S3.SS1.p1.9.9.m1.1.1"><times id="S3.SS1.p1.9.9.m1.1.1.1.cmml" xref="S3.SS1.p1.9.9.m1.1.1.1"></times><ci id="S3.SS1.p1.9.9.m1.1.1.2.cmml" xref="S3.SS1.p1.9.9.m1.1.1.2">𝐶</ci><ci id="S3.SS1.p1.9.9.m1.1.1.3.cmml" xref="S3.SS1.p1.9.9.m1.1.1.3">𝑁</ci><apply id="S3.SS1.p1.9.9.m1.1.1.4.cmml" xref="S3.SS1.p1.9.9.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.9.9.m1.1.1.4.1.cmml" xref="S3.SS1.p1.9.9.m1.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.9.9.m1.1.1.4.2.cmml" xref="S3.SS1.p1.9.9.m1.1.1.4.2">𝑇</ci><ci id="S3.SS1.p1.9.9.m1.1.1.4.3.cmml" xref="S3.SS1.p1.9.9.m1.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.9.m1.1c">CNT_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.9.m1.1d">italic_C italic_N italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>]</span> representing the count (if greater than one) for the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m1.1"><semantics id="S3.SS1.p1.10.m1.1a"><msup id="S3.SS1.p1.10.m1.1.1" xref="S3.SS1.p1.10.m1.1.1.cmml"><mi id="S3.SS1.p1.10.m1.1.1.2" xref="S3.SS1.p1.10.m1.1.1.2.cmml">i</mi><mrow id="S3.SS1.p1.10.m1.1.1.3" xref="S3.SS1.p1.10.m1.1.1.3.cmml"><mi id="S3.SS1.p1.10.m1.1.1.3.2" xref="S3.SS1.p1.10.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.10.m1.1.1.3.1" xref="S3.SS1.p1.10.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.10.m1.1.1.3.3" xref="S3.SS1.p1.10.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m1.1b"><apply id="S3.SS1.p1.10.m1.1.1.cmml" xref="S3.SS1.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m1.1.1.1.cmml" xref="S3.SS1.p1.10.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.10.m1.1.1.2.cmml" xref="S3.SS1.p1.10.m1.1.1.2">𝑖</ci><apply id="S3.SS1.p1.10.m1.1.1.3.cmml" xref="S3.SS1.p1.10.m1.1.1.3"><times id="S3.SS1.p1.10.m1.1.1.3.1.cmml" xref="S3.SS1.p1.10.m1.1.1.3.1"></times><ci id="S3.SS1.p1.10.m1.1.1.3.2.cmml" xref="S3.SS1.p1.10.m1.1.1.3.2">𝑡</ci><ci id="S3.SS1.p1.10.m1.1.1.3.3.cmml" xref="S3.SS1.p1.10.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m1.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m1.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> object in the image. We prompt GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> to collect a bucket of questions for three different object identification tasks: semantic, instance, and panoptic, corresponding to the three different image segmentation tasks. Finally, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2.F3" title="Figure 3 ‣ 2.2 Visual Understanding with LLMs ‣ 2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, we organize the images from COCO, segmentation maps from OneFormer, questions from GPT-4, and sentences containing object information into a question-answer format to construct our <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.10.12">CO</span>CO <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.10.13">S</span>egmentation <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.10.14">T</span>ext (<span class="ltx_text ltx_font_bold" id="S3.SS1.p1.10.15">COST</span>) dataset for training and evaluating MLLMs on the object identification task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Statistically, we prompt GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> to return 20 questions for each question bucket (panoptic, semantic, and instance). In total, we used 280k images from the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">train2017</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">test2017</span>, and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.3">unlabeled2017</span> splits of the COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> dataset and corresponding segmentation outputs from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> to form the visual component of the COST training dataset. Similarly, we prepare a COST validation split using the 5k images from the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.4">val2017</span> split of the COCO dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.6">Note that a similar approach can extend the COST dataset to other perception modalities. In this work, we incorporate the depth map modality into our COST dataset for the object order perception task. Particularly, we leverage the publicly available DINOv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> DPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> model to obtain depth maps for COCO images and use the panoptic mask (from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>) to estimate the depth order of objects in an image. We format the obtained ordering of objects into the text with the template: <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.4.4">“The depth order for objects present in the image is: [<math alttext="\text{OBJ}_{\text{1}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.1.m1.1"><semantics id="S3.SS1.p3.1.1.m1.1a"><msub id="S3.SS1.p3.1.1.m1.1.1" xref="S3.SS1.p3.1.1.m1.1.1.cmml"><mtext id="S3.SS1.p3.1.1.m1.1.1.2" xref="S3.SS1.p3.1.1.m1.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p3.1.1.m1.1.1.3" mathvariant="italic" xref="S3.SS1.p3.1.1.m1.1.1.3a.cmml">1</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.1.m1.1b"><apply id="S3.SS1.p3.1.1.m1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.1.m1.1.1.2a.cmml" xref="S3.SS1.p3.1.1.m1.1.1.2"><mtext id="S3.SS1.p3.1.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.1.m1.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p3.1.1.m1.1.1.3a.cmml" xref="S3.SS1.p3.1.1.m1.1.1.3"><mtext id="S3.SS1.p3.1.1.m1.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p3.1.1.m1.1.1.3">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.1.m1.1c">\text{OBJ}_{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.1.m1.1d">OBJ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>], [<math alttext="\text{OBJ}_{\text{2}}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.2.m2.1"><semantics id="S3.SS1.p3.2.2.m2.1a"><msub id="S3.SS1.p3.2.2.m2.1.1" xref="S3.SS1.p3.2.2.m2.1.1.cmml"><mtext id="S3.SS1.p3.2.2.m2.1.1.2" xref="S3.SS1.p3.2.2.m2.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p3.2.2.m2.1.1.3" mathvariant="italic" xref="S3.SS1.p3.2.2.m2.1.1.3a.cmml">2</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.2.m2.1b"><apply id="S3.SS1.p3.2.2.m2.1.1.cmml" xref="S3.SS1.p3.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.2.m2.1.1.2a.cmml" xref="S3.SS1.p3.2.2.m2.1.1.2"><mtext id="S3.SS1.p3.2.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.2.m2.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p3.2.2.m2.1.1.3a.cmml" xref="S3.SS1.p3.2.2.m2.1.1.3"><mtext id="S3.SS1.p3.2.2.m2.1.1.3.cmml" mathsize="70%" mathvariant="italic" xref="S3.SS1.p3.2.2.m2.1.1.3">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.2.m2.1c">\text{OBJ}_{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.2.m2.1d">OBJ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>], <math alttext="\ldots" class="ltx_Math" display="inline" id="S3.SS1.p3.3.3.m3.1"><semantics id="S3.SS1.p3.3.3.m3.1a"><mi id="S3.SS1.p3.3.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p3.3.3.m3.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.3.m3.1b"><ci id="S3.SS1.p3.3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.3.m3.1.1">normal-…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.3.m3.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.3.m3.1d">…</annotation></semantics></math>, [<math alttext="\text{OBJ}_{\text{J}}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.4.m4.1"><semantics id="S3.SS1.p3.4.4.m4.1a"><msub id="S3.SS1.p3.4.4.m4.1.1" xref="S3.SS1.p3.4.4.m4.1.1.cmml"><mtext id="S3.SS1.p3.4.4.m4.1.1.2" xref="S3.SS1.p3.4.4.m4.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mtext id="S3.SS1.p3.4.4.m4.1.1.3" xref="S3.SS1.p3.4.4.m4.1.1.3a.cmml">𝐽</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.4.m4.1b"><apply id="S3.SS1.p3.4.4.m4.1.1.cmml" xref="S3.SS1.p3.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.4.m4.1.1.2a.cmml" xref="S3.SS1.p3.4.4.m4.1.1.2"><mtext id="S3.SS1.p3.4.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.4.m4.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p3.4.4.m4.1.1.3a.cmml" xref="S3.SS1.p3.4.4.m4.1.1.3"><mtext id="S3.SS1.p3.4.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p3.4.4.m4.1.1.3">𝐽</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.4.m4.1c">\text{OBJ}_{\text{J}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.4.m4.1d">OBJ start_POSTSUBSCRIPT J end_POSTSUBSCRIPT</annotation></semantics></math>].”</span>, with <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.5.5">[<math alttext="\text{OBJ}_{j}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.5.m1.1"><semantics id="S3.SS1.p3.5.5.m1.1a"><msub id="S3.SS1.p3.5.5.m1.1.1" xref="S3.SS1.p3.5.5.m1.1.1.cmml"><mtext id="S3.SS1.p3.5.5.m1.1.1.2" xref="S3.SS1.p3.5.5.m1.1.1.2a.cmml">𝑂𝐵𝐽</mtext><mi id="S3.SS1.p3.5.5.m1.1.1.3" xref="S3.SS1.p3.5.5.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.5.m1.1b"><apply id="S3.SS1.p3.5.5.m1.1.1.cmml" xref="S3.SS1.p3.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.5.m1.1.1.1.cmml" xref="S3.SS1.p3.5.5.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.5.m1.1.1.2a.cmml" xref="S3.SS1.p3.5.5.m1.1.1.2"><mtext id="S3.SS1.p3.5.5.m1.1.1.2.cmml" xref="S3.SS1.p3.5.5.m1.1.1.2">𝑂𝐵𝐽</mtext></ci><ci id="S3.SS1.p3.5.5.m1.1.1.3.cmml" xref="S3.SS1.p3.5.5.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.5.m1.1c">\text{OBJ}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.5.m1.1d">OBJ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>]</span> representing the <math alttext="j^{th}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m1.1"><semantics id="S3.SS1.p3.6.m1.1a"><msup id="S3.SS1.p3.6.m1.1.1" xref="S3.SS1.p3.6.m1.1.1.cmml"><mi id="S3.SS1.p3.6.m1.1.1.2" xref="S3.SS1.p3.6.m1.1.1.2.cmml">j</mi><mrow id="S3.SS1.p3.6.m1.1.1.3" xref="S3.SS1.p3.6.m1.1.1.3.cmml"><mi id="S3.SS1.p3.6.m1.1.1.3.2" xref="S3.SS1.p3.6.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p3.6.m1.1.1.3.1" xref="S3.SS1.p3.6.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.6.m1.1.1.3.3" xref="S3.SS1.p3.6.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m1.1b"><apply id="S3.SS1.p3.6.m1.1.1.cmml" xref="S3.SS1.p3.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m1.1.1.1.cmml" xref="S3.SS1.p3.6.m1.1.1">superscript</csymbol><ci id="S3.SS1.p3.6.m1.1.1.2.cmml" xref="S3.SS1.p3.6.m1.1.1.2">𝑗</ci><apply id="S3.SS1.p3.6.m1.1.1.3.cmml" xref="S3.SS1.p3.6.m1.1.1.3"><times id="S3.SS1.p3.6.m1.1.1.3.1.cmml" xref="S3.SS1.p3.6.m1.1.1.3.1"></times><ci id="S3.SS1.p3.6.m1.1.1.3.2.cmml" xref="S3.SS1.p3.6.m1.1.1.3.2">𝑡</ci><ci id="S3.SS1.p3.6.m1.1.1.3.3.cmml" xref="S3.SS1.p3.6.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m1.1c">j^{th}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m1.1d">italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> object name. To maintain relative ordering among objects belonging to the same class, we append a count number to the second and later objects, as shown in the bottom right of <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S2.F3" title="Figure 3 ‣ 2.2 Visual Understanding with LLMs ‣ 2 Related Work ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> for <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.6">person</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.7">person-2</span>. Similar to the previous setting, we prompt GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> to return 20 questions for the object order perception task. We provide a detailed flow of obtaining ground-truth object orders in the appendix.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>VCoder for Multimodal LLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We notice that existing open-source Multimodal LLMs generally use the ViT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> from CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> as the image encoder (ImCoder) during instruction tuning. We reason that the ViT focuses mainly on salient objects because it is trained against captions, which leave out information about background regions. We argue that identifying objects in the background is critical for a Multimodal LLM to become skilled at perception. To overcome this limitation, we introduce a segmentation map as a control input&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> into our Multimodal LLM. Specifically, we use the segmentation map from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> and project it to the LLM’s embedding space using a pretrained ViT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> (from CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>) as a SegCoder and a two-layer MLP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> which we collectively refer to as our <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">V</span>ersatile en<span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.2">Coder</span> (<span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.3">VCoder</span>). This extra control from the segmentation map results in considerable performance gains on the object identification task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.F4" title="Figure 4 ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S3.SS2.p2.1.1" style="color:#FF0000;">a</span>, our VCoder adapted MLLM takes three sets of inputs: perception modalities as control inputs fed into the VCoder, an RGB image fed into an Image enCoder (and MLP), and the question from the user. The RGB image and text are tokenized to the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.2">&lt;img&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.3">&lt;query&gt;</span> tokens, respectively. VCoder is flexible at handling various perception modalities with a special token for each modality. For example, the segmentation map and depth map inputs are tokenized to <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.4">&lt;seg&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.5">&lt;depth&gt;</span> tokens, respectively. Similarly, one can incorporate more modalities with modality-specific tokens. Finally, all tokenized embeddings are concatenated and fed into the LLM to obtain the final answer. We only use the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.6">&lt;seg&gt;</span> input for the object identification task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We treat our VCoder as an adapter, added to our base MLLM, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> to obtain the final MLLM framework for experiments. Note that we only train the MLP components in the VCoder on the COST dataset. We decided to keep all other parameters fixed during training to keep the reasoning ability unaffected while achieving improved object perception performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluating MLLMs for Object Identification</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Despite the availability of various metrics&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> to measure object hallucination in vision-language models, no existing metric considers the explicit object counts while calculating their hallucination scores. We argue that object counts returned by an MLLM are a critical component that should not be overlooked while evaluating object identification performance. Therefore, we propose evaluating object identification performance in MLLMs using two metrics: count-score (<math alttext="\mathbf{CS}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">𝐂𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝐂𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathbf{CS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">bold_CS</annotation></semantics></math>) and hallucination-score (<math alttext="\mathbf{HS}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">𝐇𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐇𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathbf{HS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">bold_HS</annotation></semantics></math>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}G_{\text{dict}}&amp;=\{\text{OBJ}^{G}_{1}:\text{CNT}^{G}_{1};\cdots;%
\text{OBJ}^{G}_{N}:\text{CNT}^{G}_{N}\}\\
P_{\text{dict}}&amp;=\{\text{OBJ}^{P}_{1}:\text{CNT}^{P}_{1};\cdots;\text{OBJ}^{P}%
_{M}:\text{CNT}^{P}_{M}\}\\
\end{split}" class="ltx_Math" display="block" id="S3.E1.m1.52"><semantics id="S3.E1.m1.52a"><mtable columnspacing="0pt" displaystyle="true" id="S3.E1.m1.52.52.8" rowspacing="0pt" xref="S3.E1.m1.48.48.4.cmml"><mtr id="S3.E1.m1.52.52.8a" xref="S3.E1.m1.48.48.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.52.52.8b" xref="S3.E1.m1.48.48.4.cmml"><msub id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.48.48.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">G</mi><mtext id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1a.cmml">dict</mtext></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.52.52.8c" xref="S3.E1.m1.48.48.4.cmml"><mrow id="S3.E1.m1.50.50.6.46.24.22" xref="S3.E1.m1.48.48.4.cmml"><mi id="S3.E1.m1.50.50.6.46.24.22.23" xref="S3.E1.m1.48.48.4a.cmml"></mi><mo id="S3.E1.m1.3.3.3.3.1.1" xref="S3.E1.m1.3.3.3.3.1.1.cmml">=</mo><mrow id="S3.E1.m1.50.50.6.46.24.22.22.2" xref="S3.E1.m1.48.48.4.cmml"><mo id="S3.E1.m1.4.4.4.4.2.2" stretchy="false" xref="S3.E1.m1.48.48.4a.cmml">{</mo><msubsup id="S3.E1.m1.49.49.5.45.23.21.21.1.1" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.5.5.5.5.3.3" xref="S3.E1.m1.5.5.5.5.3.3a.cmml">OBJ</mtext><mn id="S3.E1.m1.7.7.7.7.5.5.1" xref="S3.E1.m1.7.7.7.7.5.5.1.cmml">1</mn><mi id="S3.E1.m1.6.6.6.6.4.4.1" xref="S3.E1.m1.6.6.6.6.4.4.1.cmml">G</mi></msubsup><mo id="S3.E1.m1.8.8.8.8.6.6" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.48.48.4a.cmml">:</mo><mrow id="S3.E1.m1.50.50.6.46.24.22.22.2.2" xref="S3.E1.m1.48.48.4.cmml"><mrow id="S3.E1.m1.50.50.6.46.24.22.22.2.2.2.2" xref="S3.E1.m1.48.48.4.cmml"><msubsup id="S3.E1.m1.50.50.6.46.24.22.22.2.2.1.1.1" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.9.9.9.9.7.7" xref="S3.E1.m1.9.9.9.9.7.7a.cmml">CNT</mtext><mn id="S3.E1.m1.11.11.11.11.9.9.1" xref="S3.E1.m1.11.11.11.11.9.9.1.cmml">1</mn><mi id="S3.E1.m1.10.10.10.10.8.8.1" xref="S3.E1.m1.10.10.10.10.8.8.1.cmml">G</mi></msubsup><mo id="S3.E1.m1.12.12.12.12.10.10" xref="S3.E1.m1.48.48.4a.cmml">;</mo><mi id="S3.E1.m1.13.13.13.13.11.11" mathvariant="normal" xref="S3.E1.m1.13.13.13.13.11.11.cmml">⋯</mi><mo id="S3.E1.m1.14.14.14.14.12.12" xref="S3.E1.m1.48.48.4a.cmml">;</mo><msubsup id="S3.E1.m1.50.50.6.46.24.22.22.2.2.2.2.2" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.15.15.15.15.13.13" xref="S3.E1.m1.15.15.15.15.13.13a.cmml">OBJ</mtext><mi id="S3.E1.m1.17.17.17.17.15.15.1" xref="S3.E1.m1.17.17.17.17.15.15.1.cmml">N</mi><mi id="S3.E1.m1.16.16.16.16.14.14.1" xref="S3.E1.m1.16.16.16.16.14.14.1.cmml">G</mi></msubsup></mrow><mo id="S3.E1.m1.18.18.18.18.16.16" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.18.18.18.18.16.16.cmml">:</mo><msubsup id="S3.E1.m1.50.50.6.46.24.22.22.2.2.3" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.19.19.19.19.17.17" xref="S3.E1.m1.19.19.19.19.17.17a.cmml">CNT</mtext><mi id="S3.E1.m1.21.21.21.21.19.19.1" xref="S3.E1.m1.21.21.21.21.19.19.1.cmml">N</mi><mi id="S3.E1.m1.20.20.20.20.18.18.1" xref="S3.E1.m1.20.20.20.20.18.18.1.cmml">G</mi></msubsup></mrow><mo id="S3.E1.m1.22.22.22.22.20.20" stretchy="false" xref="S3.E1.m1.48.48.4a.cmml">}</mo></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.52.52.8d" xref="S3.E1.m1.48.48.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.52.52.8e" xref="S3.E1.m1.48.48.4.cmml"><msub id="S3.E1.m1.24.24.24.2.2" xref="S3.E1.m1.48.48.4.cmml"><mi id="S3.E1.m1.23.23.23.1.1.1" xref="S3.E1.m1.23.23.23.1.1.1.cmml">P</mi><mtext id="S3.E1.m1.24.24.24.2.2.2.1" xref="S3.E1.m1.24.24.24.2.2.2.1a.cmml">dict</mtext></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.52.52.8f" xref="S3.E1.m1.48.48.4.cmml"><mrow id="S3.E1.m1.52.52.8.48.24.22" xref="S3.E1.m1.48.48.4.cmml"><mi id="S3.E1.m1.52.52.8.48.24.22.23" xref="S3.E1.m1.48.48.4a.cmml"></mi><mo id="S3.E1.m1.25.25.25.3.1.1" xref="S3.E1.m1.25.25.25.3.1.1.cmml">=</mo><mrow id="S3.E1.m1.52.52.8.48.24.22.22.2" xref="S3.E1.m1.48.48.4.cmml"><mo id="S3.E1.m1.26.26.26.4.2.2" stretchy="false" xref="S3.E1.m1.48.48.4a.cmml">{</mo><msubsup id="S3.E1.m1.51.51.7.47.23.21.21.1.1" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.27.27.27.5.3.3" xref="S3.E1.m1.27.27.27.5.3.3a.cmml">OBJ</mtext><mn id="S3.E1.m1.29.29.29.7.5.5.1" xref="S3.E1.m1.29.29.29.7.5.5.1.cmml">1</mn><mi id="S3.E1.m1.28.28.28.6.4.4.1" xref="S3.E1.m1.28.28.28.6.4.4.1.cmml">P</mi></msubsup><mo id="S3.E1.m1.30.30.30.8.6.6" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.48.48.4a.cmml">:</mo><mrow id="S3.E1.m1.52.52.8.48.24.22.22.2.2" xref="S3.E1.m1.48.48.4.cmml"><mrow id="S3.E1.m1.52.52.8.48.24.22.22.2.2.2.2" xref="S3.E1.m1.48.48.4.cmml"><msubsup id="S3.E1.m1.52.52.8.48.24.22.22.2.2.1.1.1" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.31.31.31.9.7.7" xref="S3.E1.m1.31.31.31.9.7.7a.cmml">CNT</mtext><mn id="S3.E1.m1.33.33.33.11.9.9.1" xref="S3.E1.m1.33.33.33.11.9.9.1.cmml">1</mn><mi id="S3.E1.m1.32.32.32.10.8.8.1" xref="S3.E1.m1.32.32.32.10.8.8.1.cmml">P</mi></msubsup><mo id="S3.E1.m1.34.34.34.12.10.10" xref="S3.E1.m1.48.48.4a.cmml">;</mo><mi id="S3.E1.m1.35.35.35.13.11.11" mathvariant="normal" xref="S3.E1.m1.35.35.35.13.11.11.cmml">⋯</mi><mo id="S3.E1.m1.36.36.36.14.12.12" xref="S3.E1.m1.48.48.4a.cmml">;</mo><msubsup id="S3.E1.m1.52.52.8.48.24.22.22.2.2.2.2.2" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.37.37.37.15.13.13" xref="S3.E1.m1.37.37.37.15.13.13a.cmml">OBJ</mtext><mi id="S3.E1.m1.39.39.39.17.15.15.1" xref="S3.E1.m1.39.39.39.17.15.15.1.cmml">M</mi><mi id="S3.E1.m1.38.38.38.16.14.14.1" xref="S3.E1.m1.38.38.38.16.14.14.1.cmml">P</mi></msubsup></mrow><mo id="S3.E1.m1.40.40.40.18.16.16" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.40.40.40.18.16.16.cmml">:</mo><msubsup id="S3.E1.m1.52.52.8.48.24.22.22.2.2.3" xref="S3.E1.m1.48.48.4.cmml"><mtext id="S3.E1.m1.41.41.41.19.17.17" xref="S3.E1.m1.41.41.41.19.17.17a.cmml">CNT</mtext><mi id="S3.E1.m1.43.43.43.21.19.19.1" xref="S3.E1.m1.43.43.43.21.19.19.1.cmml">M</mi><mi id="S3.E1.m1.42.42.42.20.18.18.1" xref="S3.E1.m1.42.42.42.20.18.18.1.cmml">P</mi></msubsup></mrow><mo id="S3.E1.m1.44.44.44.22.20.20" stretchy="false" xref="S3.E1.m1.48.48.4a.cmml">}</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.52b"><apply id="S3.E1.m1.48.48.4.cmml" xref="S3.E1.m1.52.52.8"><and id="S3.E1.m1.48.48.4a.cmml" xref="S3.E1.m1.50.50.6.46.24.22.23"></and><apply id="S3.E1.m1.48.48.4b.cmml" xref="S3.E1.m1.52.52.8"><eq id="S3.E1.m1.3.3.3.3.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1"></eq><apply id="S3.E1.m1.48.48.4.6.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.6.1.cmml" xref="S3.E1.m1.50.50.6.46.24.22.23">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝐺</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1a.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1"><mtext id="S3.E1.m1.2.2.2.2.2.2.1.cmml" mathsize="70%" xref="S3.E1.m1.2.2.2.2.2.2.1">dict</mtext></ci></apply><apply id="S3.E1.m1.46.46.2.2.cmml" xref="S3.E1.m1.52.52.8"><times id="S3.E1.m1.46.46.2.2.3.cmml" xref="S3.E1.m1.50.50.6.46.24.22.23"></times><apply id="S3.E1.m1.46.46.2.2.2.3.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="latexml" id="S3.E1.m1.46.46.2.2.2.3.1.cmml" xref="S3.E1.m1.52.52.8">conditional-set</csymbol><apply id="S3.E1.m1.45.45.1.1.1.1.1.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.45.45.1.1.1.1.1.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.45.45.1.1.1.1.1.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.45.45.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.5.5.5.5.3.3a.cmml" xref="S3.E1.m1.5.5.5.5.3.3"><mtext id="S3.E1.m1.5.5.5.5.3.3.cmml" xref="S3.E1.m1.5.5.5.5.3.3">OBJ</mtext></ci><ci id="S3.E1.m1.6.6.6.6.4.4.1.cmml" xref="S3.E1.m1.6.6.6.6.4.4.1">𝐺</ci></apply><cn id="S3.E1.m1.7.7.7.7.5.5.1.cmml" type="integer" xref="S3.E1.m1.7.7.7.7.5.5.1">1</cn></apply><apply id="S3.E1.m1.46.46.2.2.2.2.2.cmml" xref="S3.E1.m1.52.52.8"><ci id="S3.E1.m1.18.18.18.18.16.16.cmml" xref="S3.E1.m1.18.18.18.18.16.16">:</ci><list id="S3.E1.m1.46.46.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.52.52.8"><apply id="S3.E1.m1.46.46.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.46.46.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.9.9.9.9.7.7a.cmml" xref="S3.E1.m1.9.9.9.9.7.7"><mtext id="S3.E1.m1.9.9.9.9.7.7.cmml" xref="S3.E1.m1.9.9.9.9.7.7">CNT</mtext></ci><ci id="S3.E1.m1.10.10.10.10.8.8.1.cmml" xref="S3.E1.m1.10.10.10.10.8.8.1">𝐺</ci></apply><cn id="S3.E1.m1.11.11.11.11.9.9.1.cmml" type="integer" xref="S3.E1.m1.11.11.11.11.9.9.1">1</cn></apply><ci id="S3.E1.m1.13.13.13.13.11.11.cmml" xref="S3.E1.m1.13.13.13.13.11.11">⋯</ci><apply id="S3.E1.m1.46.46.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.46.46.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.15.15.15.15.13.13a.cmml" xref="S3.E1.m1.15.15.15.15.13.13"><mtext id="S3.E1.m1.15.15.15.15.13.13.cmml" xref="S3.E1.m1.15.15.15.15.13.13">OBJ</mtext></ci><ci id="S3.E1.m1.16.16.16.16.14.14.1.cmml" xref="S3.E1.m1.16.16.16.16.14.14.1">𝐺</ci></apply><ci id="S3.E1.m1.17.17.17.17.15.15.1.cmml" xref="S3.E1.m1.17.17.17.17.15.15.1">𝑁</ci></apply></list><apply id="S3.E1.m1.46.46.2.2.2.2.2.4.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.4.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.46.46.2.2.2.2.2.4.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.2.2.2.4.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.19.19.19.19.17.17a.cmml" xref="S3.E1.m1.19.19.19.19.17.17"><mtext id="S3.E1.m1.19.19.19.19.17.17.cmml" xref="S3.E1.m1.19.19.19.19.17.17">CNT</mtext></ci><ci id="S3.E1.m1.20.20.20.20.18.18.1.cmml" xref="S3.E1.m1.20.20.20.20.18.18.1">𝐺</ci></apply><ci id="S3.E1.m1.21.21.21.21.19.19.1.cmml" xref="S3.E1.m1.21.21.21.21.19.19.1">𝑁</ci></apply></apply></apply><apply id="S3.E1.m1.46.46.2.2.4.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.46.46.2.2.4.1.cmml" xref="S3.E1.m1.50.50.6.46.24.22.23">subscript</csymbol><ci id="S3.E1.m1.23.23.23.1.1.1.cmml" xref="S3.E1.m1.23.23.23.1.1.1">𝑃</ci><ci id="S3.E1.m1.24.24.24.2.2.2.1a.cmml" xref="S3.E1.m1.24.24.24.2.2.2.1"><mtext id="S3.E1.m1.24.24.24.2.2.2.1.cmml" mathsize="70%" xref="S3.E1.m1.24.24.24.2.2.2.1">dict</mtext></ci></apply></apply></apply><apply id="S3.E1.m1.48.48.4c.cmml" xref="S3.E1.m1.52.52.8"><eq id="S3.E1.m1.25.25.25.3.1.1.cmml" xref="S3.E1.m1.25.25.25.3.1.1"></eq><share href="#S3.E1.m1.46.46.2.2.cmml" id="S3.E1.m1.48.48.4d.cmml" xref="S3.E1.m1.50.50.6.46.24.22.23"></share><apply id="S3.E1.m1.48.48.4.4.3.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="latexml" id="S3.E1.m1.48.48.4.4.3.1.cmml" xref="S3.E1.m1.52.52.8">conditional-set</csymbol><apply id="S3.E1.m1.47.47.3.3.1.1.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.47.47.3.3.1.1.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.47.47.3.3.1.1.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.47.47.3.3.1.1.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.27.27.27.5.3.3a.cmml" xref="S3.E1.m1.27.27.27.5.3.3"><mtext id="S3.E1.m1.27.27.27.5.3.3.cmml" xref="S3.E1.m1.27.27.27.5.3.3">OBJ</mtext></ci><ci id="S3.E1.m1.28.28.28.6.4.4.1.cmml" xref="S3.E1.m1.28.28.28.6.4.4.1">𝑃</ci></apply><cn id="S3.E1.m1.29.29.29.7.5.5.1.cmml" type="integer" xref="S3.E1.m1.29.29.29.7.5.5.1">1</cn></apply><apply id="S3.E1.m1.48.48.4.4.2.2.cmml" xref="S3.E1.m1.52.52.8"><ci id="S3.E1.m1.40.40.40.18.16.16.cmml" xref="S3.E1.m1.40.40.40.18.16.16">:</ci><list id="S3.E1.m1.48.48.4.4.2.2.2.3.cmml" xref="S3.E1.m1.52.52.8"><apply id="S3.E1.m1.48.48.4.4.2.2.1.1.1.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.1.1.1.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.48.48.4.4.2.2.1.1.1.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.31.31.31.9.7.7a.cmml" xref="S3.E1.m1.31.31.31.9.7.7"><mtext id="S3.E1.m1.31.31.31.9.7.7.cmml" xref="S3.E1.m1.31.31.31.9.7.7">CNT</mtext></ci><ci id="S3.E1.m1.32.32.32.10.8.8.1.cmml" xref="S3.E1.m1.32.32.32.10.8.8.1">𝑃</ci></apply><cn id="S3.E1.m1.33.33.33.11.9.9.1.cmml" type="integer" xref="S3.E1.m1.33.33.33.11.9.9.1">1</cn></apply><ci id="S3.E1.m1.35.35.35.13.11.11.cmml" xref="S3.E1.m1.35.35.35.13.11.11">⋯</ci><apply id="S3.E1.m1.48.48.4.4.2.2.2.2.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.2.2.2.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.48.48.4.4.2.2.2.2.2.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.37.37.37.15.13.13a.cmml" xref="S3.E1.m1.37.37.37.15.13.13"><mtext id="S3.E1.m1.37.37.37.15.13.13.cmml" xref="S3.E1.m1.37.37.37.15.13.13">OBJ</mtext></ci><ci id="S3.E1.m1.38.38.38.16.14.14.1.cmml" xref="S3.E1.m1.38.38.38.16.14.14.1">𝑃</ci></apply><ci id="S3.E1.m1.39.39.39.17.15.15.1.cmml" xref="S3.E1.m1.39.39.39.17.15.15.1">𝑀</ci></apply></list><apply id="S3.E1.m1.48.48.4.4.2.2.4.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.4.1.cmml" xref="S3.E1.m1.52.52.8">subscript</csymbol><apply id="S3.E1.m1.48.48.4.4.2.2.4.2.cmml" xref="S3.E1.m1.52.52.8"><csymbol cd="ambiguous" id="S3.E1.m1.48.48.4.4.2.2.4.2.1.cmml" xref="S3.E1.m1.52.52.8">superscript</csymbol><ci id="S3.E1.m1.41.41.41.19.17.17a.cmml" xref="S3.E1.m1.41.41.41.19.17.17"><mtext id="S3.E1.m1.41.41.41.19.17.17.cmml" xref="S3.E1.m1.41.41.41.19.17.17">CNT</mtext></ci><ci id="S3.E1.m1.42.42.42.20.18.18.1.cmml" xref="S3.E1.m1.42.42.42.20.18.18.1">𝑃</ci></apply><ci id="S3.E1.m1.43.43.43.21.19.19.1.cmml" xref="S3.E1.m1.43.43.43.21.19.19.1">𝑀</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.52c">\begin{split}G_{\text{dict}}&amp;=\{\text{OBJ}^{G}_{1}:\text{CNT}^{G}_{1};\cdots;%
\text{OBJ}^{G}_{N}:\text{CNT}^{G}_{N}\}\\
P_{\text{dict}}&amp;=\{\text{OBJ}^{P}_{1}:\text{CNT}^{P}_{1};\cdots;\text{OBJ}^{P}%
_{M}:\text{CNT}^{P}_{M}\}\\
\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.52d">start_ROW start_CELL italic_G start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT end_CELL start_CELL = { OBJ start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; ⋯ ; OBJ start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT : CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } end_CELL end_ROW start_ROW start_CELL italic_P start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT end_CELL start_CELL = { OBJ start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; ⋯ ; OBJ start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT : CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT } end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.10">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.F5" title="Figure 5 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>, given a ground-truth sentence (<math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_G</annotation></semantics></math>) and an MLLM predicted response (<math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_P</annotation></semantics></math>), we first extract the object words (nouns) and their corresponding count from both text samples and represent them in a dictionary form with keys as the object noun and the value as the corresponding object’s count as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.E1" title="1 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Eq.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> with <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_N</annotation></semantics></math> and <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_M</annotation></semantics></math> representing the number of different object nouns in the <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m5.1"><semantics id="S3.SS3.p3.5.m5.1a"><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m5.1d">italic_G</annotation></semantics></math> and <math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p3.6.m6.1"><semantics id="S3.SS3.p3.6.m6.1a"><mi id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><ci id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.6.m6.1d">italic_P</annotation></semantics></math> respectively. Next, we perform one-to-one matching between the counts for keys with <math alttext="G_{\text{dict}}" class="ltx_Math" display="inline" id="S3.SS3.p3.7.m7.1"><semantics id="S3.SS3.p3.7.m7.1a"><msub id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">G</mi><mtext id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3a.cmml">dict</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">𝐺</ci><ci id="S3.SS3.p3.7.m7.1.1.3a.cmml" xref="S3.SS3.p3.7.m7.1.1.3"><mtext id="S3.SS3.p3.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p3.7.m7.1.1.3">dict</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">G_{\text{dict}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.7.m7.1d">italic_G start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="P_{\text{dict}}" class="ltx_Math" display="inline" id="S3.SS3.p3.8.m8.1"><semantics id="S3.SS3.p3.8.m8.1a"><msub id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><mi id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml">P</mi><mtext id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3a.cmml">dict</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2">𝑃</ci><ci id="S3.SS3.p3.8.m8.1.1.3a.cmml" xref="S3.SS3.p3.8.m8.1.1.3"><mtext id="S3.SS3.p3.8.m8.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p3.8.m8.1.1.3">dict</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">P_{\text{dict}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.8.m8.1d">italic_P start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT</annotation></semantics></math> as the reference for Count Score (<math alttext="\mathbf{CS}" class="ltx_Math" display="inline" id="S3.SS3.p3.9.m9.1"><semantics id="S3.SS3.p3.9.m9.1a"><mi id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml">𝐂𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><ci id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1">𝐂𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">\mathbf{CS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.9.m9.1d">bold_CS</annotation></semantics></math>) and Hallucination Score (<math alttext="\mathbf{HS}" class="ltx_Math" display="inline" id="S3.SS3.p3.10.m10.1"><semantics id="S3.SS3.p3.10.m10.1a"><mi id="S3.SS3.p3.10.m10.1.1" xref="S3.SS3.p3.10.m10.1.1.cmml">𝐇𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.10.m10.1b"><ci id="S3.SS3.p3.10.m10.1.1.cmml" xref="S3.SS3.p3.10.m10.1.1">𝐇𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.10.m10.1c">\mathbf{HS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.10.m10.1d">bold_HS</annotation></semantics></math>), respectively, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.E2" title="2 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Eq.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}&amp;\mathbf{CS}=\frac{100}{N}\sum_{i=1}^{N}\begin{cases}\frac{\min(%
\text{CNT}^{G}_{i},\text{CNT}^{P}_{i})}{\max(\text{CNT}^{G}_{i},\text{CNT}^{P}%
_{i})}&amp;\text{if }I(\text{OBJ}^{G}_{i},P_{\text{dict}})\\
0&amp;\text{otherwise}\end{cases}\\
&amp;\mathbf{HS}=\frac{100}{M}\sum_{j=1}^{M}\begin{cases}1-\frac{\min(\text{CNT}^{%
P}_{j},\text{CNT}^{G}_{j})}{\max(\text{CNT}^{P}_{j},\text{CNT}^{G}_{j})}&amp;\text%
{if }I(\text{OBJ}^{P}_{j},G_{\text{dict}})\\
1&amp;\text{otherwise}\end{cases}\\
&amp;I(\text{OBJ},D)=\begin{cases}\text{True}&amp;\text{if }\text{OBJ}\text{ is in }%
\texttt{keys}(D)\\
\text{False}&amp;\text{otherwise}\end{cases}\\
\end{split}" class="ltx_Math" display="block" id="S3.E2.m1.34"><semantics id="S3.E2.m1.34a"><mtable columnspacing="0pt" displaystyle="true" id="S3.E2.m1.34.34" rowspacing="0pt" xref="S3.E2.m1.34.35.1.cmml"><mtr id="S3.E2.m1.34.34a" xref="S3.E2.m1.34.35.1.cmml"><mtd id="S3.E2.m1.34.34b" xref="S3.E2.m1.34.35.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.34.34c" xref="S3.E2.m1.34.35.1.cmml"><mrow id="S3.E2.m1.19.19.19.11.11" xref="S3.E2.m1.34.35.1.cmml"><mi id="S3.E2.m1.13.13.13.5.5.5" xref="S3.E2.m1.13.13.13.5.5.5.cmml">𝐂𝐒</mi><mo id="S3.E2.m1.14.14.14.6.6.6" xref="S3.E2.m1.14.14.14.6.6.6.cmml">=</mo><mrow id="S3.E2.m1.19.19.19.11.11.12" xref="S3.E2.m1.34.35.1.cmml"><mfrac id="S3.E2.m1.15.15.15.7.7.7" xref="S3.E2.m1.15.15.15.7.7.7.cmml"><mn id="S3.E2.m1.15.15.15.7.7.7.2" xref="S3.E2.m1.15.15.15.7.7.7.2.cmml">100</mn><mi id="S3.E2.m1.15.15.15.7.7.7.3" xref="S3.E2.m1.15.15.15.7.7.7.3.cmml">N</mi></mfrac><mo id="S3.E2.m1.19.19.19.11.11.12.1" xref="S3.E2.m1.34.35.1a.cmml">⁢</mo><mrow id="S3.E2.m1.19.19.19.11.11.12.2" xref="S3.E2.m1.34.35.1.cmml"><munderover id="S3.E2.m1.19.19.19.11.11.12.2.1" xref="S3.E2.m1.34.35.1.cmml"><mo id="S3.E2.m1.16.16.16.8.8.8" movablelimits="false" rspace="0em" xref="S3.E2.m1.16.16.16.8.8.8.cmml">∑</mo><mrow id="S3.E2.m1.17.17.17.9.9.9.1" xref="S3.E2.m1.17.17.17.9.9.9.1.cmml"><mi id="S3.E2.m1.17.17.17.9.9.9.1.2" xref="S3.E2.m1.17.17.17.9.9.9.1.2.cmml">i</mi><mo id="S3.E2.m1.17.17.17.9.9.9.1.1" xref="S3.E2.m1.17.17.17.9.9.9.1.1.cmml">=</mo><mn id="S3.E2.m1.17.17.17.9.9.9.1.3" xref="S3.E2.m1.17.17.17.9.9.9.1.3.cmml">1</mn></mrow><mi id="S3.E2.m1.18.18.18.10.10.10.1" xref="S3.E2.m1.18.18.18.10.10.10.1.cmml">N</mi></munderover><mrow id="S3.E2.m1.4.4.4.4.4.4" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mo id="S3.E2.m1.4.4.4.4.4.4.5" xref="S3.E2.m1.19.19.19.11.11.11.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E2.m1.4.4.4.4.4.4.4" rowspacing="0pt" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mtr id="S3.E2.m1.4.4.4.4.4.4.4a" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4.4.4.4.4b" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mstyle displaystyle="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">min</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml">⁡</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.3.cmml">G</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.3.cmml">P</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.4.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.4.1.cmml">max</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml">⁡</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.3.cmml">G</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml">,</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.3.cmml">P</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml">)</mo></mrow></mrow></mfrac></mstyle></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4.4.4.4.4c" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4a.cmml">if&nbsp;</mtext><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3.cmml">⁢</mo><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.5" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.5.cmml">I</mi><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3a" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.3.cmml"><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.3" stretchy="false" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.3.cmml">(</mo><msubsup id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2a.cmml">OBJ</mtext><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.3.cmml">i</mi><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.3.cmml">G</mi></msubsup><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.4" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.2.cmml">P</mi><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3a.cmml">dict</mtext></msub><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.5" stretchy="false" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.3.cmml">)</mo></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.4.4.4.4.4.4.4d" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4.4.4.4.4e" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mn id="S3.E2.m1.3.3.3.3.3.3.3.3.1.1" xref="S3.E2.m1.3.3.3.3.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.4.4.4.4.4.4.4f" xref="S3.E2.m1.19.19.19.11.11.11.1.cmml"><mtext id="S3.E2.m1.4.4.4.4.4.4.4.4.2.1" xref="S3.E2.m1.4.4.4.4.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.34.34d" xref="S3.E2.m1.34.35.1.cmml"><mtd id="S3.E2.m1.34.34e" xref="S3.E2.m1.34.35.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.34.34f" xref="S3.E2.m1.34.35.1.cmml"><mrow id="S3.E2.m1.26.26.26.11.11" xref="S3.E2.m1.34.35.1.cmml"><mi id="S3.E2.m1.20.20.20.5.5.5" xref="S3.E2.m1.20.20.20.5.5.5.cmml">𝐇𝐒</mi><mo id="S3.E2.m1.21.21.21.6.6.6" xref="S3.E2.m1.21.21.21.6.6.6.cmml">=</mo><mrow id="S3.E2.m1.26.26.26.11.11.12" xref="S3.E2.m1.34.35.1.cmml"><mfrac id="S3.E2.m1.22.22.22.7.7.7" xref="S3.E2.m1.22.22.22.7.7.7.cmml"><mn id="S3.E2.m1.22.22.22.7.7.7.2" xref="S3.E2.m1.22.22.22.7.7.7.2.cmml">100</mn><mi id="S3.E2.m1.22.22.22.7.7.7.3" xref="S3.E2.m1.22.22.22.7.7.7.3.cmml">M</mi></mfrac><mo id="S3.E2.m1.26.26.26.11.11.12.1" xref="S3.E2.m1.34.35.1a.cmml">⁢</mo><mrow id="S3.E2.m1.26.26.26.11.11.12.2" xref="S3.E2.m1.34.35.1.cmml"><munderover id="S3.E2.m1.26.26.26.11.11.12.2.1" xref="S3.E2.m1.34.35.1.cmml"><mo id="S3.E2.m1.23.23.23.8.8.8" movablelimits="false" rspace="0em" xref="S3.E2.m1.23.23.23.8.8.8.cmml">∑</mo><mrow id="S3.E2.m1.24.24.24.9.9.9.1" xref="S3.E2.m1.24.24.24.9.9.9.1.cmml"><mi id="S3.E2.m1.24.24.24.9.9.9.1.2" xref="S3.E2.m1.24.24.24.9.9.9.1.2.cmml">j</mi><mo id="S3.E2.m1.24.24.24.9.9.9.1.1" xref="S3.E2.m1.24.24.24.9.9.9.1.1.cmml">=</mo><mn id="S3.E2.m1.24.24.24.9.9.9.1.3" xref="S3.E2.m1.24.24.24.9.9.9.1.3.cmml">1</mn></mrow><mi id="S3.E2.m1.25.25.25.10.10.10.1" xref="S3.E2.m1.25.25.25.10.10.10.1.cmml">M</mi></munderover><mrow id="S3.E2.m1.8.8.8.4.4.4" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mo id="S3.E2.m1.8.8.8.4.4.4.5" xref="S3.E2.m1.26.26.26.11.11.11.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E2.m1.8.8.8.4.4.4.4" rowspacing="0pt" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mtr id="S3.E2.m1.8.8.8.4.4.4.4a" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.8.8.8.4.4.4.4b" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mrow id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.8" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.8.cmml">1</mn><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.7" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.7.cmml">−</mo><mstyle displaystyle="false" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.cmml"><mfrac id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6a" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.cmml"><mrow id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml"><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.cmml">min</mi><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3a" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml">⁡</mo><mrow id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml"><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.3" stretchy="false" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml">(</mo><msubsup id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.cmml"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.3.cmml">j</mi><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.3.cmml">P</mi></msubsup><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.4" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msubsup id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.cmml"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.3.cmml">j</mi><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.3.cmml">G</mi></msubsup><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.5" stretchy="false" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml"><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.4.4.1" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.4.4.1.cmml">max</mi><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3a" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml">⁡</mo><mrow id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml"><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.3" stretchy="false" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml">(</mo><msubsup id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.cmml"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.3.cmml">j</mi><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.3.cmml">P</mi></msubsup><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.4" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml">,</mo><msubsup id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.cmml"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2a.cmml">CNT</mtext><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.3.cmml">j</mi><mi id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.3" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.3.cmml">G</mi></msubsup><mo id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.5" stretchy="false" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml">)</mo></mrow></mrow></mfrac></mstyle></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.8.8.8.4.4.4.4c" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mrow id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.cmml"><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4a.cmml">if&nbsp;</mtext><mo id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3.cmml">⁢</mo><mi id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.5" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.5.cmml">I</mi><mo id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3a" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3.cmml">⁢</mo><mrow id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.3.cmml"><mo id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.3" stretchy="false" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.3.cmml">(</mo><msubsup id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.cmml"><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2a.cmml">OBJ</mtext><mi id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.3" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.3.cmml">j</mi><mi id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.3" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.3.cmml">P</mi></msubsup><mo id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.4" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.3.cmml">,</mo><msub id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.cmml"><mi id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.2" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.2.cmml">G</mi><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3a.cmml">dict</mtext></msub><mo id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.5" stretchy="false" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.3.cmml">)</mo></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.8.8.8.4.4.4.4d" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.8.8.8.4.4.4.4e" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mn id="S3.E2.m1.7.7.7.3.3.3.3.3.1.1" xref="S3.E2.m1.7.7.7.3.3.3.3.3.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.8.8.8.4.4.4.4f" xref="S3.E2.m1.26.26.26.11.11.11.1.cmml"><mtext id="S3.E2.m1.8.8.8.4.4.4.4.4.2.1" xref="S3.E2.m1.8.8.8.4.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.34.34g" xref="S3.E2.m1.34.35.1.cmml"><mtd id="S3.E2.m1.34.34h" xref="S3.E2.m1.34.35.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.34.34i" xref="S3.E2.m1.34.35.1.cmml"><mrow id="S3.E2.m1.34.34.34.12.12" xref="S3.E2.m1.34.35.1.cmml"><mrow id="S3.E2.m1.34.34.34.12.12.13" xref="S3.E2.m1.34.35.1.cmml"><mi id="S3.E2.m1.27.27.27.5.5.5" xref="S3.E2.m1.27.27.27.5.5.5.cmml">I</mi><mo id="S3.E2.m1.34.34.34.12.12.13.1" xref="S3.E2.m1.34.35.1a.cmml">⁢</mo><mrow id="S3.E2.m1.34.34.34.12.12.13.2" xref="S3.E2.m1.34.35.1.cmml"><mo id="S3.E2.m1.28.28.28.6.6.6" stretchy="false" xref="S3.E2.m1.34.35.1a.cmml">(</mo><mtext id="S3.E2.m1.29.29.29.7.7.7" xref="S3.E2.m1.29.29.29.7.7.7a.cmml">OBJ</mtext><mo id="S3.E2.m1.30.30.30.8.8.8" xref="S3.E2.m1.34.35.1a.cmml">,</mo><mi id="S3.E2.m1.31.31.31.9.9.9" xref="S3.E2.m1.31.31.31.9.9.9.cmml">D</mi><mo id="S3.E2.m1.32.32.32.10.10.10" stretchy="false" xref="S3.E2.m1.34.35.1a.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.33.33.33.11.11.11" xref="S3.E2.m1.33.33.33.11.11.11.cmml">=</mo><mrow id="S3.E2.m1.12.12.12.4.4.4" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mo id="S3.E2.m1.12.12.12.4.4.4.5" xref="S3.E2.m1.34.34.34.12.12.12.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E2.m1.12.12.12.4.4.4.4" rowspacing="0pt" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtr id="S3.E2.m1.12.12.12.4.4.4.4a" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.12.12.12.4.4.4.4b" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtext id="S3.E2.m1.9.9.9.1.1.1.1.1.1.1" xref="S3.E2.m1.9.9.9.1.1.1.1.1.1.1a.cmml">True</mtext></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.12.12.12.4.4.4.4c" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mrow id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.cmml"><mrow id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml"><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3a" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml">if&nbsp;</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3b" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml">OBJ</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3c" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml">&nbsp;is in&nbsp;</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3d" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml">𝚔𝚎𝚢𝚜</mtext></mrow><mo id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.2" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.4.2" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.cmml"><mo id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.4.2.1" stretchy="false" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.cmml">(</mo><mi id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.1" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.1.cmml">D</mi><mo id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.4.2.2" stretchy="false" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.cmml">)</mo></mrow></mrow></mtd></mtr><mtr id="S3.E2.m1.12.12.12.4.4.4.4d" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.12.12.12.4.4.4.4e" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtext id="S3.E2.m1.11.11.11.3.3.3.3.3.1.1" xref="S3.E2.m1.11.11.11.3.3.3.3.3.1.1a.cmml">False</mtext></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E2.m1.12.12.12.4.4.4.4f" xref="S3.E2.m1.34.34.34.12.12.12.1.cmml"><mtext id="S3.E2.m1.12.12.12.4.4.4.4.4.2.1" xref="S3.E2.m1.12.12.12.4.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E2.m1.34b"><apply id="S3.E2.m1.34.35.1.cmml" xref="S3.E2.m1.34.34"><and id="S3.E2.m1.34.35.1a.cmml" xref="S3.E2.m1.34.34b"></and><apply id="S3.E2.m1.34.35.1b.cmml" xref="S3.E2.m1.34.34"><eq id="S3.E2.m1.14.14.14.6.6.6.cmml" xref="S3.E2.m1.14.14.14.6.6.6"></eq><ci id="S3.E2.m1.13.13.13.5.5.5.cmml" xref="S3.E2.m1.13.13.13.5.5.5">𝐂𝐒</ci><apply id="S3.E2.m1.34.35.1.4.cmml" xref="S3.E2.m1.34.34"><times id="S3.E2.m1.34.35.1.4.1.cmml" xref="S3.E2.m1.34.34b"></times><apply id="S3.E2.m1.15.15.15.7.7.7.cmml" xref="S3.E2.m1.15.15.15.7.7.7"><divide id="S3.E2.m1.15.15.15.7.7.7.1.cmml" xref="S3.E2.m1.15.15.15.7.7.7"></divide><cn id="S3.E2.m1.15.15.15.7.7.7.2.cmml" type="integer" xref="S3.E2.m1.15.15.15.7.7.7.2">100</cn><ci id="S3.E2.m1.15.15.15.7.7.7.3.cmml" xref="S3.E2.m1.15.15.15.7.7.7.3">𝑁</ci></apply><apply id="S3.E2.m1.34.35.1.4.3.cmml" xref="S3.E2.m1.34.34"><apply id="S3.E2.m1.34.35.1.4.3.1.cmml" xref="S3.E2.m1.34.34"><csymbol cd="ambiguous" id="S3.E2.m1.34.35.1.4.3.1.1.cmml" xref="S3.E2.m1.34.34b">superscript</csymbol><apply id="S3.E2.m1.34.35.1.4.3.1.2.cmml" xref="S3.E2.m1.34.34"><csymbol cd="ambiguous" id="S3.E2.m1.34.35.1.4.3.1.2.1.cmml" xref="S3.E2.m1.34.34b">subscript</csymbol><sum id="S3.E2.m1.16.16.16.8.8.8.cmml" xref="S3.E2.m1.16.16.16.8.8.8"></sum><apply id="S3.E2.m1.17.17.17.9.9.9.1.cmml" xref="S3.E2.m1.17.17.17.9.9.9.1"><eq id="S3.E2.m1.17.17.17.9.9.9.1.1.cmml" xref="S3.E2.m1.17.17.17.9.9.9.1.1"></eq><ci id="S3.E2.m1.17.17.17.9.9.9.1.2.cmml" xref="S3.E2.m1.17.17.17.9.9.9.1.2">𝑖</ci><cn id="S3.E2.m1.17.17.17.9.9.9.1.3.cmml" type="integer" xref="S3.E2.m1.17.17.17.9.9.9.1.3">1</cn></apply></apply><ci id="S3.E2.m1.18.18.18.10.10.10.1.cmml" xref="S3.E2.m1.18.18.18.10.10.10.1">𝑁</ci></apply><apply id="S3.E2.m1.34.35.1.4.3.2.cmml" xref="S3.E2.m1.34.34"><times id="S3.E2.m1.34.35.1.4.3.2.1.cmml" xref="S3.E2.m1.34.34b"></times><apply id="S3.E2.m1.19.19.19.11.11.11.1.cmml" xref="S3.E2.m1.4.4.4.4.4.4"><csymbol cd="latexml" id="S3.E2.m1.19.19.19.11.11.11.1.1.cmml" xref="S3.E2.m1.4.4.4.4.4.4.5">cases</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"><divide id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></divide><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3"><min id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></min><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.2">CNT</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2.3">𝐺</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.2">CNT</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.2.3">𝑃</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.2.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3"><max id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.4.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.4.1"></max><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.2">CNT</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.2.3">𝐺</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.5.2.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.2">CNT</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.2.3">𝑃</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.6.3.2.2.3">𝑖</ci></apply></apply></apply><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1"><times id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.3"></times><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4a.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.4">if&nbsp;</mtext></ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.5.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.5">𝐼</ci><interval closure="open" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2"><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2a.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.2">OBJ</mtext></ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.2.3">𝐺</ci></apply><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.2">𝑃</ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3a.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3.cmml" mathsize="70%" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.2.2.2.3">dict</mtext></ci></apply></interval></apply><cn id="S3.E2.m1.3.3.3.3.3.3.3.3.1.1.cmml" type="integer" xref="S3.E2.m1.3.3.3.3.3.3.3.3.1.1">0</cn><ci id="S3.E2.m1.4.4.4.4.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.4.4.4.4.4.4.4.4.2.1"><mtext id="S3.E2.m1.4.4.4.4.4.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4.4.4.4.4.4.2.1">otherwise</mtext></ci></apply><ci id="S3.E2.m1.20.20.20.5.5.5.cmml" xref="S3.E2.m1.20.20.20.5.5.5">𝐇𝐒</ci></apply></apply></apply></apply><apply id="S3.E2.m1.34.35.1c.cmml" xref="S3.E2.m1.34.34"><eq id="S3.E2.m1.21.21.21.6.6.6.cmml" xref="S3.E2.m1.21.21.21.6.6.6"></eq><share href="#S3.E2.m1.34.35.1.4.cmml" id="S3.E2.m1.34.35.1d.cmml" xref="S3.E2.m1.34.34b"></share><apply id="S3.E2.m1.34.35.1.6.cmml" xref="S3.E2.m1.34.34"><times id="S3.E2.m1.34.35.1.6.1.cmml" xref="S3.E2.m1.34.34b"></times><apply id="S3.E2.m1.22.22.22.7.7.7.cmml" xref="S3.E2.m1.22.22.22.7.7.7"><divide id="S3.E2.m1.22.22.22.7.7.7.1.cmml" xref="S3.E2.m1.22.22.22.7.7.7"></divide><cn id="S3.E2.m1.22.22.22.7.7.7.2.cmml" type="integer" xref="S3.E2.m1.22.22.22.7.7.7.2">100</cn><ci id="S3.E2.m1.22.22.22.7.7.7.3.cmml" xref="S3.E2.m1.22.22.22.7.7.7.3">𝑀</ci></apply><apply id="S3.E2.m1.34.35.1.6.3.cmml" xref="S3.E2.m1.34.34"><apply id="S3.E2.m1.34.35.1.6.3.1.cmml" xref="S3.E2.m1.34.34"><csymbol cd="ambiguous" id="S3.E2.m1.34.35.1.6.3.1.1.cmml" xref="S3.E2.m1.34.34b">superscript</csymbol><apply id="S3.E2.m1.34.35.1.6.3.1.2.cmml" xref="S3.E2.m1.34.34"><csymbol cd="ambiguous" id="S3.E2.m1.34.35.1.6.3.1.2.1.cmml" xref="S3.E2.m1.34.34b">subscript</csymbol><sum id="S3.E2.m1.23.23.23.8.8.8.cmml" xref="S3.E2.m1.23.23.23.8.8.8"></sum><apply id="S3.E2.m1.24.24.24.9.9.9.1.cmml" xref="S3.E2.m1.24.24.24.9.9.9.1"><eq id="S3.E2.m1.24.24.24.9.9.9.1.1.cmml" xref="S3.E2.m1.24.24.24.9.9.9.1.1"></eq><ci id="S3.E2.m1.24.24.24.9.9.9.1.2.cmml" xref="S3.E2.m1.24.24.24.9.9.9.1.2">𝑗</ci><cn id="S3.E2.m1.24.24.24.9.9.9.1.3.cmml" type="integer" xref="S3.E2.m1.24.24.24.9.9.9.1.3">1</cn></apply></apply><ci id="S3.E2.m1.25.25.25.10.10.10.1.cmml" xref="S3.E2.m1.25.25.25.10.10.10.1">𝑀</ci></apply><apply id="S3.E2.m1.34.35.1.6.3.2.cmml" xref="S3.E2.m1.34.34"><times id="S3.E2.m1.34.35.1.6.3.2.1.cmml" xref="S3.E2.m1.34.34b"></times><apply id="S3.E2.m1.26.26.26.11.11.11.1.cmml" xref="S3.E2.m1.8.8.8.4.4.4"><csymbol cd="latexml" id="S3.E2.m1.26.26.26.11.11.11.1.1.cmml" xref="S3.E2.m1.8.8.8.4.4.4.5">cases</csymbol><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1"><minus id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.7"></minus><cn id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.8.cmml" type="integer" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.8">1</cn><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6"><divide id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.7.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6"></divide><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3"><min id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.1.1.1"></min><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1">superscript</csymbol><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2a.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2.cmml" mathsize="70%" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.2">CNT</mtext></ci><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.2.3">𝑃</ci></apply><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.2.2.2.1.1.3">𝑗</ci></apply><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2">subscript</csymbol><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2">superscript</csymbol><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2a.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2.cmml" mathsize="70%" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.2">CNT</mtext></ci><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.2.3">𝐺</ci></apply><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.3.3.3.2.2.3">𝑗</ci></apply></apply><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.4.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3"><max id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.4.4.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.4.4.1"></max><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1">subscript</csymbol><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1">superscript</csymbol><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2a.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2.cmml" mathsize="70%" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.2">CNT</mtext></ci><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.2.3">𝑃</ci></apply><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.5.5.2.1.1.3">𝑗</ci></apply><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2">subscript</csymbol><apply id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.1.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2">superscript</csymbol><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2a.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2"><mtext id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2.cmml" mathsize="70%" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.2">CNT</mtext></ci><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.2.3">𝐺</ci></apply><ci id="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.3.cmml" xref="S3.E2.m1.5.5.5.1.1.1.1.1.1.1.6.6.3.2.2.3">𝑗</ci></apply></apply></apply></apply><apply id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1"><times id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.3"></times><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4a.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4"><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.4">if&nbsp;</mtext></ci><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.5.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.5">𝐼</ci><interval closure="open" id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.3.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2"><apply id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2a.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2"><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.2">OBJ</mtext></ci><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.2.3">𝑃</ci></apply><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.1.1.1.3">𝑗</ci></apply><apply id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.1.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.2.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.2">𝐺</ci><ci id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3a.cmml" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3"><mtext id="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3.cmml" mathsize="70%" xref="S3.E2.m1.6.6.6.2.2.2.2.2.2.1.2.2.2.3">dict</mtext></ci></apply></interval></apply><cn id="S3.E2.m1.7.7.7.3.3.3.3.3.1.1.cmml" type="integer" xref="S3.E2.m1.7.7.7.3.3.3.3.3.1.1">1</cn><ci id="S3.E2.m1.8.8.8.4.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.8.8.8.4.4.4.4.4.2.1"><mtext id="S3.E2.m1.8.8.8.4.4.4.4.4.2.1.cmml" xref="S3.E2.m1.8.8.8.4.4.4.4.4.2.1">otherwise</mtext></ci></apply><ci id="S3.E2.m1.27.27.27.5.5.5.cmml" xref="S3.E2.m1.27.27.27.5.5.5">𝐼</ci><interval closure="open" id="S3.E2.m1.34.35.1.6.3.2.4.cmml" xref="S3.E2.m1.34.34"><ci id="S3.E2.m1.29.29.29.7.7.7a.cmml" xref="S3.E2.m1.29.29.29.7.7.7"><mtext id="S3.E2.m1.29.29.29.7.7.7.cmml" xref="S3.E2.m1.29.29.29.7.7.7">OBJ</mtext></ci><ci id="S3.E2.m1.31.31.31.9.9.9.cmml" xref="S3.E2.m1.31.31.31.9.9.9">𝐷</ci></interval></apply></apply></apply></apply><apply id="S3.E2.m1.34.35.1e.cmml" xref="S3.E2.m1.34.34"><eq id="S3.E2.m1.33.33.33.11.11.11.cmml" xref="S3.E2.m1.33.33.33.11.11.11"></eq><share href="#S3.E2.m1.34.35.1.6.cmml" id="S3.E2.m1.34.35.1f.cmml" xref="S3.E2.m1.34.34b"></share><apply id="S3.E2.m1.34.34.34.12.12.12.1.cmml" xref="S3.E2.m1.12.12.12.4.4.4"><csymbol cd="latexml" id="S3.E2.m1.34.34.34.12.12.12.1.1.cmml" xref="S3.E2.m1.12.12.12.4.4.4.5">cases</csymbol><ci id="S3.E2.m1.9.9.9.1.1.1.1.1.1.1a.cmml" xref="S3.E2.m1.9.9.9.1.1.1.1.1.1.1"><mtext id="S3.E2.m1.9.9.9.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.9.9.9.1.1.1.1.1.1.1">True</mtext></ci><apply id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1"><times id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.2.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.2"></times><ci id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3e.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3"><mrow id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3"><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3a.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3">if&nbsp;</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3b.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3">OBJ</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3c.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3">&nbsp;is in&nbsp;</mtext><mtext id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3d.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.3">𝚔𝚎𝚢𝚜</mtext></mrow></ci><ci id="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.10.10.10.2.2.2.2.2.2.1.1">𝐷</ci></apply><ci id="S3.E2.m1.11.11.11.3.3.3.3.3.1.1a.cmml" xref="S3.E2.m1.11.11.11.3.3.3.3.3.1.1"><mtext id="S3.E2.m1.11.11.11.3.3.3.3.3.1.1.cmml" xref="S3.E2.m1.11.11.11.3.3.3.3.3.1.1">False</mtext></ci><ci id="S3.E2.m1.12.12.12.4.4.4.4.4.2.1a.cmml" xref="S3.E2.m1.12.12.12.4.4.4.4.4.2.1"><mtext id="S3.E2.m1.12.12.12.4.4.4.4.4.2.1.cmml" xref="S3.E2.m1.12.12.12.4.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.34c">\begin{split}&amp;\mathbf{CS}=\frac{100}{N}\sum_{i=1}^{N}\begin{cases}\frac{\min(%
\text{CNT}^{G}_{i},\text{CNT}^{P}_{i})}{\max(\text{CNT}^{G}_{i},\text{CNT}^{P}%
_{i})}&amp;\text{if }I(\text{OBJ}^{G}_{i},P_{\text{dict}})\\
0&amp;\text{otherwise}\end{cases}\\
&amp;\mathbf{HS}=\frac{100}{M}\sum_{j=1}^{M}\begin{cases}1-\frac{\min(\text{CNT}^{%
P}_{j},\text{CNT}^{G}_{j})}{\max(\text{CNT}^{P}_{j},\text{CNT}^{G}_{j})}&amp;\text%
{if }I(\text{OBJ}^{P}_{j},G_{\text{dict}})\\
1&amp;\text{otherwise}\end{cases}\\
&amp;I(\text{OBJ},D)=\begin{cases}\text{True}&amp;\text{if }\text{OBJ}\text{ is in }%
\texttt{keys}(D)\\
\text{False}&amp;\text{otherwise}\end{cases}\\
\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.34d">start_ROW start_CELL end_CELL start_CELL bold_CS = divide start_ARG 100 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT { start_ROW start_CELL divide start_ARG roman_min ( CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG roman_max ( CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG end_CELL start_CELL if italic_I ( OBJ start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL bold_HS = divide start_ARG 100 end_ARG start_ARG italic_M end_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT { start_ROW start_CELL 1 - divide start_ARG roman_min ( CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG roman_max ( CNT start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , CNT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG end_CELL start_CELL if italic_I ( OBJ start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT dict end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL otherwise end_CELL end_ROW end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL italic_I ( OBJ , italic_D ) = { start_ROW start_CELL True end_CELL start_CELL if roman_OBJ is in typewriter_keys ( italic_D ) end_CELL end_ROW start_ROW start_CELL False end_CELL start_CELL otherwise end_CELL end_ROW end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Count Score (CS)</span>. It represents the percentage of correct object counts predicted by the MLLM with respect to the ground-truth sentence. The higher the <span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.2">CS</span>, the better.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Hallucination Score (HS)</span>. It represents the percentage of extra object counts predicted by the MLLM that do not exist in the ground-truth sentence. The lower the <span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.2">HS</span>, the better.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Note that due to the one-to-one word-matching nature of our evaluation, we manually define a mapping between the categories in COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> and their synonyms&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. For example, we replace words like <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.1">man, woman, child, kid, boy, girl</span>, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p6.1.2">etc.</em> with the word <span class="ltx_text ltx_font_italic" id="S3.SS3.p6.1.3">person</span> in the MLLM’s response before evaluation.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="539" id="S3.F5.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x5.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.5.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F5.6.2" style="font-size:90%;">Evaluation Metrics for Object Identification<span class="ltx_text ltx_font_medium" id="S3.F5.6.2.1">. We compare the object counts in the ground truth and prediction to calculate a count score (</span>CS<span class="ltx_text ltx_font_medium" id="S3.F5.6.2.2">) and a hallucination score (</span>HS<span class="ltx_text ltx_font_medium" id="S3.F5.6.2.3">).</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.58" style="width:541.7pt;height:307pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.58.58">
<tbody><tr class="ltx_tr" id="S3.T1.58.58.59">
<td class="ltx_td ltx_border_r" id="S3.T1.58.58.59.1"></td>
<td class="ltx_td ltx_border_r" id="S3.T1.58.58.59.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="2" id="S3.T1.58.58.59.3">Semantic</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="2" id="S3.T1.58.58.59.4">Instance</td>
<td class="ltx_td ltx_align_center" colspan="2" id="S3.T1.58.58.59.5">Panoptic</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.6.6.6.7">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.8">Input Tokens</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.1">CS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.2.2.2">HS (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">↓</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3">CS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.3.m1.1a"><mo id="S3.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.4.4.4">HS (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.m1.1"><semantics id="S3.T1.4.4.4.4.m1.1a"><mo id="S3.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S3.T1.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.m1.1d">↓</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5">CS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.m1.1"><semantics id="S3.T1.5.5.5.5.m1.1a"><mo id="S3.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S3.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.6">HS (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.m1.1"><semantics id="S3.T1.6.6.6.6.m1.1a"><mo id="S3.T1.6.6.6.6.m1.1.1" stretchy="false" xref="S3.T1.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.m1.1b"><ci id="S3.T1.6.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.m1.1d">↓</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.58.58.60">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="8" id="S3.T1.58.58.60.1">
<span class="ltx_text ltx_font_italic" id="S3.T1.58.58.60.1.1">Closed Model, Open API</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.10.10.10.5">GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.10.10.10.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.1.m1.1a"><mo id="S3.T1.7.7.7.1.m1.1.1" stretchy="false" xref="S3.T1.7.7.7.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><ci id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.8.8.8.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.8.8.8.2.1.m1.1"><semantics id="S3.T1.8.8.8.2.1.m1.1a"><mo id="S3.T1.8.8.8.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.8.8.8.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.2.1.m1.1b"><ci id="S3.T1.8.8.8.2.1.m1.1.1.cmml" xref="S3.T1.8.8.8.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.9.9.9.3.m2.1"><semantics id="S3.T1.9.9.9.3.m2.1a"><mo id="S3.T1.9.9.9.3.m2.1.1" stretchy="false" xref="S3.T1.9.9.9.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.3.m2.1b"><ci id="S3.T1.9.9.9.3.m2.1.1.cmml" xref="S3.T1.9.9.9.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.10.10.10.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.10.10.10.4.2.m1.1"><semantics id="S3.T1.10.10.10.4.2.m1.1a"><mo id="S3.T1.10.10.10.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.10.10.10.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.4.2.m1.1b"><ci id="S3.T1.10.10.10.4.2.m1.1.1.cmml" xref="S3.T1.10.10.10.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.10.10.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.10.10.10.6">—</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.10.10.10.7">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.10.10.10.8">—</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.10.10.10.9">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.10.10.10.10">38.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.10.10.10.11">83.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.58.58.61">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="8" id="S3.T1.58.58.61.1">
<span class="ltx_text ltx_font_italic" id="S3.T1.58.58.61.1.1">Existing Open-Source Multimodal LLMs</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.14.14.14">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.14.14.14.5">MiniGPT-4 LLaMA-2-7b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.14.14.14.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.11.11.11.1.m1.1"><semantics id="S3.T1.11.11.11.1.m1.1a"><mo id="S3.T1.11.11.11.1.m1.1.1" stretchy="false" xref="S3.T1.11.11.11.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.11.1.m1.1b"><ci id="S3.T1.11.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.11.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.11.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.11.11.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.12.12.12.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.12.12.12.2.1.m1.1"><semantics id="S3.T1.12.12.12.2.1.m1.1a"><mo id="S3.T1.12.12.12.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.12.12.12.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.12.2.1.m1.1b"><ci id="S3.T1.12.12.12.2.1.m1.1.1.cmml" xref="S3.T1.12.12.12.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.12.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.12.12.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.13.13.13.3.m2.1"><semantics id="S3.T1.13.13.13.3.m2.1a"><mo id="S3.T1.13.13.13.3.m2.1.1" stretchy="false" xref="S3.T1.13.13.13.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.13.3.m2.1b"><ci id="S3.T1.13.13.13.3.m2.1.1.cmml" xref="S3.T1.13.13.13.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.13.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.13.13.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.14.14.14.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.14.14.14.4.2.m1.1"><semantics id="S3.T1.14.14.14.4.2.m1.1a"><mo id="S3.T1.14.14.14.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.14.14.14.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.14.4.2.m1.1b"><ci id="S3.T1.14.14.14.4.2.m1.1.1.cmml" xref="S3.T1.14.14.14.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.14.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.14.14.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.14.14.14.6">6.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.14.14.14.7">92.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.14.14.14.8">5.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.14.14.14.9">97.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.14.14.14.10">6.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.14.14.14.11">94.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.18.18.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.18.18.18.5">InstructBLIP Vicuna-7b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.18.18.18.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.15.15.15.1.m1.1"><semantics id="S3.T1.15.15.15.1.m1.1a"><mo id="S3.T1.15.15.15.1.m1.1.1" stretchy="false" xref="S3.T1.15.15.15.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.15.1.m1.1b"><ci id="S3.T1.15.15.15.1.m1.1.1.cmml" xref="S3.T1.15.15.15.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.15.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.15.15.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.16.16.16.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.16.16.16.2.1.m1.1"><semantics id="S3.T1.16.16.16.2.1.m1.1a"><mo id="S3.T1.16.16.16.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.16.16.16.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.16.16.16.2.1.m1.1b"><ci id="S3.T1.16.16.16.2.1.m1.1.1.cmml" xref="S3.T1.16.16.16.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.16.16.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.16.16.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.17.17.17.3.m2.1"><semantics id="S3.T1.17.17.17.3.m2.1a"><mo id="S3.T1.17.17.17.3.m2.1.1" stretchy="false" xref="S3.T1.17.17.17.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.17.17.17.3.m2.1b"><ci id="S3.T1.17.17.17.3.m2.1.1.cmml" xref="S3.T1.17.17.17.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.17.17.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.17.17.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.18.18.18.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.18.18.18.4.2.m1.1"><semantics id="S3.T1.18.18.18.4.2.m1.1a"><mo id="S3.T1.18.18.18.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.18.18.18.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.18.18.18.4.2.m1.1b"><ci id="S3.T1.18.18.18.4.2.m1.1.1.cmml" xref="S3.T1.18.18.18.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.18.18.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.18.18.18.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.18.18.18.6">14.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.18.18.18.7">85.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.18.18.18.8">25.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.18.18.18.9">91.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.18.18.18.10">17.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.18.18.18.11">91.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.22.22.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.22.22.22.5">LLaVA-1.5-7b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.22.22.22.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.19.19.19.1.m1.1"><semantics id="S3.T1.19.19.19.1.m1.1a"><mo id="S3.T1.19.19.19.1.m1.1.1" stretchy="false" xref="S3.T1.19.19.19.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.19.19.19.1.m1.1b"><ci id="S3.T1.19.19.19.1.m1.1.1.cmml" xref="S3.T1.19.19.19.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.19.19.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.19.19.19.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.20.20.20.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.20.20.20.2.1.m1.1"><semantics id="S3.T1.20.20.20.2.1.m1.1a"><mo id="S3.T1.20.20.20.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.20.20.20.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.20.20.20.2.1.m1.1b"><ci id="S3.T1.20.20.20.2.1.m1.1.1.cmml" xref="S3.T1.20.20.20.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.20.20.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.20.20.20.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.21.21.21.3.m2.1"><semantics id="S3.T1.21.21.21.3.m2.1a"><mo id="S3.T1.21.21.21.3.m2.1.1" stretchy="false" xref="S3.T1.21.21.21.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.21.21.21.3.m2.1b"><ci id="S3.T1.21.21.21.3.m2.1.1.cmml" xref="S3.T1.21.21.21.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.21.21.21.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.21.21.21.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.22.22.22.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.22.22.22.4.2.m1.1"><semantics id="S3.T1.22.22.22.4.2.m1.1a"><mo id="S3.T1.22.22.22.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.22.22.22.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.22.22.22.4.2.m1.1b"><ci id="S3.T1.22.22.22.4.2.m1.1.1.cmml" xref="S3.T1.22.22.22.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.22.22.22.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.22.22.22.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.22.22.22.6">30.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.22.22.22.7">60.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.22.22.22.8">50.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.22.22.22.9">75.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.22.22.22.10">38.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.22.22.22.11">67.3</td>
</tr>
<tr class="ltx_tr" id="S3.T1.26.26.26">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.26.26.26.5">LLaVA-1.5-13b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.26.26.26.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.23.23.23.1.m1.1"><semantics id="S3.T1.23.23.23.1.m1.1a"><mo id="S3.T1.23.23.23.1.m1.1.1" stretchy="false" xref="S3.T1.23.23.23.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.23.23.23.1.m1.1b"><ci id="S3.T1.23.23.23.1.m1.1.1.cmml" xref="S3.T1.23.23.23.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.23.23.23.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.23.23.23.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.24.24.24.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.24.24.24.2.1.m1.1"><semantics id="S3.T1.24.24.24.2.1.m1.1a"><mo id="S3.T1.24.24.24.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.24.24.24.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.24.24.24.2.1.m1.1b"><ci id="S3.T1.24.24.24.2.1.m1.1.1.cmml" xref="S3.T1.24.24.24.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.24.24.24.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.24.24.24.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.25.25.25.3.m2.1"><semantics id="S3.T1.25.25.25.3.m2.1a"><mo id="S3.T1.25.25.25.3.m2.1.1" stretchy="false" xref="S3.T1.25.25.25.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.25.25.25.3.m2.1b"><ci id="S3.T1.25.25.25.3.m2.1.1.cmml" xref="S3.T1.25.25.25.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.25.25.25.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.25.25.25.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.26.26.26.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.26.26.26.4.2.m1.1"><semantics id="S3.T1.26.26.26.4.2.m1.1a"><mo id="S3.T1.26.26.26.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.26.26.26.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.26.26.26.4.2.m1.1b"><ci id="S3.T1.26.26.26.4.2.m1.1.1.cmml" xref="S3.T1.26.26.26.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.26.26.26.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.26.26.26.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.26.26.26.6">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.26.26.26.7">69.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.26.26.26.8">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.26.26.26.9">75.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.26.26.26.10">35.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.26.26.26.11">68.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.30.30.30">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.30.30.30.5">CogVLM-17b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.30.30.30.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.27.27.27.1.m1.1"><semantics id="S3.T1.27.27.27.1.m1.1a"><mo id="S3.T1.27.27.27.1.m1.1.1" stretchy="false" xref="S3.T1.27.27.27.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.27.27.27.1.m1.1b"><ci id="S3.T1.27.27.27.1.m1.1.1.cmml" xref="S3.T1.27.27.27.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.27.27.27.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.27.27.27.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.28.28.28.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.28.28.28.2.1.m1.1"><semantics id="S3.T1.28.28.28.2.1.m1.1a"><mo id="S3.T1.28.28.28.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.28.28.28.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.28.28.28.2.1.m1.1b"><ci id="S3.T1.28.28.28.2.1.m1.1.1.cmml" xref="S3.T1.28.28.28.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.28.28.28.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.28.28.28.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.29.29.29.3.m2.1"><semantics id="S3.T1.29.29.29.3.m2.1a"><mo id="S3.T1.29.29.29.3.m2.1.1" stretchy="false" xref="S3.T1.29.29.29.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.29.29.29.3.m2.1b"><ci id="S3.T1.29.29.29.3.m2.1.1.cmml" xref="S3.T1.29.29.29.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.29.29.29.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.29.29.29.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.30.30.30.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.30.30.30.4.2.m1.1"><semantics id="S3.T1.30.30.30.4.2.m1.1a"><mo id="S3.T1.30.30.30.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.30.30.30.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.30.30.30.4.2.m1.1b"><ci id="S3.T1.30.30.30.4.2.m1.1.1.cmml" xref="S3.T1.30.30.30.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.30.30.30.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.30.30.30.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.30.30.30.6">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.30.30.30.7">67.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.30.30.30.8">43.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.30.30.30.9">86.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.30.30.30.10">40.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.30.30.30.11">75.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.58.58.62">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="8" id="S3.T1.58.58.62.1">
<span class="ltx_text ltx_font_italic" id="S3.T1.58.58.62.1.1">Baselines trained on the <span class="ltx_text ltx_font_bold" id="S3.T1.58.58.62.1.1.1">COST</span> dataset</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.34.34.34">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.34.34.34.5">COST IT LLaVA-1.5-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.34.34.34.4">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.31.31.31.1.m1.1"><semantics id="S3.T1.31.31.31.1.m1.1a"><mo id="S3.T1.31.31.31.1.m1.1.1" stretchy="false" xref="S3.T1.31.31.31.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.31.31.31.1.m1.1b"><ci id="S3.T1.31.31.31.1.m1.1.1.cmml" xref="S3.T1.31.31.31.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.31.31.31.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.31.31.31.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.32.32.32.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.32.32.32.2.1.m1.1"><semantics id="S3.T1.32.32.32.2.1.m1.1a"><mo id="S3.T1.32.32.32.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.32.32.32.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.32.32.32.2.1.m1.1b"><ci id="S3.T1.32.32.32.2.1.m1.1.1.cmml" xref="S3.T1.32.32.32.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.32.32.32.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.32.32.32.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.33.33.33.3.m2.1"><semantics id="S3.T1.33.33.33.3.m2.1a"><mo id="S3.T1.33.33.33.3.m2.1.1" stretchy="false" xref="S3.T1.33.33.33.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.33.33.33.3.m2.1b"><ci id="S3.T1.33.33.33.3.m2.1.1.cmml" xref="S3.T1.33.33.33.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.33.33.33.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.33.33.33.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.34.34.34.4.2">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.34.34.34.4.2.m1.1"><semantics id="S3.T1.34.34.34.4.2.m1.1a"><mo id="S3.T1.34.34.34.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.34.34.34.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.34.34.34.4.2.m1.1b"><ci id="S3.T1.34.34.34.4.2.m1.1.1.cmml" xref="S3.T1.34.34.34.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.34.34.34.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.34.34.34.4.2.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.34.34.34.6">78.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.34.34.34.7">22.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.34.34.34.8">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.34.34.34.9">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.34.34.34.10">71.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.34.34.34.11">28.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.40.40.40">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.40.40.40.7">Soft-Prompted LLaVA-1.5-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.40.40.40.6">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.35.35.35.1.m1.1"><semantics id="S3.T1.35.35.35.1.m1.1a"><mo id="S3.T1.35.35.35.1.m1.1.1" stretchy="false" xref="S3.T1.35.35.35.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.35.35.35.1.m1.1b"><ci id="S3.T1.35.35.35.1.m1.1.1.cmml" xref="S3.T1.35.35.35.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.35.35.35.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.35.35.35.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.36.36.36.2.1">prompt<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.36.36.36.2.1.m1.1"><semantics id="S3.T1.36.36.36.2.1.m1.1a"><mo id="S3.T1.36.36.36.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.36.36.36.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.36.36.36.2.1.m1.1b"><ci id="S3.T1.36.36.36.2.1.m1.1.1.cmml" xref="S3.T1.36.36.36.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.36.36.36.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.36.36.36.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.37.37.37.3.m2.1"><semantics id="S3.T1.37.37.37.3.m2.1a"><mo id="S3.T1.37.37.37.3.m2.1.1" stretchy="false" xref="S3.T1.37.37.37.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.37.37.37.3.m2.1b"><ci id="S3.T1.37.37.37.3.m2.1.1.cmml" xref="S3.T1.37.37.37.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.37.37.37.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.37.37.37.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.38.38.38.4.2">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.38.38.38.4.2.m1.1"><semantics id="S3.T1.38.38.38.4.2.m1.1a"><mo id="S3.T1.38.38.38.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.38.38.38.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.38.38.38.4.2.m1.1b"><ci id="S3.T1.38.38.38.4.2.m1.1.1.cmml" xref="S3.T1.38.38.38.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.38.38.38.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.38.38.38.4.2.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.39.39.39.5.m3.1"><semantics id="S3.T1.39.39.39.5.m3.1a"><mo id="S3.T1.39.39.39.5.m3.1.1" stretchy="false" xref="S3.T1.39.39.39.5.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.39.39.39.5.m3.1b"><ci id="S3.T1.39.39.39.5.m3.1.1.cmml" xref="S3.T1.39.39.39.5.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.39.39.39.5.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.39.39.39.5.m3.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.40.40.40.6.3">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.40.40.40.6.3.m1.1"><semantics id="S3.T1.40.40.40.6.3.m1.1a"><mo id="S3.T1.40.40.40.6.3.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.40.40.40.6.3.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.40.40.40.6.3.m1.1b"><ci id="S3.T1.40.40.40.6.3.m1.1.1.cmml" xref="S3.T1.40.40.40.6.3.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.40.40.40.6.3.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.40.40.40.6.3.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.40.40.40.8">36.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.40.40.40.9">56.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.40.40.40.10">18.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.40.40.40.11">72.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.40.40.40.12">26.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.40.40.40.13">63.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.46.46.46">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.46.46.46.7">ImCoder LLaVA-1.5-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.46.46.46.6">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.41.41.41.1.m1.1"><semantics id="S3.T1.41.41.41.1.m1.1a"><mo id="S3.T1.41.41.41.1.m1.1.1" stretchy="false" xref="S3.T1.41.41.41.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.41.41.41.1.m1.1b"><ci id="S3.T1.41.41.41.1.m1.1.1.cmml" xref="S3.T1.41.41.41.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.41.41.41.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.41.41.41.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.42.42.42.2.1">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.42.42.42.2.1.m1.1"><semantics id="S3.T1.42.42.42.2.1.m1.1a"><mo id="S3.T1.42.42.42.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.42.42.42.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.42.42.42.2.1.m1.1b"><ci id="S3.T1.42.42.42.2.1.m1.1.1.cmml" xref="S3.T1.42.42.42.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.42.42.42.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.42.42.42.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.43.43.43.3.m2.1"><semantics id="S3.T1.43.43.43.3.m2.1a"><mo id="S3.T1.43.43.43.3.m2.1.1" stretchy="false" xref="S3.T1.43.43.43.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.43.43.43.3.m2.1b"><ci id="S3.T1.43.43.43.3.m2.1.1.cmml" xref="S3.T1.43.43.43.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.43.43.43.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.43.43.43.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.44.44.44.4.2">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.44.44.44.4.2.m1.1"><semantics id="S3.T1.44.44.44.4.2.m1.1a"><mo id="S3.T1.44.44.44.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.44.44.44.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.44.44.44.4.2.m1.1b"><ci id="S3.T1.44.44.44.4.2.m1.1.1.cmml" xref="S3.T1.44.44.44.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.44.44.44.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.44.44.44.4.2.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.45.45.45.5.m3.1"><semantics id="S3.T1.45.45.45.5.m3.1a"><mo id="S3.T1.45.45.45.5.m3.1.1" stretchy="false" xref="S3.T1.45.45.45.5.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.45.45.45.5.m3.1b"><ci id="S3.T1.45.45.45.5.m3.1.1.cmml" xref="S3.T1.45.45.45.5.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.45.45.45.5.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.45.45.45.5.m3.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.46.46.46.6.3">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.46.46.46.6.3.m1.1"><semantics id="S3.T1.46.46.46.6.3.m1.1a"><mo id="S3.T1.46.46.46.6.3.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.46.46.46.6.3.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.46.46.46.6.3.m1.1b"><ci id="S3.T1.46.46.46.6.3.m1.1.1.cmml" xref="S3.T1.46.46.46.6.3.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.46.46.46.6.3.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.46.46.46.6.3.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.46.46.46.8">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.46.46.46.9">22.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.46.46.46.10">64.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.46.46.46.11">29.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.46.46.46.12">70.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.46.46.46.13">27.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.58.58.63">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="8" id="S3.T1.58.58.63.1">
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.58.58.63.1.1">VCoder<span class="ltx_text ltx_font_medium" id="S3.T1.58.58.63.1.1.1"> augmented LLaVA-1.5</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.52.52.52">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.52.52.52.7">
<span class="ltx_text ltx_font_bold" id="S3.T1.52.52.52.7.1">VCoder</span> LLaVA-1.5-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.52.52.52.6">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.47.47.47.1.m1.1"><semantics id="S3.T1.47.47.47.1.m1.1a"><mo id="S3.T1.47.47.47.1.m1.1.1" stretchy="false" xref="S3.T1.47.47.47.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.47.47.47.1.m1.1b"><ci id="S3.T1.47.47.47.1.m1.1.1.cmml" xref="S3.T1.47.47.47.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.47.47.47.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.47.47.47.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.48.48.48.2.1">seg<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.48.48.48.2.1.m1.1"><semantics id="S3.T1.48.48.48.2.1.m1.1a"><mo id="S3.T1.48.48.48.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.48.48.48.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.48.48.48.2.1.m1.1b"><ci id="S3.T1.48.48.48.2.1.m1.1.1.cmml" xref="S3.T1.48.48.48.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.48.48.48.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.48.48.48.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.49.49.49.3.m2.1"><semantics id="S3.T1.49.49.49.3.m2.1a"><mo id="S3.T1.49.49.49.3.m2.1.1" stretchy="false" xref="S3.T1.49.49.49.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.49.49.49.3.m2.1b"><ci id="S3.T1.49.49.49.3.m2.1.1.cmml" xref="S3.T1.49.49.49.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.49.49.49.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.49.49.49.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.50.50.50.4.2">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.50.50.50.4.2.m1.1"><semantics id="S3.T1.50.50.50.4.2.m1.1a"><mo id="S3.T1.50.50.50.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.50.50.50.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.50.50.50.4.2.m1.1b"><ci id="S3.T1.50.50.50.4.2.m1.1.1.cmml" xref="S3.T1.50.50.50.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.50.50.50.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.50.50.50.4.2.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.51.51.51.5.m3.1"><semantics id="S3.T1.51.51.51.5.m3.1a"><mo id="S3.T1.51.51.51.5.m3.1.1" stretchy="false" xref="S3.T1.51.51.51.5.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.51.51.51.5.m3.1b"><ci id="S3.T1.51.51.51.5.m3.1.1.cmml" xref="S3.T1.51.51.51.5.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.51.51.51.5.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.51.51.51.5.m3.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.52.52.52.6.3">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.52.52.52.6.3.m1.1"><semantics id="S3.T1.52.52.52.6.3.m1.1a"><mo id="S3.T1.52.52.52.6.3.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.52.52.52.6.3.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.52.52.52.6.3.m1.1b"><ci id="S3.T1.52.52.52.6.3.m1.1.1.cmml" xref="S3.T1.52.52.52.6.3.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.52.52.52.6.3.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.52.52.52.6.3.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.52.52.52.8"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.8.1">88.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.52.52.52.9"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.9.1">10.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.52.52.52.10"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.10.1">71.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.52.52.52.11"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.11.1">26.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.52.52.52.12"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.12.1">86.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.52.52.52.13"><span class="ltx_text ltx_framed_underline" id="S3.T1.52.52.52.13.1">12.8</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.58.58.58">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.58.58.58.7">
<span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.7.1">VCoder</span> LLaVA-1.5-13b</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.58.58.58.6">
<math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.53.53.53.1.m1.1"><semantics id="S3.T1.53.53.53.1.m1.1a"><mo id="S3.T1.53.53.53.1.m1.1.1" stretchy="false" xref="S3.T1.53.53.53.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.53.53.53.1.m1.1b"><ci id="S3.T1.53.53.53.1.m1.1.1.cmml" xref="S3.T1.53.53.53.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.53.53.53.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.53.53.53.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.54.54.54.2.1">seg<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.54.54.54.2.1.m1.1"><semantics id="S3.T1.54.54.54.2.1.m1.1a"><mo id="S3.T1.54.54.54.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.54.54.54.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.54.54.54.2.1.m1.1b"><ci id="S3.T1.54.54.54.2.1.m1.1.1.cmml" xref="S3.T1.54.54.54.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.54.54.54.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.54.54.54.2.1.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.55.55.55.3.m2.1"><semantics id="S3.T1.55.55.55.3.m2.1a"><mo id="S3.T1.55.55.55.3.m2.1.1" stretchy="false" xref="S3.T1.55.55.55.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.55.55.55.3.m2.1b"><ci id="S3.T1.55.55.55.3.m2.1.1.cmml" xref="S3.T1.55.55.55.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.55.55.55.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.55.55.55.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.56.56.56.4.2">img<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.56.56.56.4.2.m1.1"><semantics id="S3.T1.56.56.56.4.2.m1.1a"><mo id="S3.T1.56.56.56.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.56.56.56.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.56.56.56.4.2.m1.1b"><ci id="S3.T1.56.56.56.4.2.m1.1.1.cmml" xref="S3.T1.56.56.56.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.56.56.56.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.56.56.56.4.2.m1.1d">⟩</annotation></semantics></math></span> + <math alttext="\langle" class="ltx_Math" display="inline" id="S3.T1.57.57.57.5.m3.1"><semantics id="S3.T1.57.57.57.5.m3.1a"><mo id="S3.T1.57.57.57.5.m3.1.1" stretchy="false" xref="S3.T1.57.57.57.5.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S3.T1.57.57.57.5.m3.1b"><ci id="S3.T1.57.57.57.5.m3.1.1.cmml" xref="S3.T1.57.57.57.5.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.57.57.57.5.m3.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.57.57.57.5.m3.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.T1.58.58.58.6.3">query<math alttext="\rangle" class="ltx_Math" display="inline" id="S3.T1.58.58.58.6.3.m1.1"><semantics id="S3.T1.58.58.58.6.3.m1.1a"><mo id="S3.T1.58.58.58.6.3.m1.1.1" mathvariant="normal" stretchy="false" xref="S3.T1.58.58.58.6.3.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S3.T1.58.58.58.6.3.m1.1b"><ci id="S3.T1.58.58.58.6.3.m1.1.1.cmml" xref="S3.T1.58.58.58.6.3.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.58.58.58.6.3.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.58.58.58.6.3.m1.1d">⟩</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.58.58.58.8"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.8.1">89.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.58.58.58.9"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.9.1">10.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.58.58.58.10"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.10.1">73.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.58.58.58.11"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.11.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.58.58.58.12"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.12.1">87.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.58.58.58.13"><span class="ltx_text ltx_font_bold" id="S3.T1.58.58.58.13.1">11.6</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.71.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T1.60.1" style="font-size:90%;">Comparison to baseline Multimodal LLMs on the COST validation dataset for Object Identification.<span class="ltx_text ltx_font_medium" id="S3.T1.60.1.2"> We compare our VCoder to existing off-the-shelf baseline MLLMs: MiniGPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>, InstructBLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, and CogVLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>. We also train three different variants of LLaVA-1.5 on the COST dataset: <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.2.1">COST IT</span> mixes the COST training data with the instruction tuning data; <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.2.2">Soft-Prompted</span> uses a set of learnable tokens, and <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.2.3">ImCoder</span> uses an RGB image as the control input. Our </span>VCoder<span class="ltx_text ltx_font_medium" id="S3.T1.60.1.1"> adapted LLaVA-1.5 performs the best on all three object perception tasks. Note: <math alttext="\langle\cdot\rangle" class="ltx_Math" display="inline" id="S3.T1.60.1.1.m1.1"><semantics id="S3.T1.60.1.1.m1.1b"><mrow id="S3.T1.60.1.1.m1.1.2.2" xref="S3.T1.60.1.1.m1.1.2.1.cmml"><mo id="S3.T1.60.1.1.m1.1.2.2.1" mathvariant="normal" stretchy="false" xref="S3.T1.60.1.1.m1.1.2.1.1.cmml">⟨</mo><mo id="S3.T1.60.1.1.m1.1.1" lspace="0em" mathvariant="normal" rspace="0em" xref="S3.T1.60.1.1.m1.1.1.cmml">⋅</mo><mo id="S3.T1.60.1.1.m1.1.2.2.2" mathvariant="normal" stretchy="false" xref="S3.T1.60.1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.60.1.1.m1.1c"><apply id="S3.T1.60.1.1.m1.1.2.1.cmml" xref="S3.T1.60.1.1.m1.1.2.2"><csymbol cd="latexml" id="S3.T1.60.1.1.m1.1.2.1.1.cmml" xref="S3.T1.60.1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.T1.60.1.1.m1.1.1.cmml" xref="S3.T1.60.1.1.m1.1.1">normal-⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.60.1.1.m1.1d">\langle\cdot\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.T1.60.1.1.m1.1e">⟨ ⋅ ⟩</annotation></semantics></math> denotes input tokens to LLM with <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.1.1">seg</span> representing segmentation map, <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.1.2">img</span> representing RGB image, <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.1.3">prompt</span> representing learnable prompt, and <span class="ltx_text ltx_font_italic" id="S3.T1.60.1.1.4">query</span> representing the user question. We also evaluate the performance of GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> on the COST dataset using the publicly accessible paid API released by OpenAI. Our VCoder-adapted LLaVA-1.5 shows the best performance on object identification among all MLLMs.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We use LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> as our base MLLM. LLaVA-1.5 uses CLIP-ViT-L-336px&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> as the image encoder (ImCoder) with a two-layer MLP as projection and Vicuna-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> as the LLM. Inside our VCoder, we also use a CLIP-ViT-L-336px to encode the control inputs and project the features into the LLM embedding space using modality-specific two-layer MLPs. We resize the visual inputs to 336<math alttext="\times" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">×</annotation></semantics></math>336 resolution (corresponds to 576 tokens) for our MLLM. During training, we load the instruction-tuned weights from LLaVA-1.5 and keep those frozen while only tuning the MLP component of our VCoder. We use the publicly available OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> model trained on COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> with DiNAT-L&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> backbone to obtain the segmentation map. For getting depth maps, we use the publicly available ViT-L/14 distilled variant of DINOv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> DPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> trained on the NYUd&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> dataset. In this section, we discuss our results on the object identification task. Please refer to <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S5" title="5 Object Order Perception with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> for our results on the object order perception task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Training Details.</span> We train our VCoder-adapted LLaVA-1.5 framework for two epochs on the COST training dataset with a batch size 256 and a learning rate of 1<math alttext="e^{-3}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msup id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">e</mi><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mo id="S4.SS1.p1.1.m1.1.1.3a" xref="S4.SS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝑒</ci><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><minus id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3"></minus><cn id="S4.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">e^{-3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math>. For other training hyperparameters, we follow the settings used during the instruction-tuning stage in LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, we uniformly sample each object identification task (semantic, instance, and panoptic) during training. We also use the corresponding segmentation map from OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> as input to the VCoder during training and inference. On 8 A100 GPUs, it takes 8 and 14 hours to train our VCoder with the 7b and 13b variants of LLaVA-1.5 as the base MLLM, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Evaluation Details.</span> We evaluate all MLLMs on the COST validation set. We separately evaluate semantic, instance, and panoptic object identification tasks while randomly sampling questions from the corresponding task’s question bucket. Note that for evaluating all off-the-shelf MLLMs, we experiment with various prompts and finally use the prompt: <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">“[QUESTION]. Return the answer in the paragraph format: ‘The objects present in the image are: …’ and then list the objects with their count in word format (if greater than 1) in front of them, like ’two people’.”</span>, where [QUESTION] is the randomly sampled question from the object identification task bucket.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Baselines.</span> We compare the performance of VCoder to open-source Multimodal LLMs, namely, MiniGPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>, InstructBLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, and CogVLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> on the COST validation set in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.T1" title="Table 1 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>. Furthermore, we also provide three additional baselines, all trained for two epochs:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_framed_underline" id="S4.SS2.p2.1.1">COST IT LLaVA-1.5</span>: We mix the COST training data with the instruction tuning data used in LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and finetune a LLaVA-1.5 model from scratch following the settings from Liu <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.2">et al.</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.4"><span class="ltx_text ltx_framed_underline" id="S4.SS2.p3.4.3">Soft-Prompted LLaVA-1.5</span>: We prepend 576 learnable tokens (<math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mo id="S4.SS2.p3.1.m1.1.1" stretchy="false" xref="S4.SS2.p3.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.1">prompt<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS2.p3.2.1.m1.1"><semantics id="S4.SS2.p3.2.1.m1.1a"><mo id="S4.SS2.p3.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.SS2.p3.2.1.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.1.m1.1b"><ci id="S4.SS2.p3.2.1.m1.1.1.cmml" xref="S4.SS2.p3.2.1.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.1.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.1.m1.1d">⟩</annotation></semantics></math></span>) to the LLM input and tune only the <math alttext="\langle" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m2.1"><semantics id="S4.SS2.p3.3.m2.1a"><mo id="S4.SS2.p3.3.m2.1.1" stretchy="false" xref="S4.SS2.p3.3.m2.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m2.1b"><ci id="S4.SS2.p3.3.m2.1.1.cmml" xref="S4.SS2.p3.3.m2.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m2.1c">\langle</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m2.1d">⟨</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S4.SS2.p3.4.2">prompt<math alttext="\rangle" class="ltx_Math" display="inline" id="S4.SS2.p3.4.2.m1.1"><semantics id="S4.SS2.p3.4.2.m1.1a"><mo id="S4.SS2.p3.4.2.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.SS2.p3.4.2.m1.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.2.m1.1b"><ci id="S4.SS2.p3.4.2.m1.1.1.cmml" xref="S4.SS2.p3.4.2.m1.1.1">normal-⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.2.m1.1c">\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.2.m1.1d">⟩</annotation></semantics></math></span> parameters on the COST training dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_framed_underline" id="S4.SS2.p4.1.1">ImCoder LLaVA-1.5</span>: We use an RGB image as the control input instead of a segmentation map and train VCoder on the COST training dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.T1" title="Table 1 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>, we notice that all existing MLLMs perform poorly on our COST validation set, demonstrating their inability to count and identify objects accurately. Note that existing MLLMs perform relatively better on instance object identification, reaffirming our claim that MLLMs are better at detecting salient objects than background objects. Although the baselines trained on the COST dataset perform relatively better, they still lag in performance compared to the VCoder. Notably, a segmentation map performs considerably better than using an RGB image as the control input, proving the segmentation map’s vitality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1">Comparison to GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>.</span> We utilize OpenAI’s newly released <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.2">gpt-4-vision-preview<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote1.1.1.1">1</span></span><a class="ltx_ref ltx_href" href="https://platform.openai.com/docs/guides/vision" title="">https://platform.openai.com/docs/guides/vision</a></span></span></span></span> API to obtain responses from GPT-4V. Our experiments show that GPT-4V’s responses are consistent across all object identification tasks, closely aligning with the panoptic identification task. Therefore, we compare our VCoder to GPT-4V only on the panoptic object identification to reduce API requests due to a daily limit of 500 API requests during this project. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.T1" title="Table 1 ‣ 3.3 Evaluating MLLMs for Object Identification ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>, GPT-4V&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> lags behind our VCoder by a considerable margin, reaffirming our claim that existing MLLMs cannot perform accurate object-level perception.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table ltx_flex_size_1 ltx_align_center" id="S4.T2">
<div class="ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer" id="S4.SS2.1.1" style="width:297.4pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS2.1.1.1">
<tbody><tr class="ltx_tr" id="S4.SS2.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.1.1.1.1.2">Method</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.1.1.1.1.1">Depth Score (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.1.1.1.1.1.m1.1"><semantics id="S4.SS2.1.1.1.1.1.m1.1a"><mo id="S4.SS2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.SS2.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.1.1.1.1.1.m1.1b"><ci id="S4.SS2.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS2.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.1.1.1.1.1.m1.1d">↓</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.SS2.1.1.1.2.1">LLaVA-1.5-7b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.1.1.1.2.2">166.1</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.1.1.1.3.1">LLaVA-1.5-13b&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.1.1.1.3.2">227.2</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.SS2.1.1.1.4.1">
<span class="ltx_text ltx_font_bold" id="S4.SS2.1.1.1.4.1.1">VCoder</span>-DS LLaVA-1.5-7b</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.1.1.1.4.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.1.1.1.4.2.1">65.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.SS2.1.1.1.5.1">
<span class="ltx_text ltx_font_bold" id="S4.SS2.1.1.1.5.1.1">VCoder</span>-DS LLaVA-1.5-13b</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.1.1.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.1.1.1.5.2.1">63.3</span></td>
</tr>
</tbody></table>
</span></div>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.5.2" style="font-size:90%;">Performance on Object Order Perception.<span class="ltx_text ltx_font_medium" id="S4.T2.5.2.1"> Our VCoder LLaVA-1.5 considerably outperforms LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, owing to the usage of control inputs and training on the COST dataset.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Object Order Perception with MLLMs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.F4" title="Figure 4 ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, multiple perception modalities can be leveraged to improve object perception in MLLMs with our VCoder. This section presents our experiments with our VCoder using the segmentation and depth maps as the control inputs. We term the resulting MLLM as VCoder-DS LLaVA-1.5. Intuitively, predicting the object order implicitly means identifying the objects in an image. Therefore, for the object order perception task (<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S3.F4" title="Figure 4 ‣ 3 Object Identification with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S5.p1.1.1" style="color:#FF0000;">b</span>), we use both <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.2">&lt;depth&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.3">&lt;seg&gt;</span> inputs, while only the <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.4">&lt;seg&gt;</span> input as the additional control for object identification.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">During training, we use a mixture of datasets, including the object identification and object order perception components from the COST dataset. We also use about 200k image-conversation (along with the corresponding segmentation map obtained using OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>) pairs randomly sampled from the instruction tuning data used in LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. We train our VCoder for one epoch following the same hyperparameter settings mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S4" title="4 Experiments ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S4.T2" title="Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, our VCoder-DS LLaVA-1.5 significantly outperforms the base MLLM, LLaVA-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> on the COST validation set. For quantitatively evaluating the performance of MLLMs on the depth order perception task, we calculate a depth score (<span class="ltx_text ltx_font_bold" id="S5.p3.1.1">DS</span>) using the absolute difference between the position of objects in the ground truth and prediction. We provide more details about computing the depth score in the appendix.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="954" id="S5.F6.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F6.4.2" style="font-size:90%;">Failure Case.<span class="ltx_text ltx_font_medium" id="S5.F6.4.2.1"> VCoder returns the wrong response when the input segmentation mask (control input) is inaccurate.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Despite the improved object perception performance after training our VCoder on the COST dataset, certain limitations remain to be addressed for future work. Firstly, we build our COST dataset using OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, which can only perceive objects belonging to a limited number of categories due to being trained on a closed-set vocabulary dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. For real-world applications, it is imperative to develop an object perception benchmark for MLLMs covering many more classes with varying granularity than those in the COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. Secondly, the count, hallucination, and depth scores use one-to-one word matching, which requires manually defining a mapping between synonymous words. It will be promising to explore ways to overcome manually defined synonym mappings. Lastly, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S5.F6" title="Figure 6 ‣ 5 Object Order Perception with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>, the inaccuracy in the segmentation map may result in the VCoder’s failure. Exploring ways to reduce the over-dependency on control inputs to handle inaccurate context from the perception modalities would be interesting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work analyzes the object-level perception skills of Multimodal Large Language Models (VLMMs). Although MLLMs are good visual reasoners, they need to improve at the simple yet fundamental task of object perception. To improve object perception ability in MLLMs, we propose the COST dataset for training and evaluating MLLMs at the object perception task. We benchmark different off-the-shelf MLLMs and GPT-4V on our COST dataset and observe their lousy performance. Consequently, we propose using perception modalities as control inputs and a Versatile vision enCoders (<span class="ltx_text ltx_font_bold" id="S7.p1.1.1">VCoder</span>) as an adapter for projecting the control inputs to the LLM embedding space. Our VCoder can easily be extended to leverage various modalities as the control inputs depending on the task. To quantify the object-level perception ability in MLLMs, we introduce a Count-Score (<span class="ltx_text ltx_font_bold" id="S7.p1.1.2">CS</span>), a Hallucination-Score (<span class="ltx_text ltx_font_bold" id="S7.p1.1.3">HS</span>), and a Depth-Score (<span class="ltx_text ltx_font_bold" id="S7.p1.1.4">DS</span>). We adapted LLaVA-1.5 with VCoder, only trained the VCoder on our COST dataset, and demonstrated its improved performance at the object perception task while retaining the reasoning performance. We hope our work can inspire the research community to focus on developing object perception datasets for MLLMs and develop vision systems that are equally good at perception and reasoning in the future.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1">We would like to extend our gratitude to Eric Zhang and Kai Wang (JJ’s labmates) for an insightful discussion before the start of the project and valuable feedback on the design of Figure 2. We also thank the ML Center at Georgia Tech for generously supporting this work.


<span class="ltx_text" id="S7.SS0.SSS0.Px1.p1.1.1" style="font-size:90%;"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Agrawal et&nbsp;al. [2015]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C.&nbsp;Lawrence Zitnick, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Vqa: Visual question answering, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Alayrac et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Almazrouei et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Falcon-40B: an open large language model with state-of-the-art performance.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Awadalla et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang&nbsp;Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Openflamingo: An open-source framework for training large autoregressive vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Bai et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Carion et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Changpinyo et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Chen et&nbsp;al. [2015]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan&nbsp;L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Semantic image segmentation with deep convolutional nets and fully connected crfs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cheng et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Bowen Cheng, Maxwell&nbsp;D Collins, Yukun Zhu, Ting Liu, Thomas&nbsp;S Huang, Hartwig Adam, and Liang-Chieh Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib9.11.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Cheng et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Bowen Cheng, Ishan Misra, Alexander&nbsp;G. Schwing, Alexander Kirillov, and Rohit Girdhar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Masked-attention mask transformer for universal image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Chiang et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and Eric&nbsp;P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.4.4.1" style="font-size:90%;">Contributors [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.6.1" style="font-size:90%;">
OpenCompass Contributors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">Opencompass: A universal evaluation platform for foundation models.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-compass/opencompass" style="font-size:90%;" title="">https://github.com/open-compass/opencompass</a><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Cootes et&nbsp;al. [2001]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Timothy&nbsp;F Cootes, Gareth&nbsp;J Edwards, and Christopher&nbsp;J Taylor.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Active appearance models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.9.1" style="font-size:90%;">IEEE Transactions on pattern analysis and machine intelligence</em><span class="ltx_text" id="bib.bib13.10.2" style="font-size:90%;">, 23(6):681–685, 2001.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Dai et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng&nbsp;Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Dosovitskiy et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Eigen et&nbsp;al. [2014]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
David Eigen, Christian Puhrsch, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Depth map prediction from a single image using a multi-scale deep network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 2366–2374, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Fu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Mme: A comprehensive evaluation benchmark for multimodal large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Girdhar et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van&nbsp;der Maaten, Armand Joulin, and Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Omnivore: A Single Model for Many Visual Modalities.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib18.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.4.4.1" style="font-size:90%;">Hassani and Shi [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.6.1" style="font-size:90%;">
Ali Hassani and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">Dilated neighborhood attention transformer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.8.1" style="font-size:90%;">arXiv:2209.15001</em><span class="ltx_text" id="bib.bib19.9.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Hassani et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Neighborhood attention transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib20.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">He et&nbsp;al. [2016]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib21.11.3" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">He et&nbsp;al. [2017]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib22.11.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Huang et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais&nbsp;Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Language is not all you need: Aligning perception with language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">ArXiv</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, abs/2302.14045, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.4.4.1" style="font-size:90%;">Hudson and Manning [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.6.1" style="font-size:90%;">
Drew&nbsp;A Hudson and Christopher&nbsp;D Manning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.8.1" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib24.9.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Jain et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Semask: Semantically masking transformer backbones for effective semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Jain et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">OneFormer: One Transformer to Rule Universal Image Segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Jin et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Yadong Mu, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Unified language-vision pretraining in llm with dynamic discrete visual tokenization.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Kirillov et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Panoptic feature pyramid networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib28.11.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Koh et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Jing&nbsp;Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Generating images with multimodal language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.1" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.4.4.1" style="font-size:90%;">Kuhn [1991]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.6.1" style="font-size:90%;">
Deanna Kuhn.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.7.1" style="font-size:90%;">The Skills of Argument</em><span class="ltx_text" id="bib.bib30.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.9.1" style="font-size:90%;">Cambridge University Press, 1991.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Laina et&nbsp;al. [2016]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Deeper depth prediction with fully convolutional residual networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">2016 Fourth International Conference on 3D Vision (3DV)</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, pages 239–248. IEEE, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.4.4.1" style="font-size:90%;">[32]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.6.1" style="font-size:90%;">
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">Gradient-based learning applied to document recognition.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.8.1" style="font-size:90%;">Proceedings of the IEEE</em><span class="ltx_text" id="bib.bib32.9.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Mimic-it: Multi-modal in-context instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Otter: A multi-modal model with in-context instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.03726</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023c]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023d]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne&nbsp;Xin Zhao, and Ji-Rong Wen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Evaluating object hallucination in large vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">EMNLP</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Lin et&nbsp;al. [2014]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C.&nbsp;Lawrence Zitnick, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.14566</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Aligning large multi-modal model with robust instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.14565</em><span class="ltx_text" id="bib.bib40.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2023c]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong&nbsp;Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Improved baselines with visual instruction tuning, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2023d]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong&nbsp;Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted windows.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Long et&nbsp;al. [2015]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Fully convolutional networks for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib44.11.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Lovenia et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib45.8.2" style="font-size:90%;">Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">Negative object presence evaluation (nope) to measure object hallucination in vision-language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib45.11.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Lu et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Neural baby talk.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.4.4.1" style="font-size:90%;">Moravec [1988]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.6.1" style="font-size:90%;">
H. Moravec.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">Mind children: The future of robot and human intelligence.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Harvard University Press, 1988.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Mou et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.4.4.1" style="font-size:90%;">Nathan&nbsp;Silberman and Fergus [2012]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.6.1" style="font-size:90%;">
Pushmeet&nbsp;Kohli Nathan&nbsp;Silberman, Derek&nbsp;Hoiem and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.9.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib49.10.3" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.4.4.1" style="font-size:90%;">OpenAI [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.6.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib50.7.2" style="font-size:90%;">OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Chatgpt.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" style="font-size:90%;" title="">https://chat.openai.com/</a><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.4.4.1" style="font-size:90%;">OpenAI [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.6.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Oquab et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy&nbsp;V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Dinov2: Learning robust visual features without supervision, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Ordonez et&nbsp;al. [2011]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Im2text: Describing images using 1 million captioned photographs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib53.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib53.11.3" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Peng et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">Kosmos-2: Grounding multimodal large language models to the world.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.9.1" style="font-size:90%;">ArXiv</em><span class="ltx_text" id="bib.bib54.10.2" style="font-size:90%;">, abs/2306, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Radford et&nbsp;al. [2021a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Alec Radford, Jong&nbsp;Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.10.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib55.11.3" style="font-size:90%;">, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Radford et&nbsp;al. [2021b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Alec Radford, Jong&nbsp;Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib56.10.2" style="font-size:90%;">, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Ranftl et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">Vision transformers for dense prediction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib57.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib57.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Ren et&nbsp;al. [2015]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Faster R-CNN: Towards real-time object detection with region proposal networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Rohrbach et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Anna Rohrbach, Lisa&nbsp;Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Object hallucination in image captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib59.10.2" style="font-size:90%;">EMNLP</em><span class="ltx_text" id="bib.bib59.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">Schuhmann et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib60.10.2" style="font-size:90%;">NeurIPS Workshops 2021</em><span class="ltx_text" id="bib.bib60.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Sun et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Generative pretraining in multimodality.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib61.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.4.4.1" style="font-size:90%;">Toshev and Szegedy [2014]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.6.1" style="font-size:90%;">
Alexander Toshev and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">Deeppose: Human pose estimation via deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib62.9.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib62.10.3" style="font-size:90%;">, pages 1653–1660, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Touvron et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Touvron et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">Llama 2: Open foundation and fine-tuned chat models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib64.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Tsimpoukelli et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S.&nbsp;M.&nbsp;Ali Eslami, Oriol Vinyals, and Felix Hill.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Multimodal few-shot learning with frozen language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib65.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib65.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Tu et&nbsp;al. [2005]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Z. Tu, Xiangrong Chen, Alan Yuille, and Song Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Image parsing: Unifying segmentation, detection, and recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">IJCV</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.4.4.1" style="font-size:90%;">Viola and Jones [2001]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.6.1" style="font-size:90%;">
Paul Viola and Michael Jones.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">Rapid object detection using a boosted cascade of simple features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib67.9.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib67.10.3" style="font-size:90%;">, 2001.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Wang et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Cogvlm: Visual expert for pretrained language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib68.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Xie et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose&nbsp;M. Alvarez, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Segformer: Simple and efficient design for semantic segmentation with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib69.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Xu et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Prompt-free diffusion: Taking” text” out of text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.16223</em><span class="ltx_text" id="bib.bib70.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="font-size:90%;">Xu et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">
Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Versatile diffusion: Text, images and variations all in one diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib71.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib71.11.3" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Ye et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">mplug-owl: Modularization empowers large language models with multimodality, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Zeng et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">What matters in training a gpt4-style language model with multimodal inputs?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.02469</em><span class="ltx_text" id="bib.bib73.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Zhang et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib74.11.3" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Zhang et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh&nbsp;Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Siren’s song in the ai ocean: A survey on hallucination in large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib75.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Zheng et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Kaizhi Zheng, Xuehai He, and Xin&nbsp;Eric Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Minigpt-5: Interleaved vision-and-language generation via generative vokens.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib76.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Zheng et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric.&nbsp;P Xing, Hao Zhang, Joseph&nbsp;E. Gonzalez, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Zhu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib78.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Zhu et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object detection.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.9.1" style="font-size:90%;">arXiv</em><span class="ltx_text" id="bib.bib79.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:144%;">Appendix</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">In this appendix, we first present our analysis of the effect of the quality of the segmentation map (control input) on the VCoder’s performance in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A1" title="Appendix A Control Through Segmentation Map ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">A</span></a>. Next, we provide details about obtaining ground-truth texts for the object order perception task along with the process to compute the depth score in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2" title="Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">B</span></a>. Lastly, we share analysis on the per-image object counts about the COST dataset in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A3" title="Appendix C Object Counts in COST Dataset ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Appendix</span>&nbsp;<span class="ltx_text ltx_ref_tag">C</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Control Through Segmentation Map</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We study the effect of segmentation map quality on object identification performance. Specifically, instead of using DiNAT-L OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> to obtain the segmentation map, we use the relatively worse segmentation models: ResNet-50&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> based Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, Panoptic-FPN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, and Swin-L&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> based Mask2Former&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> for the instance and panoptic object identification task, respectively. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2.T1" title="Table I ‣ B.2 Depth Score ‣ Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Tab.</span>&nbsp;<span class="ltx_text ltx_ref_tag">I</span></a>, we notice a considerable drop in performance with maps from Mask R-CNN and Panoptic FPN. However, the drop in performance is much lower with maps from a relatively newer and better Mask2Former model, demonstrating the importance of the segmentation map’s quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Object Order Perception</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we present the process of obtaining the ground truth ordering of objects in an image using segmentation and depth maps. Then, we share details about the logic used to compute the depth score (<span class="ltx_text ltx_font_bold" id="A2.p1.1.1">DS</span>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Obtaining Ground Truth</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">To obtain the ground truth order for objects in an image, we utilize the fact that each pixel in a depth map (from DINOv2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> DPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>) represents the distance&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> of that pixel from the camera. Therefore, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2.F1" title="Figure I ‣ B.2 Depth Score ‣ Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">I</span></a>, we use the binary object masks (from OneFormer’s&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> panoptic prediction) to first obtain the corresponding regions in the depth map. Next, for each object region, we calculate the maximum pixel value representing the distance of the object’s farthest point from the camera. Finally, we sort the values obtained in the previous in an ascending order to obtain the final order, starting with the closest object and ending with the farthest object. As mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#S5" title="5 Object Order Perception with MLLMs ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Sec.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>, we append a number to the object name to represent the relative order of objects belonging to the same category.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Depth Score</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A2.F2" title="Figure II ‣ B.2 Depth Score ‣ Appendix B Object Order Perception ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">II</span></a>, we share the <span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.1">python</span> code to compute the depth score given the ground truth and prediction for object orders in an image. Particularly, we first obtain the position of objects belonging to all categories and then compute the absolute difference using the position values for objects belonging to the same category in the ground truth and prediction. Note that to handle different numbers of objects in the prediction and ground truth, we use the position value as 100 for unmatched objects. We average the obtained score over all images to obtain the final depth score.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A2.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T1.2" style="width:317.0pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.6pt,7.2pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T1.2.2">
<tbody><tr class="ltx_tr" id="A2.T1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T1.2.2.2.3">Seg Model</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T1.2.2.2.4">Year</td>
<td class="ltx_td ltx_align_center" id="A2.T1.1.1.1.1">CS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T1.1.1.1.1.m1.1"><semantics id="A2.T1.1.1.1.1.m1.1a"><mo id="A2.T1.1.1.1.1.m1.1.1" stretchy="false" xref="A2.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A2.T1.1.1.1.1.m1.1b"><ci id="A2.T1.1.1.1.1.m1.1.1.cmml" xref="A2.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="A2.T1.2.2.2.2">HS (<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T1.2.2.2.2.m1.1"><semantics id="A2.T1.2.2.2.2.m1.1a"><mo id="A2.T1.2.2.2.2.m1.1.1" stretchy="false" xref="A2.T1.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A2.T1.2.2.2.2.m1.1b"><ci id="A2.T1.2.2.2.2.m1.1.1.cmml" xref="A2.T1.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T1.2.2.2.2.m1.1d">↓</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" id="A2.T1.2.2.3.1">
<span class="ltx_text ltx_font_italic" id="A2.T1.2.2.3.1.1">Instance Object Identification</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A2.T1.2.2.4.1">OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T1.2.2.4.2">CVPR 2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.2.2.4.3"><span class="ltx_text ltx_font_bold" id="A2.T1.2.2.4.3.1">71.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.2.2.4.4"><span class="ltx_text ltx_font_bold" id="A2.T1.2.2.4.4.1">26.9</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T1.2.2.5.1">Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T1.2.2.5.2">ICCV 2017</td>
<td class="ltx_td ltx_align_center" id="A2.T1.2.2.5.3">61.9 <span class="ltx_text" id="A2.T1.2.2.5.3.1" style="color:#BF0040;">(-9.2)</span>
</td>
<td class="ltx_td ltx_align_center" id="A2.T1.2.2.5.4">39.8 <span class="ltx_text" id="A2.T1.2.2.5.4.1" style="color:#BF0040;">(+12.9)</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="A2.T1.2.2.6.1">
<span class="ltx_text ltx_font_italic" id="A2.T1.2.2.6.1.1">Panoptic Object Identification</span></td>
<td class="ltx_td ltx_border_t" id="A2.T1.2.2.6.2"></td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A2.T1.2.2.7.1">OneFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T1.2.2.7.2">CVPR 2023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.2.2.7.3"><span class="ltx_text ltx_font_bold" id="A2.T1.2.2.7.3.1">86.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T1.2.2.7.4"><span class="ltx_text ltx_font_bold" id="A2.T1.2.2.7.4.1">12.8</span></td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T1.2.2.8.1">Mask2Former&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T1.2.2.8.2">CVPR 2022</td>
<td class="ltx_td ltx_align_center" id="A2.T1.2.2.8.3">76.5 <span class="ltx_text" id="A2.T1.2.2.8.3.1" style="color:#BF0040;">(-9.5)</span>
</td>
<td class="ltx_td ltx_align_center" id="A2.T1.2.2.8.4">26.1 <span class="ltx_text" id="A2.T1.2.2.8.4.1" style="color:#BF0040;">(+13.3)</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T1.2.2.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A2.T1.2.2.9.1">Panoptic FPN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T1.2.2.9.2">CVPR 2019</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T1.2.2.9.3">64.2 <span class="ltx_text" id="A2.T1.2.2.9.3.1" style="color:#BF0040;">(-21.8)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T1.2.2.9.4">33.3 <span class="ltx_text" id="A2.T1.2.2.9.4.1" style="color:#BF0040;">(+20.5)</span>
</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A2.T1.5.1.1" style="font-size:90%;">Table I</span>: </span><span class="ltx_text ltx_font_bold" id="A2.T1.6.2" style="font-size:90%;">Ablation on Quality of Segmentation Map<span class="ltx_text ltx_font_medium" id="A2.T1.6.2.1">. Using segmentation maps from older models like Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> and Panoptic-FPN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> as the control input results in a performance drop due to the relatively low quality of the maps.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="665" id="A2.F1.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x7.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F1.3.1.1" style="font-size:90%;">Figure I</span>: </span><span class="ltx_text ltx_font_bold" id="A2.F1.4.2" style="font-size:90%;">Data Engine to obtain Object Order GT<span class="ltx_text ltx_font_medium" id="A2.F1.4.2.1">. We calculate the maximum pixel value inside each object’s region using the depth and segmentation maps. We sort the obtained values in an ascending order to obtain the final object order.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F2">
<div class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_listing" id="A2.F2.2" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,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">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.1.1.1" style="font-size:50%;color:#808080;">1</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx1.2" style="font-size:80%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.3" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.4" style="font-size:80%;">calculate_per_image_depth_score</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.5" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.6" style="font-size:80%;">gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.7" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9" style="font-size:80%;">pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.10" style="font-size:80%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.1.1.1" style="font-size:50%;color:#808080;">2</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3" style="font-size:80%;">position_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.4" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.5" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.6" style="font-size:80%;">order_num</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.8" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.10" style="font-size:80%;">_get_order</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.11" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.12" style="font-size:80%;">gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.13" style="font-size:80%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.1.1.1" style="font-size:50%;color:#808080;">3</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3" style="font-size:80%;">position_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.4" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.5" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.6" style="font-size:80%;">_</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.8" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.10" style="font-size:80%;">_get_order</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.11" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.12" style="font-size:80%;">pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.13" style="font-size:80%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.1.1.1" style="font-size:50%;color:#808080;">4</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3" style="font-size:80%;">depth_distance</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.7" style="font-size:80%;">[]</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.1.1.1" style="font-size:50%;color:#808080;">5</span></span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.1.1.1" style="font-size:50%;color:#808080;">6</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx6.3" style="font-size:80%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx6.5" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx6.7" style="font-size:80%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.8" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.9" style="font-size:80%;">position_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.10" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.11" style="font-size:80%;">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.12" style="font-size:80%;">():</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.1.1.1" style="font-size:50%;color:#808080;">7</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.3" style="font-size:80%;color:#FF00FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5" style="font-size:80%;">position_pred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.7" style="font-size:80%;color:#FF00FF;">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.8" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.9" style="font-size:80%;color:#FF00FF;">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.10" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.11" style="font-size:80%;">None</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.12" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.13" style="font-size:80%;color:#FF00FF;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.14" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx7.15" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.16" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.17" style="font-size:80%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.18" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.19" style="font-size:80%;">position_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.20" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.21" style="font-size:80%;">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.22" style="font-size:80%;">():</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.1.1.1" style="font-size:50%;color:#808080;">8</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.3" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.7" style="font-size:80%;">position_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.8" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx8.9" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.10" style="font-size:80%;">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.1.1.1" style="font-size:50%;color:#808080;">9</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.3" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.7" style="font-size:80%;">position_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.8" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx9.9" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.10" style="font-size:80%;">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.1.1.1" style="font-size:50%;color:#808080;">10</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx10.3" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.1">&nbsp;</span>pad<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.2">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.3">&nbsp;</span>object<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.4">&nbsp;</span>specific<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.5">&nbsp;</span>position<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.6">&nbsp;</span>list<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.7">&nbsp;</span>to<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.8">&nbsp;</span>make<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.9">&nbsp;</span>with<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.10">&nbsp;</span>100<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.11">&nbsp;</span>to<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.12">&nbsp;</span>make<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.13">&nbsp;</span>them<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.14">&nbsp;</span>equal<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.15">&nbsp;</span>for<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.16">&nbsp;</span>prediction<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.17">&nbsp;</span>and<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.18">&nbsp;</span>ground-truth</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1.1.1" style="font-size:50%;color:#808080;">11</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx11.3" style="font-size:80%;color:#FF00FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx11.5" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.6" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.7" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.8" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.11" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx11.12" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.13" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.14" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.15" style="font-size:80%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1.1.1" style="font-size:50%;color:#808080;">12</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.3" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.5" style="font-size:80%;">extend</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.6" style="font-size:80%;">([100]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.8" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.10" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx12.11" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.12" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.13" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.14" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.15" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.16" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.17" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx12.18" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.20" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.21" style="font-size:80%;">)))</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.1.1.1" style="font-size:50%;color:#808080;">13</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx13.3" style="font-size:80%;color:#FF00FF;">elif</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx13.5" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.7" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.8" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.10" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.11" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx13.12" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.13" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.14" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.15" style="font-size:80%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.1.1.1" style="font-size:50%;color:#808080;">14</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.4" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5" style="font-size:80%;">extend</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6" style="font-size:80%;">([100]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.8" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.10" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx14.11" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.12" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.13" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.14" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.15" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.16" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.17" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx14.18" style="font-size:80%;color:#FF00FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.20" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.21" style="font-size:80%;">)))</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.1.1.1" style="font-size:50%;color:#808080;">15</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx15.3" style="font-size:80%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.5" style="font-size:80%;">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.6" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.8" style="font-size:80%;">j</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx15.10" style="font-size:80%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.11" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx15.12" style="font-size:80%;color:#FF00FF;">zip</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.13" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.14" style="font-size:80%;">order_gt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.15" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.16" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.17" style="font-size:80%;">order_pred</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.18" style="font-size:80%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.1.1.1" style="font-size:50%;color:#808080;">16</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3" style="font-size:80%;">depth_distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.4" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5" style="font-size:80%;">append</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.6" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx16.7" style="font-size:80%;color:#FF00FF;">abs</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.8" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.9" style="font-size:80%;">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.10" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.11" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.12" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.13" style="font-size:80%;">j</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.14" style="font-size:80%;">))</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.1.1.1" style="font-size:50%;color:#808080;">17</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx17.3" style="font-size:80%;color:#FF00FF;">else</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.4" style="font-size:80%;">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.1.1.1" style="font-size:50%;color:#808080;">18</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.3" style="font-size:80%;">depth_distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.5" style="font-size:80%;">append</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.6" style="font-size:80%;">(100)</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.1.1.1" style="font-size:50%;color:#808080;">19</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx19.3" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.1">&nbsp;</span>normalize<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.2">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.3">&nbsp;</span>score<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.4">&nbsp;</span>based<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.5">&nbsp;</span>on<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.6">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.7">&nbsp;</span>total<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.8">&nbsp;</span>number<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.9">&nbsp;</span>of<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.10">&nbsp;</span>objects<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.11">&nbsp;</span>in<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.12">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx19.3.13">&nbsp;</span>image</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.1.1.1" style="font-size:50%;color:#808080;">20</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx20.3" style="font-size:80%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx20.5" style="font-size:80%;color:#FF00FF;">sum</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.6" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.7" style="font-size:80%;">depth_distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.8" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.10" style="font-size:80%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.11" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.12" style="font-size:80%;">order_num</span>
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.1.1.1" style="font-size:50%;color:#808080;">21</span></span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.1.1.1" style="font-size:50%;color:#808080;">22</span></span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx22.2" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.1">&nbsp;</span>helper<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.2">&nbsp;</span>function<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.3">&nbsp;</span>to<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.4">&nbsp;</span>calculate<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.5">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.6">&nbsp;</span>order<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.7">&nbsp;</span>position<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.8">&nbsp;</span>of<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.9">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.10">&nbsp;</span>objects<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.11">&nbsp;</span>in<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.12">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx22.2.13">&nbsp;</span>image</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.1.1.1" style="font-size:50%;color:#808080;">23</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx23.2" style="font-size:80%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.3" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.4" style="font-size:80%;">_get_order</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.5" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.6" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.7" style="font-size:80%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.1.1.1" style="font-size:50%;color:#808080;">24</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.3" style="font-size:80%;">order_num</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.7" style="font-size:80%;">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.8" style="font-size:80%;">&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx24.9" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx24.9.1">&nbsp;</span>order<span class="ltx_text ltx_lst_space" id="lstnumberx24.9.2">&nbsp;</span>number<span class="ltx_text ltx_lst_space" id="lstnumberx24.9.3">&nbsp;</span>of<span class="ltx_text ltx_lst_space" id="lstnumberx24.9.4">&nbsp;</span>the<span class="ltx_text ltx_lst_space" id="lstnumberx24.9.5">&nbsp;</span>object</span>
</div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.1.1.1" style="font-size:50%;color:#808080;">25</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.3" style="font-size:80%;">positions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.7" style="font-size:80%;">{}</span>
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.1.1.1" style="font-size:50%;color:#808080;">26</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx26.3" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx26.3.1">&nbsp;</span>obtain<span class="ltx_text ltx_lst_space" id="lstnumberx26.3.2">&nbsp;</span>object<span class="ltx_text ltx_lst_space" id="lstnumberx26.3.3">&nbsp;</span>nouns</span>
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.1.1.1" style="font-size:50%;color:#808080;">27</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.3" style="font-size:80%;">nouns</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.7" style="font-size:80%;">_obtain_nouns</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.8" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.9" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.10" style="font-size:80%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx28">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.1.1.1" style="font-size:50%;color:#808080;">28</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx28.3" style="font-size:80%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.5" style="font-size:80%;">noun</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx28.7" style="font-size:80%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.8" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.9" style="font-size:80%;">nouns</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.10" style="font-size:80%;">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx29">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.1.1.1" style="font-size:50%;color:#808080;">29</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx29.3" style="font-size:80%;color:#009900;">#<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.1">&nbsp;</span>obtain<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.2">&nbsp;</span>only<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.3">&nbsp;</span>object<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.4">&nbsp;</span>noun<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.5">&nbsp;</span>(person)<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.6">&nbsp;</span>from<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.7">&nbsp;</span>words<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.8">&nbsp;</span>like<span class="ltx_text ltx_lst_space" id="lstnumberx29.3.9">&nbsp;</span>person-2</span>
</div>
<div class="ltx_listingline" id="lstnumberx30">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.1.1.1" style="font-size:50%;color:#808080;">30</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx30.3" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.5" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.7" style="font-size:80%;">noun</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.8" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.9" style="font-size:80%;">split</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.10" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx30.11" style="font-size:80%;color:#9400D1;">"-"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.12" style="font-size:80%;">)[0].</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.13" style="font-size:80%;">strip</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.14" style="font-size:80%;">()</span>
</div>
<div class="ltx_listingline" id="lstnumberx31">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.1.1.1" style="font-size:50%;color:#808080;">31</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx31.3" style="font-size:80%;color:#FF00FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx31.5" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx31.7" style="font-size:80%;color:#FF00FF;">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.8" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx31.9" style="font-size:80%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.10" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.11" style="font-size:80%;">positions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.12" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.13" style="font-size:80%;">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.14" style="font-size:80%;">():</span>
</div>
<div class="ltx_listingline" id="lstnumberx32">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.1.1.1" style="font-size:50%;color:#808080;">32</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.3" style="font-size:80%;">positions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.4" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx32.5" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.6" style="font-size:80%;">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.8" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.10" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.11" style="font-size:80%;">order_num</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.12" style="font-size:80%;">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx33">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.1.1.1" style="font-size:50%;color:#808080;">33</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx33.3" style="font-size:80%;color:#FF00FF;">else</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.4" style="font-size:80%;">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx34">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.1.1.1" style="font-size:50%;color:#808080;">34</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.3" style="font-size:80%;">positions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.4" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx34.5" style="font-size:80%;color:#FF00FF;">object</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.6" style="font-size:80%;">].</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.7" style="font-size:80%;">append</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.8" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.9" style="font-size:80%;">order_num</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.10" style="font-size:80%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.1.1.1" style="font-size:50%;color:#808080;">35</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.3" style="font-size:80%;">order_num</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.5" style="font-size:80%;">+=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.6" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.7" style="font-size:80%;">1</span>
</div>
<div class="ltx_listingline" id="lstnumberx36">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.1.1.1" style="font-size:50%;color:#808080;">36</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.2" style="font-size:80%;">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx36.3" style="font-size:80%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.4" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx36.5" style="font-size:80%;">positions</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.6" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.7" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx36.8" style="font-size:80%;">order_num</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.9" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.10" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.11" style="font-size:80%;">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.12" style="font-size:80%;">1</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F2.3.1.1" style="font-size:90%;">Figure II</span>: </span><span class="ltx_text" id="A2.F2.4.2" style="font-size:90%;">Computing Depth Score for a given Image.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Object Counts in COST Dataset</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We show the plots for the per-image total object count distribution in the <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.1">train</span> and <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">val</span> splits of our COST dataset in <a class="ltx_ref" href="https://arxiv.org/html/2312.14233v1/#A3.F3" title="Figure III ‣ Appendix C Object Counts in COST Dataset ‣ VCoder: Versatile Vision Encoders for Multimodal Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span>&nbsp;<span class="ltx_text ltx_ref_tag">III</span></a>. We observe that there exists a long tail beyond the object count of 25. Based on this observation, we express the need for a more scaled effort at collecting object-level perception datasets for training MLLMs to make them excel (without extra pre-processing) at counting in cluttered scenes that may contain many more objects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="A3.F3.g1" src="./VCoder_ Versatile Vision Encoders for Multimodal Large Language Models_files/x8.png" width="664">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F3.3.1.1" style="font-size:90%;">Figure III</span>: </span><span class="ltx_text ltx_font_bold" id="A3.F3.4.2" style="font-size:90%;">Total Object Counts per image in the COST <span class="ltx_text ltx_font_typewriter" id="A3.F3.4.2.1">train</span> and <span class="ltx_text ltx_font_typewriter" id="A3.F3.4.2.2">val</span> splits.<span class="ltx_text ltx_font_medium" id="A3.F3.4.2.3"> We observe that our COST dataset does not include images with more than 60 objects and has a long tail beyond the object count of 25.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
          <div class="ltx_page_logo">
              Generated by
              <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                  <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                      L
                      <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                      T
                      <span style="position: relative; bottom: -0.4ex;">E</span>
                  </span>
                  <span class="ltx_font_smallcaps">xml</span>
                  <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
              </a>
          </div></div><footer id="footer" class="ltx_document">
          <div class="keyboard-glossary">
              <h2>Instructions for reporting errors</h2>
              <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
              <ul>
                  <li>Click the "Report Issue" button.</li>
                  <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                  <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                  <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
              </ul>
              <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
              <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
          </div>
      </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none;">Report Issue for Selection</button></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 4);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 3px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #F5F7F9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #F6F6F6;
  --float-ball-more-button-background-color: #FFFFFF;
  --float-ball-more-button-svg-color: #6C6F73;
  color-scheme: light;
  --service-bg-hover:#F7FAFF;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --float-ball-more-button-background-color: #191919;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
    color-scheme: dark;
    --service-bg-hover:#22292F;
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --float-ball-more-button-background-color: #191919;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  color-scheme: dark;
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 1130px;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}

@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 992px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);

}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --popup-trial-pro-background-color: #222222;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
    --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
    --service-select-border-color: #2c2c2c;
    --service-select-selected-background-color: #333333;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.translate-service {
  color: var(--text-black-2);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

.widget-item {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 59px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-container {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 30px;
  width: 100%;
  margin-bottom: 4px;
}

.widget-title-container {
  display: flex;
  align-items: flex-start;
  justify-content: center;
  height: 24px;
  width: 100%;
  padding-bottom: 4px;
}

.widget-icon {
  margin-bottom: 4px;
  display: flex;
  justify-content: center;
}

.widget-title {
  color: var(--text-gray-6);
  font-size: 12px;
  text-align: center;
  width: 100%;
  font-weight: 400;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding: 0 2px 2px;
}

.widget-item svg {
  fill: var(--text-gray-2);
}

.setting svg {
  fill: var(--text-gray-6);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: 0;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: 100%;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
}
.more-container {
  position: relative;
}
.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  z-index: 2147483647;
  top: 335px;
  width: 56px;
  display: flex;
  flex-direction: column;
  display: none;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ED6D8F;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-50%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 16px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-right: 8px;
}

.imt-fb-more-button {
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}

/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 12px 0 0 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
}

.imt-manga-feedback {
  cursor: pointer;
  margin: 9px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 372px;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 16px auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " dir="ltr" style="z-index: 2147483647; pointer-events: none; top: 911px; display: flex;"><div title="关闭悬浮球" class="btn-animate" style="transform: translateX(100%); padding: 4px; cursor: pointer;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-btn  right btn-animate " dir="ltr" style="transform: translateX(15px); opacity: 0.7;"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg hidden="true" class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg></div></div></div></div><div hidden="" class="imt-manga-button imt-no-events btn-animate " id="manga-button" style="transform: translateX(8px);"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg><svg hidden="true" class="imt-manga-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#68CD52"></circle><path d="M1.40857 5.87858L2.24148 5.18962L4.15344 6.64214C4.15344 6.64214 6.33547 4.15566 9.00658 2.48145L9.32541 2.87514C9.32541 2.87514 6.28665 5.55844 4.71735 9.07881L1.40857 5.87858Z" fill="white"></path></svg><svg class="imt-float-ball-loading" hidden="true" width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin: 9px;"><path d="M9.42859 0C9.84288 0 10.1929 0.387143 10.1929 0.847143V3.99429C10.1929 4.45429 9.84431 4.84143 9.42859 4.84143C9.01431 4.84143 8.66431 4.45571 8.66431 3.99429V0.847143C8.66431 0.387143 9.01288 0 9.42859 0Z" fill="#E9E9E9"></path><path d="M14.1301 1.38877C14.5158 1.62591 14.6301 2.12163 14.4258 2.52305L12.9515 5.19448C12.901 5.28714 12.8325 5.36876 12.75 5.43455C12.6675 5.50035 12.5727 5.54898 12.4712 5.5776C12.3696 5.60621 12.2634 5.61424 12.1586 5.60119C12.0539 5.58814 11.9529 5.55429 11.8615 5.50163C11.6787 5.38432 11.5468 5.20237 11.4923 4.9921C11.4377 4.78184 11.4645 4.55874 11.5672 4.36734L13.0415 1.69591C13.2686 1.29448 13.7443 1.15305 14.1301 1.38877Z" fill="#989697"></path><path d="M17.4685 4.75707C17.5813 4.95451 17.6123 5.18824 17.5549 5.40825C17.4975 5.62826 17.3563 5.81705 17.1614 5.93422L14.4971 7.52564C14.0971 7.76993 13.6014 7.62422 13.3657 7.20707C13.2532 7.00994 13.2222 6.77667 13.2793 6.55702C13.3365 6.33737 13.4771 6.14874 13.6714 6.03136L16.3357 4.43993C16.7371 4.21993 17.2557 4.34136 17.4685 4.7585V4.75707Z" fill="#9B999A"></path><path d="M18.8572 9.42835C18.8572 9.84263 18.47 10.1926 18.01 10.1926H14.8629C14.4029 10.1926 14.0157 9.84406 14.0157 9.42835C14.0157 9.01406 14.4029 8.66406 14.8629 8.66406H18.01C18.47 8.66406 18.8572 9.01263 18.8572 9.42835Z" fill="#A3A1A2"></path><path d="M17.4686 14.1303C17.3515 14.3134 17.1697 14.4455 16.9594 14.5003C16.7491 14.5552 16.5259 14.5286 16.3343 14.426L13.6629 12.9517C13.5702 12.9012 13.4886 12.8327 13.4228 12.7503C13.357 12.6678 13.3084 12.573 13.2798 12.4714C13.2512 12.3698 13.2431 12.2636 13.2562 12.1589C13.2692 12.0542 13.3031 11.9532 13.3558 11.8617C13.4731 11.6789 13.655 11.547 13.8653 11.4925C14.0755 11.4379 14.2986 11.4647 14.49 11.5674L17.1615 13.0417C17.5629 13.2689 17.7043 13.7446 17.4686 14.1303Z" fill="#ABA9AA"></path><path opacity="0.7" d="M14.1 17.4686C13.9026 17.5814 13.6689 17.6124 13.4489 17.555C13.2288 17.4976 13.04 17.3564 12.9229 17.1615L11.3315 14.4972C11.0872 14.0972 11.2329 13.6015 11.65 13.3658C11.8472 13.2533 12.0804 13.2224 12.3001 13.2795C12.5197 13.3366 12.7084 13.4773 12.8257 13.6715L14.4172 16.3358C14.6372 16.7372 14.5157 17.2558 14.0986 17.4686H14.1Z" fill="#B2B2B2"></path><path opacity="0.6" d="M9.42859 18.8571C9.01431 18.8571 8.66431 18.4699 8.66431 18.0099V14.8628C8.66431 14.4028 9.01288 14.0156 9.42859 14.0156C9.84288 14.0156 10.1929 14.4028 10.1929 14.8628V18.0099C10.1929 18.4699 9.84431 18.8571 9.42859 18.8571Z" fill="#BAB8B9"></path><path opacity="0.5" d="M4.72717 17.4685C4.5441 17.3514 4.41195 17.1696 4.35713 16.9593C4.30231 16.749 4.32885 16.5258 4.43145 16.3342L5.90574 13.6628C5.95622 13.5701 6.02472 13.4885 6.1072 13.4227C6.18969 13.3569 6.2845 13.3083 6.38606 13.2797C6.48762 13.251 6.59387 13.243 6.69857 13.2561C6.80327 13.2691 6.90431 13.303 6.99574 13.3556C7.38145 13.5914 7.49431 14.0885 7.29002 14.4899L5.81574 17.1614C5.5886 17.5628 5.11288 17.7042 4.72717 17.4685Z" fill="#C2C0C1"></path><path opacity="0.4" d="M1.38862 14.1002C1.27584 13.9027 1.24483 13.669 1.30223 13.449C1.35964 13.229 1.50089 13.0402 1.69576 12.923L4.36004 11.3316C4.76004 11.0873 5.25576 11.233 5.49147 11.6502C5.60393 11.8473 5.63491 12.0806 5.5778 12.3002C5.52069 12.5199 5.38 12.7085 5.18576 12.8259L2.52004 14.4173C2.12004 14.6373 1.60004 14.5159 1.38862 14.0987V14.1002Z" fill="#CBCBCB"></path><path d="M0 9.42835C0 9.01406 0.387143 8.66406 0.847143 8.66406H3.99429C4.45429 8.66406 4.84143 9.01263 4.84143 9.42835C4.84143 9.84263 4.45571 10.1926 3.99429 10.1926H0.847143C0.387143 10.1926 0 9.84406 0 9.42835Z" fill="#D2D2D2"></path><path opacity="0.2" d="M1.38852 4.72705C1.50561 4.54398 1.68746 4.41183 1.89774 4.35701C2.10803 4.30219 2.33125 4.32873 2.52281 4.43133L5.19424 5.90562C5.28689 5.9561 5.36851 6.0246 5.43431 6.10708C5.5001 6.18957 5.54874 6.28438 5.57735 6.38594C5.60597 6.48749 5.61399 6.59375 5.60094 6.69845C5.5879 6.80315 5.55405 6.90419 5.50138 6.99562C5.38407 7.17844 5.20212 7.31029 4.99186 7.36484C4.78159 7.4194 4.55849 7.39263 4.3671 7.2899L1.69567 5.81562C1.29424 5.58847 1.15281 5.11276 1.38852 4.72705Z" fill="#DADADA"></path><path d="M4.75719 1.38849C4.95463 1.27571 5.18837 1.24471 5.40838 1.30211C5.62838 1.35952 5.81718 1.50077 5.93434 1.69564L7.52577 4.35992C7.77005 4.75992 7.62434 5.25564 7.20719 5.49135C7.01006 5.60381 6.77679 5.63479 6.55714 5.57768C6.33749 5.52056 6.14886 5.37988 6.03148 5.18564L4.44005 2.51992C4.22005 2.11992 4.34148 1.59992 4.75862 1.38849H4.75719Z" fill="#E2E2E2"></path></svg><div style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.6861 1L15.2353 4.54635V7.11765V14.6471V15.5882C15.2353 15.9627 15.0866 16.3217 14.8218 16.5865C14.557 16.8513 14.198 17 13.8235 17H4.41176C4.03734 17 3.67825 16.8513 3.4135 16.5865C3.14874 16.3217 3 15.9627 3 15.5882V14.6471V7.11765V2.41176C3 2.03734 3.14874 1.67825 3.4135 1.4135C3.67825 1.14874 4.03734 1 4.41176 1H11.6861ZM11.8692 3.17882V4.74212H13.4334L11.8692 3.17882ZM4.41171 15.5882V14.647V2.41176H10.4574L10.4578 6.15341H13.8235V14.647V15.5882H4.41171ZM12.7739 7.51746H5.46094V8.6155H12.7739V7.51746ZM5.46094 9.98805H12.7739V11.0861H5.46094V9.98805ZM9.5127 12.36H5.46094V13.458H9.5127V12.36Z" fill="#666666"></path></svg></div></div></div><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><div class="imt-fb-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M7.55741 1L10.0685 1.00329C10.8482 1.00471 11.4802 1.63624 11.4812 2.41647L11.4821 2.82588C11.9687 3.0278 12.4297 3.28671 12.8553 3.59718L13.1913 3.40329C13.516 3.21676 13.9013 3.1665 14.2629 3.26352C14.6246 3.36055 14.933 3.59695 15.1207 3.92094L16.3795 6.09365C16.5601 6.40546 16.6149 6.7744 16.5328 7.12523C16.4507 7.47606 16.2378 7.78235 15.9376 7.98165L15.8609 8.02871L15.5235 8.22353C15.5819 8.76273 15.5736 9.30708 15.4986 9.84424L15.7372 9.98259C16.0496 10.1631 16.2812 10.4561 16.3848 10.8017C16.4884 11.1472 16.456 11.5193 16.2944 11.8419L16.2553 11.9153L15.076 13.9576C14.8955 14.27 14.6025 14.5017 14.2569 14.6053C13.9113 14.7088 13.5392 14.6765 13.2167 14.5148L13.1433 14.4753L12.8172 14.2871C12.4074 14.5817 11.9651 14.8283 11.4991 15.0221L11.4995 15.5831C11.5 15.9434 11.3629 16.2904 11.1163 16.5532C10.8697 16.816 10.5321 16.9748 10.1725 16.9972L10.0831 17L7.57153 16.9967C7.19697 16.9961 6.83793 16.847 6.57312 16.5821C6.30831 16.3172 6.15932 15.9581 6.15883 15.5835L6.15788 14.9073C5.76852 14.7244 5.39771 14.5044 5.05059 14.2504L4.44918 14.5967C4.12448 14.7834 3.73902 14.8337 3.37726 14.7367C3.01551 14.6397 2.70698 14.4032 2.5193 14.0791L1.26047 11.9064C1.07996 11.5945 1.02522 11.2255 1.10742 10.8747C1.18962 10.5238 1.40257 10.2176 1.70283 10.0184L1.77906 9.97129L2.3913 9.61835C2.34424 9.17129 2.34188 8.71765 2.38706 8.26494L1.70753 7.87247C1.39506 7.69207 1.16331 7.39911 1.05965 7.05351C0.955998 6.70791 0.988275 6.33577 1.14989 6.01318L1.18941 5.93976L2.36871 3.89741C2.54919 3.58502 2.84218 3.35337 3.18777 3.2498C3.53336 3.14624 3.90547 3.17859 4.228 3.34023L4.30141 3.37976L4.89436 3.72188C5.28027 3.42082 5.69854 3.1637 6.14141 2.95529L6.14047 2.41694C6.14001 2.05657 6.27707 1.7096 6.52367 1.44681C6.77028 1.18403 7.10786 1.02523 7.46753 1.00282L7.55741 1ZM7.55553 2.41506L7.55694 3.85271L6.74377 4.23576C6.39553 4.39906 6.06706 4.60094 5.764 4.83718L5.01247 5.424L3.62941 4.62494L3.59365 4.60518L2.41483 6.64753L3.88636 7.49694L3.79506 8.40612C3.75968 8.7598 3.76078 9.11619 3.79836 9.46965L3.8953 10.3854L2.48494 11.1976L3.7433 13.3704L5.14377 12.5647L5.88636 13.1087C6.15997 13.309 6.45231 13.4823 6.7593 13.6264L7.57106 14.008L7.57388 15.5816L10.0845 15.5849L10.0831 14.0791L10.9555 13.7158C11.3216 13.5635 11.6689 13.3698 11.9908 13.1384L12.7329 12.6047L13.8506 13.2499L15.0289 11.2075L13.9654 10.592L14.0972 9.64847C14.1561 9.22659 14.1628 8.79904 14.1169 8.37553L14.0181 7.45882L15.1555 6.80235L13.8967 4.62965L12.7645 5.28235L12.0214 4.74024C11.686 4.4956 11.3229 4.29152 10.9395 4.13224L10.0689 3.77082L10.0666 2.41835L7.55553 2.41506ZM10.3715 6.47624C11.0214 6.85201 11.4955 7.47036 11.6898 8.19547C11.8841 8.92058 11.7827 9.69316 11.4078 10.3435C11.2223 10.6654 10.9752 10.9476 10.6805 11.1739C10.3859 11.4002 10.0495 11.5662 9.69068 11.6623C9.33183 11.7585 8.95754 11.7829 8.58923 11.7343C8.22092 11.6856 7.86582 11.5648 7.54424 11.3788C6.89445 11.003 6.4204 10.3846 6.2262 9.65948C6.032 8.93438 6.13352 8.16184 6.50847 7.51153C6.69395 7.18963 6.94107 6.90746 7.23571 6.68117C7.53034 6.45488 7.86671 6.28891 8.22556 6.19275C8.58441 6.09659 8.9587 6.07213 9.32701 6.12077C9.69532 6.16942 10.0504 6.29021 10.372 6.47624H10.3715ZM7.73388 8.21835C7.54638 8.54388 7.49567 8.9305 7.5929 9.29336C7.69012 9.65623 7.92733 9.96571 8.25247 10.1539C8.41305 10.2468 8.59037 10.3071 8.77429 10.3314C8.9582 10.3557 9.14511 10.3435 9.32431 10.2956C9.50351 10.2476 9.67149 10.1647 9.81864 10.0517C9.96579 9.93877 10.0892 9.7979 10.1819 9.63718C10.5588 8.98353 10.356 8.15435 9.73435 7.74494L9.66377 7.70118L9.59035 7.66165C9.26834 7.49988 8.89663 7.46742 8.55145 7.57093C8.20626 7.67444 7.91375 7.90608 7.73388 8.21835Z" fill="#666666"></path></svg></div></div></div></div><div hidden="" class="imt-fb-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block; opacity: 1;"><div><svg class="imt-manga-feedback" width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9996 3C15.1684 3 15.3356 3.03326 15.4916 3.09787C15.6476 3.16248 15.7893 3.25719 15.9087 3.37658C16.0281 3.49597 16.1228 3.6377 16.1874 3.79369C16.252 3.94968 16.2853 4.11687 16.2853 4.28571V12.8571C16.2853 13.026 16.252 13.1932 16.1874 13.3492C16.1228 13.5052 16.0281 13.6469 15.9087 13.7663C15.7893 13.8857 15.6476 13.9804 15.4916 14.045C15.3356 14.1096 15.1684 14.1429 14.9996 14.1429H8.3233L5.3773 16.0736C5.31264 16.1159 5.23773 16.14 5.1605 16.1433C5.08327 16.1465 5.00659 16.1288 4.9386 16.0921C4.8706 16.0553 4.81382 16.0008 4.77426 15.9344C4.73469 15.868 4.71383 15.7922 4.71387 15.7149V14.1429H2.99958C2.83074 14.1429 2.66355 14.1096 2.50756 14.045C2.35157 13.9804 2.20983 13.8857 2.09044 13.7663C1.97105 13.6469 1.87635 13.5052 1.81174 13.3492C1.74712 13.1932 1.71387 13.026 1.71387 12.8571V4.28571C1.71387 3.94472 1.84933 3.61769 2.09044 3.37658C2.33156 3.13546 2.65859 3 2.99958 3H14.9996ZM14.9996 4.28571H2.99958V12.8571H5.99958V14.1287L7.93972 12.8571H14.9996V4.28571ZM9.54815 8.57143V9.85714H5.99958V8.57143H9.54815ZM11.9996 6V7.28571H5.99958V6H11.9996Z" fill="#6C6F73"></path></svg></div></div></div></div></div></template></div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>