2.1. Foundational Models for Depth and Flow

In the tasks of monocular depth (MDE) and flow estimation,
well-generalizable foundation models have replaced early
deep learning approaches ++ref++[ (Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Pro- ceedings of the IEEE international conference on computer vision, pages 2758–2766, 2015. 2), (Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency. on computer vision and pattern recognition, pages 270–279, 2017. 2), (Cl´ement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation. national conference on computer vision, pages 3828–3838, 2019. 2) ]++ref++ in the last years. For
MDE, DepthAnything ++ref++[ (Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of 10 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371–10381, 2024. 2) ]++ref++ uses a data engine to construct
a large corpus of automatically annotated data to learn rel-
ative depth estimation. Additional fine-tuning allows for
metric depth estimates. DepthAnythingV2 ++ref++[ (Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao- gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any- thing v2. arXiv preprint arXiv:2406.09414, 2024. 2) ]++ref++ finetunes
the previous model using synthetic data for better perfor-
mance. Metric3D ++ref++[ (Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9043–9053, 2023. 2) ]++ref++ and Metric3Dv2 ++ref++[ (Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 2) ]++ref++ transform im-
ages to canonical camera intrinsics with a fixed focal length.
DepthPro ++ref++[ (Aleksei Bochkovskii, Ama¨el Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than a second. arXiv preprint arXiv:2410.02073, 2024. 2) ]++ref++ proposes a two-stage training curriculum with
a second stage solely on synthetic data to sharpen bound-
ary predictions. DepthCrafter ++ref++[ (Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 2) ]++ref++ leverages a conditional
diffusion model to predict temporally consistent depth maps

for videos. In this work, we utilize UniDepth ++ref++[ (Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106–10116, 2024. 2, 6) ]++ref++ for metric
MDE, which uses a geometric invariance loss on different
image augmentation to enforce consistency.

RAFT ++ref++[ (Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part II 16, pages 402–419. Springer, 2020. 2) ]++ref++ presented the state of the art for optical flow
It improved previous meth-
estimation for a long time.
ods by introducing a recurrent look-up operator on corre-
lation volumes to iteratively refine flow predictions with-
out needing coarse-to-fine flow pyramids. GMFlow ++ref++[ (Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8121–8130, 2022. 2) ]++ref++
avoids correlation volumes and instead leverages the prop-
erties of transformers for global matching on feature maps.
This removes the need for iterative steps to improve runtime
performance. UniMatch ++ref++[ (Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2, 6) ]++ref++ extends GMFlow network by
tasks of disparity and depth prediction to enable cross-task
transfer learning of a single transformer network.

We rely on both off-the-shelf MDE and Optical Flow
networks to benefit from strong geometric priors during
training and inference.

2.2. SfM and SLAM

For many decades, the problem of recovering camera pa-
rameters and geometry from images has been formulated
as the Structure-from-Motion (SfM) pipeline ++ref++[ (Richard Hartley and Andrew Zisserman. Multiple view ge- ometry in computer vision. Cambridge university press, 2003. 2), (John Oliensis. A critique of structure-from-motion algo- rithms. Computer Vision and Image Understanding, 80(2): 172–214, 2000. 2), (Onur ¨Ozyes¸il, Vladislav Voroninski, Ronen Basri, and Amit Singer. A survey of structure from motion*. Acta Numerica, 26:305–364, 2017. 2 9) ]++ref++.
While many different implementations of the SfM pipeline
exist, COLMAP ++ref++[ (Johannes L Schonberger and Jan-Michael Frahm. Structure- In Proceedings of the IEEE con- from-motion revisited. ference on computer vision and pattern recognition, pages 4104–4113, 2016. 1, 2) ]++ref++ has emerged as the standard due to
its robustness and flexibility. One of the drawbacks of SfM
methods is their high computational cost. Simultaneous Lo-
cation and Mapping (SLAM) ++ref++[ (Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. Lsd- slam: Large-scale direct monocular slam. In European con- ference on computer vision, pages 834–849. Springer, 2014. 2), (Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611–625, 2017. 2), (Raul Mur-Artal and Juan D Tard´os. Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras. IEEE transactions on robotics, 33(5):1255–1262, 2017. 2), (Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam IEEE transactions on robotics, 31(5):1147–1163, system. 2015. 2) ]++ref++ approaches em-
ploy a similar pipeline to SfM but focus on the efficient pro-
cessing of consecutive video frames. In recent years, these
classical optimization-based approaches were enhanced by
learned components ++ref++[ (Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6684–6692, 2017. 2), (Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224–236, 2018. 2), (Vincent Leroy, Yohann Cabon, and J´erˆome Revaud. Ground- arXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. 2), (Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallab- hula, and Daniel Cremers. Learning correspondence uncer- tainty via differentiable nonlinear least squares. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13102–13112, 2023. 2), (Ren´e Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV), pages 284–299, 2018. 2), (Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. In Proceedings of matching with graph neural networks. the IEEE/CVF conference on computer vision and pattern recognition, pages 4938–4947, 2020.), (Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, et al. Back to the feature: Learning robust camera localization from pixels to pose. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 3247–3257, 2021. 2), (Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry. In Proceedings of the European conference on computer vision (ECCV), pages 817–833, 2018. 2) ]++ref++. However,
relying on epipolar geometry ++ref++[ (Richard I Hartley. In defense of the eight-point algorithm. IEEE Transactions on pattern analysis and machine intelli- gence, 19(6):580–593, 1997. 2) ]++ref++ or photometric consis-
tency ++ref++[ (Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611–625, 2017. 2) ]++ref++ makes them susceptible to high error on highly
dynamic scenes. The strong focus on self driving data pro-
vided datasets with mostly static environments ++ref++[ (Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- In Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 11621–11631, 2020. 2), (Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research, 32(11):1231–1237, 2013. 2), (Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2446–2454, 2020. 2) ]++ref++,
an assumption that does not hold for casual videos.

2.3. Learning Based SfM and SLAM

Largely learning-based methods started to replace classical
SLAM and SfM systems due to improved robustness ++ref++[ (Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neu- ral information processing systems, 34:16558–16569, 2021. 2, 6) ]++ref++.
DROID-SLAM extends the framework of RAFT ++ref++[ (Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part II 16, pages 402–419. Springer, 2020. 2) ]++ref++ by an
update operator on both depth and pose estimates. A final
differentiable bundle adjustment (BA) layer produces the fi-
nal pose estimates. ParticleSfM ++ref++[ (Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523–542. Springer, 2022. 2, 6) ]++ref++ utilizes dense corre-
spondences inside a BA framework to optimize poses. The
dense correspondences are initialized from optical flow, and
dynamic points are filtered using trajectory-based motion
segmentation. CasualSAM ++ref++[ (Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Ru- binstein, Noah Snavely, and William T Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 20–37. Springer, 2022. 2, 6) ]++ref++ predicts both depth and
movement from images to get frame-to-frame motion. A
global optimization aligns the scale of the prediction and
refines the poses. Dust3R ++ref++[ (Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi- sion made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20697–20709, 2024. 1, 2) ]++ref++ is a dense multi-view stereo
method that regresses point coordinates between an im-
age pair. This allows it to be extended to either SfM or

SLAM. FlowMap ++ref++[ (Cameron Smith, David Charatan, Ayush Tewari, and Vin- cent Sitzmann. Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent. arXiv:2404.15259, 2024. 2, 3, 4) ]++ref++ proposes to reconstruct a scene by
overfitting a depth network to it and aligning depth maps
via correspondences from flow or point tracking. LEAP-
VO ++ref++[ (Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry. on Computer Vision and Pattern Recognition, pages 19844– 19853, 2024. 2, 3, 6) ]++ref++ combines visual and temporal information of video
sequences to improve the tracking accuracy of points and
identify occluded and dynamic points. A sliding window
bundle adjustment then optimizes the poses. The concurrent
work of MonST3R ++ref++[ (Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam- pani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming- Hsuan Yang. Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion. arXiv:2410.03825, 2024. 3, 6) ]++ref++ finetunes Dust3r on mostly syn-
thetic data to generalize it to dynamic scenes. While these
works achieve impressive progress, they generally obtain
poses from aligning depth and point maps or by optimizing
them per-scene. This makes it hard to inject prior informa-
tion about camera motion. In contrast, our method uses a
neural network to predict a trajectory, which can effectively
learn priors over realistic camera motions.

