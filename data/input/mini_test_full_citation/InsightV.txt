2.1. Vision-Language Reasoning

Recent advancements in MLLMs ++ref++[ (Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understand- ing, localization, text reading, and beyond. 2023. 2), (Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun- yuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2), (Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6), (Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 26296–26306, 2024. 1, 5), (Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6), (Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 1, 2), (Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial- temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 2), (Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 1, 2, 6), (QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6) ]++ref++
have equipped these models with robust capabilities across
diverse domains, including visual understanding ++ref++[ (Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6), (QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6) ]++ref++,
mathematics ++ref++[ (Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: A foundational and multimodal mathe- matical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7126–7133, 2023. 2) ]++ref++, college-level questions ++ref++[ (Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks. In CVPR, pages 24185–24198, 2024. 1, 2) ]++ref++, and scien-
tific inquiries. In visual understanding, most research ++ref++[ (Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Mon- key: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 2), (Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6), (Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Ji- wen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 2), (Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5, 6), (Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. 2) ]++ref++ emphasizes fine-grained detail analysis
and localization, training models to perform visual reason-
ing with tailored datasets to enhance interpretive capabili-
ties. For mathematics and expert-level reasoning, existing
methods ++ref++[ (Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric prob- lem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 2), (Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual in- struction tuning. arXiv preprint arXiv:2407.08739, 2024. 2, 3), (Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yim- ing Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. 2, 5) ]++ref++ predominantly derive from Chain-of-
Thought ++ref++[ (Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824– 24837, 2022. 1, 2) ]++ref++ approaches, training MLLMs to generate step-
by-step reasoning across various subjects. However, these
approaches often focus primarily on improving dataset qual-
ity through Chain-of-Thought, overlooking the importance
of structured reasoning paths and extended reasoning chains
in advancing model reasoning capabilities. Additionally,
significant challenges arise when relying on a single model
to manage the entire reasoning process for complex tasks,
underscoring the need for a multi-agent system to decom-
pose and enhance this process. In this work, we tackle these
challenges by introducing a scalable reasoning data gener-
ation pipeline and implementing a multi-agent system for
reasoning and summarization decomposition, enhancing the
overall reasoning capabilities of existing MLLMs.

2.2. Vision-Language Alignment

To align the model more closely with human preferences,
several alignment techniques are employed for MLLMs. A
widely used approach is Reinforcement Learning from Hu-
man Feedback ++ref++[ (Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2) ]++ref++ (RLHF), which iteratively refines the
model’s responses based on human feedback, enhancing
both response quality and interpretability. To further improve
MLLM capabilities, Direct Preference Optimization ++ref++[ (Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct prefer- ence optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, 5) ]++ref++
(DPO) is introduced to simplify the alignment process. By di-
rectly training on human preference data, DPO optimizes the
model’s outputs to better match human-selected responses.
However, traditional DPO is primarily focused on offline
scenarios, and as the model evolves, the effectiveness of
this approach may significantly diminish. To address this,
Iterative DPO ++ref++[ (Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak lan- guage models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 2) ]++ref++ has been proposed, which optimizes pref-

2

erence pairs through DPO at each iteration. It then generates
new preference pairs for the next iteration using the updated
model and evaluates them with a reward model. In this pa-
per, we use iterative DPO to achieve stronger alignment and
enhance the model’s reasoning capabilities.

