2.1. Human Motion Generation

Human motion generation has progressed significantly in
recent years ++ref++[ (HP-GAN: Probabilistic 3D human motion prediction via GAN), (Implicit neural representations for variable length human motion generation), (Chopin, N), (Flexible motion in-betweening with diffusion models), (Motionlcm: Real-time controllable motion generation via latent consistency model), (Momask: Generative masked mod- eling of 3d human motions), (A recurrent variational autoen- coder for human motion synthesis), (Nemf: Neural motion fields for kinematic ani- mation), (MoGlow: Probabilistic and controllable motion synthesis using normalising flows), (Mmm: Generative masked motion model), (Mmm: Generative masked motion model), (Guibas), (Human motion gen- eration using wasserstein GAN), (Human motion dif- fusion model), (Transflower: probabilistic autoregressive dance gen- eration with multimodal attention), (ActFormer: A gan-based transformer towards general action-conditioned 3d human motion generation), (Motiondif- fuse: Text-driven human motion generation with diffusion model), (Re- modiffuse: Retrieval-augmented motion diffusion model), (Human motion generation: A survey) ]++ref++ leveraging a variety of conditioning sig-
nals such as text ++ref++[ (Executing your commands via motion diffusion in latent space), (Synthesis of compositional animations from textual descriptions), (TM2T: Stochastic and tokenized modeling for the reciprocal gener- ation of 3d human motions and texts), (Act as you wish: Fine-grained control of mo- tion diffusion model with hierarchical semantic graphs) ]++ref++, actions ++ref++[ (Ac- tion2Motion: Conditioned generation of 3D human mo- In ACM International Conference on Multimedia tions) ]++ref++, speech ++ref++[ (Listen, denoise, action! audio-driven motion synthesis with diffusion models), (Taming diffusion models for audio-driven co-speech gesture generation) ]++ref++,
music ++ref++[ (Ai choreographer: Music conditioned 3d dance generation with aist++), (Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory), (You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection), (Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis), (Edge: In Proceedings of Editable dance generation from music), (Transflower: probabilistic autoregressive dance gen- eration with multimodal attention) ]++ref++, and scenes/objects ++ref++[ (Stochastic scene-aware motion prediction), (NIFTY: Neural object interaction fields for guided human motion synthesis), (HUMANISE: Language-conditioned hu- man motion generation in 3d scenes), (Generating human interaction motions in scenes with text control), (Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P´erez Pellitero, and Gerard Pons-Moll) ]++ref++. Recently, multimodal motion generation has
also gained attention ++ref++[ (Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls), (M3gpt: An ad- vanced multimodal, multitask framework for motion com- prehension and generation), (Large motion model for unified multi-modal motion generation), (Ude: A unified driving engine for human motion generation) ]++ref++ enabling multiple input
modalities. However, most existing methods focus solely
on generative tasks without supporting estimation. For in-
stance, the method ++ref++[ (Large motion model for unified multi-modal motion generation) ]++ref++ supports video input but treats it as
a generative task, resulting in motions that loosely imitate
video content rather than precisely matching it. In contrast,
our method jointly handles generation and estimation tasks,
yielding more precise video-conditioned results.

For long-sequence motion generation, existing works
mostly rely on ad-hoc post-processing techniques to stitch
separately generated fixed-length motions ++ref++[ (Black, and G¨ul Varol), (Black, G¨ul Varol, Xue Bin Peng, and Davis Rempe), (Hauptmann, and Jungdam Won), (Diffcollage: Parallel generation of large content with diffusion models) ]++ref++. In
contrast, our method introduces a novel diffusion-based ar-
chitecture enabling seamless generation of arbitrary-length
motions conditioned on multiple modalities without com-
plex post-processing.

Existing datasets, such as AMASS ++ref++[ (Troje, Ger- ard Pons-Moll, and Michael J) ]++ref++, are limited in
size and diversity. To address the scarcity of 3D data,
Motion-X ++ref++[ (Motion-x: A large- scale 3d expressive whole-body human motion dataset) ]++ref++ and MotionBank ++ref++[ (Mo- tionbank: A large-scale video motion benchmark with disen- tangled rule-based annotations, 2024) ]++ref++ augment datasets us-
ing 2D videos and 3D pose estimation models ++ref++[ (Wham: Reconstructing world-grounded humans with accu- rate 3d motion), (Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras) ]++ref++, but
the resulting motions often contain artifacts.
In contrast,
our method directly leverages in-the-wild videos with 2D
annotations without explicit 3D reconstruction, reducing re-
liance on noisy data and enhancing robustness and diversity.

2.2. Human Motion Estimation

Human pose estimation from images ++ref++[ (Black, David W), (Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation), (Neural localizer fields for continuous 3d human pose and shape estimation) ]++ref++, videos ++ref++[ (Be- yond static features for temporally consistent 3d human pose and shape from a video), (Humans in 4d: Re- In constructing and tracking humans with transformers), (VIBE: Video inference for human body pose and shape estimation) ]++ref++, or even sparse marker data ++ref++[ (Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head- mounted camera), (Sparseposer: Real-time full- body motion reconstruction from sparse data), (Transpose: Real-time 3d human translation and pose estimation with six inertial sensors) ]++ref++ has been
studied extensively in the literature. Recent works focus pri-
marily on estimating global human motion in world-space
coordinates ++ref++[ (Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal), (Coin: Control-inpainting diffusion prior for human and camera In ECCV, pages 426–446), (Wham: Reconstructing world-grounded humans with accu- rate 3d motion), (Tram: Global trajectory and motion of 3d humans from in- the-wild videos), (Decoupling human and camera motion from videos in the wild), (Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras) ]++ref++. This is an inherently
ill-posed problem, hence these methods leverage generative
priors and SLAM methods to constrain human and camera
motions, respectively. However, these methods typically in-
volve computationally expensive optimization or separate
post-processing steps.

More recent approaches aim to estimate global human
motion in a feed-forward manner ++ref++[ (World-grounded human motion recovery via gravity-view coordinates), (Wham: Reconstructing world-grounded humans with accu- rate 3d motion), (Tram: Global trajectory and motion of 3d humans from in- the-wild videos), (Rohm: Robust human motion reconstruction via diffusion) ]++ref++, offer-
ing faster solutions. Our method extends this direction by
jointly modeling generation and estimation within a uni-
fied diffusion framework. This integration leverages shared
representations and generative priors during training to pro-
duce more plausible estimations.