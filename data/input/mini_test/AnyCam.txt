2.1. Foundational Models for Depth and Flow

In the tasks of monocular depth (MDE) and flow estimation,
well-generalizable foundation models have replaced early
deep learning approaches ++ref++[ (Flownet: Learning optical flow with convolutional networks), (Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency), (Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation) ]++ref++ in the last years. For
MDE, DepthAnything ++ref++[ (Depth anything: Unleashing the power of large-scale unlabeled data) ]++ref++ uses a data engine to construct
a large corpus of automatically annotated data to learn rel-
ative depth estimation. Additional fine-tuning allows for
metric depth estimates. DepthAnythingV2 ++ref++[ (Depth any- thing v2) ]++ref++ finetunes
the previous model using synthetic data for better perfor-
mance. Metric3D ++ref++[ (Metric3d: Towards zero-shot metric 3d prediction from a single image) ]++ref++ and Metric3Dv2 ++ref++[ (Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation) ]++ref++ transform im-
ages to canonical camera intrinsics with a fixed focal length.
DepthPro ++ref++[ (Depth pro: Sharp monocular metric depth in less than a second) ]++ref++ proposes a two-stage training curriculum with
a second stage solely on synthetic data to sharpen bound-
ary predictions. DepthCrafter ++ref++[ (Depthcrafter: Generating consistent long depth sequences for open-world videos) ]++ref++ leverages a conditional
diffusion model to predict temporally consistent depth maps

for videos. In this work, we utilize UniDepth ++ref++[ (Unidepth: Universal monocular metric depth estimation) ]++ref++ for metric
MDE, which uses a geometric invariance loss on different
image augmentation to enforce consistency.

RAFT ++ref++[ (Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow) ]++ref++ presented the state of the art for optical flow
It improved previous meth-
estimation for a long time.
ods by introducing a recurrent look-up operator on corre-
lation volumes to iteratively refine flow predictions with-
out needing coarse-to-fine flow pyramids. GMFlow ++ref++[ (Gmflow: Learning optical flow via global matching) ]++ref++
avoids correlation volumes and instead leverages the prop-
erties of transformers for global matching on feature maps.
This removes the need for iterative steps to improve runtime
performance. UniMatch ++ref++[ (Unifying flow, stereo and depth estimation) ]++ref++ extends GMFlow network by
tasks of disparity and depth prediction to enable cross-task
transfer learning of a single transformer network.

We rely on both off-the-shelf MDE and Optical Flow
networks to benefit from strong geometric priors during
training and inference.

2.2. SfM and SLAM

For many decades, the problem of recovering camera pa-
rameters and geometry from images has been formulated
as the Structure-from-Motion (SfM) pipeline ++ref++[ (Multiple view ge- ometry in computer vision), (A critique of structure-from-motion algo- rithms), (A survey of structure from motion*) ]++ref++.
While many different implementations of the SfM pipeline
exist, COLMAP ++ref++[ (Structure- In Proceedings of the IEEE con- from-motion revisited) ]++ref++ has emerged as the standard due to
its robustness and flexibility. One of the drawbacks of SfM
methods is their high computational cost. Simultaneous Lo-
cation and Mapping (SLAM) ++ref++[ (Lsd- slam: Large-scale direct monocular slam), (Direct sparse odometry), (Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras), (Orb-slam: a versatile and accurate monocular slam IEEE transactions on robotics, 31(5):1147–1163, system) ]++ref++ approaches em-
ploy a similar pipeline to SfM but focus on the efficient pro-
cessing of consecutive video frames. In recent years, these
classical optimization-based approaches were enhanced by
learned components ++ref++[ (Dsac-differentiable ransac for camera localization), (Superpoint: Self-supervised interest point detection and description), (Ground- arXiv preprint ing image matching in 3d with mast3r), (Learning correspondence uncer- tainty via differentiable nonlinear least squares), (Deep fundamental matrix estimation), (In Proceedings of matching with graph neural networks), (Back to the feature: Learning robust camera localization from pixels to pose), (Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry) ]++ref++. However,
relying on epipolar geometry ++ref++[ (In defense of the eight-point algorithm) ]++ref++ or photometric consis-
tency ++ref++[ (Direct sparse odometry) ]++ref++ makes them susceptible to high error on highly
dynamic scenes. The strong focus on self driving data pro-
vided datasets with mostly static environments ++ref++[ (nuscenes: A multi- In Proceedings of modal dataset for autonomous driving), (Vision meets robotics: The kitti dataset), (Scalability in perception for autonomous driving: Waymo open dataset) ]++ref++,
an assumption that does not hold for casual videos.

2.3. Learning Based SfM and SLAM

Largely learning-based methods started to replace classical
SLAM and SfM systems due to improved robustness ++ref++[ (Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras) ]++ref++.
DROID-SLAM extends the framework of RAFT ++ref++[ (Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow) ]++ref++ by an
update operator on both depth and pose estimates. A final
differentiable bundle adjustment (BA) layer produces the fi-
nal pose estimates. ParticleSfM ++ref++[ (Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild) ]++ref++ utilizes dense corre-
spondences inside a BA framework to optimize poses. The
dense correspondences are initialized from optical flow, and
dynamic points are filtered using trajectory-based motion
segmentation. CasualSAM ++ref++[ (Structure and motion from casual videos) ]++ref++ predicts both depth and
movement from images to get frame-to-frame motion. A
global optimization aligns the scale of the prediction and
refines the poses. Dust3R ++ref++[ (Dust3r: Geometric 3d vi- sion made easy) ]++ref++ is a dense multi-view stereo
method that regresses point coordinates between an im-
age pair. This allows it to be extended to either SfM or

SLAM. FlowMap ++ref++[ (Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent) ]++ref++ proposes to reconstruct a scene by
overfitting a depth network to it and aligning depth maps
via correspondences from flow or point tracking. LEAP-
VO ++ref++[ (Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry) ]++ref++ combines visual and temporal information of video
sequences to improve the tracking accuracy of points and
identify occluded and dynamic points. A sliding window
bundle adjustment then optimizes the poses. The concurrent
work of MonST3R ++ref++[ (Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion) ]++ref++ finetunes Dust3r on mostly syn-
thetic data to generalize it to dynamic scenes. While these
works achieve impressive progress, they generally obtain
poses from aligning depth and point maps or by optimizing
them per-scene. This makes it hard to inject prior informa-
tion about camera motion. In contrast, our method uses a
neural network to predict a trajectory, which can effectively
learn priors over realistic camera motions.