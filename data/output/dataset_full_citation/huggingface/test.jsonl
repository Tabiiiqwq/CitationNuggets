{"paper_id": "deep_learning_results", "input_text": "2. Related Work\n\n2.1. Human Motion Generation\n\nHuman motion generation has progressed significantly in\nrecent years  leveraging a variety of conditioning sig-\nnals such as text , actions , speech ,\nmusic , and scenes/objects . Recently, multimodal motion generation has\nalso gained attention  enabling multiple input\nmodalities. However, most existing methods focus solely\non generative tasks without supporting estimation. For in-\nstance, the method  supports video input but treats it as\na generative task, resulting in motions that loosely imitate\nvideo content rather than precisely matching it. In contrast,\nour method jointly handles generation and estimation tasks,\nyielding more precise video-conditioned results.\n\nFor long-sequence motion generation, existing works\nmostly rely on ad-hoc post-processing techniques to stitch\nseparately generated fixed-length motions . In\ncontrast, our method introduces a novel diffusion-based ar-\nchitecture enabling seamless generation of arbitrary-length\nmotions conditioned on multiple modalities without com-\nplex post-processing.\n\nExisting datasets, such as AMASS , are limited in\nsize and diversity. To address the scarcity of 3D data,\nMotion-X  and MotionBank  augment datasets us-\ning 2D videos and 3D pose estimation models , but\nthe resulting motions often contain artifacts.\nIn contrast,\nour method directly leverages in-the-wild videos with 2D\nannotations without explicit 3D reconstruction, reducing re-\nliance on noisy data and enhancing robustness and diversity.\n\n2.2. Human Motion Estimation\n\nHuman pose estimation from images , videos , or even sparse marker data  has been\nstudied extensively in the literature. Recent works focus pri-\nmarily on estimating global human motion in world-space\ncoordinates . This is an inherently\nill-posed problem, hence these methods leverage generative\npriors and SLAM methods to constrain human and camera\nmotions, respectively. However, these methods typically in-\nvolve computationally expensive optimization or separate\npost-processing steps.\n\nMore recent approaches aim to estimate global human\nmotion in a feed-forward manner , offer-\ning faster solutions. Our method extends this direction by\njointly modeling generation and estimation within a uni-\nfied diffusion framework. This integration leverages shared\nrepresentations and generative priors during training to pro-\nduce more plausible estimations.\n\n\n", "expected_text": "2. Related Work\n\n2.1. Human Motion Generation\n\nHuman motion generation has progressed significantly in\nrecent years [CITATION_0] leveraging a variety of conditioning sig-\nnals such as text [CITATION_1], actions [CITATION_2], speech [CITATION_3],\nmusic [CITATION_4], and scenes/objects [CITATION_5]. Recently, multimodal motion generation has\nalso gained attention [CITATION_6] enabling multiple input\nmodalities. However, most existing methods focus solely\non generative tasks without supporting estimation. For in-\nstance, the method [CITATION_7] supports video input but treats it as\na generative task, resulting in motions that loosely imitate\nvideo content rather than precisely matching it. In contrast,\nour method jointly handles generation and estimation tasks,\nyielding more precise video-conditioned results.\n\nFor long-sequence motion generation, existing works\nmostly rely on ad-hoc post-processing techniques to stitch\nseparately generated fixed-length motions [CITATION_8]. In\ncontrast, our method introduces a novel diffusion-based ar-\nchitecture enabling seamless generation of arbitrary-length\nmotions conditioned on multiple modalities without com-\nplex post-processing.\n\nExisting datasets, such as AMASS [CITATION_9], are limited in\nsize and diversity. To address the scarcity of 3D data,\nMotion-X [CITATION_10] and MotionBank [CITATION_11] augment datasets us-\ning 2D videos and 3D pose estimation models [CITATION_12], but\nthe resulting motions often contain artifacts.\nIn contrast,\nour method directly leverages in-the-wild videos with 2D\nannotations without explicit 3D reconstruction, reducing re-\nliance on noisy data and enhancing robustness and diversity.\n\n2.2. Human Motion Estimation\n\nHuman pose estimation from images [CITATION_13], videos [CITATION_14], or even sparse marker data [CITATION_15] has been\nstudied extensively in the literature. Recent works focus pri-\nmarily on estimating global human motion in world-space\ncoordinates [CITATION_16]. This is an inherently\nill-posed problem, hence these methods leverage generative\npriors and SLAM methods to constrain human and camera\nmotions, respectively. However, these methods typically in-\nvolve computationally expensive optimization or separate\npost-processing steps.\n\nMore recent approaches aim to estimate global human\nmotion in a feed-forward manner [CITATION_17], offer-\ning faster solutions. Our method extends this direction by\njointly modeling generation and estimation within a uni-\nfied diffusion framework. This integration leverages shared\nrepresentations and generative priors during training to pro-\nduce more plausible estimations.\n\n\n", "citation_info": {"content": [["Emad Barsoum, John Kender, and Zicheng Liu. HP-GAN: Probabilistic 3D human motion prediction via GAN. In CVPR Workshops, 2018. 2", "Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda. Implicit neural representations for variable length human motion generation. In ECCV, 2022. 2", "B. Chopin, N. Otberdout, M. Daoudi, and A. Bartolo. Human motion prediction using manifold-aware wasserstein gan. In FG, 2021. 2", "Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, and Michiel van de Panne. Flexible motion in-betweening with diffusion models. SIGGRAPH, 2024.", "Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV, 2024. 2", "Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked mod- eling of 3d human motions. 2023. 2", "Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and Taku Komura. A recurrent variational autoen- coder for human motion synthesis. In BMVC, 2017. 2", "Chengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. Nemf: Neural motion fields for kinematic ani- mation. In NeurIPS, 2022. 2", "Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. MoGlow: Probabilistic and controllable motion synthesis using normalising flows. ACM Transactions on Graphics (TOG), 2020. 2", "Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In CVPR, 2024. 2", "Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In CVPR, 2024. 2", "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas. Humor: 3d human motion model for robust pose estimation. In ICCV, 2021. 2, 5 10", "Ayumi Shiobara and Makoto Murakami. Human motion gen- eration using wasserstein GAN. In International Conference on Digital Signal Processing (ICDSP), 2021. 2", "Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion dif- fusion model. In ICLR, 2023. 2, 5, 6, 7, 8, 13", "Guillermo Valle-P\u00b4erez, Gustav Eje Henter, Jonas Beskow, Andr\u00b4e Holzapfel, Pierre-Yves Oudeyer, and Simon Alexan- derson. Transflower: probabilistic autoregressive dance gen- eration with multimodal attention. ACM Transactions on Graphics (TOG), 2021. 2", "Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng, and Wei Wu. ActFormer: A gan-based transformer towards general action-conditioned 3d human motion generation. In ICCV, 2023. 2", "Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif- fuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2", "Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re- modiffuse: Retrieval-augmented motion diffusion model. In arXiv preprint arXiv:2304.01116, 2023. 2", "Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. TPAMI, 2023. 2"], ["Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your commands via motion diffusion in latent space. In CVPR, 2023. 2", "Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In ICCV, 2021. 2", "Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. TM2T: Stochastic and tokenized modeling for the reciprocal gener- ation of 3d human motions and texts. In ECCV, 2022. 2", "Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Yang Wei, and Li Yuan. Act as you wish: Fine-grained control of mo- tion diffusion model with hierarchical semantic graphs. In NeurIPs, 2023. 2"], ["Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac- tion2Motion: Conditioned generation of 3D human mo- In ACM International Conference on Multimedia tions. (ACMMM), 2020. 2"], ["Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG), 2023. 2", "Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models for audio-driven co-speech gesture generation. In CVPR, 2023. 2"], ["Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In ICCV, 2021. 2, 6, 7, 12, 13", "Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory. Computer Vision and Pattern Recognition, pages 11050\u2013 11059, 2022. 2, 7, 13", "Jiangxin Sun, Chunyu Wang, Huang Hu, Hanjiang Lai, Zhi Jin, and Jian-Fang Hu. You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection. In NeurIPS, 2022. 2", "Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis. In ACM International Conference on Multimedia (ACMMM), 2018. 2", "Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: In Proceedings of Editable dance generation from music. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448\u2013458, 2023. 2, 6, 7, 12, 13", "Guillermo Valle-P\u00b4erez, Gustav Eje Henter, Jonas Beskow, Andr\u00b4e Holzapfel, Pierre-Yves Oudeyer, and Simon Alexan- derson. Transflower: probabilistic autoregressive dance gen- eration with multimodal attention. ACM Transactions on Graphics (TOG), 2021. 2"], ["Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In ICCV, 2021. 2", "Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. NIFTY: Neural object interaction fields for guided human motion synthesis. arXiv:2307.07511, 2023. 2", "Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. HUMANISE: Language-conditioned hu- man motion generation in 3d scenes. In Neural Information Processing Systems (NeurIPS), 2022. 3", "Hongwei Yi, Justus Thies, Michael J Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control. In European Conference on Com- puter Vision, pages 246\u2013263. Springer, 2024. 3", "Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya A. Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P\u00b4erez Pellitero, and Gerard Pons-Moll. Force: Dataset and method for intuitive physics guided human-object interac- tion. In International Conference on 3D Vision (3DV), 2025. 3"], ["Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls. arXiv preprint arXiv:2407.21136, 2024. 3", "Mingshuang Luo, Ruibing Hou, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan. M3gpt: An ad- vanced multimodal, multitask framework for motion com- prehension and generation. In NeurIPs, 2024. 3", "Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In ECCV, 2024. 3", "Zixiang Zhou and Baoyuan Wang. Ude: A unified driving engine for human motion generation. In CVPR, 2023. 3"], ["Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In ECCV, 2024. 3"], ["Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00a8ul Varol. TEACH: Temporal action composition for 3D humans. In International Conference on 3D Vision (3DV), 2022. 3", "Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, G\u00a8ul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In CVPR Workshop on Human Motion Generation, 2024. 3", "Yijun Qian, Jack Urbanek, Alexander G. Hauptmann, and Jungdam Won. Breaking the limits of text-conditioned 3D In ICCV, motion synthesis with elaborative descriptions. 2023. 3", "Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In CVPR, 2023. 3"], ["Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In ICCV, 2019. 3, 6, 12"], ["Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: A large- scale 3d expressive whole-body human motion dataset. In NeurIPS, 2023. 2, 3, 6, 7, 12, 13"], ["Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Mo- tionbank: A large-scale video motion benchmark with disen- tangled rule-based annotations, 2024. 3"], ["Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12", "Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras. In CVPR, 2022. 3, 7"], ["Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018. 3", "Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In CVPR, 2021. 3, 7, 12, 13", "Istv\u00b4an S\u00b4ar\u00b4andi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. 2024. 3"], ["Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Be- yond static features for temporally consistent 3d human pose and shape from a video. In CVPR, 2021. 3", "Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Re- In constructing and tracking humans with transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14783\u201314794, 2023. 3, 7", "Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. VIBE: Video inference for human body pose and shape estimation. In CVPR, 2020. 3, 7, 13"], ["Jiye Lee and Hanbyul Joo. Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head- mounted camera. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2024. 3", "Jose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano. Sparseposer: Real-time full- body motion reconstruction from sparse data. ACM Trans- actions on Graphics, 2023. 3", "Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. ACM Transactions on Graphics, 2021. 3"], ["Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. PACE: Human and motion estimation from in-the- wild videos. In 3DV, 2024. 2, 3, 7", "Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, and Umar Iqbal. Coin: Control-inpainting diffusion prior for human and camera In ECCV, pages 426\u2013446. Springer, motion estimation. 2024. 2, 3, 7", "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12", "Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in- the-wild videos. In European Conference on Computer Vi- sion, pages 467\u2013487. Springer, 2024. 3, 6, 7, 12", "Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In CVPR, 2023. 2, 3, 7", "Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras. In CVPR, 2022. 3, 7"], ["Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia 2024 Conference Papers, pages 1\u201311, 2024. 3, 7, 12", "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12", "Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in- the-wild videos. In European Conference on Computer Vi- sion, pages 467\u2013487. Springer, 2024. 3, 6, 7, 12", "Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexan- der Winkler, Petr Kadlecek, Siyu Tang, and Federica Bogo. Rohm: Robust human motion reconstruction via diffusion. In CVPR, 2024. 3 11"]], "positions": [116, 177, 187, 196, 204, 225, 292, 451, 876, 1113, 1195, 1211, 1277, 1587, 1596, 1625, 1766, 2128]}}
{"paper_id": "InsightV", "input_text": "2.1. Vision-Language Reasoning\n\nRecent advancements in MLLMs \nhave equipped these models with robust capabilities across\ndiverse domains, including visual understanding ,\nmathematics , college-level questions , and scien-\ntific inquiries. In visual understanding, most research  emphasizes fine-grained detail analysis\nand localization, training models to perform visual reason-\ning with tailored datasets to enhance interpretive capabili-\nties. For mathematics and expert-level reasoning, existing\nmethods  predominantly derive from Chain-of-\nThought  approaches, training MLLMs to generate step-\nby-step reasoning across various subjects. However, these\napproaches often focus primarily on improving dataset qual-\nity through Chain-of-Thought, overlooking the importance\nof structured reasoning paths and extended reasoning chains\nin advancing model reasoning capabilities. Additionally,\nsignificant challenges arise when relying on a single model\nto manage the entire reasoning process for complex tasks,\nunderscoring the need for a multi-agent system to decom-\npose and enhance this process. In this work, we tackle these\nchallenges by introducing a scalable reasoning data gener-\nation pipeline and implementing a multi-agent system for\nreasoning and summarization decomposition, enhancing the\noverall reasoning capabilities of existing MLLMs.\n\n2.2. Vision-Language Alignment\n\nTo align the model more closely with human preferences,\nseveral alignment techniques are employed for MLLMs. A\nwidely used approach is Reinforcement Learning from Hu-\nman Feedback  (RLHF), which iteratively refines the\nmodel\u2019s responses based on human feedback, enhancing\nboth response quality and interpretability. To further improve\nMLLM capabilities, Direct Preference Optimization \n(DPO) is introduced to simplify the alignment process. By di-\nrectly training on human preference data, DPO optimizes the\nmodel\u2019s outputs to better match human-selected responses.\nHowever, traditional DPO is primarily focused on offline\nscenarios, and as the model evolves, the effectiveness of\nthis approach may significantly diminish. To address this,\nIterative DPO  has been proposed, which optimizes pref-\n\n2\n\n\ference pairs through DPO at each iteration. It then generates\nnew preference pairs for the next iteration using the updated\nmodel and evaluates them with a reward model. In this pa-\nper, we use iterative DPO to achieve stronger alignment and\nenhance the model\u2019s reasoning capabilities.\n\n", "expected_text": "2.1. Vision-Language Reasoning\n\nRecent advancements in MLLMs [CITATION_0]\nhave equipped these models with robust capabilities across\ndiverse domains, including visual understanding [CITATION_1],\nmathematics [CITATION_2], college-level questions [CITATION_3], and scien-\ntific inquiries. In visual understanding, most research [CITATION_4] emphasizes fine-grained detail analysis\nand localization, training models to perform visual reason-\ning with tailored datasets to enhance interpretive capabili-\nties. For mathematics and expert-level reasoning, existing\nmethods [CITATION_5] predominantly derive from Chain-of-\nThought [CITATION_6] approaches, training MLLMs to generate step-\nby-step reasoning across various subjects. However, these\napproaches often focus primarily on improving dataset qual-\nity through Chain-of-Thought, overlooking the importance\nof structured reasoning paths and extended reasoning chains\nin advancing model reasoning capabilities. Additionally,\nsignificant challenges arise when relying on a single model\nto manage the entire reasoning process for complex tasks,\nunderscoring the need for a multi-agent system to decom-\npose and enhance this process. In this work, we tackle these\nchallenges by introducing a scalable reasoning data gener-\nation pipeline and implementing a multi-agent system for\nreasoning and summarization decomposition, enhancing the\noverall reasoning capabilities of existing MLLMs.\n\n2.2. Vision-Language Alignment\n\nTo align the model more closely with human preferences,\nseveral alignment techniques are employed for MLLMs. A\nwidely used approach is Reinforcement Learning from Hu-\nman Feedback [CITATION_7] (RLHF), which iteratively refines the\nmodel\u2019s responses based on human feedback, enhancing\nboth response quality and interpretability. To further improve\nMLLM capabilities, Direct Preference Optimization [CITATION_8]\n(DPO) is introduced to simplify the alignment process. By di-\nrectly training on human preference data, DPO optimizes the\nmodel\u2019s outputs to better match human-selected responses.\nHowever, traditional DPO is primarily focused on offline\nscenarios, and as the model evolves, the effectiveness of\nthis approach may significantly diminish. To address this,\nIterative DPO [CITATION_9] has been proposed, which optimizes pref-\n\n2\n\n\ference pairs through DPO at each iteration. It then generates\nnew preference pairs for the next iteration using the updated\nmodel and evaluates them with a reward model. In this pa-\nper, we use iterative DPO to achieve stronger alignment and\nenhance the model\u2019s reasoning capabilities.\n\n", "citation_info": {"content": [["Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understand- ing, localization, text reading, and beyond. 2023. 2", "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun- yuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2", "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6", "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 26296\u201326306, 2024. 1, 5", "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6", "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 1, 2", "Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial- temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 2", "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 1, 2, 6", "QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6"], ["Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6", "QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6"], ["Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: A foundational and multimodal mathe- matical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7126\u20137133, 2023. 2"], ["Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks. In CVPR, pages 24185\u201324198, 2024. 1, 2"], ["Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Mon- key: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 2", "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6", "Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Ji- wen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 2", "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5, 6", "Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. 2"], ["Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric prob- lem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 2", "Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual in- struction tuning. arXiv preprint arXiv:2407.08739, 2024. 2, 3", "Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yim- ing Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. 2, 5"], ["Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u2013 24837, 2022. 1, 2"], ["Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2"], ["Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct prefer- ence optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, 5"], ["Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak lan- guage models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 2"]], "positions": [61, 169, 183, 209, 278, 507, 552, 1562, 1767, 2136]}}
{"paper_id": "SceneCentric", "input_text": "Approaches for unsupervised segmentation tasks have been\nsignificantly influenced by the literature on self-supervised\nlearning (SSL) and low-level vision tasks (e.g., optical flow\nestimation), which we review first.\nSelf-supervised representation learning focuses on learn-\ning generic feature extractors from unlabeled data, aiming\nfor expressive features that facilitate a broad range of down-\nstream tasks . To that end, various self-supervised pre-\ntext tasks have been proposed . The development\nof Vision Transformers (ViTs)  shaped current pretext\ntasks while allowing for data-scalable training .\nCurrent approaches typically train ViTs on contrastive , negative-free , clustering-\nbased , or masked modeling  pretext\ntasks. Recent state-of-the-art models (e.g., DINO ) of-\nfer semantically rich and dense features suitable for unsu-\npervised scene understanding .\n\nUnsupervised optical flow is concerned with learning op-\ntical flow estimation without the need for ground-truth data.\nWhile early deep networks relied on synthetic ground-truth\nflow for supervision , the domain gap to real\nvideos, among other factors, has prompted the development\nof unsupervised deep optical flow pipelines . Current unsupervised optical flow methods (e.g.,\nSMURF ) offer accurate flow estimates, fast inference,\nand generalization to various real-world domains.\n\nUnsupervised instance segmentation aims to discover and\nsegment object instances in images . Recent work\n bootstraps class-agnostic instance segmen-\ntation networks using pseudo labels extracted from SSL fea-\ntures on object-centric data. TokenCut  applies normal-\nized cuts [N-Cut, 61] to DINO features, providing a fore-\nground pseudo mask. CutLER  proposes MaskCut by\niteratively applying N-Cuts, retrieving up to three pseudo\nmasks per image. A second stream of works uses motion\ncues to obtain an unsupervised signal for object discov-\nery . SF2SE3  clusters scene\nflow from consecutive stereo frames into independent rigid\nobject motions in SE (3) space, improving object segmen-\ntation and motion accuracy. MOD-UV  uses motion\nsegmentation for pseudo labeling and multi-stage training.\n\nUnsupervised semantic segmentation is approached by\nearly deep learning methods via representation learning\n. STEGO  leverages the self-supervised\nDINO features as an inductive prior and distills the fea-\ntures into a lower-dimensional space before unsupervised\nprobing. Later,  proposed improvements to the\nfeature distillation or probing . DepthG  extends\nSTEGO by spatially correlating the feature maps with depth\nmaps and furthest point sampling in the contrastive loss.\nDiffSeg  utilizes Stable Diffusion  and iterative at-\ntention merging for unsupervised semantic segmentation.\n\nUnsupervised panoptic segmentation is a nascent re-\nsearch avenue following recent advancements in unsuper-\nvised semantic and instance segmentation. To the best of\nour knowledge, U2Seg  is the only method to date to approach unsupervised panoptic segmentation. U2Seg\nleverages STEGO  and CutLER  to create panoptic\npseudo labels for training a panoptic network. However, its\ndependence on CutLER\u2019s MaskCut approach significantly\nIn contrast, we\nlimits its accuracy on scene-centric data.\npresent the first unsupervised panoptic approach that learns\ndirectly from scene-centric data, addressing key limitations\nof U2Seg and MaskCut.\n\n", "expected_text": "Approaches for unsupervised segmentation tasks have been\nsignificantly influenced by the literature on self-supervised\nlearning (SSL) and low-level vision tasks (e.g., optical flow\nestimation), which we review first.\nSelf-supervised representation learning focuses on learn-\ning generic feature extractors from unlabeled data, aiming\nfor expressive features that facilitate a broad range of down-\nstream tasks [CITATION_0]. To that end, various self-supervised pre-\ntext tasks have been proposed [CITATION_1]. The development\nof Vision Transformers (ViTs) [CITATION_2] shaped current pretext\ntasks while allowing for data-scalable training [CITATION_3].\nCurrent approaches typically train ViTs on contrastive [CITATION_4], negative-free [CITATION_5], clustering-\nbased [CITATION_6], or masked modeling [CITATION_7] pretext\ntasks. Recent state-of-the-art models (e.g., DINO [CITATION_8]) of-\nfer semantically rich and dense features suitable for unsu-\npervised scene understanding [CITATION_9].\n\nUnsupervised optical flow is concerned with learning op-\ntical flow estimation without the need for ground-truth data.\nWhile early deep networks relied on synthetic ground-truth\nflow for supervision [CITATION_10], the domain gap to real\nvideos, among other factors, has prompted the development\nof unsupervised deep optical flow pipelines [CITATION_11]. Current unsupervised optical flow methods (e.g.,\nSMURF [CITATION_12]) offer accurate flow estimates, fast inference,\nand generalization to various real-world domains.\n\nUnsupervised instance segmentation aims to discover and\nsegment object instances in images [CITATION_13]. Recent work\n[CITATION_14] bootstraps class-agnostic instance segmen-\ntation networks using pseudo labels extracted from SSL fea-\ntures on object-centric data. TokenCut [CITATION_15] applies normal-\nized cuts [N-Cut, 61] to DINO features, providing a fore-\nground pseudo mask. CutLER [CITATION_16] proposes MaskCut by\niteratively applying N-Cuts, retrieving up to three pseudo\nmasks per image. A second stream of works uses motion\ncues to obtain an unsupervised signal for object discov-\nery [CITATION_17]. SF2SE3 [CITATION_18] clusters scene\nflow from consecutive stereo frames into independent rigid\nobject motions in SE (3) space, improving object segmen-\ntation and motion accuracy. MOD-UV [CITATION_19] uses motion\nsegmentation for pseudo labeling and multi-stage training.\n\nUnsupervised semantic segmentation is approached by\nearly deep learning methods via representation learning\n[CITATION_20]. STEGO [CITATION_21] leverages the self-supervised\nDINO features as an inductive prior and distills the fea-\ntures into a lower-dimensional space before unsupervised\nprobing. Later, [CITATION_22] proposed improvements to the\nfeature distillation or probing [CITATION_23]. DepthG [CITATION_24] extends\nSTEGO by spatially correlating the feature maps with depth\nmaps and furthest point sampling in the contrastive loss.\nDiffSeg [CITATION_25] utilizes Stable Diffusion [CITATION_26] and iterative at-\ntention merging for unsupervised semantic segmentation.\n\nUnsupervised panoptic segmentation is a nascent re-\nsearch avenue following recent advancements in unsuper-\nvised semantic and instance segmentation. To the best of\nour knowledge, U2Seg [CITATION_27] is the only method to date to approach unsupervised panoptic segmentation. U2Seg\nleverages STEGO [CITATION_28] and CutLER [CITATION_29] to create panoptic\npseudo labels for training a panoptic network. However, its\ndependence on CutLER\u2019s MaskCut approach significantly\nIn contrast, we\nlimits its accuracy on scene-centric data.\npresent the first unsupervised panoptic approach that learns\ndirectly from scene-centric data, addressing key limitations\nof U2Seg and MaskCut.\n\n", "citation_info": {"content": [["Linus Ericsson, Henry Gouk, Chen Change Loy, and Timo- thy M. Hospedales. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Trans. Signal Process., 39(3):42\u201362, 2022. 2"], ["Saleh Albelwi. Survey on self-supervised learning: Auxil- iary pretext tasks and contrastive learning methods in imag- ing. Entropy, 24(4):551, 2022. 2", "Linus Ericsson, Henry Gouk, Chen Change Loy, and Timo- thy M. Hospedales. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Trans. Signal Process., 39(3):42\u201362, 2022. 2"], ["Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16\u00d716 words: Transformers for image recognition at scale. In ICLR, 2021. 2"], ["Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660, 2021. 2, 3, 4, ii", "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009, 2022. 2"], ["Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS, pages 15509\u201315519, 2019. 2", "Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv:2003.04297 [cs.CV], 2020. 2, 6, ii", "Xinlei Chen, Saining Xie, and Kaiming He. An empiri- cal study of training self-supervised vision transformers. In CVPR, pages 9640\u20139649, 2021. 2", "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, pages 9729\u20139738, 2020. 2, 5"], ["Adrien Bardes, Jean Ponce, and Yann LeCun. VICRegL: Self-supervised learning of local visual features. In NeurIPS, pages 8799\u20138810, 2022. 2", "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660, 2021. 2, 3, 4, ii", "Xinlei Chen and Kaiming He. Exploring simple Siamese In CVPR, pages 15750\u201315758, representation learning. 2021. 2", "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, pages 21271\u201321284, 2020. 2", "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, 10 et al. DINOv2: Learning robust visual features without su- pervision. Trans. Mach. Learn. Res., 2024. 2 Unsupervised zero-shot segmentation using stable diffusion. In CVPR, pages 3554\u20133563, 2024. 3, 7"], ["Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and rep- resentation learning. In ICLR, 2020. 2", "Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, pages 132\u2013149, 2018. 2", "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learn- ing of visual features by contrasting cluster assignments. In NeurIPS, pages 9912\u20139924, 2020. 2"], ["Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese In NeurIPS, pages 40676\u201340693, masked autoencoders. 2023. 2", "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009, 2022. 2", "Duy Kien Nguyen, Yanghao Li, Vaibhav Aggarwal, Mar- tin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, and Xinlei Chen. R-MAE: Regions meet masked autoencoders. In ICLR, 2024. 2"], ["Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660, 2021. 2, 3, 4, ii"], ["Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. 2, 3, 6, 7, i, ii, iii", "Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 3124\u20133134, 2023. 2, 3, 5, 6, 7, iv, v, vi"], ["Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip H\u00e4usser, Caner Haz\u0131rba\u00b8s, Vladimir Golkov, Patrick v. d. Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learn- ing optical flow with convolutional networks. In ICCV, pages 2758\u20132766, 2015. 3", "Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, In CVPR, pages optical flow, and scene flow estimation. 4040\u20134048, 2016. 3", "Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In CVPR, pages 8934\u20138943, 2018. 3", "Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419, 2022. 3, iii, iv"], ["Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What mat- ters in unsupervised optical flow. In ECCV, pages 557\u2013572, 2020. 3", "Gal Lifshitz and Dan Raviv. Cost function unrolling in un- IEEE Trans. Pattern Anal. Mach. supervised optical flow. Intell., 46(2):869\u2013880, 2024. 3", "R\u00e9mi Marsal, Florian Chabot, Ang\u00e9lique Loesch, and Hichem Sahbi. BrightFlow: Brightness-change-aware un- supervised learning of optical flow. In WACV, pages 2061\u2013 2070, 2023. 3", "Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un- supervised learning of optical flow with a bidirectional cen- sus loss. In AAAI, pages 7251\u20137259, 2018. 3", "Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpa- nis. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. In ECCV Workshops, pages 3\u201310, 2016. 3"], ["Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia An- gelova, and Rico Jonschkowski. SMURF: Self-teaching multi-frame unsupervised RAFT with full-image warping. In CVPR, pages 3887\u20133896, 2021. 3, i, iii, iv"], ["Oriane Sim\u00e9oni, \u00c9loi Zablocki, Spyros Gidaris, Gilles Puy, and Patrick P\u00e9rez. Unsupervised object localization in the era of self-supervised ViTs: A survey. Int. J. Comput. Vis., 133(2):781\u2013808, 2025. 3"], ["Oriane Sim\u00e9oni, Gilles Puy, Huy V. Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick P\u00e9rez, Renaud Mar- let, and Jean Ponce. Localizing objects with self-supervised transformers and no labels. In BMVC, 2021. 2, 3", "Wouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with transformers for unsupervised semantic segmentation. arXiv:2206.06363 [cs.CV], 2022. 3", "Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jos\u00e9 M. \u00c1lvarez. FreeSOLO: Learning to segment objects without annota- tions. In CVPR, pages 14156\u201314166, 2022. 2, 3", "Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 3124\u20133134, 2023. 2, 3, 5, 6, 7, iv, v, vi", "XuDong Wang, Jingfeng Yang, and Trevor Darrell. Segment In NeurIPS, pages 138731\u2013 anything without supervision. 138755, 2024. 3, ix"], ["Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James L. Crowley, and Dominique Vaufrey- daz. TokenCut: Segmenting objects in images and videos with self-supervised transformer and normalized cut. IEEE Trans. Pattern Anal. Mach. Intell., 45(12):15790\u201315801, 2023. 2, 3"], ["Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 3124\u20133134, 2023. 2, 3, 5, 6, 7, iv, v, vi"], ["Subhabrata Choudhury, Laurynas Karazija, Iro Laina, An- drea Vedaldi, and Christian Rupprecht. Guess What Moves: Unsupervised video and image segmentation by anticipating motion. In BMVC, 2022. 3", "Laurynas Karazija, Subhabrata Choudhury, Iro Laina, Chris- tian Rupprecht, and Andrea Vedaldi. Unsupervised multi- object segmentation by predicting probable motion patterns. In NeurIPS, pages 2128\u20132141, 2022. 3", "Runtao Liu, Zhirong Wu, Stella Yu, and Stephen Lin. The emergence of objectness: Learning zero-shot segmentation from videos. In NeurIPS, pages 13137\u201313152, 2021. 3", "Sadra Safadoust and Fatma G\u00fcney. Multi-object discovery by low-dimensional object motion. In ICCV, pages 734\u2013744, 2023. 3", "Yihong Sun and Bharath Hariharan. MOD-UV: Learning In ECCV, mobile object detectors from unlabeled videos. pages 289\u2013307, 2024. 3, 6, 7", "Yanchao Yang, Brian Lai, and Stefano Soatto. DyStaB: Unsupervised object segmentation via dynamic-static boot- strapping. In CVPR, pages 2826\u20132836, 2021. 3"], ["Leonhard Sommer, Philipp Schr\u00f6ppel, and Thomas Brox. SF2SE3: Clustering scene flow into SE(3)-motions via pro- posal and selection. In GCPR, pages 215\u2013229, 2022. 3, 4, i"], ["Yihong Sun and Bharath Hariharan. MOD-UV: Learning In ECCV, mobile object detectors from unlabeled videos. pages 289\u2013307, 2024. 3, 6, 7"], ["Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. PiCIE: Unsupervised semantic segmentation us- In CVPR, ing invariance and equivariance in clustering. pages 16794\u201316804, 2021. 3, 6, 7, ii, iii", "Robert Harb and Patrick Kn\u00f6belreiter. InfoSeg: Unsuper- vised semantic image segmentation with mutual information maximization. In GCPR, pages 18\u201332, 2021. 3", "Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, pages 9865\u20139874, 2019. 3, 6, ii, iii"], ["Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. 2, 3, 6, 7, i, ii, iii"], ["Chanyoung Kim, Woojung Han, Dayun Ju, and Seong Jae Hwang. EAGLE: Eigen aggregation learning for object- In CVPR, centric unsupervised semantic segmentation. pages 3523\u20133533, 2024. 2, 3, 7", "Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised semantic segmentation. In CVPR, pages 19540\u201319549, 2023. 2, 3, 7", "Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo Ropinski. Unsupervised semantic segmentation through In CVPR, depth-guided feature correlation and sampling. pages 3637\u20133646, 2024. 2, 3, 4, 6, 7, i, ii, iii, v, vi"], ["Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, and Stefan Roth. Boosting unsupervised semantic segmentation with principal mask proposals. Trans. Mach. Learn. Res., 2024. 3, 7"], ["Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo Ropinski. Unsupervised semantic segmentation through In CVPR, depth-guided feature correlation and sampling. pages 3637\u20133646, 2024. 2, 3, 4, 6, 7, i, ii, iii, v, vi"], ["Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonz\u00e1lez-Franco. Diffuse, attend, and segment:"], ["Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image syn- thesis with latent diffusion models. In CVPR, pages 10684\u2013 10695, 2022. 3"], ["Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, and Trevor Darrell. Unsupervised universal image segmentation. In CVPR, pages 22744\u201322754, 2024. 1, 2, 3, 6, 7, 8, i, ii, iv, v, vi, vii, viii, ix"], ["Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. 2, 3, 6, 7, i, ii, iii"], ["Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 3124\u20133134, 2023. 2, 3, 5, 6, 7, iv, v, vi"]], "positions": [410, 484, 532, 604, 661, 677, 697, 718, 777, 872, 1074, 1201, 1258, 1449, 1463, 1606, 1708, 1903, 1912, 2079, 2260, 2268, 2430, 2492, 2501, 2635, 2662, 2924, 3022, 3034]}}
{"paper_id": "dust3r", "input_text": "For the sake of space, we summarize here the most related\nworks in 3D vision, and refer the reader to the appendix in\nSec. C for a more comprehensive review.\nStructure-from-Motion (SfM)  aims\nat reconstructing sparse 3D maps while jointly determin-\ning camera parameters from a set of images. The tradi-\ntional pipeline starts from pixel correspondences obtained\nfrom keypoint matching  between multiple\nimages to determine geometric relationships, followed by\nbundle adjustment to optimize 3D coordinates and camera\nparameters jointly. Recently, the SfM pipeline has under-\ngone substantial enhancements, particularly with the incor-\nporation of learning-based techniques into its subprocesses.\nThese improvements encompass advanced feature descrip-\ntion , more accurate image match-\ning , featuremetric refine-\nment , and neural bundle adjustment . Despite\nthese advancements, the sequential structure of the SfM\npipeline persists, making it vulnerable to noise and errors in\neach individual component.\nMultiView Stereo (MVS) is the task of densely recon-\nstructing visible surfaces, which is achieved via triangu-\nlation between multiple viewpoints.\nIn the classical for-\nmulation of MVS, all camera parameters are supposed\nto be provided as inputs. The fully handcrafted , the more recent scene optimization\nbased , or learning\nbased  approaches all de-\npend on camera parameter estimates obtained via com-\nplex calibration procedures, either during the data acqui-\nsition  or using Structure-from-Motion ap-\nproaches  for in-the-wild reconstructions. Yet, in\nreal-life scenarios, the inaccuracy of pre-estimated camera\nparameters can be detrimental for these algorithms to work\nproperly. In this work, we propose instead to directly pre-\ndict the geometry of visible surfaces without any explicit\nknowledge of the camera parameters.\nDirect RGB-to-3D. Recently, some approaches aiming at\ndirectly predicting 3D geometry from a single RGB image\nhave been proposed. Since the problem is by nature ill-posed\nwithout introducing additional assumptions, these methods\nleverage neural networks that learn strong 3D priors from\nlarge datasets to solve ambiguities. These methods can be\nclassified into two groups. The first group leverages class-\nlevel object priors. For instance, Pavllo et al.  pro-\npose to learn a model that can fully recover shape, pose, and\nappearance from a single image, given a large collection of\n2D images. While this type of approach is powerful, it does\nnot allow to infer shape on objects from unseen categories. A\nsecond group of work, closest to our method, focuses instead\non general scenes. These methods systematically build on\nor re-use existing monocular depth estimation (MDE) net-\n\nworks . Depth maps indeed encode a form\nof 3D information and, combined with camera intrinsics,\ncan straightforwardly yield pixel-aligned 3D point-clouds.\nSynSin , for example, performs new viewpoint syn-\nthesis from a single image by rendering feature-augmented\ndepthmaps knowing all camera parameters. Without cam-\nera intrinsics, one solution is to infer them by exploiting\ntemporal consistency in video frames, either by enforcing a\nglobal alignment et al.  or by leveraging differentiable\nrendering with a photometric reconstruction loss .\nAnother way is to explicitly learn to predict camera intrin-\nsics, which enables to perform metric 3D reconstruction\nfrom a single image when combined with MDE .\nAll these methods are, however, intrinsically limited by the\nquality of depth estimates, which arguably is ill-posed for\nmonocular settings.\n\nIn contrast, our network processes two viewpoints simul-\ntaneously in order to output depthmaps, or rather, pointmaps.\nIn theory, at least, this makes triangulation between rays\nfrom different viewpoint possible. Multi-view networks for\n3D reconstruction have been proposed in the past. They are\nessentially based on the idea of building a differentiable SfM\npipeline, replicating the traditional pipeline but training it\nend-to-end . For that, however, ground-truth\ncamera intrinsics are required as input, and the output is gen-\nerally a depthmap and a relative camera pose . In\ncontrast, our network has a generic architecture and outputs\npointmaps, i.e. dense 2D field of 3D points, which handle\ncamera poses implicitly and makes the regression problem\nmuch better posed.\nPointmaps. Using a collection of pointmaps as shape rep-\nresentation is quite counter-intuitive for MVS, but its us-\nage is widespread for Visual Localization tasks, either in\nscene-dependent optimization approaches  or scene-\nagnostic inference methods . Similarly, view-\nwise modeling is a common theme in monocular 3D recon-\nstruction works  and in view synthesis\nworks . The idea being to store the canonical 3D shape\nin multiple canonical views to work in image space. These\napproaches usually leverage explicit perspective camera ge-\nometry, via rendering of the canonical representation.\n\n", "expected_text": "For the sake of space, we summarize here the most related\nworks in 3D vision, and refer the reader to the appendix in\nSec. C for a more comprehensive review.\nStructure-from-Motion (SfM) [CITATION_0] aims\nat reconstructing sparse 3D maps while jointly determin-\ning camera parameters from a set of images. The tradi-\ntional pipeline starts from pixel correspondences obtained\nfrom keypoint matching [CITATION_1] between multiple\nimages to determine geometric relationships, followed by\nbundle adjustment to optimize 3D coordinates and camera\nparameters jointly. Recently, the SfM pipeline has under-\ngone substantial enhancements, particularly with the incor-\nporation of learning-based techniques into its subprocesses.\nThese improvements encompass advanced feature descrip-\ntion [CITATION_2], more accurate image match-\ning [CITATION_3], featuremetric refine-\nment [CITATION_4], and neural bundle adjustment [CITATION_5]. Despite\nthese advancements, the sequential structure of the SfM\npipeline persists, making it vulnerable to noise and errors in\neach individual component.\nMultiView Stereo (MVS) is the task of densely recon-\nstructing visible surfaces, which is achieved via triangu-\nlation between multiple viewpoints.\nIn the classical for-\nmulation of MVS, all camera parameters are supposed\nto be provided as inputs. The fully handcrafted [CITATION_6], the more recent scene optimization\nbased [CITATION_7], or learning\nbased [CITATION_8] approaches all de-\npend on camera parameter estimates obtained via com-\nplex calibration procedures, either during the data acqui-\nsition [CITATION_9] or using Structure-from-Motion ap-\nproaches [CITATION_10] for in-the-wild reconstructions. Yet, in\nreal-life scenarios, the inaccuracy of pre-estimated camera\nparameters can be detrimental for these algorithms to work\nproperly. In this work, we propose instead to directly pre-\ndict the geometry of visible surfaces without any explicit\nknowledge of the camera parameters.\nDirect RGB-to-3D. Recently, some approaches aiming at\ndirectly predicting 3D geometry from a single RGB image\nhave been proposed. Since the problem is by nature ill-posed\nwithout introducing additional assumptions, these methods\nleverage neural networks that learn strong 3D priors from\nlarge datasets to solve ambiguities. These methods can be\nclassified into two groups. The first group leverages class-\nlevel object priors. For instance, Pavllo et al. [CITATION_11] pro-\npose to learn a model that can fully recover shape, pose, and\nappearance from a single image, given a large collection of\n2D images. While this type of approach is powerful, it does\nnot allow to infer shape on objects from unseen categories. A\nsecond group of work, closest to our method, focuses instead\non general scenes. These methods systematically build on\nor re-use existing monocular depth estimation (MDE) net-\n\nworks [CITATION_12]. Depth maps indeed encode a form\nof 3D information and, combined with camera intrinsics,\ncan straightforwardly yield pixel-aligned 3D point-clouds.\nSynSin [CITATION_13], for example, performs new viewpoint syn-\nthesis from a single image by rendering feature-augmented\ndepthmaps knowing all camera parameters. Without cam-\nera intrinsics, one solution is to infer them by exploiting\ntemporal consistency in video frames, either by enforcing a\nglobal alignment et al. [CITATION_14] or by leveraging differentiable\nrendering with a photometric reconstruction loss [CITATION_15].\nAnother way is to explicitly learn to predict camera intrin-\nsics, which enables to perform metric 3D reconstruction\nfrom a single image when combined with MDE [CITATION_16].\nAll these methods are, however, intrinsically limited by the\nquality of depth estimates, which arguably is ill-posed for\nmonocular settings.\n\nIn contrast, our network processes two viewpoints simul-\ntaneously in order to output depthmaps, or rather, pointmaps.\nIn theory, at least, this makes triangulation between rays\nfrom different viewpoint possible. Multi-view networks for\n3D reconstruction have been proposed in the past. They are\nessentially based on the idea of building a differentiable SfM\npipeline, replicating the traditional pipeline but training it\nend-to-end [CITATION_17]. For that, however, ground-truth\ncamera intrinsics are required as input, and the output is gen-\nerally a depthmap and a relative camera pose [CITATION_18]. In\ncontrast, our network has a generic architecture and outputs\npointmaps, i.e. dense 2D field of 3D points, which handle\ncamera poses implicitly and makes the regression problem\nmuch better posed.\nPointmaps. Using a collection of pointmaps as shape rep-\nresentation is quite counter-intuitive for MVS, but its us-\nage is widespread for Visual Localization tasks, either in\nscene-dependent optimization approaches [CITATION_19] or scene-\nagnostic inference methods [CITATION_20]. Similarly, view-\nwise modeling is a common theme in monocular 3D recon-\nstruction works [CITATION_21] and in view synthesis\nworks [CITATION_22]. The idea being to store the canonical 3D shape\nin multiple canonical views to work in image space. These\napproaches usually leverage explicit perspective camera ge-\nometry, via rendering of the canonical representation.\n\n", "citation_info": {"content": [["David Crandall, Andrew Owens, Noah Snavely, and Daniel Huttenlocher. SfM with MRFs: Discrete-continuous opti-mization for large-scale structure from motion. PAMI, 2013. 2, 3", "Hainan Cui, Xiang Gao, Shuhan Shen, and Zhanyi Hu. Hsfm: Hybrid structure-from-motion. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, 2017. 3", "Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2004. 3, 5", "Nianjuan Jiang, Zhaopeng Cui, and Ping Tan. A global linear method for camera pose registration. In ICCV, 2013. 3", "Johannes Lutz Sch\u00a8onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Com- puter Vision and Pattern Recognition (CVPR), 2016. 2, 3, 8, 9"], ["Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krys- tian Mikolajczyk. Key. net: Keypoint detection by hand- crafted and learned cnn filters. In ICCV, pages 5836\u20135844, 2019. 3", "Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: In ECCV, pages 404\u2013417. Speeded up robust features. Springer, 2006. 3", "Chris Harris, Mike Stephens, et al. A combined corner and edge detector. In Alvey vision conference, volume 15, pages 10\u20135244. Citeseer, 1988. 3", "David G. Lowe. Distinctive image features from scale- invariant keypoints. IJCV, 2004. 2, 3", "Edward Rosten and Tom Drummond. Machine learning for high-speed corner detection. In ECCV. Springer, 2006. 3"], ["Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. In CVPR Workshops, pages 224\u2013236, 2018. 2, 3, 7, 8", "Mihai Dusmanu, Ignacio Rocco, Tom\u00b4as Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable CNN for joint description and detection of local features. In CVPR, pages 8092\u20138101, 2019. 2, 3", "J\u00b4er\u02c6ome Revaud, C\u00b4esar Roberto de Souza, Martin Humen- berger, and Philippe Weinzaepfel. R2D2: reliable and repeat- able detector and descriptor. In Neurips, pages 12405\u201312415, 2019. 2, 3", "Micha\u0142 Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy gradient. Advances in Neural Information Processing Systems, 33:14254\u201314265, 2020. 3", "Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. Lift: Learned invariant feature transform. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pages 467\u2013483. Springer, 2016. 3"], ["Daniel Barath, Dmytro Mishkin, Luca Cavalli, Paul-Edouard Sarlin, Petr Hruby, and Marc Pollefeys. Affineglue: Joint matching and robust estimation, 2023. 2, 3", "Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin Zhen, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Aspanformer: Detector-free image matching with adaptive span transformer. ECCV, 2022. 3", "Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle- feys. Lightglue: Local feature matching at light speed. In ICCV, 2023. 2, 3", "R\u00b4emi Pautrat, Iago Su\u00b4arez, Yifan Yu, Marc Pollefeys, and Viktor Larsson. GlueStick: Robust image matching by stick- ing points and lines together. In ICCV, 2023. 3", "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature match- ing with graph neural networks. In CVPR, pages 4937\u20134946, 2020. 2, 3, 7, 8", "Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 2, 3", "Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. ICLR, 2022. Quadtree attention for vision transformers. 3", "Shuzhe Wang, Juho Kannala, Marc Pollefeys, and Daniel Barath. Guiding local feature matching with surface curva- ture. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV), pages 17981\u201317991, October 2023. 3"], ["Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-perfect structure-from-motion with featuremetric refinement. In ICCV, 2021. 2, 3, 7, 8"], ["Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si- mon Lucey. BARF: bundle-adjusting neural radiance fields. In ICCV, 2021. 3", "Yuxi Xiao, Nan Xue, Tianfu Wu, and Gui-Song Xia. Level- S2fM: Structure From Motion on Neural Level Set of Im- plicit Surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3"], ["Yasutaka Furukawa and Carlos Hern\u00b4andez. Multi-view stereo: A tutorial. Found. Trends Comput. Graph. Vis., 2015. 2, 3", "Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, June 2015. 3, 9", "Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for un- structured multi-view stereo. In ECCV, 2016. 2, 3, 7, 8, 9", "Yuesong Wang, Zhaojie Zeng, Tao Guan, Wei Yang, Zhuo Chen, Wenkai Liu, Luoyuan Xu, and Yawei Luo. Adaptive patch deformation for textureless-resilient multi-view stereo. In CVPR, 2023. 3", "Zhaojie Zeng. OpenMVS. https://github.com/ [Online; accessed 19- cdcseacave/openMVS, 2015. October-2023]. 3"], ["Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometry-consistent neural implicit sur- faces learning for multi-view reconstruction. In NeurIPS, 2022. 2, 3", "Xiaoxu Meng, Weikai Chen, and Bo Yang. Neat: Learning neural implicit surfaces with arbitrary topologies from multi- view images. In CVPR, 2023. 2, 3", "Michael Niemeyer, Lars M. Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In CVPR, 2020. 3", "Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, 2021. 2, 3", "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural im- plicit surfaces by volume rendering for multi-view recon- struction. In NeurIPS, 2021. 2, 3", "Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Hf- neus: Improved surface reconstruction using high-frequency details. In NeurIPS, 2022. 2, 3", "Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In ICCV, 2021. 2, 3, 14", "Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, and Yaron Lipman. Multiview neu- ral surface reconstruction by disentangling geometry and appearance. In NeurIPS, 2020. 2, 3"], ["Xiaodong Gu*, Zhiwen Fan*, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade Cost Volume for High- resolution Multi-view Stereo and Stereo Matching. CVPR, 2020. 2, 3, 9", "Vincent Leroy, Jean-S\u00b4ebastien Franco, and Edmond Boyer. Volume sweeping: Learning photoconsistency for multi- view shape reconstruction. IJCV, 2021. 3", "Zeyu Ma, Zachary Teed, and Jia Deng. Multiview stereo with cascaded epipolar raft. In ECCV, 2022. 3, 9", "Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi- view stereo: A unified representation. In CVPR, 2022. 3", "Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, 2018. 3, 9", "Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, Zhiguo Cao, and Xin Li. Constraining depth map geometry for multi- view stereo: A dual-depth approach with saddle-shaped depth cells. ICCV, 2023. 3", "Zhe Zhang, Rui Peng, Yuxi Hu, and Ronggang Wang. Ge- omvsnet: Learning multi-view stereo with geometry percep- tion. In CVPR, 2023. 3, 9"], ["Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. IJCV, 2016. 1, 3, 8", "Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly- annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 3, 8", "Thomas Sch\u00a8ops, Johannes L. Sch\u00a8onberger, Silvano Gal- liani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high- resolution images and multi-camera videos. In CVPR, 2017. 2, 3, 8, 14", "Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d in- door scenes. In Proceedings of the International Conference on Computer Vision (ICCV), 2023. 2, 3, 6, 16, 17"], ["Nianjuan Jiang, Zhaopeng Cui, and Ping Tan. A global linear method for camera pose registration. In ICCV, 2013. 3", "Johannes Lutz Sch\u00a8onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Com- puter Vision and Pattern Recognition (CVPR), 2016. 2, 3, 8, 9"], ["Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aur\u00b4elien Lucchi. Learning generative models of textured 3d meshes from real-world images. In ICCV, 2021. 3", "Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie- Francine Moens, and Aur\u00b4elien Lucchi. Convolutional gener- ation of textured 3d meshes. In NeurIPS, 2020.", "Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. Shape, pose, and appearance from a single image via bootstrapped radiance field inversion. In CVPR, 2023. 3"], ["Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua Shen, and Ian D. Reid. Auto-rectify net- work for unsupervised indoor depth estimation. IEEE Trans. Pattern Anal. Mach. Intell., 44(12):9802\u20139813, 2022. 3, 7, 8", "Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. In ICCV, 2021. 3, 6, 7, 8, 17", "Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si- mon Chen, Yifan Liu, and Chunhua Shen. Towards accurate reconstruction of 3d scene shape from a single monocular image, 2022. 3", "Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In CVPR, 2020. 3"], ["Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In CVPR, 2020. 3"], ["Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Cheng, and Feng Zhao. Frozenrecon: Pose-free 3d scene reconstruction with frozen depth models. In ICCV, 2023. 3"], ["Cl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In CVPR, 2017. 3", "Jaime Spencer, Chris Russell, Simon Hadfield, and Richard Bowden. Kick back & relax: Learning to reconstruct the world by watching slowtv. In ICCV, 2023. 3, 7, 8"], ["Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In ICCV, 2023. 3", "Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si- mon Chen, Yifan Liu, and Chunhua Shen. Towards accurate reconstruction of 3d scene shape from a single monocular image. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022. 3"], ["Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. In ICLR, 2020. 3, 9, 15", "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and motion network for learning monocular stereo. In CVPR, pages 5622\u20135631, 2017. 3, 9, 14, 15", "Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. DeepTAM: Deep tracking and mapping with convolutional neural networks. Int. J. Comput. Vis., 128(3):756\u2013769, 2020. 3, 14"], ["Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and motion network for learning monocular stereo. In CVPR, pages 5622\u20135631, 2017. 3, 9, 14, 15", "Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. DeepTAM: Deep tracking and mapping with convolutional neural networks. Int. J. Comput. Vis., 128(3):756\u2013769, 2020. 3, 14"], ["Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. DSAC - differentiable RANSAC for camera local- ization. In CVPR, 2017. 3", "Eric Brachmann and Carsten Rother. Learning less is more - 6d camera localization via 3d surface regression. In CVPR, 2018. 3", "Eric Brachmann and Carsten Rother. Visual camera re- localization from RGB and RGB-D images using DSAC. PAMI, 2022. 3, 7, 8"], ["Jerome Revaud, Yohann Cabon, Romain Br\u00b4egier, Jong- Min Lee, and Philippe Weinzaepfel. SACReg: Scene- agnostic coordinate regression for visual localization. CoRR, abs/2307.11702, 2023. 3", "Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, and Ping Tan. Learning camera localization via dense scene matching. In CVPR, 2021. 3", "Luwei Yang, Ziqian Bai, Chengzhou Tang, Honghua Li, Yasutaka Furukawa, and Ping Tan. Sanet: Scene agnostic network for camera localization. In ICCV, 2019. 3"], ["Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learning efficient point cloud generation for dense 3d object recon- struction. In AAAI, 2018. 3", "Daeyun Shin, Charless C. Fowlkes, and Derek Hoiem. Pix- els, voxels, and views: A study of shape representations for single view 3d object shape prediction. In CVPR, 2018. 3", "Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Multi-view 3d models from single images with a convolu- tional network. In ECCV, 2016. 3", "Jinglu Wang, Bo Sun, and Yan Lu. Mvpnet: Multi-view point regression networks for 3d object reconstruction from A single image. In AAAI, 2019. 3"], ["Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In CVPR, 2020. 3"]], "positions": [186, 386, 756, 789, 818, 849, 1275, 1318, 1338, 1477, 1522, 2293, 2725, 2881, 3180, 3262, 3424, 4001, 4144, 4560, 4598, 4688, 4717]}}
{"paper_id": "sugar", "input_text": "Image-based rendering (IBR) methods rely on a set of two-\ndimensional images of a scene to generate a representation\nof the scene and render novel views. The very first novel-\nview synthesis approaches were based on light fields ,\nand developed the concept of volume rendering for novel\nviews. Their work emphasized the importance of efficiently\ntraversing volumetric data to produce realistic images.\n\nVarious scene representations have been proposed since,\nsuch as triangle meshes, point clouds, voxel grids, multi-\nplane images, or neural implicit functions.\n\nTraditional mesh-based IBR methods. Structure-from-\nmotion (SfM)\n and subsequent multi-view stereo\n(MVS)  allow for 3D reconstruction of surfaces, lead-\ning to the development of several view synthesis algorithms\nrelying on triangle meshes as the primary 3D representa-\ntion of scenes. Such algorithms consider textured triangles\nor warp and blend captured images on the mesh surface to\ngenerate novel views .\n consider deep\nlearning-based mesh representations for better view synthe-\nsis, bridging the gap between traditional graphics and mod-\nern machine learning techniques. While these mesh-based\nmethods take advantage of existing graphics hardware and\nsoftware for efficient rendering, they struggle with the cap-\nture of accurate geometry and appearance in complex re-\ngions.\n\nVolumetric IBR methods. Volumetric methods use voxel\ngrids, multiplane images, or neural networks to represent\nscenes as continuous volumetric functions of density and\ncolor. Recently, Neural Radiance Fields (NeRF)  intro-\nduced a novel scene representation based on a continuous\nvolumetric function parameterized by a multilayer percep-\ntron (MLP). NeRF produces photorealistic renderings with\nfine details and view-dependent effects, achieved through\nvolumetric ray tracing. However, the original NeRF is com-\nputationally expensive and memory intensive.\n\nTo address these challenges, several works have im-\nproved NeRF\u2019s performance and scalability. These meth-\nods leverage discretized or sparse volumetric representa-\ntions like voxel grids and hash tables as ways to store\nlearnable features acting as positional encodings for 3D\npoints , hierarchical sampling strate-\ngies , or low-rank approximations . How-\never, they still rely on volumetric ray marching, which\nis incompatible with standard graphics hardware and soft-\nware designed for rendering polygonal surfaces. Recent\nworks have proposed modifying the NeRF\u2019s representation\nof geometry and emitted radiance to allow for better recon-\nstruction of specular materials  or relighting the scene\nthrough an explicit decomposition into material and lighting\nproperties .\n\nHybrid IBR methods. Some methods build on differen-\ntiable rendering to combine the advantages of mesh-based\nand volumetric methods, and allow for surface reconstruc-\ntion as well as better editability. They use a hybrid volume-\nsurface representation, which enables high-quality meshes\nsuitable for downstream graphics applications while effi-\nciently modeling view-dependent appearance.\nIn partic-\nular, some works optimize neural signed distance func-\ntions (SDF) by training neural radiance fields in which the\ndensity is derived as a differentiable transformation of the\nSDF . A triangle mesh can finally\nbe reconstructed from the SDF by applying the Marching\nCubes algorithm . However, most of these methods do\nnot target real-time rendering.\n\nAlternatively, other approaches \u201cbake\u201d the rendering ca-\npacity of an optimized NeRF or neural SDF into a much ef-\nficient structure relying on an underlying triangle mesh \nthat could benefit from the traditional triangle rasteriza-\ntion pipeline. In particular, the recent BakedSDF  re-\nconstructs high quality meshes by optimizing a full neural\nSDF model, baking it into a high-resolution triangle mesh\nthat combines mesh rendering for interpolating features and\ndeep learning to translate these features into images, and\nfinally optimizes a view-dependent appearance model.\n\nHowever, even though it achieves real-time rendering\nand produces impressive meshes of the surface of the scene,\nthis model demands training a full neural SDF with an ar-\nchitecture identical to Mip-NeRF360 , which necessi-\ntates 48 hours of training.\n\nSimilarly, the recent method NeRFMeshing  pro-\nposes to also bake any NeRF model into a mesh structure,\nachieving real-time rendering. However, the meshing per-\nformed in this method lowers the quality of the rendering\nand results in a PSNR much lower than our method. Ad-\nditionally, this method still requires training a full NeRF\nmodel beforehand, and needs approximately an hour of\ntraining on 8 V100 NVIDIA GPUs to allow for mesh train-\ning and extraction.\n\nOur method is much faster at retrieveing a 3D mesh from\n3D Gaussian Splatting, which is itself much faster than\nNeRFs. As our experiments show, our rendering done by\nbounding Gaussians to the mesh results in higher quality\nthan previous solutions based on meshes.\n\nPoint-based IBR methods. Alternatively, point-based\nrepresentations for radiance field excel at modeling thin ge-\nometry and leverage fast point rasterization pipelines to ren-\nder images using \u03b1-blending rather than ray-marching .\nIn particular, the very recent 3D Gaussian Splatting.\nmodel  allows for optimizing and rendering scenes with\nspeed and quality never seen before.\n\n\n", "expected_text": "Image-based rendering (IBR) methods rely on a set of two-\ndimensional images of a scene to generate a representation\nof the scene and render novel views. The very first novel-\nview synthesis approaches were based on light fields [CITATION_0],\nand developed the concept of volume rendering for novel\nviews. Their work emphasized the importance of efficiently\ntraversing volumetric data to produce realistic images.\n\nVarious scene representations have been proposed since,\nsuch as triangle meshes, point clouds, voxel grids, multi-\nplane images, or neural implicit functions.\n\nTraditional mesh-based IBR methods. Structure-from-\nmotion (SfM)\n[CITATION_1] and subsequent multi-view stereo\n(MVS) [CITATION_2] allow for 3D reconstruction of surfaces, lead-\ning to the development of several view synthesis algorithms\nrelying on triangle meshes as the primary 3D representa-\ntion of scenes. Such algorithms consider textured triangles\nor warp and blend captured images on the mesh surface to\ngenerate novel views [CITATION_3].\n[CITATION_4] consider deep\nlearning-based mesh representations for better view synthe-\nsis, bridging the gap between traditional graphics and mod-\nern machine learning techniques. While these mesh-based\nmethods take advantage of existing graphics hardware and\nsoftware for efficient rendering, they struggle with the cap-\nture of accurate geometry and appearance in complex re-\ngions.\n\nVolumetric IBR methods. Volumetric methods use voxel\ngrids, multiplane images, or neural networks to represent\nscenes as continuous volumetric functions of density and\ncolor. Recently, Neural Radiance Fields (NeRF) [CITATION_5] intro-\nduced a novel scene representation based on a continuous\nvolumetric function parameterized by a multilayer percep-\ntron (MLP). NeRF produces photorealistic renderings with\nfine details and view-dependent effects, achieved through\nvolumetric ray tracing. However, the original NeRF is com-\nputationally expensive and memory intensive.\n\nTo address these challenges, several works have im-\nproved NeRF\u2019s performance and scalability. These meth-\nods leverage discretized or sparse volumetric representa-\ntions like voxel grids and hash tables as ways to store\nlearnable features acting as positional encodings for 3D\npoints [CITATION_6], hierarchical sampling strate-\ngies [CITATION_7], or low-rank approximations [CITATION_8]. How-\never, they still rely on volumetric ray marching, which\nis incompatible with standard graphics hardware and soft-\nware designed for rendering polygonal surfaces. Recent\nworks have proposed modifying the NeRF\u2019s representation\nof geometry and emitted radiance to allow for better recon-\nstruction of specular materials [CITATION_9] or relighting the scene\nthrough an explicit decomposition into material and lighting\nproperties [CITATION_10].\n\nHybrid IBR methods. Some methods build on differen-\ntiable rendering to combine the advantages of mesh-based\nand volumetric methods, and allow for surface reconstruc-\ntion as well as better editability. They use a hybrid volume-\nsurface representation, which enables high-quality meshes\nsuitable for downstream graphics applications while effi-\nciently modeling view-dependent appearance.\nIn partic-\nular, some works optimize neural signed distance func-\ntions (SDF) by training neural radiance fields in which the\ndensity is derived as a differentiable transformation of the\nSDF [CITATION_11]. A triangle mesh can finally\nbe reconstructed from the SDF by applying the Marching\nCubes algorithm [CITATION_12]. However, most of these methods do\nnot target real-time rendering.\n\nAlternatively, other approaches \u201cbake\u201d the rendering ca-\npacity of an optimized NeRF or neural SDF into a much ef-\nficient structure relying on an underlying triangle mesh [CITATION_13]\nthat could benefit from the traditional triangle rasteriza-\ntion pipeline. In particular, the recent BakedSDF [CITATION_14] re-\nconstructs high quality meshes by optimizing a full neural\nSDF model, baking it into a high-resolution triangle mesh\nthat combines mesh rendering for interpolating features and\ndeep learning to translate these features into images, and\nfinally optimizes a view-dependent appearance model.\n\nHowever, even though it achieves real-time rendering\nand produces impressive meshes of the surface of the scene,\nthis model demands training a full neural SDF with an ar-\nchitecture identical to Mip-NeRF360 [CITATION_15], which necessi-\ntates 48 hours of training.\n\nSimilarly, the recent method NeRFMeshing [CITATION_16] pro-\nposes to also bake any NeRF model into a mesh structure,\nachieving real-time rendering. However, the meshing per-\nformed in this method lowers the quality of the rendering\nand results in a PSNR much lower than our method. Ad-\nditionally, this method still requires training a full NeRF\nmodel beforehand, and needs approximately an hour of\ntraining on 8 V100 NVIDIA GPUs to allow for mesh train-\ning and extraction.\n\nOur method is much faster at retrieveing a 3D mesh from\n3D Gaussian Splatting, which is itself much faster than\nNeRFs. As our experiments show, our rendering done by\nbounding Gaussians to the mesh results in higher quality\nthan previous solutions based on meshes.\n\nPoint-based IBR methods. Alternatively, point-based\nrepresentations for radiance field excel at modeling thin ge-\nometry and leverage fast point rasterization pipelines to ren-\nder images using \u03b1-blending rather than ray-marching [CITATION_17].\nIn particular, the very recent 3D Gaussian Splatting.\nmodel [CITATION_18] allows for optimizing and rendering scenes with\nspeed and quality never seen before.\n\n\n", "citation_info": {"content": [["Marc Levoy and Pat Hanrahan. Light Field Rendering. In ACM SIGGRAPH, 1996. 3"], ["Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo Tourism: Exploring Photo Collections in 3D. In ACM SIG- GRAPH, 2006. 3, 4"], ["Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven Seitz. Multi-View Stereo for Community Photo Collections. In International Conference on Computer Vision, 2007. 3"], ["Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, and Michael Cohen. Unstructured Lumigraph Ren- dering. In ACM SIGGRAPH, 2001. 3", "Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-Viewpoint Image-Based Rendering. In ACM SIG- GRAPH, 2018. 3, 7, 2, 4", "Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Cur- less, Tom Duchamp, David H. Salesin, and Werner Stuet- zle. Surface Light Fields for 3D Photography. In ACM SIG- GRAPH, 2000. 3"], ["Gernot Riegler and Vladlen Koltun. Free View Synthesis. In European Conference on Computer Vision, 2020. 3", "Gernot Riegler and Vladlen Koltun. Stable View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3"], ["Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View In European Conference on Computer Vision, Synthesis. 2020. 1, 3"], ["Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3", "Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. ReLU Fields: The Little Non-Linearity That Could. In ACM SIGGRAPH, 2022. 3", "Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant Neural Graphics Primitives with a Mul- tiresolution Hash Encoding. In ACM SIGGRAPH, 2022. 3, 7, 8, 2", "Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance In Conference on Computer Vision Fields Reconstruction. and Pattern Recognition, 2022. 3", "Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: In Conference Radiance Fields Without Neural Networks. on Computer Vision and Pattern Recognition, 2022. 3, 7, 8"], ["Jonathan T. Barron. Mip-NeRF 360: Unbounded Anti- Aliased Neural Radiance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3, 7, 8, 2, 4", "Peter Hedman and Pratul P. Srinivasan. Baking Neural Radi- ance Fields for Real-Time View Synthesis. In International Conference on Computer Vision, 2021. 3", "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. KiloNeRF: Speeding Up Neural Radiance Fields with Thousands of Tiny MLPs. In International Conference on Computer Vision, 2021. 3", "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees For Real-Time Rendering of Neural Radiance Fields. In International Conference on Computer Vision, 2021. 3"], ["Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3"], ["Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: 10 SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering"], ["Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar- ron, Ce Liu, and Hendrik P. A. Lensch. NeRD: Neural Re- flectance Decomposition from Image Collections. In Inter- national Conference on Computer Vision, 2021. 3", "Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC: Neural Rendering of Objects from Online Image Collections. In ACM SIGGRAPH, 2022. 3", "Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. NeRV: Neural Reflectance and Visibility Fields for Relight- ing and View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3", "Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse Rendering with Spherical Gaussians for Physics-Based Material Editing and Relight- ing. In Conference on Computer Vision and Pattern Recog- nition, 2021. 3"], ["Chong Bao and Bangbang Yang, Zeng Junyi, Bao Hu- jun, Zhang Yinda, Cui Zhaopeng, and Zhang Guofeng. NeuMesh: Learning Disentangled Neural Mesh-Based Im- plicit Field for Geometry and Texture Editing. In European Conference on Computer Vision, 2022. 4", "Franc\u00b8ois Darmon, B\u00b4en\u00b4edicte Bascle, Jean-Cl\u00b4ement Devaux, Pascal Monasse, and Mathieu Aubry. Improving Neural Im- plicit Surfaces Geometry with Patch Warping. In Conference on Computer Vision and Pattern Recognition, 2022. 4", "Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H. Tay- lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-Fidelity Neural Surface Reconstruction. In Conference on Computer Vision and Pattern Recognition, 2023. 2, 4", "Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction. In International Con- ference on Computer Vision, 2021. 4", "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning Neural Im- plicit Surfaces by Volume Rendering for Multi-View Recon- In Advances in Neural Information Processing struction. Systems, 2021. 2, 4", "Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol- ume Rendering of Neural Implicit Surfaces. In Advances in Neural Information Processing Systems, 2021. 2, 4"], ["William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In ACM SIGGRAPH, 1987. 2, 4, 8 9 Structured View-Dependent Appearance for Neural Radi- ance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3"], ["Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An- drea Tagliasacchi. MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. In Conference on Computer Vision and Pattern Recognition, 2023. 3, 4, 7, 8"], ["Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, and Jonathan T. Bar- ron. BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis. In ACM SIGGRAPH, 2023. 2, 3, 4, 7, 8"], ["Jonathan T. Barron. Mip-NeRF: A Multiscale Representa- tion for Anti-Aliasing Neural Radiance Fields. In Interna- tional Conference on Computer Vision, 2021. 4, 7"], ["Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and Federico Tombari. NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes. In DV, 2023. 2, 3, 4, 7, 8"], ["Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and George Drettakis. Point-Based Neural Rendering with Per- View Optimization. In Computer Graphics Forum, 2021. 4", "Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. ADOP: Approximate Differentiable One-Pixel Point Ren- dering. In ACM SIGGRAPH, 2022. 4"], ["Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. In ACM SIGGRAPH, 2023. 1, 2, 4, 7, 8"]], "positions": [229, 628, 668, 971, 973, 1562, 2190, 2227, 2256, 2580, 2677, 3260, 3361, 3602, 3713, 4215, 4302, 5219, 5281]}}
{"paper_id": "atte", "input_text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n, ByteNet  and ConvS2S , all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions . In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations .\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks .\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as  and .\n\n\n", "expected_text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[CITATION_0], ByteNet [CITATION_1] and ConvS2S [CITATION_2], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [CITATION_3]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [CITATION_4].\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [CITATION_5].\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [CITATION_6] and [CITATION_7].\n\n\n", "citation_info": {"content": [["\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016."], ["Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."], ["Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."], ["Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."], ["Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. 10", "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.", "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.", "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017."], ["Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015."], ["\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.", "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."], ["Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."]], "positions": [97, 107, 120, 561, 1189, 1406, 1767, 1772]}}
{"paper_id": "fireplace", "input_text": "3D scene generation and object placement. Significant\nefforts have been made towards collecting 3D scene datasets\n, enabling the community to train\nand develop systems that generate and/or position elements\nwithin indoor scenes . While\nthey demonstrate that object placement rules can be dis-\ntilled from scene databases, these were not designed to han-\ndle open vocabularies of objects, and even less so to take\ninto account the level of common sense reasoning that un-\nderlies human decisions to place objects where they are\nplaced within our living environments. FirePlace intro-\nduces a method to leverage the knowledge of MLLMs to\ndo this in a training-free manner. While other works  use\n2D image priors to generate scenes and objects, they often\nhave issues preserving object identity and physical plausi-\nbility of the final object arrangement. In contrast, FirePlace\nworks with an explicit 3D scene representation, where ex-\nplicit geometric constraints are enforced.\nFoundation models for 3D graphics. More recent\nworks  have demonstrated the potential\nof involving large pretrained models for different stages of\nthe 3D graphical design process. While they demonstrate\ncapabilities in editing materials , texture , and con-\ntrolling animation , they struggle with tasks that require\ncomplex spatial reasoning, like object placement. Existing\nworks like  have attempted to position objects in a scene\n\nby directly using LLMs through predicting the position and\norientation of objects as LLM outputs. More recent works\n have demonstrated the benefit of using LLMs\nto predict constraints instead, before using a solver to solve\nfor final object placements. However, despite being able\nto create large-scale scenes, they represent each object us-\ning bounding boxes, making it impossible to express fine-\ngrained constraints between parts of objects, leading to con-\nstraints that can only explain placements of box-like objects\n(as opposed to putting a book on a shelf, or a stuffed toy on\na chair with a backrest and armrests). This design choice is\nunderstandable, since parts of objects become increasingly\nhard for LLMs to reason about. FirePlace introduces a way\nto overcome this limitation.\n\n", "expected_text": "3D scene generation and object placement. Significant\nefforts have been made towards collecting 3D scene datasets\n[CITATION_0], enabling the community to train\nand develop systems that generate and/or position elements\nwithin indoor scenes [CITATION_1]. While\nthey demonstrate that object placement rules can be dis-\ntilled from scene databases, these were not designed to han-\ndle open vocabularies of objects, and even less so to take\ninto account the level of common sense reasoning that un-\nderlies human decisions to place objects where they are\nplaced within our living environments. FirePlace intro-\nduces a method to leverage the knowledge of MLLMs to\ndo this in a training-free manner. While other works [CITATION_2] use\n2D image priors to generate scenes and objects, they often\nhave issues preserving object identity and physical plausi-\nbility of the final object arrangement. In contrast, FirePlace\nworks with an explicit 3D scene representation, where ex-\nplicit geometric constraints are enforced.\nFoundation models for 3D graphics. More recent\nworks [CITATION_3] have demonstrated the potential\nof involving large pretrained models for different stages of\nthe 3D graphical design process. While they demonstrate\ncapabilities in editing materials [CITATION_4], texture [CITATION_5], and con-\ntrolling animation [CITATION_6], they struggle with tasks that require\ncomplex spatial reasoning, like object placement. Existing\nworks like [CITATION_7] have attempted to position objects in a scene\n\nby directly using LLMs through predicting the position and\norientation of objects as LLM outputs. More recent works\n[CITATION_8] have demonstrated the benefit of using LLMs\nto predict constraints instead, before using a solver to solve\nfor final object placements. However, despite being able\nto create large-scale scenes, they represent each object us-\ning bounding boxes, making it impossible to express fine-\ngrained constraints between parts of objects, leading to con-\nstraints that can only explain placements of box-like objects\n(as opposed to putting a book on a shelf, or a stuffed toy on\na chair with a backrest and armrests). This design choice is\nunderstandable, since parts of objects become increasingly\nhard for LLMs to reason about. FirePlace introduces a way\nto overcome this limitation.\n\n", "citation_info": {"content": [["Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning arXiv preprint from rgb-d data in indoor environments. arXiv:1709.06158, 2017. 3", "Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin- qiang Zhao, et al. 3d-front: 3d furnished rooms with layouts In Proceedings of the IEEE/CVF Interna- and semantics. tional Conference on Computer Vision, pages 10933\u201310942, 2021. 3", "Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. In- finigen indoors: Photorealistic indoor scenes using procedu- ral generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21783\u2013 21794, 2024. 3, 5", "Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealistic syn- thetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 3", "Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567\u2013576, 2015. 3", "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano- lis Savva, and Thomas Funkhouser. Semantic scene com- In Proceedings of the pletion from a single depth image. IEEE conference on computer vision and pattern recogni- tion, pages 1746\u20131754, 2017. 3"], ["Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. Example-based synthesis of 3d object arrangements. ACM Transactions on Graphics (TOG), 31(6):1\u201311, 2012. 3", "Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, S\u00a8oren Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas Guibas, and Hao Zhang. Language-driven synthe- sis of 3d scenes from scene databases. ACM Transactions on Graphics (TOG), 37(6):1\u201316, 2018. 3", "Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres- sive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:12013\u201312026, 2021. 2, 3", "Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flex- ible indoor scene synthesis via deep convolutional genera- In Proceedings of the IEEE/CVF Conference tive models. on Computer Vision and Pattern Recognition, pages 6182\u2013 6190, 2019. 3", "Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An- gel X Chang, and Daniel Ritchie. Planit: Planning and in- stantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG), 38(4):1\u2013 15, 2019. 3", "Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner. Sceneformer: Indoor scene generation with transformers. In 2021 International Conference on 3D Vision (3DV), pages 106\u2013115. IEEE, 2021.", "Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of ob- jects in rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19037\u2013 19047, 2023. 2, 3", "Yixuan Yang, Junru Lu, Zixiang Zhao, Zhen Luo, James JQ Yu, Victor Sanchez, and Feng Zheng. Llplace: The 3d in- door scene layout generation and editing via large language model. arXiv preprint arXiv:2406.03866, 2024. 2, 3"], ["Shen Chen, Jiale Zhou, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, and Lei Li. Scalinggaus- sian: Enhancing 3d content creation with generative gaussian splatting. arXiv preprint arXiv:2407.19035, 2024. 3", "Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scene- dreamer: Unbounded 3d scene generation from 2d image collections. IEEE transactions on pattern analysis and ma- chine intelligence, 2023.", "Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free gen- arXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3", "Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Ad- vances in Neural Information Processing Systems, 36, 2024. 3", "Paul Henderson, Melonie de Almeida, Daniela Ivanova, Sampling 3d gaussian scenes arXiv preprint and Titas Anciukevi\u02c7cius. in seconds with latent diffusion models. arXiv:2406.13099, 2024. 3", "Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vi- sion, pages 7909\u20137920, 2023. 3", "Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, and Xihui Liu. Dreamwaltz-g: Expressive 3d gaussian avatars from skeleton-guided 2d diffusion. arXiv preprint arXiv:2409.17145, 2024. 3", "Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, and Hongsheng Li. Diffindscene: Diffusion-based 9 high-quality 3d indoor scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4526\u20134535, 2024. 3", "Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic scene genera- tion with triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28337\u201328347, 2024. 3", "Pengzhi Li, Chengshuai Tang, Qinxuan Huang, and Zhi- heng Li. Art3d: 3d gaussian splatting for text-guided artistic scenes generation. arXiv preprint arXiv:2405.10508, 2024. 3", "Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite na- ture: Perpetual view generation of natural scenes from a sin- In Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 14458\u201314467, 2021. 3", "Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. In 2024 In- ternational Conference on 3D Vision (3DV), pages 651\u2013663. IEEE, 2024. 3", "Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF Conference 3d cities. on Computer Vision and Pattern Recognition, pages 9666\u2013 9675, 2024. 3", "Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6796\u20136807, 2024. 3", "Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester Cole, De- qing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6658\u20136667, 2024. 3", "Xuening Yuan, Hongyu Yang, Yueming Zhao, and Di Huang. Dreamscape: 3d scene creation via gaussian splatting joint arXiv preprint arXiv:2404.09227, correlation modeling. 2024. 3"], ["Purvi Goel, Kuan-Chieh Wang, C Karen Liu, and Kayvon Iterative motion editing with natural language. Fatahalian. arXiv preprint arXiv:2312.11538, 2023. 2, 3", "Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions. arXiv:2306.06212, 2023. 2, 3", "Ian Huang, Guandao Yang, and Leonidas Guibas. Blender- alchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2, 3, 5", "Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, and Michael J Black. Re-thinking inverse graphics with large language models. arXiv preprint arXiv:2404.15228, 2024. 2, 3", "Zehao Wen, Zichen Liu, Srinath Sridhar, and Rao Fu. Any- home: Open-vocabulary generation of structured and tex- tured 3d homes. arXiv preprint arXiv:2312.06644, 2023. 2, 3 10"], ["Ian Huang, Guandao Yang, and Leonidas Guibas. Blender- alchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2, 3, 5"], ["Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions. arXiv:2306.06212, 2023. 2, 3"], ["Purvi Goel, Kuan-Chieh Wang, C Karen Liu, and Kayvon Iterative motion editing with natural language. Fatahalian. arXiv preprint arXiv:2312.11538, 2023. 2, 3"], ["Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar- jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual plan- ning and generation with large language models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6"], ["Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stew- art Morris, Seung Jean Yoo, Aditya Ganeshan, R Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie. Open-universe indoor scene generation using llm program synthesis and uncurated object databases. arXiv preprint arXiv:2403.09675, 2024. 2, 3", "Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Ma- chine Learning, 2024. 2, 3, 5", "Hou In Ivan Tam, Hou In Derek Pun, Austin T Wang, Angel X Chang, and Manolis Savva. Scenemotifcoder: Example-driven visual program learning for generating 3d arXiv preprint arXiv:2408.02211, object arrangements. 2024. 2, 3", "Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Kr- ishna, Lingjie Liu, et al. Holodeck: Language guided genera- tion of 3d embodied ai environments. In The IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR 2024), pages 20\u201325. IEEE/CVF, 2024. 2, 3, 5, 6"]], "positions": [114, 228, 689, 1030, 1214, 1224, 1254, 1364, 1528]}}
{"paper_id": "3dmvp", "input_text": "Our work builds upon several active areas of research,\nincluding self-supervised learning, visual pretraining for\nrobotics, and learning robotic manipulation from demon-\nstrations.\n\nSelf-supervised learning. Self-supervised learning aims to\nlearn useful representations from unlabeled data by solving\npretext tasks that do not require manual annotation. Early\nwork in this area focused on designing pretext tasks for 2D\nimages, such as solving jagsaw puzzles , constrastive\nlearning  or joint embedding approaches . Most related to our work is the masked autoencoder\n(MAE) approach proposed by He et al. , which learns\nto reconstruct randomly masked patches in an image. MAE\nhas been shown to learn transferable representations for ob-\nject detection and segmentation tasks. Furthermore, Bach-\nmann et al demonstrates MAE pretraining can be extended\nto different modalities such as semantics and depth . In\nthis work, we extend the MAE approach to multi-view 3D\nscenes, enabling us to learn 3D-aware representations that\nare useful for robotic manipulation tasks. Unlike Multi-\nMAE which learns semantics and depth through direct su-\npervision, 3D-MVP aims to learn a 3D-aware representa-\ntion from multi view images.\n\nVisual pretraining for Robotics. Visual pretraining has\ndemonstrated impressive generalization ability on computer\nvision tasks. Therefore, prior works have explored whether\nit works for robotics tasks as well. Specifically, the robotics\ncommunity has trended towards learning representations us-\ning state-of-the-art self-supervised vision algorithms on di-\nverse interaction datasets , and finetune the net-\nwork on robotics tasks . 3D-MVP\nfollows the same procedure. However, existing robotics\npretraining approaches typically learn a 2D visual encoder\n(e.g. ResNet  or ViT ), we find they are inferior than\nmanipulation policies which do explicit 3D modeling (e.g.\nRVT , Act3D ). Migrating a pretrained ViT to 3D\nmanipulation policies is nontrivial since they do not have\na 2D visual encoder. In this paper, we propose 3D-MVP,\nwhich does 3D-aware pretraining on 3D manipulation poli-\ncies, to fill the gap.\n\nLearning manipulation from demonstrations. Recent\nwork has explored using transformers for multi-task ma-\nnipulation policies that predict robot actions from visual\nand language inputs . End-to-end mod-\nels like RT-1 , GATO , and InstructRL  directly\npredict 6-DoF end-effector poses but require many demon-\n\n2\n\n\fstrations to learn spatial reasoning and generalize to new\nscenes. To better handle 3D scenes, PerAct  and C2F-\nARM  voxelize the workspace and detect the 3D voxel\ncontaining the next end-effector pose. However, precise\npose prediction requires high-resolution voxels which are\ncomputationally expensive. Recently, RVT  proposes\na multi-view transformer that attends over point cloud fea-\ntures from multiple camera views to predict actions. This\navoids explicit voxelization and enables faster training and\ninference than PerAct. Act3D  represents the scene as a\ncontinuous 3D feature field and samples points to featurize\nwith attention, allowing adaptive resolution. GNFactor \njointly optimizes a generalizable neural field for reconstruc-\ntion and a Perceiver for decision-making. In contrast, our\nproposed 3D-MVP learns 3D scene representations through\nmasked autoencoding pretraining on a large dataset of 3D\nobject models. This pretraining enables 3D-MVP to build\na rich understanding of 3D geometry and semantics prior\nto finetuning on downstream manipulation tasks. Com-\npared to RVT and Act3D which train from scratch on tar-\nget tasks, 3D-MVP\u2019s pretraining leads to improved perfor-\nmance, sample efficiency and generalization. Unlike GN-\nFactor which relies on a pretrained VLM to inject seman-\ntics, 3D-MVP directly learns 3D semantic features from ob-\nject models.\n\n\n", "expected_text": "Our work builds upon several active areas of research,\nincluding self-supervised learning, visual pretraining for\nrobotics, and learning robotic manipulation from demon-\nstrations.\n\nSelf-supervised learning. Self-supervised learning aims to\nlearn useful representations from unlabeled data by solving\npretext tasks that do not require manual annotation. Early\nwork in this area focused on designing pretext tasks for 2D\nimages, such as solving jagsaw puzzles [CITATION_0], constrastive\nlearning [CITATION_1] or joint embedding approaches [CITATION_2]. Most related to our work is the masked autoencoder\n(MAE) approach proposed by He et al. [CITATION_3], which learns\nto reconstruct randomly masked patches in an image. MAE\nhas been shown to learn transferable representations for ob-\nject detection and segmentation tasks. Furthermore, Bach-\nmann et al demonstrates MAE pretraining can be extended\nto different modalities such as semantics and depth [CITATION_4]. In\nthis work, we extend the MAE approach to multi-view 3D\nscenes, enabling us to learn 3D-aware representations that\nare useful for robotic manipulation tasks. Unlike Multi-\nMAE which learns semantics and depth through direct su-\npervision, 3D-MVP aims to learn a 3D-aware representa-\ntion from multi view images.\n\nVisual pretraining for Robotics. Visual pretraining has\ndemonstrated impressive generalization ability on computer\nvision tasks. Therefore, prior works have explored whether\nit works for robotics tasks as well. Specifically, the robotics\ncommunity has trended towards learning representations us-\ning state-of-the-art self-supervised vision algorithms on di-\nverse interaction datasets [CITATION_5], and finetune the net-\nwork on robotics tasks [CITATION_6]. 3D-MVP\nfollows the same procedure. However, existing robotics\npretraining approaches typically learn a 2D visual encoder\n(e.g. ResNet [CITATION_7] or ViT [CITATION_8]), we find they are inferior than\nmanipulation policies which do explicit 3D modeling (e.g.\nRVT [CITATION_9], Act3D [CITATION_10]). Migrating a pretrained ViT to 3D\nmanipulation policies is nontrivial since they do not have\na 2D visual encoder. In this paper, we propose 3D-MVP,\nwhich does 3D-aware pretraining on 3D manipulation poli-\ncies, to fill the gap.\n\nLearning manipulation from demonstrations. Recent\nwork has explored using transformers for multi-task ma-\nnipulation policies that predict robot actions from visual\nand language inputs [CITATION_11]. End-to-end mod-\nels like RT-1 [CITATION_12], GATO [CITATION_13], and InstructRL [CITATION_14] directly\npredict 6-DoF end-effector poses but require many demon-\n\n2\n\n\fstrations to learn spatial reasoning and generalize to new\nscenes. To better handle 3D scenes, PerAct [CITATION_15] and C2F-\nARM [CITATION_16] voxelize the workspace and detect the 3D voxel\ncontaining the next end-effector pose. However, precise\npose prediction requires high-resolution voxels which are\ncomputationally expensive. Recently, RVT [CITATION_17] proposes\na multi-view transformer that attends over point cloud fea-\ntures from multiple camera views to predict actions. This\navoids explicit voxelization and enables faster training and\ninference than PerAct. Act3D [CITATION_18] represents the scene as a\ncontinuous 3D feature field and samples points to featurize\nwith attention, allowing adaptive resolution. GNFactor [CITATION_19]\njointly optimizes a generalizable neural field for reconstruc-\ntion and a Perceiver for decision-making. In contrast, our\nproposed 3D-MVP learns 3D scene representations through\nmasked autoencoding pretraining on a large dataset of 3D\nobject models. This pretraining enables 3D-MVP to build\na rich understanding of 3D geometry and semantics prior\nto finetuning on downstream manipulation tasks. Com-\npared to RVT and Act3D which train from scratch on tar-\nget tasks, 3D-MVP\u2019s pretraining leads to improved perfor-\nmance, sample efficiency and generalization. Unlike GN-\nFactor which relies on a pretrained VLM to inject seman-\ntics, 3D-MVP directly learns 3D semantic features from ob-\nject models.\n\n\n", "citation_info": {"content": [["Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Euro- visual representations by solving jigsaw puzzles. pean conference on computer vision, pages 69\u201384. Springer, 2016. 2"], ["Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597\u20131607. PMLR, 2020. 2", "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 9729\u20139738, 2020. 2"], ["Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo- janowski, Florian Bordes, Pascal Vincent, Armand Joulin, 8 InputGTPred Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In European Conference on Com- puter Vision, pages 456\u2013473. Springer, 2022. 2", "Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo- janowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023. 2", "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Ad- vances in neural information processing systems, 33:9912\u2013 9924, 2020. 2", "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 9650\u20139660, 2021. 2", "Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020. 2", "Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. 2"], ["Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u2013 16009, 2022. 1, 2, 8"], ["Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task masked autoen- coders. In ECCV, 2022. 1, 2"], ["Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In ECCV, 2018. 2, 7", "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022. 1, 2, 7", "Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey. Understanding human hands in contact at internet scale. In CVPR, 2020. 1, 2, 7"], ["Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Ab- hinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In Conference on Robot Learning, 2023. 1, 2, 7", "Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Os- bert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. 2", "Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelli- gence? Advances in Neural Information Processing Systems, 36, 2024.", "Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea R3m: A universal visual arXiv preprint Finn, and Abhinav Gupta. representation for robot manipulation. arXiv:2203.12601, 2022. 2, 6, 7", "Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learn- ing with masked visual pre-training. In Conference on Robot Learning, pages 416\u2013426. PMLR, 2023. 1, 2, 7", "Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jin- woo Shin, and Pieter Abbeel. Multi-view masked world In International models for visual robotic manipulation. Conference on Machine Learning, pages 30613\u201330632. PMLR, 2023. 2 10", "Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. 1, 2, 6, 7"], ["Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 2"], ["Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- arXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 2, 3"], ["Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694\u2013 710. PMLR, 2023. 1, 2, 3, 4, 5, 6, 7"], ["Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Ka- terina Fragkiadaki. Act3d: Infinite resolution action detec- tion transformer for robotic manipulation. arXiv preprint arXiv:2306.17817, 2023. 2, 3"], ["Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia Pinel, Makarand Tapaswi, Ivan Laptev, and Cordelia Schmid. Instruction-driven history-aware policies for robotic manip- ulations. In Conference on Robot Learning, pages 175\u2013187. PMLR, 2023. 2", "Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision- language models. 2022. 2", "Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Al- tanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. Advances in neural information processing systems, 35:22955\u201322968, 2022. 2", "Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver- actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785\u2013799. PMLR, 2023. 1, 2, 3, 5, 6", "Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen- Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, and Dieter Fox. Shelving, stacking, hanging: Relational pose arXiv preprint diffusion for multi-modal rearrangement. arXiv:2307.04751, 2023. 2"], ["Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr- ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 2"], ["Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin- arXiv preprint genberg, et al. A generalist agent. arXiv:2205.06175, 2022. 2"], ["Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision- language models. 2022. 2"], ["Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver- actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785\u2013799. PMLR, 2023. 1, 2, 3, 5, 6"], ["Stephen James, Kentaro Wada, Tristan Laidlow, and An- drew J Davison. Coarse-to-fine q-attention: Efficient learn- ing for visual robotic manipulation via discretisation. In CVPR, 2022. 3, 5, 6"], ["Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694\u2013 710. PMLR, 2023. 1, 2, 3, 4, 5, 6, 7"], ["Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Ka- terina Fragkiadaki. Act3d: Infinite resolution action detec- tion transformer for robotic manipulation. arXiv preprint arXiv:2306.17817, 2023. 2, 3"], ["Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, and Xiaolong Wang. Gnfactor: Multi-task real robot learning In Conference on with generalizable neural feature fields. Robot Learning, pages 284\u2013301. PMLR, 2023. 3"]], "positions": [459, 483, 514, 604, 902, 1605, 1652, 1788, 1796, 1892, 1900, 2316, 2348, 2355, 2372, 2546, 2560, 2763, 2981, 3123]}}
{"paper_id": "AnyCam", "input_text": "2.1. Foundational Models for Depth and Flow\n\nIn the tasks of monocular depth (MDE) and flow estimation,\nwell-generalizable foundation models have replaced early\ndeep learning approaches  in the last years. For\nMDE, DepthAnything  uses a data engine to construct\na large corpus of automatically annotated data to learn rel-\native depth estimation. Additional fine-tuning allows for\nmetric depth estimates. DepthAnythingV2  finetunes\nthe previous model using synthetic data for better perfor-\nmance. Metric3D  and Metric3Dv2  transform im-\nages to canonical camera intrinsics with a fixed focal length.\nDepthPro  proposes a two-stage training curriculum with\na second stage solely on synthetic data to sharpen bound-\nary predictions. DepthCrafter  leverages a conditional\ndiffusion model to predict temporally consistent depth maps\n\nfor videos. In this work, we utilize UniDepth  for metric\nMDE, which uses a geometric invariance loss on different\nimage augmentation to enforce consistency.\n\nRAFT  presented the state of the art for optical flow\nIt improved previous meth-\nestimation for a long time.\nods by introducing a recurrent look-up operator on corre-\nlation volumes to iteratively refine flow predictions with-\nout needing coarse-to-fine flow pyramids. GMFlow \navoids correlation volumes and instead leverages the prop-\nerties of transformers for global matching on feature maps.\nThis removes the need for iterative steps to improve runtime\nperformance. UniMatch  extends GMFlow network by\ntasks of disparity and depth prediction to enable cross-task\ntransfer learning of a single transformer network.\n\nWe rely on both off-the-shelf MDE and Optical Flow\nnetworks to benefit from strong geometric priors during\ntraining and inference.\n\n2.2. SfM and SLAM\n\nFor many decades, the problem of recovering camera pa-\nrameters and geometry from images has been formulated\nas the Structure-from-Motion (SfM) pipeline .\nWhile many different implementations of the SfM pipeline\nexist, COLMAP  has emerged as the standard due to\nits robustness and flexibility. One of the drawbacks of SfM\nmethods is their high computational cost. Simultaneous Lo-\ncation and Mapping (SLAM)  approaches em-\nploy a similar pipeline to SfM but focus on the efficient pro-\ncessing of consecutive video frames. In recent years, these\nclassical optimization-based approaches were enhanced by\nlearned components . However,\nrelying on epipolar geometry  or photometric consis-\ntency  makes them susceptible to high error on highly\ndynamic scenes. The strong focus on self driving data pro-\nvided datasets with mostly static environments ,\nan assumption that does not hold for casual videos.\n\n2.3. Learning Based SfM and SLAM\n\nLargely learning-based methods started to replace classical\nSLAM and SfM systems due to improved robustness .\nDROID-SLAM extends the framework of RAFT  by an\nupdate operator on both depth and pose estimates. A final\ndifferentiable bundle adjustment (BA) layer produces the fi-\nnal pose estimates. ParticleSfM  utilizes dense corre-\nspondences inside a BA framework to optimize poses. The\ndense correspondences are initialized from optical flow, and\ndynamic points are filtered using trajectory-based motion\nsegmentation. CasualSAM  predicts both depth and\nmovement from images to get frame-to-frame motion. A\nglobal optimization aligns the scale of the prediction and\nrefines the poses. Dust3R  is a dense multi-view stereo\nmethod that regresses point coordinates between an im-\nage pair. This allows it to be extended to either SfM or\n\nSLAM. FlowMap  proposes to reconstruct a scene by\noverfitting a depth network to it and aligning depth maps\nvia correspondences from flow or point tracking. LEAP-\nVO  combines visual and temporal information of video\nsequences to improve the tracking accuracy of points and\nidentify occluded and dynamic points. A sliding window\nbundle adjustment then optimizes the poses. The concurrent\nwork of MonST3R  finetunes Dust3r on mostly syn-\nthetic data to generalize it to dynamic scenes. While these\nworks achieve impressive progress, they generally obtain\nposes from aligning depth and point maps or by optimizing\nthem per-scene. This makes it hard to inject prior informa-\ntion about camera motion. In contrast, our method uses a\nneural network to predict a trajectory, which can effectively\nlearn priors over realistic camera motions.\n\n\f", "expected_text": "2.1. Foundational Models for Depth and Flow\n\nIn the tasks of monocular depth (MDE) and flow estimation,\nwell-generalizable foundation models have replaced early\ndeep learning approaches [CITATION_0] in the last years. For\nMDE, DepthAnything [CITATION_1] uses a data engine to construct\na large corpus of automatically annotated data to learn rel-\native depth estimation. Additional fine-tuning allows for\nmetric depth estimates. DepthAnythingV2 [CITATION_2] finetunes\nthe previous model using synthetic data for better perfor-\nmance. Metric3D [CITATION_3] and Metric3Dv2 [CITATION_4] transform im-\nages to canonical camera intrinsics with a fixed focal length.\nDepthPro [CITATION_5] proposes a two-stage training curriculum with\na second stage solely on synthetic data to sharpen bound-\nary predictions. DepthCrafter [CITATION_6] leverages a conditional\ndiffusion model to predict temporally consistent depth maps\n\nfor videos. In this work, we utilize UniDepth [CITATION_7] for metric\nMDE, which uses a geometric invariance loss on different\nimage augmentation to enforce consistency.\n\nRAFT [CITATION_8] presented the state of the art for optical flow\nIt improved previous meth-\nestimation for a long time.\nods by introducing a recurrent look-up operator on corre-\nlation volumes to iteratively refine flow predictions with-\nout needing coarse-to-fine flow pyramids. GMFlow [CITATION_9]\navoids correlation volumes and instead leverages the prop-\nerties of transformers for global matching on feature maps.\nThis removes the need for iterative steps to improve runtime\nperformance. UniMatch [CITATION_10] extends GMFlow network by\ntasks of disparity and depth prediction to enable cross-task\ntransfer learning of a single transformer network.\n\nWe rely on both off-the-shelf MDE and Optical Flow\nnetworks to benefit from strong geometric priors during\ntraining and inference.\n\n2.2. SfM and SLAM\n\nFor many decades, the problem of recovering camera pa-\nrameters and geometry from images has been formulated\nas the Structure-from-Motion (SfM) pipeline [CITATION_11].\nWhile many different implementations of the SfM pipeline\nexist, COLMAP [CITATION_12] has emerged as the standard due to\nits robustness and flexibility. One of the drawbacks of SfM\nmethods is their high computational cost. Simultaneous Lo-\ncation and Mapping (SLAM) [CITATION_13] approaches em-\nploy a similar pipeline to SfM but focus on the efficient pro-\ncessing of consecutive video frames. In recent years, these\nclassical optimization-based approaches were enhanced by\nlearned components [CITATION_14]. However,\nrelying on epipolar geometry [CITATION_15] or photometric consis-\ntency [CITATION_16] makes them susceptible to high error on highly\ndynamic scenes. The strong focus on self driving data pro-\nvided datasets with mostly static environments [CITATION_17],\nan assumption that does not hold for casual videos.\n\n2.3. Learning Based SfM and SLAM\n\nLargely learning-based methods started to replace classical\nSLAM and SfM systems due to improved robustness [CITATION_18].\nDROID-SLAM extends the framework of RAFT [CITATION_19] by an\nupdate operator on both depth and pose estimates. A final\ndifferentiable bundle adjustment (BA) layer produces the fi-\nnal pose estimates. ParticleSfM [CITATION_20] utilizes dense corre-\nspondences inside a BA framework to optimize poses. The\ndense correspondences are initialized from optical flow, and\ndynamic points are filtered using trajectory-based motion\nsegmentation. CasualSAM [CITATION_21] predicts both depth and\nmovement from images to get frame-to-frame motion. A\nglobal optimization aligns the scale of the prediction and\nrefines the poses. Dust3R [CITATION_22] is a dense multi-view stereo\nmethod that regresses point coordinates between an im-\nage pair. This allows it to be extended to either SfM or\n\nSLAM. FlowMap [CITATION_23] proposes to reconstruct a scene by\noverfitting a depth network to it and aligning depth maps\nvia correspondences from flow or point tracking. LEAP-\nVO [CITATION_24] combines visual and temporal information of video\nsequences to improve the tracking accuracy of points and\nidentify occluded and dynamic points. A sliding window\nbundle adjustment then optimizes the poses. The concurrent\nwork of MonST3R [CITATION_25] finetunes Dust3r on mostly syn-\nthetic data to generalize it to dynamic scenes. While these\nworks achieve impressive progress, they generally obtain\nposes from aligning depth and point maps or by optimizing\nthem per-scene. This makes it hard to inject prior informa-\ntion about camera motion. In contrast, our method uses a\nneural network to predict a trajectory, which can effectively\nlearn priors over realistic camera motions.\n\n\f", "citation_info": {"content": [["Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Pro- ceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015. 2", "Cl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency. on computer vision and pattern recognition, pages 270\u2013279, 2017. 2", "Cl\u00b4ement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation. national conference on computer vision, pages 3828\u20133838, 2019. 2"], ["Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of 10 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371\u201310381, 2024. 2"], ["Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao- gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any- thing v2. arXiv preprint arXiv:2406.09414, 2024. 2"], ["Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9043\u20139053, 2023. 2"], ["Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 2"], ["Aleksei Bochkovskii, Ama\u00a8el Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than a second. arXiv preprint arXiv:2410.02073, 2024. 2"], ["Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 2"], ["Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106\u201310116, 2024. 2, 6"], ["Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020. 2"], ["Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8121\u20138130, 2022. 2"], ["Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2, 6"], ["Richard Hartley and Andrew Zisserman. Multiple view ge- ometry in computer vision. Cambridge university press, 2003. 2", "John Oliensis. A critique of structure-from-motion algo- rithms. Computer Vision and Image Understanding, 80(2): 172\u2013214, 2000. 2", "Onur \u00a8Ozyes\u00b8il, Vladislav Voroninski, Ronen Basri, and Amit Singer. A survey of structure from motion*. Acta Numerica, 26:305\u2013364, 2017. 2 9"], ["Johannes L Schonberger and Jan-Michael Frahm. Structure- In Proceedings of the IEEE con- from-motion revisited. ference on computer vision and pattern recognition, pages 4104\u20134113, 2016. 1, 2"], ["Jakob Engel, Thomas Sch\u00a8ops, and Daniel Cremers. Lsd- slam: Large-scale direct monocular slam. In European con- ference on computer vision, pages 834\u2013849. Springer, 2014. 2", "Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611\u2013625, 2017. 2", "Raul Mur-Artal and Juan D Tard\u00b4os. Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras. IEEE transactions on robotics, 33(5):1255\u20131262, 2017. 2", "Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam IEEE transactions on robotics, 31(5):1147\u20131163, system. 2015. 2"], ["Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6684\u20136692, 2017. 2", "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224\u2013236, 2018. 2", "Vincent Leroy, Yohann Cabon, and J\u00b4er\u02c6ome Revaud. Ground- arXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. 2", "Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallab- hula, and Daniel Cremers. Learning correspondence uncer- tainty via differentiable nonlinear least squares. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13102\u201313112, 2023. 2", "Ren\u00b4e Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV), pages 284\u2013299, 2018. 2", "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. In Proceedings of matching with graph neural networks. the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.", "Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, et al. Back to the feature: Learning robust camera localization from pixels to pose. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 3247\u20133257, 2021. 2", "Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry. In Proceedings of the European conference on computer vision (ECCV), pages 817\u2013833, 2018. 2"], ["Richard I Hartley. In defense of the eight-point algorithm. IEEE Transactions on pattern analysis and machine intelli- gence, 19(6):580\u2013593, 1997. 2"], ["Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611\u2013625, 2017. 2"], ["Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- In Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 11621\u201311631, 2020. 2", "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research, 32(11):1231\u20131237, 2013. 2", "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2446\u20132454, 2020. 2"], ["Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neu- ral information processing systems, 34:16558\u201316569, 2021. 2, 6"], ["Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020. 2"], ["Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523\u2013542. Springer, 2022. 2, 6"], ["Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Ru- binstein, Noah Snavely, and William T Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 20\u201337. Springer, 2022. 2, 6"], ["Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi- sion made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20697\u201320709, 2024. 1, 2"], ["Cameron Smith, David Charatan, Ayush Tewari, and Vin- cent Sitzmann. Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent. arXiv:2404.15259, 2024. 2, 3, 4"], ["Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry. on Computer Vision and Pattern Recognition, pages 19844\u2013 19853, 2024. 2, 3, 6"], ["Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam- pani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming- Hsuan Yang. Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion. arXiv:2410.03825, 2024. 3, 6"]], "positions": [186, 229, 421, 507, 523, 610, 745, 877, 995, 1266, 1469, 1913, 1986, 2167, 2382, 2422, 2452, 2606, 2803, 2846, 3004, 3226, 3389, 3546, 3698, 3936]}}
