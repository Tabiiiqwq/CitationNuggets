2. Related Work

2.1. Human Motion Generation

Human motion generation has progressed significantly in
recent years [CITATION_0] leveraging a variety of conditioning sig-
nals such as text [CITATION_1], actions [CITATION_2], speech [CITATION_3],
music [CITATION_4], and scenes/objects [CITATION_5]. Recently, multimodal motion generation has
also gained attention [CITATION_6] enabling multiple input
modalities. However, most existing methods focus solely
on generative tasks without supporting estimation. For in-
stance, the method [CITATION_7] supports video input but treats it as
a generative task, resulting in motions that loosely imitate
video content rather than precisely matching it. In contrast,
our method jointly handles generation and estimation tasks,
yielding more precise video-conditioned results.

For long-sequence motion generation, existing works
mostly rely on ad-hoc post-processing techniques to stitch
separately generated fixed-length motions [CITATION_8]. In
contrast, our method introduces a novel diffusion-based ar-
chitecture enabling seamless generation of arbitrary-length
motions conditioned on multiple modalities without com-
plex post-processing.

Existing datasets, such as AMASS [CITATION_9], are limited in
size and diversity. To address the scarcity of 3D data,
Motion-X [CITATION_10] and MotionBank [CITATION_11] augment datasets us-
ing 2D videos and 3D pose estimation models [CITATION_12], but
the resulting motions often contain artifacts.
In contrast,
our method directly leverages in-the-wild videos with 2D
annotations without explicit 3D reconstruction, reducing re-
liance on noisy data and enhancing robustness and diversity.

2.2. Human Motion Estimation

Human pose estimation from images [CITATION_13], videos [CITATION_14], or even sparse marker data [CITATION_15] has been
studied extensively in the literature. Recent works focus pri-
marily on estimating global human motion in world-space
coordinates [CITATION_16]. This is an inherently
ill-posed problem, hence these methods leverage generative
priors and SLAM methods to constrain human and camera
motions, respectively. However, these methods typically in-
volve computationally expensive optimization or separate
post-processing steps.

More recent approaches aim to estimate global human
motion in a feed-forward manner [CITATION_17], offer-
ing faster solutions. Our method extends this direction by
jointly modeling generation and estimation within a uni-
fied diffusion framework. This integration leverages shared
representations and generative priors during training to pro-
duce more plausible estimations.


