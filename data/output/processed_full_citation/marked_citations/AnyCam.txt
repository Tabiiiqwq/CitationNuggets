2.1. Foundational Models for Depth and Flow

In the tasks of monocular depth (MDE) and flow estimation,
well-generalizable foundation models have replaced early
deep learning approaches [CITATION_0] in the last years. For
MDE, DepthAnything [CITATION_1] uses a data engine to construct
a large corpus of automatically annotated data to learn rel-
ative depth estimation. Additional fine-tuning allows for
metric depth estimates. DepthAnythingV2 [CITATION_2] finetunes
the previous model using synthetic data for better perfor-
mance. Metric3D [CITATION_3] and Metric3Dv2 [CITATION_4] transform im-
ages to canonical camera intrinsics with a fixed focal length.
DepthPro [CITATION_5] proposes a two-stage training curriculum with
a second stage solely on synthetic data to sharpen bound-
ary predictions. DepthCrafter [CITATION_6] leverages a conditional
diffusion model to predict temporally consistent depth maps

for videos. In this work, we utilize UniDepth [CITATION_7] for metric
MDE, which uses a geometric invariance loss on different
image augmentation to enforce consistency.

RAFT [CITATION_8] presented the state of the art for optical flow
It improved previous meth-
estimation for a long time.
ods by introducing a recurrent look-up operator on corre-
lation volumes to iteratively refine flow predictions with-
out needing coarse-to-fine flow pyramids. GMFlow [CITATION_9]
avoids correlation volumes and instead leverages the prop-
erties of transformers for global matching on feature maps.
This removes the need for iterative steps to improve runtime
performance. UniMatch [CITATION_10] extends GMFlow network by
tasks of disparity and depth prediction to enable cross-task
transfer learning of a single transformer network.

We rely on both off-the-shelf MDE and Optical Flow
networks to benefit from strong geometric priors during
training and inference.

2.2. SfM and SLAM

For many decades, the problem of recovering camera pa-
rameters and geometry from images has been formulated
as the Structure-from-Motion (SfM) pipeline [CITATION_11].
While many different implementations of the SfM pipeline
exist, COLMAP [CITATION_12] has emerged as the standard due to
its robustness and flexibility. One of the drawbacks of SfM
methods is their high computational cost. Simultaneous Lo-
cation and Mapping (SLAM) [CITATION_13] approaches em-
ploy a similar pipeline to SfM but focus on the efficient pro-
cessing of consecutive video frames. In recent years, these
classical optimization-based approaches were enhanced by
learned components [CITATION_14]. However,
relying on epipolar geometry [CITATION_15] or photometric consis-
tency [CITATION_16] makes them susceptible to high error on highly
dynamic scenes. The strong focus on self driving data pro-
vided datasets with mostly static environments [CITATION_17],
an assumption that does not hold for casual videos.

2.3. Learning Based SfM and SLAM

Largely learning-based methods started to replace classical
SLAM and SfM systems due to improved robustness [CITATION_18].
DROID-SLAM extends the framework of RAFT [CITATION_19] by an
update operator on both depth and pose estimates. A final
differentiable bundle adjustment (BA) layer produces the fi-
nal pose estimates. ParticleSfM [CITATION_20] utilizes dense corre-
spondences inside a BA framework to optimize poses. The
dense correspondences are initialized from optical flow, and
dynamic points are filtered using trajectory-based motion
segmentation. CasualSAM [CITATION_21] predicts both depth and
movement from images to get frame-to-frame motion. A
global optimization aligns the scale of the prediction and
refines the poses. Dust3R [CITATION_22] is a dense multi-view stereo
method that regresses point coordinates between an im-
age pair. This allows it to be extended to either SfM or

SLAM. FlowMap [CITATION_23] proposes to reconstruct a scene by
overfitting a depth network to it and aligning depth maps
via correspondences from flow or point tracking. LEAP-
VO [CITATION_24] combines visual and temporal information of video
sequences to improve the tracking accuracy of points and
identify occluded and dynamic points. A sliding window
bundle adjustment then optimizes the poses. The concurrent
work of MonST3R [CITATION_25] finetunes Dust3r on mostly syn-
thetic data to generalize it to dynamic scenes. While these
works achieve impressive progress, they generally obtain
poses from aligning depth and point maps or by optimizing
them per-scene. This makes it hard to inject prior informa-
tion about camera motion. In contrast, our method uses a
neural network to predict a trajectory, which can effectively
learn priors over realistic camera motions.

