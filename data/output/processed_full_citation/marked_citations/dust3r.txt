For the sake of space, we summarize here the most related
works in 3D vision, and refer the reader to the appendix in
Sec. C for a more comprehensive review.
Structure-from-Motion (SfM) [CITATION_0] aims
at reconstructing sparse 3D maps while jointly determin-
ing camera parameters from a set of images. The tradi-
tional pipeline starts from pixel correspondences obtained
from keypoint matching [CITATION_1] between multiple
images to determine geometric relationships, followed by
bundle adjustment to optimize 3D coordinates and camera
parameters jointly. Recently, the SfM pipeline has under-
gone substantial enhancements, particularly with the incor-
poration of learning-based techniques into its subprocesses.
These improvements encompass advanced feature descrip-
tion [CITATION_2], more accurate image match-
ing [CITATION_3], featuremetric refine-
ment [CITATION_4], and neural bundle adjustment [CITATION_5]. Despite
these advancements, the sequential structure of the SfM
pipeline persists, making it vulnerable to noise and errors in
each individual component.
MultiView Stereo (MVS) is the task of densely recon-
structing visible surfaces, which is achieved via triangu-
lation between multiple viewpoints.
In the classical for-
mulation of MVS, all camera parameters are supposed
to be provided as inputs. The fully handcrafted [CITATION_6], the more recent scene optimization
based [CITATION_7], or learning
based [CITATION_8] approaches all de-
pend on camera parameter estimates obtained via com-
plex calibration procedures, either during the data acqui-
sition [CITATION_9] or using Structure-from-Motion ap-
proaches [CITATION_10] for in-the-wild reconstructions. Yet, in
real-life scenarios, the inaccuracy of pre-estimated camera
parameters can be detrimental for these algorithms to work
properly. In this work, we propose instead to directly pre-
dict the geometry of visible surfaces without any explicit
knowledge of the camera parameters.
Direct RGB-to-3D. Recently, some approaches aiming at
directly predicting 3D geometry from a single RGB image
have been proposed. Since the problem is by nature ill-posed
without introducing additional assumptions, these methods
leverage neural networks that learn strong 3D priors from
large datasets to solve ambiguities. These methods can be
classified into two groups. The first group leverages class-
level object priors. For instance, Pavllo et al. [CITATION_11] pro-
pose to learn a model that can fully recover shape, pose, and
appearance from a single image, given a large collection of
2D images. While this type of approach is powerful, it does
not allow to infer shape on objects from unseen categories. A
second group of work, closest to our method, focuses instead
on general scenes. These methods systematically build on
or re-use existing monocular depth estimation (MDE) net-

works [CITATION_12]. Depth maps indeed encode a form
of 3D information and, combined with camera intrinsics,
can straightforwardly yield pixel-aligned 3D point-clouds.
SynSin [CITATION_13], for example, performs new viewpoint syn-
thesis from a single image by rendering feature-augmented
depthmaps knowing all camera parameters. Without cam-
era intrinsics, one solution is to infer them by exploiting
temporal consistency in video frames, either by enforcing a
global alignment et al. [CITATION_14] or by leveraging differentiable
rendering with a photometric reconstruction loss [CITATION_15].
Another way is to explicitly learn to predict camera intrin-
sics, which enables to perform metric 3D reconstruction
from a single image when combined with MDE [CITATION_16].
All these methods are, however, intrinsically limited by the
quality of depth estimates, which arguably is ill-posed for
monocular settings.

In contrast, our network processes two viewpoints simul-
taneously in order to output depthmaps, or rather, pointmaps.
In theory, at least, this makes triangulation between rays
from different viewpoint possible. Multi-view networks for
3D reconstruction have been proposed in the past. They are
essentially based on the idea of building a differentiable SfM
pipeline, replicating the traditional pipeline but training it
end-to-end [CITATION_17]. For that, however, ground-truth
camera intrinsics are required as input, and the output is gen-
erally a depthmap and a relative camera pose [CITATION_18]. In
contrast, our network has a generic architecture and outputs
pointmaps, i.e. dense 2D field of 3D points, which handle
camera poses implicitly and makes the regression problem
much better posed.
Pointmaps. Using a collection of pointmaps as shape rep-
resentation is quite counter-intuitive for MVS, but its us-
age is widespread for Visual Localization tasks, either in
scene-dependent optimization approaches [CITATION_19] or scene-
agnostic inference methods [CITATION_20]. Similarly, view-
wise modeling is a common theme in monocular 3D recon-
struction works [CITATION_21] and in view synthesis
works [CITATION_22]. The idea being to store the canonical 3D shape
in multiple canonical views to work in image space. These
approaches usually leverage explicit perspective camera ge-
ometry, via rendering of the canonical representation.

