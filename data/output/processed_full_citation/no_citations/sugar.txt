Image-based rendering (IBR) methods rely on a set of two-
dimensional images of a scene to generate a representation
of the scene and render novel views. The very first novel-
view synthesis approaches were based on light fields ,
and developed the concept of volume rendering for novel
views. Their work emphasized the importance of efficiently
traversing volumetric data to produce realistic images.

Various scene representations have been proposed since,
such as triangle meshes, point clouds, voxel grids, multi-
plane images, or neural implicit functions.

Traditional mesh-based IBR methods. Structure-from-
motion (SfM)
 and subsequent multi-view stereo
(MVS)  allow for 3D reconstruction of surfaces, lead-
ing to the development of several view synthesis algorithms
relying on triangle meshes as the primary 3D representa-
tion of scenes. Such algorithms consider textured triangles
or warp and blend captured images on the mesh surface to
generate novel views .
 consider deep
learning-based mesh representations for better view synthe-
sis, bridging the gap between traditional graphics and mod-
ern machine learning techniques. While these mesh-based
methods take advantage of existing graphics hardware and
software for efficient rendering, they struggle with the cap-
ture of accurate geometry and appearance in complex re-
gions.

Volumetric IBR methods. Volumetric methods use voxel
grids, multiplane images, or neural networks to represent
scenes as continuous volumetric functions of density and
color. Recently, Neural Radiance Fields (NeRF)  intro-
duced a novel scene representation based on a continuous
volumetric function parameterized by a multilayer percep-
tron (MLP). NeRF produces photorealistic renderings with
fine details and view-dependent effects, achieved through
volumetric ray tracing. However, the original NeRF is com-
putationally expensive and memory intensive.

To address these challenges, several works have im-
proved NeRF’s performance and scalability. These meth-
ods leverage discretized or sparse volumetric representa-
tions like voxel grids and hash tables as ways to store
learnable features acting as positional encodings for 3D
points , hierarchical sampling strate-
gies , or low-rank approximations . How-
ever, they still rely on volumetric ray marching, which
is incompatible with standard graphics hardware and soft-
ware designed for rendering polygonal surfaces. Recent
works have proposed modifying the NeRF’s representation
of geometry and emitted radiance to allow for better recon-
struction of specular materials  or relighting the scene
through an explicit decomposition into material and lighting
properties .

Hybrid IBR methods. Some methods build on differen-
tiable rendering to combine the advantages of mesh-based
and volumetric methods, and allow for surface reconstruc-
tion as well as better editability. They use a hybrid volume-
surface representation, which enables high-quality meshes
suitable for downstream graphics applications while effi-
ciently modeling view-dependent appearance.
In partic-
ular, some works optimize neural signed distance func-
tions (SDF) by training neural radiance fields in which the
density is derived as a differentiable transformation of the
SDF . A triangle mesh can finally
be reconstructed from the SDF by applying the Marching
Cubes algorithm . However, most of these methods do
not target real-time rendering.

Alternatively, other approaches “bake” the rendering ca-
pacity of an optimized NeRF or neural SDF into a much ef-
ficient structure relying on an underlying triangle mesh 
that could benefit from the traditional triangle rasteriza-
tion pipeline. In particular, the recent BakedSDF  re-
constructs high quality meshes by optimizing a full neural
SDF model, baking it into a high-resolution triangle mesh
that combines mesh rendering for interpolating features and
deep learning to translate these features into images, and
finally optimizes a view-dependent appearance model.

However, even though it achieves real-time rendering
and produces impressive meshes of the surface of the scene,
this model demands training a full neural SDF with an ar-
chitecture identical to Mip-NeRF360 , which necessi-
tates 48 hours of training.

Similarly, the recent method NeRFMeshing  pro-
poses to also bake any NeRF model into a mesh structure,
achieving real-time rendering. However, the meshing per-
formed in this method lowers the quality of the rendering
and results in a PSNR much lower than our method. Ad-
ditionally, this method still requires training a full NeRF
model beforehand, and needs approximately an hour of
training on 8 V100 NVIDIA GPUs to allow for mesh train-
ing and extraction.

Our method is much faster at retrieveing a 3D mesh from
3D Gaussian Splatting, which is itself much faster than
NeRFs. As our experiments show, our rendering done by
bounding Gaussians to the mesh results in higher quality
than previous solutions based on meshes.

Point-based IBR methods. Alternatively, point-based
representations for radiance field excel at modeling thin ge-
ometry and leverage fast point rasterization pipelines to ren-
der images using α-blending rather than ray-marching .
In particular, the very recent 3D Gaussian Splatting.
model  allows for optimizing and rendering scenes with
speed and quality never seen before.


