{"paper_id": "fireplace", "input_text": "3D scene generation and object placement. Significant\nefforts have been made towards collecting 3D scene datasets\n, enabling the community to train\nand develop systems that generate and/or position elements\nwithin indoor scenes . While\nthey demonstrate that object placement rules can be dis-\ntilled from scene databases, these were not designed to han-\ndle open vocabularies of objects, and even less so to take\ninto account the level of common sense reasoning that un-\nderlies human decisions to place objects where they are\nplaced within our living environments. FirePlace intro-\nduces a method to leverage the knowledge of MLLMs to\ndo this in a training-free manner. While other works  use\n2D image priors to generate scenes and objects, they often\nhave issues preserving object identity and physical plausi-\nbility of the final object arrangement. In contrast, FirePlace\nworks with an explicit 3D scene representation, where ex-\nplicit geometric constraints are enforced.\nFoundation models for 3D graphics. More recent\nworks  have demonstrated the potential\nof involving large pretrained models for different stages of\nthe 3D graphical design process. While they demonstrate\ncapabilities in editing materials , texture , and con-\ntrolling animation , they struggle with tasks that require\ncomplex spatial reasoning, like object placement. Existing\nworks like  have attempted to position objects in a scene\n\nby directly using LLMs through predicting the position and\norientation of objects as LLM outputs. More recent works\n have demonstrated the benefit of using LLMs\nto predict constraints instead, before using a solver to solve\nfor final object placements. However, despite being able\nto create large-scale scenes, they represent each object us-\ning bounding boxes, making it impossible to express fine-\ngrained constraints between parts of objects, leading to con-\nstraints that can only explain placements of box-like objects\n(as opposed to putting a book on a shelf, or a stuffed toy on\na chair with a backrest and armrests). This design choice is\nunderstandable, since parts of objects become increasingly\nhard for LLMs to reason about. FirePlace introduces a way\nto overcome this limitation.", "expected_text": "3D scene generation and object placement. Significant\nefforts have been made towards collecting 3D scene datasets\n[CITATION_0], enabling the community to train\nand develop systems that generate and/or position elements\nwithin indoor scenes [CITATION_1]. While\nthey demonstrate that object placement rules can be dis-\ntilled from scene databases, these were not designed to han-\ndle open vocabularies of objects, and even less so to take\ninto account the level of common sense reasoning that un-\nderlies human decisions to place objects where they are\nplaced within our living environments. FirePlace intro-\nduces a method to leverage the knowledge of MLLMs to\ndo this in a training-free manner. While other works [CITATION_2] use\n2D image priors to generate scenes and objects, they often\nhave issues preserving object identity and physical plausi-\nbility of the final object arrangement. In contrast, FirePlace\nworks with an explicit 3D scene representation, where ex-\nplicit geometric constraints are enforced.\nFoundation models for 3D graphics. More recent\nworks [CITATION_3] have demonstrated the potential\nof involving large pretrained models for different stages of\nthe 3D graphical design process. While they demonstrate\ncapabilities in editing materials [CITATION_4], texture [CITATION_5], and con-\ntrolling animation [CITATION_6], they struggle with tasks that require\ncomplex spatial reasoning, like object placement. Existing\nworks like [CITATION_7] have attempted to position objects in a scene\n\nby directly using LLMs through predicting the position and\norientation of objects as LLM outputs. More recent works\n[CITATION_8] have demonstrated the benefit of using LLMs\nto predict constraints instead, before using a solver to solve\nfor final object placements. However, despite being able\nto create large-scale scenes, they represent each object us-\ning bounding boxes, making it impossible to express fine-\ngrained constraints between parts of objects, leading to con-\nstraints that can only explain placements of box-like objects\n(as opposed to putting a book on a shelf, or a stuffed toy on\na chair with a backrest and armrests). This design choice is\nunderstandable, since parts of objects become increasingly\nhard for LLMs to reason about. FirePlace introduces a way\nto overcome this limitation.", "citation_info": {"content": [["Matterport3d: Learning arXiv preprint from rgb-d data in indoor environments", "3d-front: 3d furnished rooms with layouts In Proceedings of the IEEE/CVF Interna- and semantics", "In- finigen indoors: Photorealistic indoor scenes using procedu- ral generation", "Susskind", "Sun rgb-d: A rgb-d scene understanding benchmark suite", "Semantic scene com- In Proceedings of the pletion from a single depth image"], ["Example-based synthesis of 3d object arrangements", "Language-driven synthe- sis of 3d scenes from scene databases", "Atiss: Autoregres- sive transformers for indoor scene synthesis", "Fast and flex- ible indoor scene synthesis via deep convolutional genera- In Proceedings of the IEEE/CVF Conference tive models", "Planit: Planning and in- stantiating indoor scenes with relation graph and spatial prior networks", "Sceneformer: Indoor scene generation with transformers", "Lego-net: Learning regular rearrangements of ob- jects in rooms", "Llplace: The 3d in- door scene layout generation and editing via large language model"], ["Scalinggaus- sian: Enhancing 3d content creation with generative gaussian splatting", "Scene- dreamer: Unbounded 3d scene generation from 2d image collections", "Luciddreamer: Domain-free gen- arXiv preprint eration of 3d gaussian splatting scenes", "Scenescape: Text-driven consistent scene generation", "in seconds with latent diffusion models", "Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models", "Dreamwaltz-g: Expressive 3d gaussian avatars from skeleton-guided 2d diffusion", "Diffindscene: Diffusion-based 9 high-quality 3d indoor scene generation", "Semcity: Semantic scene genera- tion with triplane diffusion", "Art3d: 3d gaussian splatting for text-guided artistic scenes generation", "Infinite na- ture: Perpetual view generation of natural scenes from a sin- In Proceedings of the IEEE/CVF International gle image", "Compositional 3d scene generation using locally conditioned diffusion", "Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF Conference 3d cities", "Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models", "Wonderjourney: In Proceedings of Going from anywhere to everywhere", "Dreamscape: 3d scene creation via gaussian splatting joint arXiv preprint arXiv:2404.09227, correlation modeling"], ["Fatahalian", "Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions", "Blender- alchemy: Editing 3d graphics with vision-language models", "Re-thinking inverse graphics with large language models", "Any- home: Open-vocabulary generation of structured and tex- tured 3d homes"], ["Blender- alchemy: Editing 3d graphics with vision-language models"], ["Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions"], ["Fatahalian"], ["Layoutgpt: Compositional visual plan- ning and generation with large language models"], ["Open-universe indoor scene generation using llm program synthesis and uncurated object databases", "Scenecraft: An llm agent for synthesizing 3d scenes as blender code", "Scenemotifcoder: Example-driven visual program learning for generating 3d arXiv preprint arXiv:2408.02211, object arrangements", "Holodeck: Language guided genera- tion of 3d embodied ai environments"]], "positions": [114, 228, 689, 1030, 1214, 1224, 1254, 1364, 1528]}}
{"paper_id": "GENMO", "input_text": "2.1. Human Motion Generation\n\nHuman motion generation has progressed significantly in\nrecent years  leveraging a variety of conditioning sig-\nnals such as text , actions , speech ,\nmusic , and scenes/objects . Recently, multimodal motion generation has\nalso gained attention  enabling multiple input\nmodalities. However, most existing methods focus solely\non generative tasks without supporting estimation. For in-\nstance, the method  supports video input but treats it as\na generative task, resulting in motions that loosely imitate\nvideo content rather than precisely matching it. In contrast,\nour method jointly handles generation and estimation tasks,\nyielding more precise video-conditioned results.\n\nFor long-sequence motion generation, existing works\nmostly rely on ad-hoc post-processing techniques to stitch\nseparately generated fixed-length motions . In\ncontrast, our method introduces a novel diffusion-based ar-\nchitecture enabling seamless generation of arbitrary-length\nmotions conditioned on multiple modalities without com-\nplex post-processing.\n\nExisting datasets, such as AMASS , are limited in\nsize and diversity. To address the scarcity of 3D data,\nMotion-X  and MotionBank  augment datasets us-\ning 2D videos and 3D pose estimation models , but\nthe resulting motions often contain artifacts.\nIn contrast,\nour method directly leverages in-the-wild videos with 2D\nannotations without explicit 3D reconstruction, reducing re-\nliance on noisy data and enhancing robustness and diversity.\n\n2.2. Human Motion Estimation\n\nHuman pose estimation from images , videos , or even sparse marker data  has been\nstudied extensively in the literature. Recent works focus pri-\nmarily on estimating global human motion in world-space\ncoordinates . This is an inherently\nill-posed problem, hence these methods leverage generative\npriors and SLAM methods to constrain human and camera\nmotions, respectively. However, these methods typically in-\nvolve computationally expensive optimization or separate\npost-processing steps.\n\nMore recent approaches aim to estimate global human\nmotion in a feed-forward manner , offer-\ning faster solutions. Our method extends this direction by\njointly modeling generation and estimation within a uni-\nfied diffusion framework. This integration leverages shared\nrepresentations and generative priors during training to pro-\nduce more plausible estimations.", "expected_text": "2.1. Human Motion Generation\n\nHuman motion generation has progressed significantly in\nrecent years [CITATION_0] leveraging a variety of conditioning sig-\nnals such as text [CITATION_1], actions [CITATION_2], speech [CITATION_3],\nmusic [CITATION_4], and scenes/objects [CITATION_5]. Recently, multimodal motion generation has\nalso gained attention [CITATION_6] enabling multiple input\nmodalities. However, most existing methods focus solely\non generative tasks without supporting estimation. For in-\nstance, the method [CITATION_7] supports video input but treats it as\na generative task, resulting in motions that loosely imitate\nvideo content rather than precisely matching it. In contrast,\nour method jointly handles generation and estimation tasks,\nyielding more precise video-conditioned results.\n\nFor long-sequence motion generation, existing works\nmostly rely on ad-hoc post-processing techniques to stitch\nseparately generated fixed-length motions [CITATION_8]. In\ncontrast, our method introduces a novel diffusion-based ar-\nchitecture enabling seamless generation of arbitrary-length\nmotions conditioned on multiple modalities without com-\nplex post-processing.\n\nExisting datasets, such as AMASS [CITATION_9], are limited in\nsize and diversity. To address the scarcity of 3D data,\nMotion-X [CITATION_10] and MotionBank [CITATION_11] augment datasets us-\ning 2D videos and 3D pose estimation models [CITATION_12], but\nthe resulting motions often contain artifacts.\nIn contrast,\nour method directly leverages in-the-wild videos with 2D\nannotations without explicit 3D reconstruction, reducing re-\nliance on noisy data and enhancing robustness and diversity.\n\n2.2. Human Motion Estimation\n\nHuman pose estimation from images [CITATION_13], videos [CITATION_14], or even sparse marker data [CITATION_15] has been\nstudied extensively in the literature. Recent works focus pri-\nmarily on estimating global human motion in world-space\ncoordinates [CITATION_16]. This is an inherently\nill-posed problem, hence these methods leverage generative\npriors and SLAM methods to constrain human and camera\nmotions, respectively. However, these methods typically in-\nvolve computationally expensive optimization or separate\npost-processing steps.\n\nMore recent approaches aim to estimate global human\nmotion in a feed-forward manner [CITATION_17], offer-\ning faster solutions. Our method extends this direction by\njointly modeling generation and estimation within a uni-\nfied diffusion framework. This integration leverages shared\nrepresentations and generative priors during training to pro-\nduce more plausible estimations.", "citation_info": {"content": [["HP-GAN: Probabilistic 3D human motion prediction via GAN", "Implicit neural representations for variable length human motion generation", "Chopin, N", "Flexible motion in-betweening with diffusion models", "Motionlcm: Real-time controllable motion generation via latent consistency model", "Momask: Generative masked mod- eling of 3d human motions", "A recurrent variational autoen- coder for human motion synthesis", "Nemf: Neural motion fields for kinematic ani- mation", "MoGlow: Probabilistic and controllable motion synthesis using normalising flows", "Mmm: Generative masked motion model", "Mmm: Generative masked motion model", "Guibas", "Human motion gen- eration using wasserstein GAN", "Human motion dif- fusion model", "Transflower: probabilistic autoregressive dance gen- eration with multimodal attention", "ActFormer: A gan-based transformer towards general action-conditioned 3d human motion generation", "Motiondif- fuse: Text-driven human motion generation with diffusion model", "Re- modiffuse: Retrieval-augmented motion diffusion model", "Human motion generation: A survey"], ["Executing your commands via motion diffusion in latent space", "Synthesis of compositional animations from textual descriptions", "TM2T: Stochastic and tokenized modeling for the reciprocal gener- ation of 3d human motions and texts", "Act as you wish: Fine-grained control of mo- tion diffusion model with hierarchical semantic graphs"], ["Ac- tion2Motion: Conditioned generation of 3D human mo- In ACM International Conference on Multimedia tions"], ["Listen, denoise, action! audio-driven motion synthesis with diffusion models", "Taming diffusion models for audio-driven co-speech gesture generation"], ["Ai choreographer: Music conditioned 3d dance generation with aist++", "Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory", "You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection", "Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis", "Edge: In Proceedings of Editable dance generation from music", "Transflower: probabilistic autoregressive dance gen- eration with multimodal attention"], ["Stochastic scene-aware motion prediction", "NIFTY: Neural object interaction fields for guided human motion synthesis", "HUMANISE: Language-conditioned hu- man motion generation in 3d scenes", "Generating human interaction motions in scenes with text control", "Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P\u00b4erez Pellitero, and Gerard Pons-Moll"], ["Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls", "M3gpt: An ad- vanced multimodal, multitask framework for motion com- prehension and generation", "Large motion model for unified multi-modal motion generation", "Ude: A unified driving engine for human motion generation"], ["Large motion model for unified multi-modal motion generation"], ["Black, and G\u00a8ul Varol", "Black, G\u00a8ul Varol, Xue Bin Peng, and Davis Rempe", "Hauptmann, and Jungdam Won", "Diffcollage: Parallel generation of large content with diffusion models"], ["Troje, Ger- ard Pons-Moll, and Michael J"], ["Motion-x: A large- scale 3d expressive whole-body human motion dataset"], ["Mo- tionbank: A large-scale video motion benchmark with disen- tangled rule-based annotations, 2024"], ["Wham: Reconstructing world-grounded humans with accu- rate 3d motion", "Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras"], ["Black, David W", "Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation", "Neural localizer fields for continuous 3d human pose and shape estimation"], ["Be- yond static features for temporally consistent 3d human pose and shape from a video", "Humans in 4d: Re- In constructing and tracking humans with transformers", "VIBE: Video inference for human body pose and shape estimation"], ["Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head- mounted camera", "Sparseposer: Real-time full- body motion reconstruction from sparse data", "Transpose: Real-time 3d human translation and pose estimation with six inertial sensors"], ["Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal", "Coin: Control-inpainting diffusion prior for human and camera In ECCV, pages 426\u2013446", "Wham: Reconstructing world-grounded humans with accu- rate 3d motion", "Tram: Global trajectory and motion of 3d humans from in- the-wild videos", "Decoupling human and camera motion from videos in the wild", "Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras"], ["World-grounded human motion recovery via gravity-view coordinates", "Wham: Reconstructing world-grounded humans with accu- rate 3d motion", "Tram: Global trajectory and motion of 3d humans from in- the-wild videos", "Rohm: Robust human motion reconstruction via diffusion"]], "positions": [99, 160, 170, 179, 187, 208, 275, 434, 859, 1096, 1178, 1194, 1260, 1570, 1579, 1608, 1749, 2111]}}
{"paper_id": "SceneCentric", "input_text": "Approaches for unsupervised segmentation tasks have been\nsignificantly influenced by the literature on self-supervised\nlearning (SSL) and low-level vision tasks (e.g., optical flow\nestimation), which we review first.\nSelf-supervised representation learning focuses on learn-\ning generic feature extractors from unlabeled data, aiming\nfor expressive features that facilitate a broad range of down-\nstream tasks . To that end, various self-supervised pre-\ntext tasks have been proposed . The development\nof Vision Transformers (ViTs)  shaped current pretext\ntasks while allowing for data-scalable training .\nCurrent approaches typically train ViTs on contrastive , negative-free , clustering-\nbased , or masked modeling  pretext\ntasks. Recent state-of-the-art models (e.g., DINO ) of-\nfer semantically rich and dense features suitable for unsu-\npervised scene understanding .\n\nUnsupervised optical flow is concerned with learning op-\ntical flow estimation without the need for ground-truth data.\nWhile early deep networks relied on synthetic ground-truth\nflow for supervision , the domain gap to real\nvideos, among other factors, has prompted the development\nof unsupervised deep optical flow pipelines . Current unsupervised optical flow methods (e.g.,\nSMURF ) offer accurate flow estimates, fast inference,\nand generalization to various real-world domains.\n\nUnsupervised instance segmentation aims to discover and\nsegment object instances in images . Recent work\n bootstraps class-agnostic instance segmen-\ntation networks using pseudo labels extracted from SSL fea-\ntures on object-centric data. TokenCut  applies normal-\nized cuts [N-Cut, 61] to DINO features, providing a fore-\nground pseudo mask. CutLER  proposes MaskCut by\niteratively applying N-Cuts, retrieving up to three pseudo\nmasks per image. A second stream of works uses motion\ncues to obtain an unsupervised signal for object discov-\nery . SF2SE3  clusters scene\nflow from consecutive stereo frames into independent rigid\nobject motions in SE (3) space, improving object segmen-\ntation and motion accuracy. MOD-UV  uses motion\nsegmentation for pseudo labeling and multi-stage training.\n\nUnsupervised semantic segmentation is approached by\nearly deep learning methods via representation learning\n. STEGO  leverages the self-supervised\nDINO features as an inductive prior and distills the fea-\ntures into a lower-dimensional space before unsupervised\nprobing. Later,  proposed improvements to the\nfeature distillation or probing . DepthG  extends\nSTEGO by spatially correlating the feature maps with depth\nmaps and furthest point sampling in the contrastive loss.\nDiffSeg  utilizes Stable Diffusion  and iterative at-\ntention merging for unsupervised semantic segmentation.\n\nUnsupervised panoptic segmentation is a nascent re-\nsearch avenue following recent advancements in unsuper-\nvised semantic and instance segmentation. To the best of\nour knowledge, U2Seg  is the only method to date to approach unsupervised panoptic segmentation. U2Seg\nleverages STEGO  and CutLER  to create panoptic\npseudo labels for training a panoptic network. However, its\ndependence on CutLER\u2019s MaskCut approach significantly\nIn contrast, we\nlimits its accuracy on scene-centric data.\npresent the first unsupervised panoptic approach that learns\ndirectly from scene-centric data, addressing key limitations\nof U2Seg and MaskCut.\n", "expected_text": "Approaches for unsupervised segmentation tasks have been\nsignificantly influenced by the literature on self-supervised\nlearning (SSL) and low-level vision tasks (e.g., optical flow\nestimation), which we review first.\nSelf-supervised representation learning focuses on learn-\ning generic feature extractors from unlabeled data, aiming\nfor expressive features that facilitate a broad range of down-\nstream tasks [CITATION_0]. To that end, various self-supervised pre-\ntext tasks have been proposed [CITATION_1]. The development\nof Vision Transformers (ViTs) [CITATION_2] shaped current pretext\ntasks while allowing for data-scalable training [CITATION_3].\nCurrent approaches typically train ViTs on contrastive [CITATION_4], negative-free [CITATION_5], clustering-\nbased [CITATION_6], or masked modeling [CITATION_7] pretext\ntasks. Recent state-of-the-art models (e.g., DINO [CITATION_8]) of-\nfer semantically rich and dense features suitable for unsu-\npervised scene understanding [CITATION_9].\n\nUnsupervised optical flow is concerned with learning op-\ntical flow estimation without the need for ground-truth data.\nWhile early deep networks relied on synthetic ground-truth\nflow for supervision [CITATION_10], the domain gap to real\nvideos, among other factors, has prompted the development\nof unsupervised deep optical flow pipelines [CITATION_11]. Current unsupervised optical flow methods (e.g.,\nSMURF [CITATION_12]) offer accurate flow estimates, fast inference,\nand generalization to various real-world domains.\n\nUnsupervised instance segmentation aims to discover and\nsegment object instances in images [CITATION_13]. Recent work\n[CITATION_14] bootstraps class-agnostic instance segmen-\ntation networks using pseudo labels extracted from SSL fea-\ntures on object-centric data. TokenCut [CITATION_15] applies normal-\nized cuts [N-Cut, 61] to DINO features, providing a fore-\nground pseudo mask. CutLER [CITATION_16] proposes MaskCut by\niteratively applying N-Cuts, retrieving up to three pseudo\nmasks per image. A second stream of works uses motion\ncues to obtain an unsupervised signal for object discov-\nery [CITATION_17]. SF2SE3 [CITATION_18] clusters scene\nflow from consecutive stereo frames into independent rigid\nobject motions in SE (3) space, improving object segmen-\ntation and motion accuracy. MOD-UV [CITATION_19] uses motion\nsegmentation for pseudo labeling and multi-stage training.\n\nUnsupervised semantic segmentation is approached by\nearly deep learning methods via representation learning\n[CITATION_20]. STEGO [CITATION_21] leverages the self-supervised\nDINO features as an inductive prior and distills the fea-\ntures into a lower-dimensional space before unsupervised\nprobing. Later, [CITATION_22] proposed improvements to the\nfeature distillation or probing [CITATION_23]. DepthG [CITATION_24] extends\nSTEGO by spatially correlating the feature maps with depth\nmaps and furthest point sampling in the contrastive loss.\nDiffSeg [CITATION_25] utilizes Stable Diffusion [CITATION_26] and iterative at-\ntention merging for unsupervised semantic segmentation.\n\nUnsupervised panoptic segmentation is a nascent re-\nsearch avenue following recent advancements in unsuper-\nvised semantic and instance segmentation. To the best of\nour knowledge, U2Seg [CITATION_27] is the only method to date to approach unsupervised panoptic segmentation. U2Seg\nleverages STEGO [CITATION_28] and CutLER [CITATION_29] to create panoptic\npseudo labels for training a panoptic network. However, its\ndependence on CutLER\u2019s MaskCut approach significantly\nIn contrast, we\nlimits its accuracy on scene-centric data.\npresent the first unsupervised panoptic approach that learns\ndirectly from scene-centric data, addressing key limitations\nof U2Seg and MaskCut.\n", "citation_info": {"content": [["Hospedales"], ["Survey on self-supervised learning: Auxil- iary pretext tasks and contrastive learning methods in imag- ing", "Hospedales"], ["An image is worth 16\u00d716 words: Transformers for image recognition at scale"], ["for two epochs on ImageNet", "Masked autoencoders are scalable vision learners"], ["Devon Hjelm, and William Buchwalter", "40 GB", "An empiri- cal study of training self-supervised vision transformers", "Momentum contrast for unsupervised visual rep- resentation learning"], ["VICRegL: Self-supervised learning of local visual features", "for two epochs on ImageNet", "Exploring simple Siamese In CVPR, pages 15750\u201315758, representation learning", "Bootstrap your own latent: A new approach to self-supervised learning", "Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, 10 et al"], ["Self-labelling via simultaneous clustering and rep- resentation learning", "Deep clustering for unsupervised learning of visual features", "Unsupervised learn- ing of visual features by contrasting cluster assignments"], ["Siamese In NeurIPS, pages 40676\u201340693, masked autoencoders", "Masked autoencoders are scalable vision learners", "Oswald, Alexander Kirillov, Cees G"], ["for two epochs on ImageNet"], ["Freeman", "and U2Seg"], ["d", "A large dataset to train convolutional networks for disparity, In CVPR, pages optical flow, and scene flow estimation", "PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume", "a supervised ana- log of SMURF"], ["Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova", "Cost function unrolling in un- IEEE Trans", "BrightFlow: Brightness-change-aware un- supervised learning of optical flow", "UnFlow: Un- supervised learning of optical flow with a bidirectional cen- sus loss", "Yu, Adam W"], ["unsupervised"], ["Unsupervised object localization in the era of self-supervised ViTs: A survey"], ["Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick P\u00e9rez, Renaud Mar- let, and Jean Ponce", "Discovering object masks with transformers for unsupervised semantic segmentation", "\u00c1lvarez", "and U2Seg", "Segment In NeurIPS, pages 138731\u2013 anything without supervision"], ["Crowley, and Dominique Vaufrey- daz"], ["and U2Seg"], ["Guess What Moves: Unsupervised video and image segmentation by anticipating motion", "Unsupervised multi- object segmentation by predicting probable motion patterns", "The emergence of objectness: Learning zero-shot segmentation from videos", "Multi-object discovery by low-dimensional object motion", "MOD-UV: Learning In ECCV, mobile object detectors from unlabeled videos", "DyStaB: Unsupervised object segmentation via dynamic-static boot- strapping"], ["3"], ["MOD-UV: Learning In ECCV, mobile object detectors from unlabeled videos"], ["PiCIE: Unsupervised semantic segmentation us- In CVPR, ing invariance and equivariance in clustering", "InfoSeg: Unsuper- vised semantic image segmentation with mutual information maximization", "Henriques, and Andrea Vedaldi"], ["Freeman"], ["EAGLE: Eigen aggregation learning for object- In CVPR, centric unsupervised semantic segmentation", "Leveraging hidden positives for unsupervised semantic segmentation", "+ CutLER"], ["Boosting unsupervised semantic segmentation with principal mask proposals"], ["+ CutLER"], ["Diffuse, attend, and segment:"], ["High-resolution image syn- thesis with latent diffusion models"], ["7"], ["Freeman"], ["and U2Seg"]], "positions": [410, 484, 532, 604, 661, 677, 697, 718, 777, 872, 1074, 1201, 1258, 1449, 1463, 1606, 1708, 1903, 1912, 2079, 2260, 2268, 2430, 2492, 2501, 2635, 2662, 2924, 3022, 3034]}}
{"paper_id": "sugar", "input_text": "Image-based rendering (IBR) methods rely on a set of two-\ndimensional images of a scene to generate a representation\nof the scene and render novel views. The very first novel-\nview synthesis approaches were based on light fields ,\nand developed the concept of volume rendering for novel\nviews. Their work emphasized the importance of efficiently\ntraversing volumetric data to produce realistic images.\n\nVarious scene representations have been proposed since,\nsuch as triangle meshes, point clouds, voxel grids, multi-\nplane images, or neural implicit functions.\n\nTraditional mesh-based IBR methods. Structure-from-\nmotion (SfM)\n and subsequent multi-view stereo\n(MVS)  allow for 3D reconstruction of surfaces, lead-\ning to the development of several view synthesis algorithms\nrelying on triangle meshes as the primary 3D representa-\ntion of scenes. Such algorithms consider textured triangles\nor warp and blend captured images on the mesh surface to\ngenerate novel views .\n consider deep\nlearning-based mesh representations for better view synthe-\nsis, bridging the gap between traditional graphics and mod-\nern machine learning techniques. While these mesh-based\nmethods take advantage of existing graphics hardware and\nsoftware for efficient rendering, they struggle with the cap-\nture of accurate geometry and appearance in complex re-\ngions.\n\nVolumetric IBR methods. Volumetric methods use voxel\ngrids, multiplane images, or neural networks to represent\nscenes as continuous volumetric functions of density and\ncolor. Recently, Neural Radiance Fields (NeRF)  intro-\nduced a novel scene representation based on a continuous\nvolumetric function parameterized by a multilayer percep-\ntron (MLP). NeRF produces photorealistic renderings with\nfine details and view-dependent effects, achieved through\nvolumetric ray tracing. However, the original NeRF is com-\nputationally expensive and memory intensive.\n\nTo address these challenges, several works have im-\nproved NeRF\u2019s performance and scalability. These meth-\nods leverage discretized or sparse volumetric representa-\ntions like voxel grids and hash tables as ways to store\nlearnable features acting as positional encodings for 3D\npoints , hierarchical sampling strate-\ngies , or low-rank approximations . How-\never, they still rely on volumetric ray marching, which\nis incompatible with standard graphics hardware and soft-\nware designed for rendering polygonal surfaces. Recent\nworks have proposed modifying the NeRF\u2019s representation\nof geometry and emitted radiance to allow for better recon-\nstruction of specular materials  or relighting the scene\nthrough an explicit decomposition into material and lighting\nproperties .\n\nHybrid IBR methods. Some methods build on differen-\ntiable rendering to combine the advantages of mesh-based\nand volumetric methods, and allow for surface reconstruc-\ntion as well as better editability. They use a hybrid volume-\nsurface representation, which enables high-quality meshes\nsuitable for downstream graphics applications while effi-\nciently modeling view-dependent appearance.\nIn partic-\nular, some works optimize neural signed distance func-\ntions (SDF) by training neural radiance fields in which the\ndensity is derived as a differentiable transformation of the\nSDF . A triangle mesh can finally\nbe reconstructed from the SDF by applying the Marching\nCubes algorithm . However, most of these methods do\nnot target real-time rendering.\n\nAlternatively, other approaches \u201cbake\u201d the rendering ca-\npacity of an optimized NeRF or neural SDF into a much ef-\nficient structure relying on an underlying triangle mesh \nthat could benefit from the traditional triangle rasteriza-\ntion pipeline. In particular, the recent BakedSDF  re-\nconstructs high quality meshes by optimizing a full neural\nSDF model, baking it into a high-resolution triangle mesh\nthat combines mesh rendering for interpolating features and\ndeep learning to translate these features into images, and\nfinally optimizes a view-dependent appearance model.\n\nHowever, even though it achieves real-time rendering\nand produces impressive meshes of the surface of the scene,\nthis model demands training a full neural SDF with an ar-\nchitecture identical to Mip-NeRF360 , which necessi-\ntates 48 hours of training.\n\nSimilarly, the recent method NeRFMeshing  pro-\nposes to also bake any NeRF model into a mesh structure,\nachieving real-time rendering. However, the meshing per-\nformed in this method lowers the quality of the rendering\nand results in a PSNR much lower than our method. Ad-\nditionally, this method still requires training a full NeRF\nmodel beforehand, and needs approximately an hour of\ntraining on 8 V100 NVIDIA GPUs to allow for mesh train-\ning and extraction.\n\nOur method is much faster at retrieveing a 3D mesh from\n3D Gaussian Splatting, which is itself much faster than\nNeRFs. As our experiments show, our rendering done by\nbounding Gaussians to the mesh results in higher quality\nthan previous solutions based on meshes.\n\nPoint-based IBR methods. Alternatively, point-based\nrepresentations for radiance field excel at modeling thin ge-\nometry and leverage fast point rasterization pipelines to ren-\nder images using \u03b1-blending rather than ray-marching .\nIn particular, the very recent 3D Gaussian Splatting.\nmodel  allows for optimizing and rendering scenes with\nspeed and quality never seen before.", "expected_text": "Image-based rendering (IBR) methods rely on a set of two-\ndimensional images of a scene to generate a representation\nof the scene and render novel views. The very first novel-\nview synthesis approaches were based on light fields [CITATION_0],\nand developed the concept of volume rendering for novel\nviews. Their work emphasized the importance of efficiently\ntraversing volumetric data to produce realistic images.\n\nVarious scene representations have been proposed since,\nsuch as triangle meshes, point clouds, voxel grids, multi-\nplane images, or neural implicit functions.\n\nTraditional mesh-based IBR methods. Structure-from-\nmotion (SfM)\n[CITATION_1] and subsequent multi-view stereo\n(MVS) [CITATION_2] allow for 3D reconstruction of surfaces, lead-\ning to the development of several view synthesis algorithms\nrelying on triangle meshes as the primary 3D representa-\ntion of scenes. Such algorithms consider textured triangles\nor warp and blend captured images on the mesh surface to\ngenerate novel views [CITATION_3].\n[CITATION_4] consider deep\nlearning-based mesh representations for better view synthe-\nsis, bridging the gap between traditional graphics and mod-\nern machine learning techniques. While these mesh-based\nmethods take advantage of existing graphics hardware and\nsoftware for efficient rendering, they struggle with the cap-\nture of accurate geometry and appearance in complex re-\ngions.\n\nVolumetric IBR methods. Volumetric methods use voxel\ngrids, multiplane images, or neural networks to represent\nscenes as continuous volumetric functions of density and\ncolor. Recently, Neural Radiance Fields (NeRF) [CITATION_5] intro-\nduced a novel scene representation based on a continuous\nvolumetric function parameterized by a multilayer percep-\ntron (MLP). NeRF produces photorealistic renderings with\nfine details and view-dependent effects, achieved through\nvolumetric ray tracing. However, the original NeRF is com-\nputationally expensive and memory intensive.\n\nTo address these challenges, several works have im-\nproved NeRF\u2019s performance and scalability. These meth-\nods leverage discretized or sparse volumetric representa-\ntions like voxel grids and hash tables as ways to store\nlearnable features acting as positional encodings for 3D\npoints [CITATION_6], hierarchical sampling strate-\ngies [CITATION_7], or low-rank approximations [CITATION_8]. How-\never, they still rely on volumetric ray marching, which\nis incompatible with standard graphics hardware and soft-\nware designed for rendering polygonal surfaces. Recent\nworks have proposed modifying the NeRF\u2019s representation\nof geometry and emitted radiance to allow for better recon-\nstruction of specular materials [CITATION_9] or relighting the scene\nthrough an explicit decomposition into material and lighting\nproperties [CITATION_10].\n\nHybrid IBR methods. Some methods build on differen-\ntiable rendering to combine the advantages of mesh-based\nand volumetric methods, and allow for surface reconstruc-\ntion as well as better editability. They use a hybrid volume-\nsurface representation, which enables high-quality meshes\nsuitable for downstream graphics applications while effi-\nciently modeling view-dependent appearance.\nIn partic-\nular, some works optimize neural signed distance func-\ntions (SDF) by training neural radiance fields in which the\ndensity is derived as a differentiable transformation of the\nSDF [CITATION_11]. A triangle mesh can finally\nbe reconstructed from the SDF by applying the Marching\nCubes algorithm [CITATION_12]. However, most of these methods do\nnot target real-time rendering.\n\nAlternatively, other approaches \u201cbake\u201d the rendering ca-\npacity of an optimized NeRF or neural SDF into a much ef-\nficient structure relying on an underlying triangle mesh [CITATION_13]\nthat could benefit from the traditional triangle rasteriza-\ntion pipeline. In particular, the recent BakedSDF [CITATION_14] re-\nconstructs high quality meshes by optimizing a full neural\nSDF model, baking it into a high-resolution triangle mesh\nthat combines mesh rendering for interpolating features and\ndeep learning to translate these features into images, and\nfinally optimizes a view-dependent appearance model.\n\nHowever, even though it achieves real-time rendering\nand produces impressive meshes of the surface of the scene,\nthis model demands training a full neural SDF with an ar-\nchitecture identical to Mip-NeRF360 [CITATION_15], which necessi-\ntates 48 hours of training.\n\nSimilarly, the recent method NeRFMeshing [CITATION_16] pro-\nposes to also bake any NeRF model into a mesh structure,\nachieving real-time rendering. However, the meshing per-\nformed in this method lowers the quality of the rendering\nand results in a PSNR much lower than our method. Ad-\nditionally, this method still requires training a full NeRF\nmodel beforehand, and needs approximately an hour of\ntraining on 8 V100 NVIDIA GPUs to allow for mesh train-\ning and extraction.\n\nOur method is much faster at retrieveing a 3D mesh from\n3D Gaussian Splatting, which is itself much faster than\nNeRFs. As our experiments show, our rendering done by\nbounding Gaussians to the mesh results in higher quality\nthan previous solutions based on meshes.\n\nPoint-based IBR methods. Alternatively, point-based\nrepresentations for radiance field excel at modeling thin ge-\nometry and leverage fast point rasterization pipelines to ren-\nder images using \u03b1-blending rather than ray-marching [CITATION_17].\nIn particular, the very recent 3D Gaussian Splatting.\nmodel [CITATION_18] allows for optimizing and rendering scenes with\nspeed and quality never seen before.", "citation_info": {"content": [["Light Field Rendering"], ["Seitz, and Richard Szeliski"], ["Multi-View Stereo for Community Photo Collections"], ["Unstructured Lumigraph Ren- dering", "Deep Blending for Free-Viewpoint Image-Based Rendering", "Wood, Daniel I"], ["Free View Synthesis", "Stable View Synthesis"], ["Srinivasan, Matthew Tancik, Jonathan T"], ["TensoRF: Tensorial Radiance Fields", "ReLU Fields: The Little Non-Linearity That Could", "Instant Neural Graphics Primitives with a Mul- tiresolution Hash Encoding", "Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance In Conference on Computer Vision Fields Reconstruction", "Plenoxels: In Conference Radiance Fields Without Neural Networks"], ["Barron", "Srinivasan", "KiloNeRF: Speeding Up Neural Radiance Fields with Thousands of Tiny MLPs", "PlenOctrees For Real-Time Rendering of Neural Radiance Fields"], ["TensoRF: Tensorial Radiance Fields"], ["Barron, and Pratul P"], ["Bar- ron, Ce Liu, and Hendrik P", "NeROIC: Neural Rendering of Objects from Online Image Collections", "Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T", "PhySG: Inverse Rendering with Spherical Gaussians for Physics-Based Material Editing and Relight- ing"], ["NeuMesh: Learning Disentangled Neural Mesh-Based Im- plicit Field for Geometry and Texture Editing", "Improving Neural Im- plicit Surfaces Geometry with Patch Warping", "Tay- lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin", "UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction", "NeuS: Learning Neural Im- plicit Surfaces by Volume Rendering for Multi-View Recon- In Advances in Neural Information Processing struction", "Vol- ume Rendering of Neural Implicit Surfaces"], ["Lorensen and Harvey E"], ["MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures"], ["Srinivasan, Richard Szeliski, and Jonathan T"], ["Barron"], ["NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes"], ["Point-Based Neural Rendering with Per- View Optimization", "ADOP: Approximate Differentiable One-Pixel Point Ren- dering"], ["3D Gaussian Splatting for Real-Time Radiance Field Rendering"]], "positions": [229, 628, 668, 971, 973, 1562, 2190, 2227, 2256, 2580, 2677, 3260, 3361, 3602, 3713, 4215, 4302, 5219, 5281]}}
{"paper_id": "atte", "input_text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n, ByteNet  and ConvS2S , all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions . In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations .\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks .\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as  and .", "expected_text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[CITATION_0], ByteNet [CITATION_1] and ConvS2S [CITATION_2], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [CITATION_3]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [CITATION_4].\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [CITATION_5].\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [CITATION_6] and [CITATION_7].", "citation_info": {"content": [["NIPS"], ["Neural machine translation in linear time"], ["Dauphin"], ["Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."], ["Long short-term memory-networks for machine reading", "A decomposable attention model", "A deep reinforced model for abstractive summarization", "A structured self-attentive sentence embedding"], ["End-to-end memory networks"], ["Neural GPUs learn algorithms", "Neural machine translation in linear time"], ["Dauphin"]], "positions": [97, 107, 120, 561, 1189, 1406, 1767, 1772]}}
{"paper_id": "InsightV", "input_text": "2.1. Vision-Language Reasoning\n\nRecent advancements in MLLMs \nhave equipped these models with robust capabilities across\ndiverse domains, including visual understanding ,\nmathematics , college-level questions , and scien-\ntific inquiries. In visual understanding, most research  emphasizes fine-grained detail analysis\nand localization, training models to perform visual reason-\ning with tailored datasets to enhance interpretive capabili-\nties. For mathematics and expert-level reasoning, existing\nmethods  predominantly derive from Chain-of-\nThought  approaches, training MLLMs to generate step-\nby-step reasoning across various subjects. However, these\napproaches often focus primarily on improving dataset qual-\nity through Chain-of-Thought, overlooking the importance\nof structured reasoning paths and extended reasoning chains\nin advancing model reasoning capabilities. Additionally,\nsignificant challenges arise when relying on a single model\nto manage the entire reasoning process for complex tasks,\nunderscoring the need for a multi-agent system to decom-\npose and enhance this process. In this work, we tackle these\nchallenges by introducing a scalable reasoning data gener-\nation pipeline and implementing a multi-agent system for\nreasoning and summarization decomposition, enhancing the\noverall reasoning capabilities of existing MLLMs.\n\n2.2. Vision-Language Alignment\n\nTo align the model more closely with human preferences,\nseveral alignment techniques are employed for MLLMs. A\nwidely used approach is Reinforcement Learning from Hu-\nman Feedback  (RLHF), which iteratively refines the\nmodel\u2019s responses based on human feedback, enhancing\nboth response quality and interpretability. To further improve\nMLLM capabilities, Direct Preference Optimization \n(DPO) is introduced to simplify the alignment process. By di-\nrectly training on human preference data, DPO optimizes the\nmodel\u2019s outputs to better match human-selected responses.\nHowever, traditional DPO is primarily focused on offline\nscenarios, and as the model evolves, the effectiveness of\nthis approach may significantly diminish. To address this,\nIterative DPO  has been proposed, which optimizes pref-\n\n2\n\n\ference pairs through DPO at each iteration. It then generates\nnew preference pairs for the next iteration using the updated\nmodel and evaluates them with a reward model. In this pa-\nper, we use iterative DPO to achieve stronger alignment and\nenhance the model\u2019s reasoning capabilities.", "expected_text": "2.1. Vision-Language Reasoning\n\nRecent advancements in MLLMs [CITATION_0]\nhave equipped these models with robust capabilities across\ndiverse domains, including visual understanding [CITATION_1],\nmathematics [CITATION_2], college-level questions [CITATION_3], and scien-\ntific inquiries. In visual understanding, most research [CITATION_4] emphasizes fine-grained detail analysis\nand localization, training models to perform visual reason-\ning with tailored datasets to enhance interpretive capabili-\nties. For mathematics and expert-level reasoning, existing\nmethods [CITATION_5] predominantly derive from Chain-of-\nThought [CITATION_6] approaches, training MLLMs to generate step-\nby-step reasoning across various subjects. However, these\napproaches often focus primarily on improving dataset qual-\nity through Chain-of-Thought, overlooking the importance\nof structured reasoning paths and extended reasoning chains\nin advancing model reasoning capabilities. Additionally,\nsignificant challenges arise when relying on a single model\nto manage the entire reasoning process for complex tasks,\nunderscoring the need for a multi-agent system to decom-\npose and enhance this process. In this work, we tackle these\nchallenges by introducing a scalable reasoning data gener-\nation pipeline and implementing a multi-agent system for\nreasoning and summarization decomposition, enhancing the\noverall reasoning capabilities of existing MLLMs.\n\n2.2. Vision-Language Alignment\n\nTo align the model more closely with human preferences,\nseveral alignment techniques are employed for MLLMs. A\nwidely used approach is Reinforcement Learning from Hu-\nman Feedback [CITATION_7] (RLHF), which iteratively refines the\nmodel\u2019s responses based on human feedback, enhancing\nboth response quality and interpretability. To further improve\nMLLM capabilities, Direct Preference Optimization [CITATION_8]\n(DPO) is introduced to simplify the alignment process. By di-\nrectly training on human preference data, DPO optimizes the\nmodel\u2019s outputs to better match human-selected responses.\nHowever, traditional DPO is primarily focused on offline\nscenarios, and as the model evolves, the effectiveness of\nthis approach may significantly diminish. To address this,\nIterative DPO [CITATION_9] has been proposed, which optimizes pref-\n\n2\n\n\ference pairs through DPO at each iteration. It then generates\nnew preference pairs for the next iteration using the updated\nmodel and evaluates them with a reward model. In this pa-\nper, we use iterative DPO to achieve stronger alignment and\nenhance the model\u2019s reasoning capabilities.", "citation_info": {"content": [["Qwen-vl: A versatile vision-language model for understand- ing, localization, text reading, and beyond", "Llava-onevision: Easy visual task transfer", "Vila: On pre-training for visual language models, 2023", "Improved baselines with visual instruction tuning", "Llava-next: Improved reason- ing, ocr, and world knowledge, 2024", "Visual instruction tuning", "Oryx mllm: On-demand spatial- temporal understanding at arbitrary resolution", "Deepseek-vl: towards real-world vision-language understanding", "Qwen2-vl: To see the world more clearly"], ["Vila: On pre-training for visual language models, 2023", "Qwen2-vl: To see the world more clearly"], ["Unimath: A foundational and multimodal mathe- matical reasoner"], ["Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks"], ["Mon- key: Image resolution and text label are important things for large multi-modal models", "Llava-next: Improved reason- ing, ocr, and world knowledge, 2024", "Chain-of-spot: Interactive reasoning improves large vision-language models", "Cambrian-1: A fully open, vision-centric exploration of multimodal llms", "Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images"], ["G-llava: Solving geometric prob- lem with multi-modal large language model", "Mavis: Mathematical visual in- struction tuning", "Improve vision language model chain-of-thought reasoning"], ["Chain-of- thought prompting elicits reasoning in large language models"], ["Training a helpful and harmless assistant with reinforcement learning from human feedback"], ["Direct prefer- ence optimization: Your language model is secretly a reward model"], ["Self-play fine-tuning converts weak lan- guage models to strong language models"]], "positions": [61, 169, 183, 209, 278, 507, 552, 1562, 1767, 2136]}}
{"paper_id": "3dmvp", "input_text": "Our work builds upon several active areas of research,\nincluding self-supervised learning, visual pretraining for\nrobotics, and learning robotic manipulation from demon-\nstrations.\n\nSelf-supervised learning. Self-supervised learning aims to\nlearn useful representations from unlabeled data by solving\npretext tasks that do not require manual annotation. Early\nwork in this area focused on designing pretext tasks for 2D\nimages, such as solving jagsaw puzzles , constrastive\nlearning  or joint embedding approaches . Most related to our work is the masked autoencoder\n(MAE) approach proposed by He et al. , which learns\nto reconstruct randomly masked patches in an image. MAE\nhas been shown to learn transferable representations for ob-\nject detection and segmentation tasks. Furthermore, Bach-\nmann et al demonstrates MAE pretraining can be extended\nto different modalities such as semantics and depth . In\nthis work, we extend the MAE approach to multi-view 3D\nscenes, enabling us to learn 3D-aware representations that\nare useful for robotic manipulation tasks. Unlike Multi-\nMAE which learns semantics and depth through direct su-\npervision, 3D-MVP aims to learn a 3D-aware representa-\ntion from multi view images.\n\nVisual pretraining for Robotics. Visual pretraining has\ndemonstrated impressive generalization ability on computer\nvision tasks. Therefore, prior works have explored whether\nit works for robotics tasks as well. Specifically, the robotics\ncommunity has trended towards learning representations us-\ning state-of-the-art self-supervised vision algorithms on di-\nverse interaction datasets , and finetune the net-\nwork on robotics tasks . 3D-MVP\nfollows the same procedure. However, existing robotics\npretraining approaches typically learn a 2D visual encoder\n(e.g. ResNet  or ViT ), we find they are inferior than\nmanipulation policies which do explicit 3D modeling (e.g.\nRVT , Act3D ). Migrating a pretrained ViT to 3D\nmanipulation policies is nontrivial since they do not have\na 2D visual encoder. In this paper, we propose 3D-MVP,\nwhich does 3D-aware pretraining on 3D manipulation poli-\ncies, to fill the gap.\n\nLearning manipulation from demonstrations. Recent\nwork has explored using transformers for multi-task ma-\nnipulation policies that predict robot actions from visual\nand language inputs . End-to-end mod-\nels like RT-1 , GATO , and InstructRL  directly\npredict 6-DoF end-effector poses but require many demon-\n\n2\n\n\fstrations to learn spatial reasoning and generalize to new\nscenes. To better handle 3D scenes, PerAct  and C2F-\nARM  voxelize the workspace and detect the 3D voxel\ncontaining the next end-effector pose. However, precise\npose prediction requires high-resolution voxels which are\ncomputationally expensive. Recently, RVT  proposes\na multi-view transformer that attends over point cloud fea-\ntures from multiple camera views to predict actions. This\navoids explicit voxelization and enables faster training and\ninference than PerAct. Act3D  represents the scene as a\ncontinuous 3D feature field and samples points to featurize\nwith attention, allowing adaptive resolution. GNFactor \njointly optimizes a generalizable neural field for reconstruc-\ntion and a Perceiver for decision-making. In contrast, our\nproposed 3D-MVP learns 3D scene representations through\nmasked autoencoding pretraining on a large dataset of 3D\nobject models. This pretraining enables 3D-MVP to build\na rich understanding of 3D geometry and semantics prior\nto finetuning on downstream manipulation tasks. Com-\npared to RVT and Act3D which train from scratch on tar-\nget tasks, 3D-MVP\u2019s pretraining leads to improved perfor-\nmance, sample efficiency and generalization. Unlike GN-\nFactor which relies on a pretrained VLM to inject seman-\ntics, 3D-MVP directly learns 3D semantic features from ob-\nject models.", "expected_text": "Our work builds upon several active areas of research,\nincluding self-supervised learning, visual pretraining for\nrobotics, and learning robotic manipulation from demon-\nstrations.\n\nSelf-supervised learning. Self-supervised learning aims to\nlearn useful representations from unlabeled data by solving\npretext tasks that do not require manual annotation. Early\nwork in this area focused on designing pretext tasks for 2D\nimages, such as solving jagsaw puzzles [CITATION_0], constrastive\nlearning [CITATION_1] or joint embedding approaches [CITATION_2]. Most related to our work is the masked autoencoder\n(MAE) approach proposed by He et al. [CITATION_3], which learns\nto reconstruct randomly masked patches in an image. MAE\nhas been shown to learn transferable representations for ob-\nject detection and segmentation tasks. Furthermore, Bach-\nmann et al demonstrates MAE pretraining can be extended\nto different modalities such as semantics and depth [CITATION_4]. In\nthis work, we extend the MAE approach to multi-view 3D\nscenes, enabling us to learn 3D-aware representations that\nare useful for robotic manipulation tasks. Unlike Multi-\nMAE which learns semantics and depth through direct su-\npervision, 3D-MVP aims to learn a 3D-aware representa-\ntion from multi view images.\n\nVisual pretraining for Robotics. Visual pretraining has\ndemonstrated impressive generalization ability on computer\nvision tasks. Therefore, prior works have explored whether\nit works for robotics tasks as well. Specifically, the robotics\ncommunity has trended towards learning representations us-\ning state-of-the-art self-supervised vision algorithms on di-\nverse interaction datasets [CITATION_5], and finetune the net-\nwork on robotics tasks [CITATION_6]. 3D-MVP\nfollows the same procedure. However, existing robotics\npretraining approaches typically learn a 2D visual encoder\n(e.g. ResNet [CITATION_7] or ViT [CITATION_8]), we find they are inferior than\nmanipulation policies which do explicit 3D modeling (e.g.\nRVT [CITATION_9], Act3D [CITATION_10]). Migrating a pretrained ViT to 3D\nmanipulation policies is nontrivial since they do not have\na 2D visual encoder. In this paper, we propose 3D-MVP,\nwhich does 3D-aware pretraining on 3D manipulation poli-\ncies, to fill the gap.\n\nLearning manipulation from demonstrations. Recent\nwork has explored using transformers for multi-task ma-\nnipulation policies that predict robot actions from visual\nand language inputs [CITATION_11]. End-to-end mod-\nels like RT-1 [CITATION_12], GATO [CITATION_13], and InstructRL [CITATION_14] directly\npredict 6-DoF end-effector poses but require many demon-\n\n2\n\n\fstrations to learn spatial reasoning and generalize to new\nscenes. To better handle 3D scenes, PerAct [CITATION_15] and C2F-\nARM [CITATION_16] voxelize the workspace and detect the 3D voxel\ncontaining the next end-effector pose. However, precise\npose prediction requires high-resolution voxels which are\ncomputationally expensive. Recently, RVT [CITATION_17] proposes\na multi-view transformer that attends over point cloud fea-\ntures from multiple camera views to predict actions. This\navoids explicit voxelization and enables faster training and\ninference than PerAct. Act3D [CITATION_18] represents the scene as a\ncontinuous 3D feature field and samples points to featurize\nwith attention, allowing adaptive resolution. GNFactor [CITATION_19]\njointly optimizes a generalizable neural field for reconstruc-\ntion and a Perceiver for decision-making. In contrast, our\nproposed 3D-MVP learns 3D scene representations through\nmasked autoencoding pretraining on a large dataset of 3D\nobject models. This pretraining enables 3D-MVP to build\na rich understanding of 3D geometry and semantics prior\nto finetuning on downstream manipulation tasks. Com-\npared to RVT and Act3D which train from scratch on tar-\nget tasks, 3D-MVP\u2019s pretraining leads to improved perfor-\nmance, sample efficiency and generalization. Unlike GN-\nFactor which relies on a pretrained VLM to inject seman-\ntics, 3D-MVP directly learns 3D semantic features from ob-\nject models.", "citation_info": {"content": [["Unsupervised learning of In Euro- visual representations by solving jigsaw puzzles"], ["A simple framework for contrastive learning of visual representations", "Momentum contrast for unsupervised visual rep- resentation learning"], ["Masked siamese networks for label-efficient learning", "Self-supervised learning from images with a joint-embedding predictive architecture", "Unsupervised learning of visual features by contrasting cluster assignments", "Emerg- ing properties in self-supervised vision transformers", "Bootstrap your own latent-a new approach to self-supervised learning", "ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer"], ["Masked autoencoders are scalable vision learners"], ["Multimae: Multi-modal multi-task masked autoen- coders"], ["Scaling egocentric vision: The epic-kitchens dataset", "Ego4d: Around the world in 3,000 hours of egocentric video", "Understanding human hands in contact at internet scale"], ["An unbiased look at datasets for visuo-motor pre-training", "Vip: Towards universal visual reward and representation via value-implicit pre-training", "Where are we in the search for an artificial visual cortex for embodied intelli- gence? Advances in Neural Information Processing Systems, 36, 2024.", "representation for robot manipulation", "Real-world robot learn- ing with masked visual pre-training", "Multi-view masked world In International models for visual robotic manipulation", "Masked visual pre-training for motor control"], ["Deep residual learning for image recognition"], ["An image is worth 16x16 words: Trans- arXiv preprint formers for image recognition at scale"], ["Rvt: Robotic view transformer for 3d object manipulation"], ["Act3d: Infinite resolution action detec- tion transformer for robotic manipulation"], ["Instruction-driven history-aware policies for robotic manip- ulations", "Instruction-following agents with jointly pre-trained vision- language models", "Behavior transformers: Cloning k modes with one stone", "Perceiver- actor: A multi-task transformer for robotic manipulation", "Shelving, stacking, hanging: Relational pose arXiv preprint diffusion for multi-modal rearrangement"], ["Rt-1: Robotics transformer for real-world control at scale"], ["A generalist agent"], ["Instruction-following agents with jointly pre-trained vision- language models"], ["Perceiver- actor: A multi-task transformer for robotic manipulation"], ["Coarse-to-fine q-attention: Efficient learn- ing for visual robotic manipulation via discretisation"], ["Rvt: Robotic view transformer for 3d object manipulation"], ["Act3d: Infinite resolution action detec- tion transformer for robotic manipulation"], ["Gnfactor: Multi-task real robot learning In Conference on with generalizable neural feature fields"]], "positions": [459, 483, 514, 604, 902, 1605, 1652, 1788, 1796, 1892, 1900, 2316, 2348, 2355, 2372, 2546, 2560, 2763, 2981, 3123]}}
{"paper_id": "AnyCam", "input_text": "2.1. Foundational Models for Depth and Flow\n\nIn the tasks of monocular depth (MDE) and flow estimation,\nwell-generalizable foundation models have replaced early\ndeep learning approaches  in the last years. For\nMDE, DepthAnything  uses a data engine to construct\na large corpus of automatically annotated data to learn rel-\native depth estimation. Additional fine-tuning allows for\nmetric depth estimates. DepthAnythingV2  finetunes\nthe previous model using synthetic data for better perfor-\nmance. Metric3D  and Metric3Dv2  transform im-\nages to canonical camera intrinsics with a fixed focal length.\nDepthPro  proposes a two-stage training curriculum with\na second stage solely on synthetic data to sharpen bound-\nary predictions. DepthCrafter  leverages a conditional\ndiffusion model to predict temporally consistent depth maps\n\nfor videos. In this work, we utilize UniDepth  for metric\nMDE, which uses a geometric invariance loss on different\nimage augmentation to enforce consistency.\n\nRAFT  presented the state of the art for optical flow\nIt improved previous meth-\nestimation for a long time.\nods by introducing a recurrent look-up operator on corre-\nlation volumes to iteratively refine flow predictions with-\nout needing coarse-to-fine flow pyramids. GMFlow \navoids correlation volumes and instead leverages the prop-\nerties of transformers for global matching on feature maps.\nThis removes the need for iterative steps to improve runtime\nperformance. UniMatch  extends GMFlow network by\ntasks of disparity and depth prediction to enable cross-task\ntransfer learning of a single transformer network.\n\nWe rely on both off-the-shelf MDE and Optical Flow\nnetworks to benefit from strong geometric priors during\ntraining and inference.\n\n2.2. SfM and SLAM\n\nFor many decades, the problem of recovering camera pa-\nrameters and geometry from images has been formulated\nas the Structure-from-Motion (SfM) pipeline .\nWhile many different implementations of the SfM pipeline\nexist, COLMAP  has emerged as the standard due to\nits robustness and flexibility. One of the drawbacks of SfM\nmethods is their high computational cost. Simultaneous Lo-\ncation and Mapping (SLAM)  approaches em-\nploy a similar pipeline to SfM but focus on the efficient pro-\ncessing of consecutive video frames. In recent years, these\nclassical optimization-based approaches were enhanced by\nlearned components . However,\nrelying on epipolar geometry  or photometric consis-\ntency  makes them susceptible to high error on highly\ndynamic scenes. The strong focus on self driving data pro-\nvided datasets with mostly static environments ,\nan assumption that does not hold for casual videos.\n\n2.3. Learning Based SfM and SLAM\n\nLargely learning-based methods started to replace classical\nSLAM and SfM systems due to improved robustness .\nDROID-SLAM extends the framework of RAFT  by an\nupdate operator on both depth and pose estimates. A final\ndifferentiable bundle adjustment (BA) layer produces the fi-\nnal pose estimates. ParticleSfM  utilizes dense corre-\nspondences inside a BA framework to optimize poses. The\ndense correspondences are initialized from optical flow, and\ndynamic points are filtered using trajectory-based motion\nsegmentation. CasualSAM  predicts both depth and\nmovement from images to get frame-to-frame motion. A\nglobal optimization aligns the scale of the prediction and\nrefines the poses. Dust3R  is a dense multi-view stereo\nmethod that regresses point coordinates between an im-\nage pair. This allows it to be extended to either SfM or\n\nSLAM. FlowMap  proposes to reconstruct a scene by\noverfitting a depth network to it and aligning depth maps\nvia correspondences from flow or point tracking. LEAP-\nVO  combines visual and temporal information of video\nsequences to improve the tracking accuracy of points and\nidentify occluded and dynamic points. A sliding window\nbundle adjustment then optimizes the poses. The concurrent\nwork of MonST3R  finetunes Dust3r on mostly syn-\nthetic data to generalize it to dynamic scenes. While these\nworks achieve impressive progress, they generally obtain\nposes from aligning depth and point maps or by optimizing\nthem per-scene. This makes it hard to inject prior informa-\ntion about camera motion. In contrast, our method uses a\nneural network to predict a trajectory, which can effectively\nlearn priors over realistic camera motions.", "expected_text": "2.1. Foundational Models for Depth and Flow\n\nIn the tasks of monocular depth (MDE) and flow estimation,\nwell-generalizable foundation models have replaced early\ndeep learning approaches [CITATION_0] in the last years. For\nMDE, DepthAnything [CITATION_1] uses a data engine to construct\na large corpus of automatically annotated data to learn rel-\native depth estimation. Additional fine-tuning allows for\nmetric depth estimates. DepthAnythingV2 [CITATION_2] finetunes\nthe previous model using synthetic data for better perfor-\nmance. Metric3D [CITATION_3] and Metric3Dv2 [CITATION_4] transform im-\nages to canonical camera intrinsics with a fixed focal length.\nDepthPro [CITATION_5] proposes a two-stage training curriculum with\na second stage solely on synthetic data to sharpen bound-\nary predictions. DepthCrafter [CITATION_6] leverages a conditional\ndiffusion model to predict temporally consistent depth maps\n\nfor videos. In this work, we utilize UniDepth [CITATION_7] for metric\nMDE, which uses a geometric invariance loss on different\nimage augmentation to enforce consistency.\n\nRAFT [CITATION_8] presented the state of the art for optical flow\nIt improved previous meth-\nestimation for a long time.\nods by introducing a recurrent look-up operator on corre-\nlation volumes to iteratively refine flow predictions with-\nout needing coarse-to-fine flow pyramids. GMFlow [CITATION_9]\navoids correlation volumes and instead leverages the prop-\nerties of transformers for global matching on feature maps.\nThis removes the need for iterative steps to improve runtime\nperformance. UniMatch [CITATION_10] extends GMFlow network by\ntasks of disparity and depth prediction to enable cross-task\ntransfer learning of a single transformer network.\n\nWe rely on both off-the-shelf MDE and Optical Flow\nnetworks to benefit from strong geometric priors during\ntraining and inference.\n\n2.2. SfM and SLAM\n\nFor many decades, the problem of recovering camera pa-\nrameters and geometry from images has been formulated\nas the Structure-from-Motion (SfM) pipeline [CITATION_11].\nWhile many different implementations of the SfM pipeline\nexist, COLMAP [CITATION_12] has emerged as the standard due to\nits robustness and flexibility. One of the drawbacks of SfM\nmethods is their high computational cost. Simultaneous Lo-\ncation and Mapping (SLAM) [CITATION_13] approaches em-\nploy a similar pipeline to SfM but focus on the efficient pro-\ncessing of consecutive video frames. In recent years, these\nclassical optimization-based approaches were enhanced by\nlearned components [CITATION_14]. However,\nrelying on epipolar geometry [CITATION_15] or photometric consis-\ntency [CITATION_16] makes them susceptible to high error on highly\ndynamic scenes. The strong focus on self driving data pro-\nvided datasets with mostly static environments [CITATION_17],\nan assumption that does not hold for casual videos.\n\n2.3. Learning Based SfM and SLAM\n\nLargely learning-based methods started to replace classical\nSLAM and SfM systems due to improved robustness [CITATION_18].\nDROID-SLAM extends the framework of RAFT [CITATION_19] by an\nupdate operator on both depth and pose estimates. A final\ndifferentiable bundle adjustment (BA) layer produces the fi-\nnal pose estimates. ParticleSfM [CITATION_20] utilizes dense corre-\nspondences inside a BA framework to optimize poses. The\ndense correspondences are initialized from optical flow, and\ndynamic points are filtered using trajectory-based motion\nsegmentation. CasualSAM [CITATION_21] predicts both depth and\nmovement from images to get frame-to-frame motion. A\nglobal optimization aligns the scale of the prediction and\nrefines the poses. Dust3R [CITATION_22] is a dense multi-view stereo\nmethod that regresses point coordinates between an im-\nage pair. This allows it to be extended to either SfM or\n\nSLAM. FlowMap [CITATION_23] proposes to reconstruct a scene by\noverfitting a depth network to it and aligning depth maps\nvia correspondences from flow or point tracking. LEAP-\nVO [CITATION_24] combines visual and temporal information of video\nsequences to improve the tracking accuracy of points and\nidentify occluded and dynamic points. A sliding window\nbundle adjustment then optimizes the poses. The concurrent\nwork of MonST3R [CITATION_25] finetunes Dust3r on mostly syn-\nthetic data to generalize it to dynamic scenes. While these\nworks achieve impressive progress, they generally obtain\nposes from aligning depth and point maps or by optimizing\nthem per-scene. This makes it hard to inject prior informa-\ntion about camera motion. In contrast, our method uses a\nneural network to predict a trajectory, which can effectively\nlearn priors over realistic camera motions.", "citation_info": {"content": [["Flownet: Learning optical flow with convolutional networks", "Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency", "Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation"], ["Depth anything: Unleashing the power of large-scale unlabeled data"], ["Depth any- thing v2"], ["Metric3d: Towards zero-shot metric 3d prediction from a single image"], ["Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation"], ["Depth pro: Sharp monocular metric depth in less than a second"], ["Depthcrafter: Generating consistent long depth sequences for open-world videos"], ["Unidepth: Universal monocular metric depth estimation"], ["Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow"], ["Gmflow: Learning optical flow via global matching"], ["Unifying flow, stereo and depth estimation"], ["Multiple view ge- ometry in computer vision", "A critique of structure-from-motion algo- rithms", "A survey of structure from motion*"], ["Structure- In Proceedings of the IEEE con- from-motion revisited"], ["Lsd- slam: Large-scale direct monocular slam", "Direct sparse odometry", "Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras", "5"], ["Dsac-differentiable ransac for camera localization", "Superpoint: Self-supervised interest point detection and description", "Ground- arXiv preprint ing image matching in 3d with mast3r", "Learning correspondence uncer- tainty via differentiable nonlinear least squares", "Deep fundamental matrix estimation", "In Proceedings of matching with graph neural networks", "Back to the feature: Learning robust camera localization from pixels to pose", "Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry"], ["In defense of the eight-point algorithm"], ["Direct sparse odometry"], ["nuscenes: A multi- In Proceedings of modal dataset for autonomous driving", "Vision meets robotics: The kitti dataset", "Scalability in perception for autonomous driving: Waymo open dataset"], ["Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras"], ["Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow"], ["Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild"], ["Structure and motion from casual videos"], ["Dust3r: Geometric 3d vi- sion made easy"], ["Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent"], ["Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry"], ["Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion"]], "positions": [186, 229, 421, 507, 523, 610, 745, 877, 995, 1266, 1469, 1913, 1986, 2167, 2382, 2422, 2452, 2606, 2803, 2846, 3004, 3226, 3389, 3546, 3698, 3936]}}
