[
  {
    "paper_id": "deep_learning_results",
    "pred_citations": [
      [
        "Human Motion Generation: A Survey",
        "TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation"
      ],
      [
        "Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation",
        "MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model"
      ],
      [
        "MultiAct: Long-Term 3D Human Motion Generation from Multiple Action Labels",
        "Weakly-supervised Action Transition Learning for Stochastic Human Motion Prediction"
      ],
      [
        "Freeform Body Motion Generation from Speech",
        "Generating Holistic 3D Human Motion from Speech"
      ],
      [
        "MIDGET: Music Conditioned 3D Dance Generation",
        "Bidirectional Autoregressive Diffusion Model for Dance Generation"
      ],
      [
        "MotionChain: Conversational Motion Controllers via Multimodal Prompts",
        "MagicAvatar: Multimodal Avatar Generation and Animation"
      ],
      [
        "Video Motion Graphs",
        "Ego-Body Pose Estimation via Ego-Head Pose Estimation"
      ],
      [
        "AMG: Avatar Motion Guided Video Generation",
        "Move-in-2D: 2D-Conditioned Human Motion Generation"
      ],
      [
        "AMASS: Archive of Motion Capture as Surface Shapes"
      ],
      [
        "Motion-X: Realistic and Diverse 3D Human Motion Generation",
        "MotionBank: A Benchmark for Human Motion Analysis"
      ],
      [
        "Motion-X: Realistic and Diverse 3D Human Motion Generation",
        "MotionBank: A Benchmark for Human Motion Analysis"
      ],
      [
        "Generating Holistic 3D Human Motion from Speech"
      ],
      [
        "VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over"
      ],
      [
        "AMG: Avatar Motion Guided Video Generation"
      ],
      [
        "Orientation-Aware Leg Movement Learning for Action-Driven Human Motion Prediction"
      ],
      [
        "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE"
      ]
    ],
    "gt_citations": [
      [
        "Emad Barsoum, John Kender, and Zicheng Liu. HP-GAN: Probabilistic 3D human motion prediction via GAN. In CVPR Workshops, 2018. 2",
        "Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda. Implicit neural representations for variable length human motion generation. In ECCV, 2022. 2",
        "B. Chopin, N. Otberdout, M. Daoudi, and A. Bartolo. Human motion prediction using manifold-aware wasserstein gan. In FG, 2021. 2",
        "Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, and Michiel van de Panne. Flexible motion in-betweening with diffusion models. SIGGRAPH, 2024.",
        "Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV, 2024. 2",
        "Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked mod- eling of 3d human motions. 2023. 2",
        "Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe Yearsley, and Taku Komura. A recurrent variational autoen- coder for human motion synthesis. In BMVC, 2017. 2",
        "Chengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. Nemf: Neural motion fields for kinematic ani- mation. In NeurIPS, 2022. 2",
        "Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. MoGlow: Probabilistic and controllable motion synthesis using normalising flows. ACM Transactions on Graphics (TOG), 2020. 2",
        "Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In CVPR, 2024. 2",
        "Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In CVPR, 2024. 2",
        "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas. Humor: 3d human motion model for robust pose estimation. In ICCV, 2021. 2, 5 10",
        "Ayumi Shiobara and Makoto Murakami. Human motion gen- eration using wasserstein GAN. In International Conference on Digital Signal Processing (ICDSP), 2021. 2",
        "Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion dif- fusion model. In ICLR, 2023. 2, 5, 6, 7, 8, 13",
        "Guillermo Valle-P\u00b4erez, Gustav Eje Henter, Jonas Beskow, Andr\u00b4e Holzapfel, Pierre-Yves Oudeyer, and Simon Alexan- derson. Transflower: probabilistic autoregressive dance gen- eration with multimodal attention. ACM Transactions on Graphics (TOG), 2021. 2",
        "Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng, and Wei Wu. ActFormer: A gan-based transformer towards general action-conditioned 3d human motion generation. In ICCV, 2023. 2",
        "Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif- fuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2",
        "Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re- modiffuse: Retrieval-augmented motion diffusion model. In arXiv preprint arXiv:2304.01116, 2023. 2",
        "Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. TPAMI, 2023. 2"
      ],
      [
        "Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your commands via motion diffusion in latent space. In CVPR, 2023. 2",
        "Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In ICCV, 2021. 2",
        "Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. TM2T: Stochastic and tokenized modeling for the reciprocal gener- ation of 3d human motions and texts. In ECCV, 2022. 2",
        "Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Yang Wei, and Li Yuan. Act as you wish: Fine-grained control of mo- tion diffusion model with hierarchical semantic graphs. In NeurIPs, 2023. 2"
      ],
      [
        "Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac- tion2Motion: Conditioned generation of 3D human mo- In ACM International Conference on Multimedia tions. (ACMMM), 2020. 2"
      ],
      [
        "Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG), 2023. 2",
        "Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models for audio-driven co-speech gesture generation. In CVPR, 2023. 2"
      ],
      [
        "Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In ICCV, 2021. 2, 6, 7, 12, 13",
        "Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory. Computer Vision and Pattern Recognition, pages 11050\u2013 11059, 2022. 2, 7, 13",
        "Jiangxin Sun, Chunyu Wang, Huang Hu, Hanjiang Lai, Zhi Jin, and Jian-Fang Hu. You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection. In NeurIPS, 2022. 2",
        "Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis. In ACM International Conference on Multimedia (ACMMM), 2018. 2",
        "Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: In Proceedings of Editable dance generation from music. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448\u2013458, 2023. 2, 6, 7, 12, 13",
        "Guillermo Valle-P\u00b4erez, Gustav Eje Henter, Jonas Beskow, Andr\u00b4e Holzapfel, Pierre-Yves Oudeyer, and Simon Alexan- derson. Transflower: probabilistic autoregressive dance gen- eration with multimodal attention. ACM Transactions on Graphics (TOG), 2021. 2"
      ],
      [
        "Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In ICCV, 2021. 2",
        "Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. NIFTY: Neural object interaction fields for guided human motion synthesis. arXiv:2307.07511, 2023. 2",
        "Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. HUMANISE: Language-conditioned hu- man motion generation in 3d scenes. In Neural Information Processing Systems (NeurIPS), 2022. 3",
        "Hongwei Yi, Justus Thies, Michael J Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control. In European Conference on Com- puter Vision, pages 246\u2013263. Springer, 2024. 3",
        "Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya A. Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P\u00b4erez Pellitero, and Gerard Pons-Moll. Force: Dataset and method for intuitive physics guided human-object interac- tion. In International Conference on 3D Vision (3DV), 2025. 3"
      ],
      [
        "Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls. arXiv preprint arXiv:2407.21136, 2024. 3",
        "Mingshuang Luo, Ruibing Hou, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan. M3gpt: An ad- vanced multimodal, multitask framework for motion com- prehension and generation. In NeurIPs, 2024. 3",
        "Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In ECCV, 2024. 3",
        "Zixiang Zhou and Baoyuan Wang. Ude: A unified driving engine for human motion generation. In CVPR, 2023. 3"
      ],
      [
        "Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In ECCV, 2024. 3"
      ],
      [
        "Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00a8ul Varol. TEACH: Temporal action composition for 3D humans. In International Conference on 3D Vision (3DV), 2022. 3",
        "Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, G\u00a8ul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In CVPR Workshop on Human Motion Generation, 2024. 3",
        "Yijun Qian, Jack Urbanek, Alexander G. Hauptmann, and Jungdam Won. Breaking the limits of text-conditioned 3D In ICCV, motion synthesis with elaborative descriptions. 2023. 3",
        "Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In CVPR, 2023. 3"
      ],
      [
        "Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In ICCV, 2019. 3, 6, 12"
      ],
      [
        "Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: A large- scale 3d expressive whole-body human motion dataset. In NeurIPS, 2023. 2, 3, 6, 7, 12, 13"
      ],
      [
        "Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Mo- tionbank: A large-scale video motion benchmark with disen- tangled rule-based annotations, 2024. 3"
      ],
      [
        "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12",
        "Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras. In CVPR, 2022. 3, 7"
      ],
      [
        "Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018. 3",
        "Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In CVPR, 2021. 3, 7, 12, 13",
        "Istv\u00b4an S\u00b4ar\u00b4andi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. 2024. 3"
      ],
      [
        "Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Be- yond static features for temporally consistent 3d human pose and shape from a video. In CVPR, 2021. 3",
        "Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Re- In constructing and tracking humans with transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14783\u201314794, 2023. 3, 7",
        "Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. VIBE: Video inference for human body pose and shape estimation. In CVPR, 2020. 3, 7, 13"
      ],
      [
        "Jiye Lee and Hanbyul Joo. Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head- mounted camera. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2024. 3",
        "Jose Luis Ponton, Haoran Yun, Andreas Aristidou, Carlos Andujar, and Nuria Pelechano. Sparseposer: Real-time full- body motion reconstruction from sparse data. ACM Trans- actions on Graphics, 2023. 3",
        "Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. ACM Transactions on Graphics, 2021. 3"
      ],
      [
        "Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. PACE: Human and motion estimation from in-the- wild videos. In 3DV, 2024. 2, 3, 7",
        "Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, and Umar Iqbal. Coin: Control-inpainting diffusion prior for human and camera In ECCV, pages 426\u2013446. Springer, motion estimation. 2024. 2, 3, 7",
        "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12",
        "Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in- the-wild videos. In European Conference on Computer Vi- sion, pages 467\u2013487. Springer, 2024. 3, 6, 7, 12",
        "Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In CVPR, 2023. 2, 3, 7",
        "Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras. In CVPR, 2022. 3, 7"
      ],
      [
        "Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia 2024 Conference Papers, pages 1\u201311, 2024. 3, 7, 12",
        "Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J Black. Wham: Reconstructing world-grounded humans with accu- rate 3d motion. In CVPR, 2024. 3, 7, 12",
        "Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in- the-wild videos. In European Conference on Computer Vi- sion, pages 467\u2013487. Springer, 2024. 3, 6, 7, 12",
        "Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexan- der Winkler, Petr Kadlecek, Siyu Tang, and Federica Bogo. Rohm: Robust human motion reconstruction via diffusion. In CVPR, 2024. 3 11"
      ]
    ]
  },
  {
    "paper_id": "InsightV",
    "pred_citations": [
      [
        "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
        "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
        "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models"
      ],
      [
        "Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense",
        "XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning"
      ],
      [
        "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
        "ALERT: Adapting Language Models to Reasoning Tasks"
      ],
      [
        "Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense"
      ],
      [
        "GAMR: A Guided Attention Model for (visual) Reasoning",
        "Solving Math Word Problems via Cooperative Reasoning induced Language Models"
      ],
      [
        "Solving Math Word Problems via Cooperative Reasoning induced Language Models"
      ],
      [
        "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing",
        "From \"Thumbs Up\" to \"10 out of 10\": Reconsidering Scalar Feedback in Interactive Reinforcement Learning"
      ],
      [
        "GLISp-r: A preference-based optimization algorithm with convergence guarantees"
      ],
      [
        "Some things are more CRINGE than others: Iterative Preference Optimization with the Pairwise Cringe Loss"
      ]
    ],
    "gt_citations": [
      [
        "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understand- ing, localization, text reading, and beyond. 2023. 2",
        "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chun- yuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2",
        "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6",
        "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 26296\u201326306, 2024. 1, 5",
        "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6",
        "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 1, 2",
        "Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial- temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 2",
        "Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 1, 2, 6",
        "QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6"
      ],
      [
        "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6",
        "QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6"
      ],
      [
        "Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: A foundational and multimodal mathe- matical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7126\u20137133, 2023. 2"
      ],
      [
        "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks. In CVPR, pages 24185\u201324198, 2024. 1, 2"
      ],
      [
        "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Mon- key: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 2",
        "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reason- ing, ocr, and world knowledge, 2024. 1, 2, 5, 6",
        "Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Ji- wen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 2",
        "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5, 6",
        "Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. 2"
      ],
      [
        "Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric prob- lem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 2",
        "Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual in- struction tuning. arXiv preprint arXiv:2407.08739, 2024. 2, 3",
        "Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yim- ing Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. 2, 5"
      ],
      [
        "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u2013 24837, 2022. 1, 2"
      ],
      [
        "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2"
      ],
      [
        "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct prefer- ence optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, 5"
      ],
      [
        "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak lan- guage models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 2"
      ]
    ]
  },
  {
    "paper_id": "sugar",
    "pred_citations": [
      [
        "Light Field Rendering"
      ],
      [
        "A One Stop 3D Target Reconstruction and multilevel Segmentation Method"
      ],
      [
        "Rethinking the Multi-view Stereo from the Perspective of Rendering-based Augmentation"
      ],
      [
        "VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis"
      ],
      [
        "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
      ],
      [
        "Compressing Explicit Voxel Grid Representations: fast NeRFs become also small"
      ],
      [
        "L\u2080-Sampler: An L\u2080 Model Guided Volume Sampling for NeRF"
      ],
      [
        "Neural Fields meet Explicit Geometric Representation for Inverse Rendering of Urban Scenes"
      ],
      [
        "A Marching Cubes Algorithm for Surface Reconstruction from Volumetric Data"
      ],
      [
        "BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis"
      ],
      [
        "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields"
      ],
      [
        "3D Gaussian Splatting for Real-Time Radiance Field Synthesis"
      ]
    ],
    "gt_citations": [
      [
        "Marc Levoy and Pat Hanrahan. Light Field Rendering. In ACM SIGGRAPH, 1996. 3"
      ],
      [
        "Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo Tourism: Exploring Photo Collections in 3D. In ACM SIG- GRAPH, 2006. 3, 4"
      ],
      [
        "Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven Seitz. Multi-View Stereo for Community Photo Collections. In International Conference on Computer Vision, 2007. 3"
      ],
      [
        "Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, and Michael Cohen. Unstructured Lumigraph Ren- dering. In ACM SIGGRAPH, 2001. 3",
        "Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-Viewpoint Image-Based Rendering. In ACM SIG- GRAPH, 2018. 3, 7, 2, 4",
        "Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Cur- less, Tom Duchamp, David H. Salesin, and Werner Stuet- zle. Surface Light Fields for 3D Photography. In ACM SIG- GRAPH, 2000. 3"
      ],
      [
        "Gernot Riegler and Vladlen Koltun. Free View Synthesis. In European Conference on Computer Vision, 2020. 3",
        "Gernot Riegler and Vladlen Koltun. Stable View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3"
      ],
      [
        "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View In European Conference on Computer Vision, Synthesis. 2020. 1, 3"
      ],
      [
        "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3",
        "Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. ReLU Fields: The Little Non-Linearity That Could. In ACM SIGGRAPH, 2022. 3",
        "Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant Neural Graphics Primitives with a Mul- tiresolution Hash Encoding. In ACM SIGGRAPH, 2022. 3, 7, 8, 2",
        "Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance In Conference on Computer Vision Fields Reconstruction. and Pattern Recognition, 2022. 3",
        "Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: In Conference Radiance Fields Without Neural Networks. on Computer Vision and Pattern Recognition, 2022. 3, 7, 8"
      ],
      [
        "Jonathan T. Barron. Mip-NeRF 360: Unbounded Anti- Aliased Neural Radiance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3, 7, 8, 2, 4",
        "Peter Hedman and Pratul P. Srinivasan. Baking Neural Radi- ance Fields for Real-Time View Synthesis. In International Conference on Computer Vision, 2021. 3",
        "Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. KiloNeRF: Speeding Up Neural Radiance Fields with Thousands of Tiny MLPs. In International Conference on Computer Vision, 2021. 3",
        "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees For Real-Time Rendering of Neural Radiance Fields. In International Conference on Computer Vision, 2021. 3"
      ],
      [
        "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3"
      ],
      [
        "Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: 10 SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering"
      ],
      [
        "Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar- ron, Ce Liu, and Hendrik P. A. Lensch. NeRD: Neural Re- flectance Decomposition from Image Collections. In Inter- national Conference on Computer Vision, 2021. 3",
        "Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC: Neural Rendering of Objects from Online Image Collections. In ACM SIGGRAPH, 2022. 3",
        "Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. NeRV: Neural Reflectance and Visibility Fields for Relight- ing and View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3",
        "Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse Rendering with Spherical Gaussians for Physics-Based Material Editing and Relight- ing. In Conference on Computer Vision and Pattern Recog- nition, 2021. 3"
      ],
      [
        "Chong Bao and Bangbang Yang, Zeng Junyi, Bao Hu- jun, Zhang Yinda, Cui Zhaopeng, and Zhang Guofeng. NeuMesh: Learning Disentangled Neural Mesh-Based Im- plicit Field for Geometry and Texture Editing. In European Conference on Computer Vision, 2022. 4",
        "Franc\u00b8ois Darmon, B\u00b4en\u00b4edicte Bascle, Jean-Cl\u00b4ement Devaux, Pascal Monasse, and Mathieu Aubry. Improving Neural Im- plicit Surfaces Geometry with Patch Warping. In Conference on Computer Vision and Pattern Recognition, 2022. 4",
        "Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H. Tay- lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-Fidelity Neural Surface Reconstruction. In Conference on Computer Vision and Pattern Recognition, 2023. 2, 4",
        "Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction. In International Con- ference on Computer Vision, 2021. 4",
        "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning Neural Im- plicit Surfaces by Volume Rendering for Multi-View Recon- In Advances in Neural Information Processing struction. Systems, 2021. 2, 4",
        "Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol- ume Rendering of Neural Implicit Surfaces. In Advances in Neural Information Processing Systems, 2021. 2, 4"
      ],
      [
        "William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In ACM SIGGRAPH, 1987. 2, 4, 8 9 Structured View-Dependent Appearance for Neural Radi- ance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3"
      ],
      [
        "Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An- drea Tagliasacchi. MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. In Conference on Computer Vision and Pattern Recognition, 2023. 3, 4, 7, 8"
      ],
      [
        "Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, and Jonathan T. Bar- ron. BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis. In ACM SIGGRAPH, 2023. 2, 3, 4, 7, 8"
      ],
      [
        "Jonathan T. Barron. Mip-NeRF: A Multiscale Representa- tion for Anti-Aliasing Neural Radiance Fields. In Interna- tional Conference on Computer Vision, 2021. 4, 7"
      ],
      [
        "Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and Federico Tombari. NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes. In DV, 2023. 2, 3, 4, 7, 8"
      ],
      [
        "Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and George Drettakis. Point-Based Neural Rendering with Per- View Optimization. In Computer Graphics Forum, 2021. 4",
        "Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. ADOP: Approximate Differentiable One-Pixel Point Ren- dering. In ACM SIGGRAPH, 2022. 4"
      ],
      [
        "Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. In ACM SIGGRAPH, 2023. 1, 2, 4, 7, 8"
      ]
    ]
  },
  {
    "paper_id": "atte",
    "pred_citations": [
      [
        "One weird trick for parallelizing convolutional neural networks"
      ],
      [
        "Neural Machine Translation in Linear Time"
      ],
      [
        "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"
      ],
      [
        "3D Graph Convolutional Networks with Temporal Graphs: A Spatial Information Free Framework For Traffic Forecasting"
      ],
      [
        "Recasting Self-Attention with Holographic Reduced Representations"
      ],
      [
        "Attention-based Memory Selection Recurrent Network for Language Modeling"
      ],
      [
        "Applying the Transformer to Character-level Transduction"
      ],
      [
        "Learning Transductions and Alignments with RNN Seq2seq Models"
      ]
    ],
    "gt_citations": [
      [
        "\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016."
      ],
      [
        "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."
      ],
      [
        "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."
      ],
      [
        "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001."
      ],
      [
        "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. 10",
        "Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.",
        "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.",
        "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017."
      ],
      [
        "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015."
      ],
      [
        "\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.",
        "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017."
      ],
      [
        "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017."
      ]
    ]
  },
  {
    "paper_id": "fireplace",
    "pred_citations": [
      [
        "Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions",
        "SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction"
      ],
      [
        "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement",
        "Gaussian Splatting is an Effective Data Generator for 3D Object Detection"
      ],
      [
        "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting"
      ],
      [
        "Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features",
        "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models"
      ],
      [
        "OBJECT 3DIT: Language-guided 3D-aware Image Editing",
        "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing"
      ],
      [
        "ClipFace: Text-guided Editing of Textured 3D Morphable Models",
        "TEXTure: Text-Guided Texturing of 3D Shapes"
      ],
      [
        "FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model",
        "I2V3D: Controllable image-to-video generation with 3D guidance"
      ],
      [
        "ET3D: Efficient Text-to-3D Generation via Multi-View Distillation",
        "MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion"
      ]
    ],
    "gt_citations": [
      [
        "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning arXiv preprint from rgb-d data in indoor environments. arXiv:1709.06158, 2017. 3",
        "Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin- qiang Zhao, et al. 3d-front: 3d furnished rooms with layouts In Proceedings of the IEEE/CVF Interna- and semantics. tional Conference on Computer Vision, pages 10933\u201310942, 2021. 3",
        "Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. In- finigen indoors: Photorealistic indoor scenes using procedu- ral generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21783\u2013 21794, 2024. 3, 5",
        "Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: A photorealistic syn- thetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 3",
        "Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567\u2013576, 2015. 3",
        "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano- lis Savva, and Thomas Funkhouser. Semantic scene com- In Proceedings of the pletion from a single depth image. IEEE conference on computer vision and pattern recogni- tion, pages 1746\u20131754, 2017. 3"
      ],
      [
        "Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. Example-based synthesis of 3d object arrangements. ACM Transactions on Graphics (TOG), 31(6):1\u201311, 2012. 3",
        "Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, S\u00a8oren Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas Guibas, and Hao Zhang. Language-driven synthe- sis of 3d scenes from scene databases. ACM Transactions on Graphics (TOG), 37(6):1\u201316, 2018. 3",
        "Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregres- sive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:12013\u201312026, 2021. 2, 3",
        "Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flex- ible indoor scene synthesis via deep convolutional genera- In Proceedings of the IEEE/CVF Conference tive models. on Computer Vision and Pattern Recognition, pages 6182\u2013 6190, 2019. 3",
        "Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, An- gel X Chang, and Daniel Ritchie. Planit: Planning and in- stantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG), 38(4):1\u2013 15, 2019. 3",
        "Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie\u00dfner. Sceneformer: Indoor scene generation with transformers. In 2021 International Conference on 3D Vision (3DV), pages 106\u2013115. IEEE, 2021.",
        "Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of ob- jects in rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19037\u2013 19047, 2023. 2, 3",
        "Yixuan Yang, Junru Lu, Zixiang Zhao, Zhen Luo, James JQ Yu, Victor Sanchez, and Feng Zheng. Llplace: The 3d in- door scene layout generation and editing via large language model. arXiv preprint arXiv:2406.03866, 2024. 2, 3"
      ],
      [
        "Shen Chen, Jiale Zhou, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, and Lei Li. Scalinggaus- sian: Enhancing 3d content creation with generative gaussian splatting. arXiv preprint arXiv:2407.19035, 2024. 3",
        "Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scene- dreamer: Unbounded 3d scene generation from 2d image collections. IEEE transactions on pattern analysis and ma- chine intelligence, 2023.",
        "Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free gen- arXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3",
        "Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Ad- vances in Neural Information Processing Systems, 36, 2024. 3",
        "Paul Henderson, Melonie de Almeida, Daniela Ivanova, Sampling 3d gaussian scenes arXiv preprint and Titas Anciukevi\u02c7cius. in seconds with latent diffusion models. arXiv:2406.13099, 2024. 3",
        "Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie\u00dfner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vi- sion, pages 7909\u20137920, 2023. 3",
        "Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, and Xihui Liu. Dreamwaltz-g: Expressive 3d gaussian avatars from skeleton-guided 2d diffusion. arXiv preprint arXiv:2409.17145, 2024. 3",
        "Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, and Hongsheng Li. Diffindscene: Diffusion-based 9 high-quality 3d indoor scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4526\u20134535, 2024. 3",
        "Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic scene genera- tion with triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28337\u201328347, 2024. 3",
        "Pengzhi Li, Chengshuai Tang, Qinxuan Huang, and Zhi- heng Li. Art3d: 3d gaussian splatting for text-guided artistic scenes generation. arXiv preprint arXiv:2405.10508, 2024. 3",
        "Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite na- ture: Perpetual view generation of natural scenes from a sin- In Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 14458\u201314467, 2021. 3",
        "Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. In 2024 In- ternational Conference on 3D Vision (3DV), pages 651\u2013663. IEEE, 2024. 3",
        "Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF Conference 3d cities. on Computer Vision and Pattern Recognition, pages 9666\u2013 9675, 2024. 3",
        "Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6796\u20136807, 2024. 3",
        "Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester Cole, De- qing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6658\u20136667, 2024. 3",
        "Xuening Yuan, Hongyu Yang, Yueming Zhao, and Di Huang. Dreamscape: 3d scene creation via gaussian splatting joint arXiv preprint arXiv:2404.09227, correlation modeling. 2024. 3"
      ],
      [
        "Purvi Goel, Kuan-Chieh Wang, C Karen Liu, and Kayvon Iterative motion editing with natural language. Fatahalian. arXiv preprint arXiv:2312.11538, 2023. 2, 3",
        "Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions. arXiv:2306.06212, 2023. 2, 3",
        "Ian Huang, Guandao Yang, and Leonidas Guibas. Blender- alchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2, 3, 5",
        "Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, and Michael J Black. Re-thinking inverse graphics with large language models. arXiv preprint arXiv:2404.15228, 2024. 2, 3",
        "Zehao Wen, Zichen Liu, Srinath Sridhar, and Rao Fu. Any- home: Open-vocabulary generation of structured and tex- tured 3d homes. arXiv preprint arXiv:2312.06644, 2023. 2, 3 10"
      ],
      [
        "Ian Huang, Guandao Yang, and Leonidas Guibas. Blender- alchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2, 3, 5"
      ],
      [
        "Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions. arXiv:2306.06212, 2023. 2, 3"
      ],
      [
        "Purvi Goel, Kuan-Chieh Wang, C Karen Liu, and Kayvon Iterative motion editing with natural language. Fatahalian. arXiv preprint arXiv:2312.11538, 2023. 2, 3"
      ],
      [
        "Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar- jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual plan- ning and generation with large language models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6"
      ],
      [
        "Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stew- art Morris, Seung Jean Yoo, Aditya Ganeshan, R Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie. Open-universe indoor scene generation using llm program synthesis and uncurated object databases. arXiv preprint arXiv:2403.09675, 2024. 2, 3",
        "Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Ma- chine Learning, 2024. 2, 3, 5",
        "Hou In Ivan Tam, Hou In Derek Pun, Austin T Wang, Angel X Chang, and Manolis Savva. Scenemotifcoder: Example-driven visual program learning for generating 3d arXiv preprint arXiv:2408.02211, object arrangements. 2024. 2, 3",
        "Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Kr- ishna, Lingjie Liu, et al. Holodeck: Language guided genera- tion of 3d embodied ai environments. In The IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR 2024), pages 20\u201325. IEEE/CVF, 2024. 2, 3, 5, 6"
      ]
    ]
  },
  {
    "paper_id": "3dmvp",
    "pred_citations": [
      [
        "Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer",
        "JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks",
        "Learning Image Representations by Completing Damaged Jigsaw Puzzles"
      ],
      [
        "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
        "From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning",
        "Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled Autoencoder for Mixed Tabular Datasets"
      ],
      [
        "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
        "From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning"
      ],
      [
        "Masked Autoencoders Are Scalable Vision Learners",
        "Masked Autoencoders As Spatiotemporal Learners",
        "GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds"
      ],
      [
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
        "RVT: Robotic View Transformer for 3D Object Manipulation"
      ],
      [
        "Learned Camera Gain and Exposure Control for Improved Visual Feature Detection and Matching",
        "myGym: Modular Toolkit for Visuomotor Robotic Tasks"
      ],
      [
        "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
        "EL-VIT: Probing Vision Transformer with Interactive Visualization"
      ],
      [
        "RVT: Robotic View Transformer for 3D Object Manipulation",
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation"
      ],
      [
        "RVT: Robotic View Transformer for 3D Object Manipulation",
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation"
      ],
      [
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
        "RVT: Robotic View Transformer for 3D Object Manipulation"
      ],
      [
        "Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation"
      ],
      [
        "Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation"
      ],
      [
        "Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation"
      ],
      [
        "Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation"
      ],
      [
        "RVT: Robotic View Transformer for 3D Object Manipulation",
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation"
      ],
      [
        "Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation"
      ],
      [
        "RVT: Robotic View Transformer for 3D Object Manipulation",
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation"
      ],
      [
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
        "RVT: Robotic View Transformer for 3D Object Manipulation"
      ],
      [
        "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
        "RVT: Robotic View Transformer for 3D Object Manipulation"
      ]
    ],
    "gt_citations": [
      [
        "Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Euro- visual representations by solving jigsaw puzzles. pean conference on computer vision, pages 69\u201384. Springer, 2016. 2"
      ],
      [
        "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597\u20131607. PMLR, 2020. 2",
        "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 9729\u20139738, 2020. 2"
      ],
      [
        "Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo- janowski, Florian Bordes, Pascal Vincent, Armand Joulin, 8 InputGTPred Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In European Conference on Com- puter Vision, pages 456\u2013473. Springer, 2022. 2",
        "Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo- janowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023. 2",
        "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Ad- vances in neural information processing systems, 33:9912\u2013 9924, 2020. 2",
        "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 9650\u20139660, 2021. 2",
        "Jean-Bastien Grill, Florian Strub, Florent Altch\u00b4e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020. 2",
        "Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. 2"
      ],
      [
        "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u2013 16009, 2022. 1, 2, 8"
      ],
      [
        "Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task masked autoen- coders. In ECCV, 2022. 1, 2"
      ],
      [
        "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In ECCV, 2018. 2, 7",
        "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022. 1, 2, 7",
        "Dandan Shan, Jiaqi Geng, Michelle Shu, and David Fouhey. Understanding human hands in contact at internet scale. In CVPR, 2020. 1, 2, 7"
      ],
      [
        "Sudeep Dasari, Mohan Kumar Srirama, Unnat Jain, and Ab- hinav Gupta. An unbiased look at datasets for visuo-motor pre-training. In Conference on Robot Learning, 2023. 1, 2, 7",
        "Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Os- bert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. 2",
        "Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelli- gence? Advances in Neural Information Processing Systems, 36, 2024.",
        "Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea R3m: A universal visual arXiv preprint Finn, and Abhinav Gupta. representation for robot manipulation. arXiv:2203.12601, 2022. 2, 6, 7",
        "Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learn- ing with masked visual pre-training. In Conference on Robot Learning, pages 416\u2013426. PMLR, 2023. 1, 2, 7",
        "Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jin- woo Shin, and Pieter Abbeel. Multi-view masked world In International models for visual robotic manipulation. Conference on Machine Learning, pages 30613\u201330632. PMLR, 2023. 2 10",
        "Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control. arXiv preprint arXiv:2203.06173, 2022. 1, 2, 6, 7"
      ],
      [
        "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 2"
      ],
      [
        "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- arXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 2, 3"
      ],
      [
        "Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694\u2013 710. PMLR, 2023. 1, 2, 3, 4, 5, 6, 7"
      ],
      [
        "Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Ka- terina Fragkiadaki. Act3d: Infinite resolution action detec- tion transformer for robotic manipulation. arXiv preprint arXiv:2306.17817, 2023. 2, 3"
      ],
      [
        "Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia Pinel, Makarand Tapaswi, Ivan Laptev, and Cordelia Schmid. Instruction-driven history-aware policies for robotic manip- ulations. In Conference on Robot Learning, pages 175\u2013187. PMLR, 2023. 2",
        "Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision- language models. 2022. 2",
        "Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Al- tanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. Advances in neural information processing systems, 35:22955\u201322968, 2022. 2",
        "Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver- actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785\u2013799. PMLR, 2023. 1, 2, 3, 5, 6",
        "Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen- Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, and Dieter Fox. Shelving, stacking, hanging: Relational pose arXiv preprint diffusion for multi-modal rearrangement. arXiv:2307.04751, 2023. 2"
      ],
      [
        "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr- ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 2"
      ],
      [
        "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin- arXiv preprint genberg, et al. A generalist agent. arXiv:2205.06175, 2022. 2"
      ],
      [
        "Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel. Instruction-following agents with jointly pre-trained vision- language models. 2022. 2"
      ],
      [
        "Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver- actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785\u2013799. PMLR, 2023. 1, 2, 3, 5, 6"
      ],
      [
        "Stephen James, Kentaro Wada, Tristan Laidlow, and An- drew J Davison. Coarse-to-fine q-attention: Efficient learn- ing for visual robotic manipulation via discretisation. In CVPR, 2022. 3, 5, 6"
      ],
      [
        "Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, and Dieter Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694\u2013 710. PMLR, 2023. 1, 2, 3, 4, 5, 6, 7"
      ],
      [
        "Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Ka- terina Fragkiadaki. Act3d: Infinite resolution action detec- tion transformer for robotic manipulation. arXiv preprint arXiv:2306.17817, 2023. 2, 3"
      ],
      [
        "Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, and Xiaolong Wang. Gnfactor: Multi-task real robot learning In Conference on with generalizable neural feature fields. Robot Learning, pages 284\u2013301. PMLR, 2023. 3"
      ]
    ]
  },
  {
    "paper_id": "AnyCam",
    "pred_citations": [
      [
        "Distilling Monocular Foundation Model for Fine-grained Depth Completion",
        "A Simple yet Effective Test-Time Adaptation for Zero-Shot Monocular Metric Depth Estimation",
        "Unsupervised monocular stereo matching"
      ],
      [
        "A Simple yet Effective Test-Time Adaptation for Zero-Shot Monocular Metric Depth Estimation",
        "FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume",
        "Error Diagnosis of Deep Monocular Depth Estimation Models"
      ],
      [
        "SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint",
        "A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation"
      ],
      [
        "Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image",
        "Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation",
        "SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation"
      ],
      [
        "Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation",
        "Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image"
      ],
      [
        "SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint",
        "FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume"
      ],
      [
        "FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume",
        "Error Diagnosis of Deep Monocular Depth Estimation Models"
      ],
      [
        "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"
      ],
      [
        "GMFlow: Learning Optical Flow via Global Matching"
      ],
      [
        "Unifying Flow, Stereo and Depth Estimation"
      ],
      [
        "Unifying Flow, Stereo and Depth Estimation"
      ],
      [
        "Structure-from-Motion Revisited",
        "OpenMVG: An Open Multiple View Geometry library",
        "Multiview Structure-from-Motion Made Easy"
      ],
      [
        "COLMAP: A General-Purpose Structure-from-Motion and Multi-View Stereo"
      ],
      [
        "ORB-SLAM: A Versatile and Accurate Monocular SLAM System",
        "PTAM (Parallel Tracking and Mapping) with RGB-D Sensors"
      ],
      [
        "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "Learning Depth from Single Images with Deep Neural Network Embedding Focal Points"
      ],
      [
        "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "Learning a Discriminative Feature Network for Semantic Segmentation and Monocular Depth Estimation"
      ],
      [
        "KITTI Vision Benchmark Suite"
      ],
      [
        "Droid-SLAM: A Versatile and Accurate Visual SLAM System",
        "DROID-SLAM with RGB-D: Distributed Real-Time Object Discovery"
      ],
      [
        "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"
      ],
      [
        "Particle SfM for 3D Reconstruction",
        "Dense Object Repositioning using ParticleSfM coupled with local descriptors"
      ],
      [
        "CasualSAM: Stochastic Spot and Motion Extraction Framework for 3D Object Interactions",
        "Simultaneous Appearance and Motion Segmentation from Video Data"
      ],
      [
        "Dust3R: Dynamic Unsupervised Structure-from-Motion Revisited",
        "Comprehensive Dense Stereo Matching Using Deep Features and the Dust3R Framework"
      ],
      [
        "Dense Monocular Motion Segmentation Using Optical Flow and Pseudo Depth Map: A Zero-Shot Approach",
        "FlowMap: Fast Dense Optical Flow for 3D Scene Understanding"
      ],
      [
        "LEAP-VO: Learning and Predicting Visual Odometry with Video Sequences",
        "LEAP: Learning Articulated Motion and Pose Transitions"
      ],
      [
        "Unsupervised Learning of Monocular Depth Prediction with Deferred Initialization"
      ]
    ],
    "gt_citations": [
      [
        "Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Pro- ceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015. 2",
        "Cl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency. on computer vision and pattern recognition, pages 270\u2013279, 2017. 2",
        "Cl\u00b4ement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation. national conference on computer vision, pages 3828\u20133838, 2019. 2"
      ],
      [
        "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of 10 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10371\u201310381, 2024. 2"
      ],
      [
        "Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao- gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any- thing v2. arXiv preprint arXiv:2406.09414, 2024. 2"
      ],
      [
        "Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9043\u20139053, 2023. 2"
      ],
      [
        "Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 2"
      ],
      [
        "Aleksei Bochkovskii, Ama\u00a8el Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than a second. arXiv preprint arXiv:2410.02073, 2024. 2"
      ],
      [
        "Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 2"
      ],
      [
        "Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106\u201310116, 2024. 2, 6"
      ],
      [
        "Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020. 2"
      ],
      [
        "Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8121\u20138130, 2022. 2"
      ],
      [
        "Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2, 6"
      ],
      [
        "Richard Hartley and Andrew Zisserman. Multiple view ge- ometry in computer vision. Cambridge university press, 2003. 2",
        "John Oliensis. A critique of structure-from-motion algo- rithms. Computer Vision and Image Understanding, 80(2): 172\u2013214, 2000. 2",
        "Onur \u00a8Ozyes\u00b8il, Vladislav Voroninski, Ronen Basri, and Amit Singer. A survey of structure from motion*. Acta Numerica, 26:305\u2013364, 2017. 2 9"
      ],
      [
        "Johannes L Schonberger and Jan-Michael Frahm. Structure- In Proceedings of the IEEE con- from-motion revisited. ference on computer vision and pattern recognition, pages 4104\u20134113, 2016. 1, 2"
      ],
      [
        "Jakob Engel, Thomas Sch\u00a8ops, and Daniel Cremers. Lsd- slam: Large-scale direct monocular slam. In European con- ference on computer vision, pages 834\u2013849. Springer, 2014. 2",
        "Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611\u2013625, 2017. 2",
        "Raul Mur-Artal and Juan D Tard\u00b4os. Orb-slam2: An open- source slam system for monocular, stereo, and rgb-d cam- eras. IEEE transactions on robotics, 33(5):1255\u20131262, 2017. 2",
        "Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam IEEE transactions on robotics, 31(5):1147\u20131163, system. 2015. 2"
      ],
      [
        "Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6684\u20136692, 2017. 2",
        "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224\u2013236, 2018. 2",
        "Vincent Leroy, Yohann Cabon, and J\u00b4er\u02c6ome Revaud. Ground- arXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. 2",
        "Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallab- hula, and Daniel Cremers. Learning correspondence uncer- tainty via differentiable nonlinear least squares. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13102\u201313112, 2023. 2",
        "Ren\u00b4e Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV), pages 284\u2013299, 2018. 2",
        "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. In Proceedings of matching with graph neural networks. the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.",
        "Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, et al. Back to the feature: Learning robust camera localization from pixels to pose. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 3247\u20133257, 2021. 2",
        "Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry. In Proceedings of the European conference on computer vision (ECCV), pages 817\u2013833, 2018. 2"
      ],
      [
        "Richard I Hartley. In defense of the eight-point algorithm. IEEE Transactions on pattern analysis and machine intelli- gence, 19(6):580\u2013593, 1997. 2"
      ],
      [
        "Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611\u2013625, 2017. 2"
      ],
      [
        "Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- In Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 11621\u201311631, 2020. 2",
        "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research, 32(11):1231\u20131237, 2013. 2",
        "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2446\u20132454, 2020. 2"
      ],
      [
        "Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neu- ral information processing systems, 34:16558\u201316569, 2021. 2, 6"
      ],
      [
        "Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer Vision\u2013ECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020. 2"
      ],
      [
        "Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild. In European Conference on Computer Vision, pages 523\u2013542. Springer, 2022. 2, 6"
      ],
      [
        "Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Ru- binstein, Noah Snavely, and William T Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 20\u201337. Springer, 2022. 2, 6"
      ],
      [
        "Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vi- sion made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20697\u201320709, 2024. 1, 2"
      ],
      [
        "Cameron Smith, David Charatan, Ayush Tewari, and Vin- cent Sitzmann. Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent. arXiv:2404.15259, 2024. 2, 3, 4"
      ],
      [
        "Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry. on Computer Vision and Pattern Recognition, pages 19844\u2013 19853, 2024. 2, 3, 6"
      ],
      [
        "Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jam- pani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming- Hsuan Yang. Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion. arXiv:2410.03825, 2024. 3, 6"
      ]
    ]
  }
]