{
  "S1": {
    "title": "1Introduction",
    "text": "Benchmarking is important for tracking the progress the field of natural language processing (NLP) has made in various tasks. In the past few years, large-scale multilingual benchmarks like XTREME Hu et\u00a0al. ( 2020 ) , XTREME-R Ruder et\u00a0al. ( 2021 ) , and XGLUE Liang et\u00a0al. ( 2020 ) have played a pivotal role in evaluating the multilingual capabilities of NLP models. These efforts have sought to make model evaluation more accessible to researchers and representative of a variety of languages Song et\u00a0al. ( 2023 ) . However, most benchmarks have focused on the standard varieties of languages, largely neglecting non-standard dialects and language varieties Blasi et\u00a0al. ( 2022 ) . Report issue for preceding element\nWe refer to non-standard dialects and language varieties simply as varieties , and sometimes include low-resource related languages, writing system variants, and other kinds of variation. Varieties contain subtle but notable variations in vocabulary, pronunciation, orthography and grammar, reflecting regional, social, and cultural differences Chambers and Trudgill ( 1998 ) . The non-standard nature of these language varieties oftentimes contributes to the scarcity of substantial datasets that accurately capture these variations Hedderich et\u00a0al. ( 2021 ) . As a result, they have often been absent from widely adopted benchmarks, even from admirable efforts like XTREME-up Ruder et\u00a0al. ( 2023 ) , GLUECoS Khanuja et\u00a0al. ( 2020 ) and CreoleVal Lent et\u00a0al. ( 2023 ) , which focus on under-resourced, code-switched, and creole languages, respectively. Report issue for preceding element\nIt is currently challenging to accurately test the robustness of multilingual models on a large suite of varieties without establishing an NLP evaluation framework covering multiple language clusters (each of which contains standard languages alongside its closely related varieties ). Report issue for preceding element\nTo this end, we create DialectBench , a large-scale benchmark covering 40 language clusters with 281 varieties , spanning 10 NLP tasks. We observe that the performance disparity between different varieties of the same language cluster becomes more pronounced when we shift from zero-shot evaluation to fine-tuning on variety data, because of uneven data availability across varieties .\nCertain language clusters exhibit varying performance across downstream tasks within the same category, due to low-resource limitations.\nAdditionally, we improve the dialectal task coverage for natural language inference by constructing a translate-test evaluation dataset. Putting these all together, DialectBench serves as a comprehensive suite that attains a multifaceted purpose: identifying broader limitations in dialectal NLP, while reflecting on potential areas for improvement. Report issue for preceding element",
    "masked_text": "Benchmarking is important for tracking the progress the field of natural language processing (NLP) has made in various tasks. In the past few years, large-scale multilingual benchmarks like XTREME [CITATION], XTREME-R [CITATION], and XGLUE [CITATION] have played a pivotal role in evaluating the multilingual capabilities of NLP models. These efforts have sought to make model evaluation more accessible to researchers and representative of a variety of languages [CITATION]. However, most benchmarks have focused on the standard varieties of languages, largely neglecting non-standard dialects and language varieties [CITATION].Report issue for preceding element\nWe refer to non-standard dialects and language varieties simply as varieties, and sometimes include low-resource related languages, writing system variants, and other kinds of variation. Varieties contain subtle but notable variations in vocabulary, pronunciation, orthography and grammar, reflecting regional, social, and cultural differences [CITATION]. The non-standard nature of these language varieties oftentimes contributes to the scarcity of substantial datasets that accurately capture these variations [CITATION]. As a result, they have often been absent from widely adopted benchmarks, even from admirable efforts like XTREME-up [CITATION], GLUECoS [CITATION] and CreoleVal [CITATION], which focus on under-resourced, code-switched, and creole languages, respectively. Report issue for preceding element\nIt is currently challenging to accurately test the robustness of multilingual models on a large suite of varieties without establishing an NLP evaluation framework covering multiple language clusters (each of which contains standard languages alongside its closely related varieties).Report issue for preceding element\nTo this end, we create DialectBench, a large-scale benchmark covering 40 language clusters with 281 varieties, spanning 10 NLP tasks. We observe that the performance disparity between different varieties of the same language cluster becomes more pronounced when we shift from zero-shot evaluation to fine-tuning on variety data, because of uneven data availability across varieties. Certain language clusters exhibit varying performance across downstream tasks within the same category, due to low-resource limitations. Additionally, we improve the dialectal task coverage for natural language inference by constructing a translate-test evaluation dataset. Putting these all together, DialectBench serves as a comprehensive suite that attains a multifaceted purpose: identifying broader limitations in dialectal NLP, while reflecting on potential areas for improvement. Report issue for preceding element",
    "citations": [
      {
        "tag": "Ruder et\u00a0al. (2023)",
        "title": "XTREME-UP: A user-centric scarce-data benchmark for under-represented languages.",
        "authors": "Sebastian Ruder, Jonathan\u00a0H. Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel\u00a0A. Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana\u00a0L. Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David\u00a0I. Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle, Melvin Johnson, Dmitry Panteleev, and Partha Talukdar. 2023.",
        "journal": ""
      },
      {
        "tag": "Lent et\u00a0al. (2023)",
        "title": "CreoleVal: Multilingual multitask benchmarks for creoles.",
        "authors": "Heather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen, Marcell Fekete, Esther Ploeger, Li\u00a0Zhou, Hans\u00a0Erik Heje, Diptesh Kanojia, Paul Belony, Marcel Bollmann, Lo\u00efc Grobol, Miryam de\u00a0Lhoneux, Daniel Hershcovich, Michel DeGraff, Anders S\u00f8gaard, and Johannes Bjerva. 2023.",
        "journal": ""
      },
      {
        "tag": "Ruder et\u00a0al. (2021)",
        "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation.",
        "authors": "Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021.",
        "journal": "InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215\u201310245, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."
      },
      {
        "tag": "Song et\u00a0al. (2023)",
        "title": "GlobalBench: A benchmark for global progress in natural language processing.",
        "authors": "Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta\u00a0Indra Winata, Alham\u00a0Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, and Graham Neubig. 2023.",
        "journal": ""
      },
      {
        "tag": "Hu et\u00a0al. (2020)",
        "title": "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.",
        "authors": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020.",
        "journal": "InProceedings of the 37th International Conference on Machine Learning, volume 119 ofProceedings of Machine Learning Research, pages 4411\u20134421. PMLR."
      },
      {
        "tag": "Khanuja et\u00a0al. (2020)",
        "title": "GLUECoS: An evaluation benchmark for code-switched NLP.",
        "authors": "Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury. 2020.",
        "journal": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3575\u20133585, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Blasi et\u00a0al. (2022)",
        "title": "Systematic inequalities in language technology performance across the world\u2019s languages.",
        "authors": "Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486\u20135505, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Hedderich et\u00a0al. (2021)",
        "title": "A survey on recent approaches for natural language processing in low-resource scenarios.",
        "authors": "Michael\u00a0A. Hedderich, Lukas Lange, Heike Adel, Jannik Str\u00f6tgen, and Dietrich Klakow. 2021.",
        "journal": "InProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2545\u20132568, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Liang et\u00a0al. (2020)",
        "title": "XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation.",
        "authors": "Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020.",
        "journal": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6008\u20136018, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Chambers and Trudgill (1998)",
        "title": "Dialectology.",
        "authors": "Jack\u00a0K. Chambers and Peter Trudgill. 1998.",
        "journal": "Cambridge University Press."
      }
    ]
  },
  "S2": {
    "title": "2DialectBench",
    "text": "DialectBench is a benchmark created to unify dialectal datasets across different tasks to foster research on language varieties and non-standard dialects. Below we describe the design choices we undertook to achieve this goal. This includes our language variety and task selection procedures, data collation methods, and evaluation principles. Report issue for preceding element\nWe first looked through papers published in the ACL Anthology 2 2 2 https://aclanthology.org from the last 10 years to find usable language resources, as well as commonly used online data repositories Littauer ( n.d. ) . We selected languages that have well-established, high-resourced varieties. Varieties may vary by location, ethnicity, or other factors. We also found instances where varieties are classified by writing system or even by genre (e.g., Twitter). When varying by location, varieties may be classified by different datasets at different levels of granularity , sometimes country, region, or city. In some cases, we found resources with two or more varieties within one dataset (e.g., the UD_Portuguese-Bosque dependency treebank Rademaker et\u00a0al. ( 2017 ) includes examples from both European and Brazilian Portuguese variants. To incorporate all these cases under one paradigm, we formulate a cluster - variety mapping procedure. Report issue for preceding element\nWe construct several language clusters comprising of both high-resourced varieties and their low-resourced counterparts. We use the Glottolog language database Nordhoff ( 2012 ) to define clusters and assign varieties as outlined in Figure\u00a01 . This design choice enables us to keep varieties that are closely related in terms of either mutual intelligibility, phylogenetic similarity or geographic proximity within the same cluster . Hence, all cluster varieties always root back to the closest common linguistic ancestor and the whole cluster maps to an established phylogenetic subtree. For example, Fiji Hindi and Hindi, with Hindustani 3 3 3 https://glottolog.org/resource/languoid/id/hind1270 as their closest common ancestor, are placed in the Hindustani cluster . Report issue for preceding element\nWe primarily use the Glottocode language identification scheme Hammarstr\u00f6m and Forkel ( 2022 ) , ensuring a standardized naming scheme across all varieties .\nFor instance, AAE variety from TwitterAAE Blodgett et\u00a0al. ( 2018 ) dependency parsing dataset is renamed as African American Vernacular English with a corresponding Glottocode afri1276 . In cases where Glottocodes are unavailable, like for spoken English from South India, we substitute with the immediate ancestor Glottocode ( indi1255 ) and further categorize the varieties using the following metadata identifiers: Report issue for preceding element 1. Area (a): the region where the variety is spoken or where its dataset was collected. Report issue for preceding element 2. Language register (r): frozen, formal, consultative, casual, and intimate. Report issue for preceding element 3. Language mode (m): written, spoken, and signed language. Report issue for preceding element 4. Orthography (o): In DialectBench this is only specific to Sinitic varieties . This could be either traditional or simplified. Report issue for preceding element 5. Identifier (i): Dataset-specific metadata, could be domain (eg. twitter). Report issue for preceding element\nArea (a): the region where the variety is spoken or where its dataset was collected. Report issue for preceding element\nLanguage register (r): frozen, formal, consultative, casual, and intimate. Report issue for preceding element\nLanguage mode (m): written, spoken, and signed language. Report issue for preceding element\nOrthography (o): In DialectBench this is only specific to Sinitic varieties . This could be either traditional or simplified. Report issue for preceding element\nIdentifier (i): Dataset-specific metadata, could be domain (eg. twitter). Report issue for preceding element\nWe encapsulate all this information, into a naming convention, and use the template: {Glottocode name}-(a:{},r:{},m:{},o:{},i:{}). 4 4 4 For example, mandarin chinese (a:mainland, o:simplified refers to Mandarin Chinese (mand1415) spoken in Mainland China and written in simplified characters. Report issue for preceding element\nEach cluster will often have a high-resourced variety usually with the largest speaker population. We choose this high-resourced variety as the cluster representative . This selection might vary across downstream tasks depending on the data availability. We primarily utilize this representative variety to evaluate the performance gap across cluster varieties , and also rely on it for transfer-learning in resource-scarce settings. Sometimes, the members of a cluster are considered closely related languages, and sometimes dialects; to avoid making this distinction, we refer to all the members of a cluster simply as varieties of the cluster representative . Report issue for preceding element\nIn selecting tasks, we maintain a balanced approach, promoting task diversity while also including tasks that require diverse levels of textual understanding.\nIn the end, our complete list of tasks are as follows: Report issue for preceding element 1. Dependency parsing (DEP parsing) Report issue for preceding element 2. Parts-of-speech tagging (POS tagging) Report issue for preceding element 3. Named entity recognition (NER) Report issue for preceding element 4. Dialect identification (DId) Report issue for preceding element 5. Sentiment analysis (SA) Report issue for preceding element 6. Topic classification (TC) Report issue for preceding element 7. Natural language inference (NLI) Report issue for preceding element 8. Multiple-choice machine reading comprehension (MRC) Report issue for preceding element 9. Extractive question answering (EQA) Report issue for preceding element 10. Machine translation (MT) Report issue for preceding element\nDependency parsing (DEP parsing) Report issue for preceding element\nParts-of-speech tagging (POS tagging) Report issue for preceding element\nNamed entity recognition (NER) Report issue for preceding element\nDialect identification (DId) Report issue for preceding element\nSentiment analysis (SA) Report issue for preceding element\nTopic classification (TC) Report issue for preceding element\nNatural language inference (NLI) Report issue for preceding element\nMultiple-choice machine reading comprehension (MRC) Report issue for preceding element\nExtractive question answering (EQA) Report issue for preceding element\nMachine translation (MT) Report issue for preceding element\nIn Table 1 , we present the task and dataset details. We mostly keep the datasets in their originally published form (except for varieties renaming). For NLI, we use the existing English test set of XNLI Conneau et\u00a0al. ( 2018 ) and construct a multilingual dialect-focused translated evaluation dataset. We refer to this as translate-test NLI. Report issue for preceding element\nOn the ground level, we evaluate existing NLP systems on text-based tasks using standard evaluation metrics (e.g., UAS for parsing, F1 for classification tasks, BLEU for translation).\nAt a global level, we believe a sustainable NLP system should be user-focused while providing substantial (i) linguistic utility and (ii) demographic utility Song et\u00a0al. ( 2023 ); Blasi et\u00a0al. ( 2022 ) . Blasi et\u00a0al. ( 2022 ) defined the utility of a task and language, as the corresponding performance normalized by the best possible performance (usually human-level performance). Demographic utility considers the demand for a language technology within a specific language, where the demand is proportional to the number of speakers of that language. Linguistic utility , on the other hand, asserts that \u201call languages are created equal\u201d regardless of the number of speakers, and hence all languages in the world should receive identical weights. Report issue for preceding element\nOverall, we want to capture the performance gap between language clusters (e.g., Anglic vs.\u00a0Italian Romance) as well as within language clusters (e.g., Norwegian Bokm\u00e5l vs.\u00a0Nynorsk). To attain this, we define the performance gap metrics in \u00a73.3 . We also vary the experimental settings in \u00a73.2 , including zero-shot and few-shot cross-lingual transfer, as well as fine-tuning with similar high-resource languages. This is essential given that we lack clean annotated data in many varieties . Report issue for preceding element",
    "masked_text": "DialectBench is a benchmark created to unify dialectal datasets across different tasks to foster research on language varieties and non-standard dialects. Below we describe the design choices we undertook to achieve this goal. This includes our language variety and task selection procedures, data collation methods, and evaluation principles.Report issue for preceding element\nWe first looked through papers published in the ACL Anthology222https://aclanthology.org from the last 10 years to find usable language resources, as well as commonly used online data repositories [CITATION]. We selected languages that have well-established, high-resourced varieties. Varieties may vary by location, ethnicity, or other factors. We also found instances where varieties are classified by writing system or even by genre (e.g., Twitter). When varying by location, varieties may be classified by different datasets at different levels of granularity, sometimes country, region, or city. In some cases, we found resources with two or more varieties within one dataset (e.g., the UD_Portuguese-Bosque dependency treebank [CITATION] includes examples from both European and Brazilian Portuguese variants. To incorporate all these cases under one paradigm, we formulate a cluster-variety mapping procedure.Report issue for preceding element\nWe construct several language clusters comprising of both high-resourced varieties and their low-resourced counterparts. We use the Glottolog language database [CITATION] to define clusters and assign varieties as outlined in Figure 1. This design choice enables us to keep varieties that are closely related in terms of either mutual intelligibility, phylogenetic similarity or geographic proximity within the same cluster. Hence, all cluster varieties always root back to the closest common linguistic ancestor and the whole cluster maps to an established phylogenetic subtree. For example, Fiji Hindi and Hindi, with Hindustani333https://glottolog.org/resource/languoid/id/hind1270 as their closest common ancestor, are placed in the Hindustani cluster.Report issue for preceding element\nWe primarily use the Glottocode language identification scheme [CITATION], ensuring a standardized naming scheme across all varieties. For instance, AAE variety from TwitterAAE [CITATION] dependency parsing dataset is renamed as African American Vernacular English with a corresponding Glottocode afri1276. In cases where Glottocodes are unavailable, like for spoken English from South India, we substitute with the immediate ancestor Glottocode (indi1255) and further categorize the varieties using the following metadata identifiers:Report issue for preceding element 1. Area (a): the region where the variety is spoken or where its dataset was collected.Report issue for preceding element 2. Language register (r): frozen, formal, consultative, casual, and intimate.Report issue for preceding element 3. Language mode (m): written, spoken, and signed language.Report issue for preceding element 4. Orthography (o): In DialectBench this is only specific to Sinitic varieties. This could be either traditional or simplified.Report issue for preceding element 5. Identifier (i): Dataset-specific metadata, could be domain (eg. twitter).Report issue for preceding element\nArea (a): the region where the variety is spoken or where its dataset was collected.Report issue for preceding element\nLanguage register (r): frozen, formal, consultative, casual, and intimate.Report issue for preceding element\nLanguage mode (m): written, spoken, and signed language.Report issue for preceding element\nOrthography (o): In DialectBench this is only specific to Sinitic varieties. This could be either traditional or simplified.Report issue for preceding element\nIdentifier (i): Dataset-specific metadata, could be domain (eg. twitter).Report issue for preceding element\nWe encapsulate all this information, into a naming convention, and use the template: {Glottocode name}-(a:{},r:{},m:{},o:{},i:{}). 444For example, mandarin chinese (a:mainland, o:simplified refers to Mandarin Chinese (mand1415) spoken in Mainland China and written in simplified characters.Report issue for preceding element\nEach cluster will often have a high-resourced variety usually with the largest speaker population. We choose this high-resourced variety as the cluster representative. This selection might vary across downstream tasks depending on the data availability. We primarily utilize this representative variety to evaluate the performance gap across cluster varieties, and also rely on it for transfer-learning in resource-scarce settings. Sometimes, the members of a cluster are considered closely related languages, and sometimes dialects; to avoid making this distinction, we refer to all the members of a cluster simply as varieties of the cluster representative.Report issue for preceding element\nIn selecting tasks, we maintain a balanced approach, promoting task diversity while also including tasks that require diverse levels of textual understanding. In the end, our complete list of tasks are as follows:Report issue for preceding element 1. Dependency parsing (DEP parsing)Report issue for preceding element 2. Parts-of-speech tagging (POS tagging)Report issue for preceding element 3. Named entity recognition (NER)Report issue for preceding element 4. Dialect identification (DId) Report issue for preceding element 5. Sentiment analysis (SA)Report issue for preceding element 6. Topic classification (TC)Report issue for preceding element 7. Natural language inference (NLI)Report issue for preceding element 8. Multiple-choice machine reading comprehension (MRC)Report issue for preceding element 9. Extractive question answering (EQA)Report issue for preceding element 10. Machine translation (MT)Report issue for preceding element\nDependency parsing (DEP parsing)Report issue for preceding element\nParts-of-speech tagging (POS tagging)Report issue for preceding element\nNamed entity recognition (NER)Report issue for preceding element\nDialect identification (DId) Report issue for preceding element\nSentiment analysis (SA)Report issue for preceding element\nTopic classification (TC)Report issue for preceding element\nNatural language inference (NLI)Report issue for preceding element\nMultiple-choice machine reading comprehension (MRC)Report issue for preceding element\nExtractive question answering (EQA)Report issue for preceding element\nMachine translation (MT)Report issue for preceding element\nIn Table 1, we present the task and dataset details. We mostly keep the datasets in their originally published form (except for varieties renaming). For NLI, we use the existing English test set of XNLI [CITATION] and construct a multilingual dialect-focused translated evaluation dataset. We refer to this as translate-test NLI. Report issue for preceding element\nOn the ground level, we evaluate existing NLP systems on text-based tasks using standard evaluation metrics (e.g., UAS for parsing, F1 for classification tasks, BLEU for translation). At a global level, we believe a sustainable NLP system should be user-focused while providing substantial (i) linguistic utility and (ii) demographic utility [CITATION]. [CITATION] defined the utility of a task and language, as the corresponding performance normalized by the best possible performance (usually human-level performance). Demographic utility considers the demand for a language technology within a specific language, where the demand is proportional to the number of speakers of that language. Linguistic utility, on the other hand, asserts that \u201call languages are created equal\u201d regardless of the number of speakers, and hence all languages in the world should receive identical weights.Report issue for preceding element\nOverall, we want to capture the performance gap between language clusters (e.g., Anglic vs. Italian Romance) as well as within language clusters (e.g., Norwegian Bokm\u00e5l vs. Nynorsk). To attain this, we define the performance gap metrics in \u00a73.3 . We also vary the experimental settings in \u00a73.2, including zero-shot and few-shot cross-lingual transfer, as well as fine-tuning with similar high-resource languages. This is essential given that we lack clean annotated data in many varieties.Report issue for preceding element",
    "citations": [
      {
        "tag": "Song et\u00a0al. (2023)",
        "title": "GlobalBench: A benchmark for global progress in natural language processing.",
        "authors": "Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta\u00a0Indra Winata, Alham\u00a0Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, and Graham Neubig. 2023.",
        "journal": ""
      },
      {
        "tag": "Nordhoff (2012)",
        "title": "Linked data for linguistic diversity research: Glottolog/langdoc and asjp.",
        "authors": "Sebastian Nordhoff. 2012.",
        "journal": "In Christian Chiarcos, Sebastian Nordhoff, and Sebastian Hellmann, editors,Linked Data in Linguistics. Representing and Connecting Language Data and Language Metadata, pages 191\u2013200. Springer, Heidelberg."
      },
      {
        "tag": "Blodgett et\u00a0al. (2018)",
        "title": "Twitter Universal Dependency parsing for African-American and mainstream American English.",
        "authors": "Su\u00a0Lin Blodgett, Johnny Wei, and Brendan O\u2019Connor. 2018.",
        "journal": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1415\u20131425, Melbourne, Australia. Association for Computational Linguistics."
      },
      {
        "tag": "Conneau et\u00a0al. (2018)",
        "title": "XNLI: Evaluating cross-lingual sentence representations.",
        "authors": "Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.",
        "journal": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Blasi et\u00a0al. (2022)",
        "title": "Systematic inequalities in language technology performance across the world\u2019s languages.",
        "authors": "Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486\u20135505, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Rademaker et\u00a0al. (2017)",
        "title": "Universal Dependencies for Portuguese.",
        "authors": "Alexandre Rademaker, Fabricio Chalub, Livy Real, Cl\u00e1udia Freitas, Eckhard Bick, and Valeria de\u00a0Paiva. 2017.",
        "journal": "InProceedings of the Fourth International Conference on Dependency Linguistics (Depling), pages 197\u2013206, Pisa, Italy."
      },
      {
        "tag": "Littauer (n.d.)",
        "title": "Low resource languages: Resources for conservation, development, and documentation of low resource (human) languages.",
        "authors": "Richard Littauer. n.d.",
        "journal": "https://github.com/RichardLitt/low-resource-languages."
      },
      {
        "tag": "Hammarstr\u00f6m and Forkel (2022)",
        "title": "Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information.",
        "authors": "Harald Hammarstr\u00f6m and Robert Forkel. 2022.",
        "journal": "Semantic Web Journal, 13(6):917\u2013924."
      }
    ]
  },
  "S3": {
    "title": "3Experiments",
    "text": "Here, we report the entire process involved in creating and evaluating baselines for all of the tasks and varieties in DialectBench . Additionally, we define a dialectal gap metric to analyze performance disparities within and across clusters . Report issue for preceding element\nWe evaluate using two multilingual models: mBERT Devlin et\u00a0al. ( 2019 ) and XLM-R Conneau et\u00a0al. ( 2020 ) for all tasks except MT. For MT, we do zero-shot evaluation with NLLB NLLB Team et\u00a0al. ( 2022 ) , using both the 600M and 1.3B variants. In addition, we use Mistral 7B Jiang et\u00a0al. ( 2023 ) to evaluate the current capability of LLMs on multilingual and dialectal understanding tasks.\nOur main goal is collating dialectal data across different languages and tasks under a single platform, hence we do not optimize for the best model performance. Rather, we focus on understanding and reporting the current state of performance on all DialectBench varieties . Report issue for preceding element\nTraining and evaluation procedures are largely determined by the availability of training or evaluation data for each task. Report issue for preceding element\nFor any cluster C \ud835\udc36 C italic_C , let C \u00af \u00af \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}} over\u00af start_ARG italic_C end_ARG be the highest-resourced variety (which is usually the cluster representative ) of C \ud835\udc36 C italic_C .\nIn addition, for any variety v \u2208 C \ud835\udc63 \ud835\udc36 v\\in C italic_v \u2208 italic_C , we write v \u00af \u00af \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}} over\u00af start_ARG italic_v end_ARG for the highest-resourced variety, that is, v \u00af = C \u00af \u00af \ud835\udc63 \u00af \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}={\\color[rgb]{%\n0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke%\n{0}\\pgfsys@color@gray@fill{0}\\bar{C}} over\u00af start_ARG italic_v end_ARG = over\u00af start_ARG italic_C end_ARG .\nFor any varieties t \ud835\udc61 t italic_t and v \ud835\udc63 v italic_v , let \ud835\udcae t \u2062 ( v ) subscript \ud835\udcae \ud835\udc61 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(v)} caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_v ) be the raw evaluation score of a system fine-tuned on t \ud835\udc61 t italic_t and tested on v \ud835\udc63 v italic_v (higher is better). Report issue for preceding element\nWe use six general approaches for task-specific model training: Report issue for preceding element 1. In- variety fine-tuning ( \ud835\udcae v \u2062 ( v ) subscript \ud835\udcae \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)} caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) ): In cases where there is available training data for a variety v \ud835\udc63 v italic_v , we fine-tune the base model on v \ud835\udc63 v italic_v . This primarily applies to tasks such as POS tagging and dependency parsing. Report issue for preceding element 2. In- cluster fine-tuning ( \ud835\udcae v \u00af \u2062 ( v ) subscript \ud835\udcae \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{{\\color[rgb%\n]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}}(v)} caligraphic_S start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG end_POSTSUBSCRIPT ( italic_v ) ): In- variety fine-tuning is quite resource-intensive when we have a large number of varieties within a cluster C \ud835\udc36 C italic_C . In such cases, we fine-tune the base model on C \u00af \u00af \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}} over\u00af start_ARG italic_C end_ARG . Then we evaluate this model on each variety v \u2208 C \ud835\udc63 \ud835\udc36 v\\in C italic_v \u2208 italic_C . This is the most common setting in our experiments, as it allows us to evaluate the dialectal performance gap without increasing the computation cost. For the DId task, we typically use a dataset of sentences annotated with variety labels to fine-tune one dialect-identification model for each language cluster . Report issue for preceding element 3. Combined fine-tuning ( \ud835\udcae \u2112 \u2062 ( v ) subscript \ud835\udcae \u2112 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\mathcal{L}%\n}(v)} caligraphic_S start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT ( italic_v ) ): Unlike in the previous two methods, where each training set contains data from a single language cluster , we fine-tune our baseline question-answering (EQA and MRC) models using the SD-QA Faisal et\u00a0al. ( 2021 ) and Belebele Bandarkar et\u00a0al. ( 2023 ) datasets respectively, both of which contain training data in multiple standard varieties only and test data in other varieties . The SD-QA training data ( \u2112 \u2112 \\mathcal{L} caligraphic_L in the notation) contains questions in 9 standard varieties ( \u2112 = { eng, ara, ben, fin, ind, swa, kor, rus, tel } \u2112 eng, ara, ben, fin, ind, swa, kor, rus, tel \\mathcal{L}=\\{\\text{eng, ara, ben, fin, ind, swa, kor, rus, tel}\\} caligraphic_L = { eng, ara, ben, fin, ind, swa, kor, rus, tel } ), while Belebele assembles data from 6 distinct multiple-choice QA datasets in standard English ( \u2112 = { eng } \u2112 eng \\mathcal{L}=\\{\\text{eng}\\} caligraphic_L = { eng } ). Report issue for preceding element Task In- variety FT Report issue for preceding element In- cluster FT Report issue for preceding element Combined FT Report issue for preceding element Zero-shot Report issue for preceding element No ref-erence Report issue for preceding element In-context learning Report issue for preceding element DEP \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element POS \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element NER \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element EQA \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element MRC \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element NLI \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element TC \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element SA \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element DI \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element MT \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element \u2713 \u2713 \\checkmark \u2713 Report issue for preceding element Table 2: Task-specific training and evaluation procedure. Report issue for preceding element 4. Zero-shot evaluation ( \ud835\udcae eng \u2062 ( v ) subscript \ud835\udcae eng \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{eng}}%\n(v)} caligraphic_S start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( italic_v ) ): For certain varieties , obtaining training data even for in- cluster fine-tuning can be a challenge. Fortunately, English training data is always available for the datasets we study, so we use English to fill the gaps when we lack in- variety , in- cluster , or combined training data. At the same time, we aim to assess the feasibility of using this zero-shot cross-lingual transfer in reducing any existing performance gap across varieties . So we perform zero-shot cross-lingual transfer from English to each variety for 6 tasks in total. We only leave out those tasks such as dialect identification that explicitly require in- cluster training data. Report issue for preceding element 5. In-context learning ( \ud835\udcae icl \u2062 ( v ) subscript \ud835\udcae icl \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{icl}}%\n(v)} caligraphic_S start_POSTSUBSCRIPT icl end_POSTSUBSCRIPT ( italic_v ) ): When evaluating large language models, we do not fine-tune them but instead rely on prompting and in-context learning. For this, we provide instructions and 5 examples in English as exemplars, followed by a prompt for predicting the test examples. Employing Mistral 7B Jiang et\u00a0al. ( 2023 ) , we assess the present effectiveness of a close-to-state-of-the-art LLM on language varieties . The task-specific example prompts are reported in Appendix I . Report issue for preceding element Table 2 summarizes the task-specific training procedures that we employ based on data availability.\nNote that, for MT, we perform zero-shot evaluation specifically in the translation direction, ( standard variety to English ) tested on ( dialectal variety to English ).\nBut evaluation is a challenge because human-created reference translations into or out of non-standard varieties are usually very limited. Therefore, we adopt an evaluation protocol from previous work (Alam et\u00a0al., 2023 ) that uses pseudo-references. Given x \ud835\udc65 x italic_x , a sentence in a variety and x \u00af \u00af \ud835\udc65 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{x}} over\u00af start_ARG italic_x end_ARG , the translation of x \ud835\udc65 x italic_x into the standard variety, let y \ud835\udc66 y italic_y be the output of the MT system on input x \ud835\udc65 x italic_x and y \u00af \u00af \ud835\udc66 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{y}} over\u00af start_ARG italic_y end_ARG be the output on input x \u00af \u00af \ud835\udc65 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{x}} over\u00af start_ARG italic_x end_ARG . Then we measure the quality of y \ud835\udc66 y italic_y (using, e.g., BLEU) compared against y \u00af \u00af \ud835\udc66 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{y}} over\u00af start_ARG italic_y end_ARG as a pseudo-reference. Report issue for preceding element\nIn- variety fine-tuning ( \ud835\udcae v \u2062 ( v ) subscript \ud835\udcae \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)} caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) ): In cases where there is available training data for a variety v \ud835\udc63 v italic_v , we fine-tune the base model on v \ud835\udc63 v italic_v . This primarily applies to tasks such as POS tagging and dependency parsing. Report issue for preceding element\nIn- cluster fine-tuning ( \ud835\udcae v \u00af \u2062 ( v ) subscript \ud835\udcae \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{{\\color[rgb%\n]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}}(v)} caligraphic_S start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG end_POSTSUBSCRIPT ( italic_v ) ): In- variety fine-tuning is quite resource-intensive when we have a large number of varieties within a cluster C \ud835\udc36 C italic_C . In such cases, we fine-tune the base model on C \u00af \u00af \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}} over\u00af start_ARG italic_C end_ARG . Then we evaluate this model on each variety v \u2208 C \ud835\udc63 \ud835\udc36 v\\in C italic_v \u2208 italic_C . This is the most common setting in our experiments, as it allows us to evaluate the dialectal performance gap without increasing the computation cost. For the DId task, we typically use a dataset of sentences annotated with variety labels to fine-tune one dialect-identification model for each language cluster . Report issue for preceding element\nCombined fine-tuning ( \ud835\udcae \u2112 \u2062 ( v ) subscript \ud835\udcae \u2112 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\mathcal{L}%\n}(v)} caligraphic_S start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT ( italic_v ) ): Unlike in the previous two methods, where each training set contains data from a single language cluster , we fine-tune our baseline question-answering (EQA and MRC) models using the SD-QA Faisal et\u00a0al. ( 2021 ) and Belebele Bandarkar et\u00a0al. ( 2023 ) datasets respectively, both of which contain training data in multiple standard varieties only and test data in other varieties . The SD-QA training data ( \u2112 \u2112 \\mathcal{L} caligraphic_L in the notation) contains questions in 9 standard varieties ( \u2112 = { eng, ara, ben, fin, ind, swa, kor, rus, tel } \u2112 eng, ara, ben, fin, ind, swa, kor, rus, tel \\mathcal{L}=\\{\\text{eng, ara, ben, fin, ind, swa, kor, rus, tel}\\} caligraphic_L = { eng, ara, ben, fin, ind, swa, kor, rus, tel } ), while Belebele assembles data from 6 distinct multiple-choice QA datasets in standard English ( \u2112 = { eng } \u2112 eng \\mathcal{L}=\\{\\text{eng}\\} caligraphic_L = { eng } ). Report issue for preceding element\nZero-shot evaluation ( \ud835\udcae eng \u2062 ( v ) subscript \ud835\udcae eng \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{eng}}%\n(v)} caligraphic_S start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( italic_v ) ): For certain varieties , obtaining training data even for in- cluster fine-tuning can be a challenge. Fortunately, English training data is always available for the datasets we study, so we use English to fill the gaps when we lack in- variety , in- cluster , or combined training data. At the same time, we aim to assess the feasibility of using this zero-shot cross-lingual transfer in reducing any existing performance gap across varieties . So we perform zero-shot cross-lingual transfer from English to each variety for 6 tasks in total. We only leave out those tasks such as dialect identification that explicitly require in- cluster training data. Report issue for preceding element\nIn-context learning ( \ud835\udcae icl \u2062 ( v ) subscript \ud835\udcae icl \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{icl}}%\n(v)} caligraphic_S start_POSTSUBSCRIPT icl end_POSTSUBSCRIPT ( italic_v ) ): When evaluating large language models, we do not fine-tune them but instead rely on prompting and in-context learning. For this, we provide instructions and 5 examples in English as exemplars, followed by a prompt for predicting the test examples. Employing Mistral 7B Jiang et\u00a0al. ( 2023 ) , we assess the present effectiveness of a close-to-state-of-the-art LLM on language varieties . The task-specific example prompts are reported in Appendix I . Report issue for preceding element\nTo quantify the performance disparity across various resource-specific settings, language clusters and varieties , we introduce a dialect performance gap metric \ud835\udca2 t \u2062 ( u , v ) subscript \ud835\udca2 \ud835\udc61 \ud835\udc62 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)} caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ) : the relative decrease in performance of a system fine-tuned on variety t \ud835\udc61 t italic_t , tested on variety v \ud835\udc63 v italic_v compared to a baseline variety u \ud835\udc62 u italic_u : Report issue for preceding element \ud835\udca2 t \u2062 ( u , v ) = \ud835\udcae t \u2062 ( u ) \u2212 \ud835\udcae t \u2062 ( v ) \ud835\udcae t \u2062 ( u ) subscript \ud835\udca2 \ud835\udc61 \ud835\udc62 \ud835\udc63 subscript \ud835\udcae \ud835\udc61 \ud835\udc62 subscript \ud835\udcae \ud835\udc61 \ud835\udc63 subscript \ud835\udcae \ud835\udc61 \ud835\udc62 \\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0%\n}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)}=%\n\\frac{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(u)}-{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(v)}}{{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(u)}} caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ) = divide start_ARG caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u ) - caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_v ) end_ARG start_ARG caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u ) end_ARG with a special case for in- variety fine-tuning: \ud835\udca2 in-variety \u2062 ( u , v ) = \ud835\udcae u \u2062 ( u ) \u2212 \ud835\udcae v \u2062 ( v ) \ud835\udcae u \u2062 ( u ) . subscript \ud835\udca2 in-variety \ud835\udc62 \ud835\udc63 subscript \ud835\udcae \ud835\udc62 \ud835\udc62 subscript \ud835\udcae \ud835\udc63 \ud835\udc63 subscript \ud835\udcae \ud835\udc62 \ud835\udc62 \\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0%\n}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}}(u,v)}=\\frac{{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{u}(u)}-{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)}}{{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{u}(u)}}. caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_v ) = divide start_ARG caligraphic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_u ) - caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) end_ARG start_ARG caligraphic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_u ) end_ARG . For the baseline score, we use either the score on the standard variety for each cluster ( u = v \u00af \ud835\udc62 \u00af \ud835\udc63 u={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}} italic_u = over\u00af start_ARG italic_v end_ARG ), or, in the zero-shot setting, the score on the language used for fine-tuning, namely English ( u = t = eng \ud835\udc62 \ud835\udc61 eng u=t=\\text{eng} italic_u = italic_t = eng ). Rather than computing an absolute gap, we opt for a relative gap (i.e., dividing by the baseline score).\nWe also indicate whether the training setting is zero-shot ( t = eng \ud835\udc61 eng t=\\text{eng} italic_t = eng ) or fine-tuned on in- variety , in- cluster , or assembled data.\nPutting all these together, we compute the following three variations of dialectal inequality. Report issue for preceding element\n1. \ud835\udca2 eng \u2062 ( eng , v ) subscript \ud835\udca2 eng eng \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n(\\text{eng},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ) : We calculate this metric to get a comprehensive measurement of global disparity across all varieties in a resource-limited environment (zero-shot transfer from English). Report issue for preceding element 2. \ud835\udca2 eng \u2062 ( v \u00af , v ) subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ) : Using this variation, we keep the setting fixed as zero-shot and calculate the gap between the representative variety and any other variety . Report issue for preceding element 3. \ud835\udca2 t \u2062 ( v \u00af , v ) subscript \ud835\udca2 \ud835\udc61 \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}({\\color[%\nrgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)} caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ) : The two aforementioned metrics shed light on the extent of the variety performance gap in a resource-limited setting. To gain a more comprehensive perspective, we additionally compute another metric, this time utilizing the availability of resources. The computation approach remains as straightforward as before. We just use fine-tuning on a variety t \ud835\udc61 t italic_t instead of zero-shot transfer from Standard English: t = in- variety \ud835\udc61 in- variety t=\\text{in-{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}} italic_t = in- roman_variety for in- variety fine-tuning, t = v \u00af \ud835\udc61 \u00af \ud835\udc63 t={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}} italic_t = over\u00af start_ARG italic_v end_ARG for in- cluster fine-tuning, or t \ud835\udc61 t italic_t is some set of varieties for combined fine-tuning. Report issue for preceding element For all three \ud835\udca2 \ud835\udca2 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}} caligraphic_G metrics, we compute them at the variety level and then average them at the cluster level: Report issue for preceding element \ud835\udca2 t \u2062 ( u , C ) subscript \ud835\udca2 \ud835\udc61 \ud835\udc62 \ud835\udc36 \\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0%\n}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,C)} caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_C ) = 1 | C | \u2062 \u2211 v \u2208 C \ud835\udca2 t \u2062 ( u , v ) absent 1 \ud835\udc36 subscript \ud835\udc63 \ud835\udc36 subscript \ud835\udca2 \ud835\udc61 \ud835\udc62 \ud835\udc63 \\displaystyle=\\frac{1}{|C|}\\sum_{v\\in C}{\\color[rgb]{0,0,0}\\definecolor[named]%\n{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}%\n\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)} = divide start_ARG 1 end_ARG start_ARG | italic_C | end_ARG \u2211 start_POSTSUBSCRIPT italic_v \u2208 italic_C end_POSTSUBSCRIPT caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ) \ud835\udca2 in-variety \u2062 ( u , C ) subscript \ud835\udca2 in-variety \ud835\udc62 \ud835\udc36 \\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0%\n}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{%\n\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}}(u,C)} caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_C ) = 1 | C | \u2062 \u2211 v \u2208 C \ud835\udca2 in-variety \u2062 ( u , v ) . absent 1 \ud835\udc36 subscript \ud835\udc63 \ud835\udc36 subscript \ud835\udca2 in-variety \ud835\udc62 \ud835\udc63 \\displaystyle=\\frac{1}{|C|}\\sum_{v\\in C}{\\color[rgb]{0,0,0}\\definecolor[named]%\n{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}%\n\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{\\color[rgb]{0,0,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}%\n\\pgfsys@color@gray@fill{0}variety}}}(u,v)}. = divide start_ARG 1 end_ARG start_ARG | italic_C | end_ARG \u2211 start_POSTSUBSCRIPT italic_v \u2208 italic_C end_POSTSUBSCRIPT caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_v ) .\n\ud835\udca2 eng \u2062 ( eng , v ) subscript \ud835\udca2 eng eng \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n(\\text{eng},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ) : We calculate this metric to get a comprehensive measurement of global disparity across all varieties in a resource-limited environment (zero-shot transfer from English). Report issue for preceding element\n\ud835\udca2 eng \u2062 ( v \u00af , v ) subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ) : Using this variation, we keep the setting fixed as zero-shot and calculate the gap between the representative variety and any other variety . Report issue for preceding element\n\ud835\udca2 t \u2062 ( v \u00af , v ) subscript \ud835\udca2 \ud835\udc61 \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}({\\color[%\nrgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)} caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ) : The two aforementioned metrics shed light on the extent of the variety performance gap in a resource-limited setting. To gain a more comprehensive perspective, we additionally compute another metric, this time utilizing the availability of resources. The computation approach remains as straightforward as before. We just use fine-tuning on a variety t \ud835\udc61 t italic_t instead of zero-shot transfer from Standard English: t = in- variety \ud835\udc61 in- variety t=\\text{in-{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}} italic_t = in- roman_variety for in- variety fine-tuning, t = v \u00af \ud835\udc61 \u00af \ud835\udc63 t={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}} italic_t = over\u00af start_ARG italic_v end_ARG for in- cluster fine-tuning, or t \ud835\udc61 t italic_t is some set of varieties for combined fine-tuning. Report issue for preceding element",
    "masked_text": "Here, we report the entire process involved in creating and evaluating baselines for all of the tasks and varieties in DialectBench. Additionally, we define a dialectal gap metric to analyze performance disparities within and across clusters.Report issue for preceding element\nWe evaluate using two multilingual models: mBERT [CITATION] and XLM-R [CITATION] for all tasks except MT. For MT, we do zero-shot evaluation with NLLB [CITATION], using both the 600M and 1.3B variants. In addition, we use Mistral 7B [CITATION] to evaluate the current capability of LLMs on multilingual and dialectal understanding tasks. Our main goal is collating dialectal data across different languages and tasks under a single platform, hence we do not optimize for the best model performance. Rather, we focus on understanding and reporting the current state of performance on all DialectBench varieties. Report issue for preceding element\nTraining and evaluation procedures are largely determined by the availability of training or evaluation data for each task.Report issue for preceding element\nFor any cluster C\ud835\udc36Citalic_C, let C\u00af\u00af\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}}over\u00af start_ARG italic_C end_ARG be the highest-resourced variety (which is usually the cluster representative) of C\ud835\udc36Citalic_C. In addition, for any variety v\u2208C\ud835\udc63\ud835\udc36v\\in Citalic_v \u2208 italic_C, we write v\u00af\u00af\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}over\u00af start_ARG italic_v end_ARG for the highest-resourced variety, that is, v\u00af=C\u00af\u00af\ud835\udc63\u00af\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}={\\color[rgb]{% 0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke% {0}\\pgfsys@color@gray@fill{0}\\bar{C}}over\u00af start_ARG italic_v end_ARG = over\u00af start_ARG italic_C end_ARG. For any varieties t\ud835\udc61titalic_t and v\ud835\udc63vitalic_v, let \ud835\udcaet\u2062(v)subscript\ud835\udcae\ud835\udc61\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(v)}caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_v ) be the raw evaluation score of a system fine-tuned on t\ud835\udc61titalic_t and tested on v\ud835\udc63vitalic_v (higher is better).Report issue for preceding element\nWe use six general approaches for task-specific model training: Report issue for preceding element 1. In-variety fine-tuning (\ud835\udcaev\u2062(v)subscript\ud835\udcae\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)}caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v )): In cases where there is available training data for a variety v\ud835\udc63vitalic_v, we fine-tune the base model on v\ud835\udc63vitalic_v. This primarily applies to tasks such as POS tagging and dependency parsing.Report issue for preceding element 2. In-cluster fine-tuning (\ud835\udcaev\u00af\u2062(v)subscript\ud835\udcae\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{{\\color[rgb% ]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}}(v)}caligraphic_S start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG end_POSTSUBSCRIPT ( italic_v )): In-variety fine-tuning is quite resource-intensive when we have a large number of varieties within a cluster C\ud835\udc36Citalic_C. In such cases, we fine-tune the base model on C\u00af\u00af\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}}over\u00af start_ARG italic_C end_ARG. Then we evaluate this model on each variety v\u2208C\ud835\udc63\ud835\udc36v\\in Citalic_v \u2208 italic_C. This is the most common setting in our experiments, as it allows us to evaluate the dialectal performance gap without increasing the computation cost. For the DId task, we typically use a dataset of sentences annotated with variety labels to fine-tune one dialect-identification model for each language cluster.Report issue for preceding element 3. Combined fine-tuning (\ud835\udcae\u2112\u2062(v)subscript\ud835\udcae\u2112\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\mathcal{L}% }(v)}caligraphic_S start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT ( italic_v )): Unlike in the previous two methods, where each training set contains data from a single language cluster, we fine-tune our baseline question-answering (EQA and MRC) models using the SD-QA [CITATION] and Belebele [CITATION] datasets respectively, both of which contain training data in multiple standard varieties only and test data in other varieties. The SD-QA training data (\u2112\u2112\\mathcal{L}caligraphic_L in the notation) contains questions in 9 standard varieties (\u2112={eng, ara, ben, fin, ind, swa, kor, rus, tel}\u2112eng, ara, ben, fin, ind, swa, kor, rus, tel\\mathcal{L}=\\{\\text{eng, ara, ben, fin, ind, swa, kor, rus, tel}\\}caligraphic_L = { eng, ara, ben, fin, ind, swa, kor, rus, tel }), while Belebele assembles data from 6 distinct multiple-choice QA datasets in standard English (\u2112={eng}\u2112eng\\mathcal{L}=\\{\\text{eng}\\}caligraphic_L = { eng }). Report issue for preceding element Task In-variety FTReport issue for preceding element In-cluster FTReport issue for preceding element Combined FTReport issue for preceding element Zero-shotReport issue for preceding element No ref-erenceReport issue for preceding element In-context learningReport issue for preceding element DEP \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element POS \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element NER \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element EQA \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element MRC \u2713\u2713\\checkmark\u2713Report issue for preceding element NLI \u2713\u2713\\checkmark\u2713Report issue for preceding element TC \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element SA \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element DI \u2713\u2713\\checkmark\u2713Report issue for preceding element MT \u2713\u2713\\checkmark\u2713Report issue for preceding element \u2713\u2713\\checkmark\u2713Report issue for preceding element Table 2: Task-specific training and evaluation procedure. Report issue for preceding element 4. Zero-shot evaluation (\ud835\udcaeeng\u2062(v)subscript\ud835\udcaeeng\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{eng}}% (v)}caligraphic_S start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( italic_v )): For certain varieties, obtaining training data even for in-cluster fine-tuning can be a challenge. Fortunately, English training data is always available for the datasets we study, so we use English to fill the gaps when we lack in-variety, in-cluster, or combined training data. At the same time, we aim to assess the feasibility of using this zero-shot cross-lingual transfer in reducing any existing performance gap across varieties. So we perform zero-shot cross-lingual transfer from English to each variety for 6 tasks in total. We only leave out those tasks such as dialect identification that explicitly require in-cluster training data.Report issue for preceding element 5. In-context learning (\ud835\udcaeicl\u2062(v)subscript\ud835\udcaeicl\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{icl}}% (v)}caligraphic_S start_POSTSUBSCRIPT icl end_POSTSUBSCRIPT ( italic_v )): When evaluating large language models, we do not fine-tune them but instead rely on prompting and in-context learning. For this, we provide instructions and 5 examples in English as exemplars, followed by a prompt for predicting the test examples. Employing Mistral 7B [CITATION], we assess the present effectiveness of a close-to-state-of-the-art LLM on language varieties. The task-specific example prompts are reported in Appendix I.Report issue for preceding element Table 2 summarizes the task-specific training procedures that we employ based on data availability. Note that, for MT, we perform zero-shot evaluation specifically in the translation direction, (standard variety to English) tested on (dialectal variety to English). But evaluation is a challenge because human-created reference translations into or out of non-standard varieties are usually very limited. Therefore, we adopt an evaluation protocol from previous work [CITATION] that uses pseudo-references. Given x\ud835\udc65xitalic_x, a sentence in a variety and x\u00af\u00af\ud835\udc65{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{x}}over\u00af start_ARG italic_x end_ARG, the translation of x\ud835\udc65xitalic_x into the standard variety, let y\ud835\udc66yitalic_y be the output of the MT system on input x\ud835\udc65xitalic_x and y\u00af\u00af\ud835\udc66{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{y}}over\u00af start_ARG italic_y end_ARG be the output on input x\u00af\u00af\ud835\udc65{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{x}}over\u00af start_ARG italic_x end_ARG. Then we measure the quality of y\ud835\udc66yitalic_y (using, e.g., BLEU) compared against y\u00af\u00af\ud835\udc66{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{y}}over\u00af start_ARG italic_y end_ARG as a pseudo-reference.Report issue for preceding element\nIn-variety fine-tuning (\ud835\udcaev\u2062(v)subscript\ud835\udcae\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)}caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v )): In cases where there is available training data for a variety v\ud835\udc63vitalic_v, we fine-tune the base model on v\ud835\udc63vitalic_v. This primarily applies to tasks such as POS tagging and dependency parsing.Report issue for preceding element\nIn-cluster fine-tuning (\ud835\udcaev\u00af\u2062(v)subscript\ud835\udcae\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{{\\color[rgb% ]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}}(v)}caligraphic_S start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG end_POSTSUBSCRIPT ( italic_v )): In-variety fine-tuning is quite resource-intensive when we have a large number of varieties within a cluster C\ud835\udc36Citalic_C. In such cases, we fine-tune the base model on C\u00af\u00af\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{C}}over\u00af start_ARG italic_C end_ARG. Then we evaluate this model on each variety v\u2208C\ud835\udc63\ud835\udc36v\\in Citalic_v \u2208 italic_C. This is the most common setting in our experiments, as it allows us to evaluate the dialectal performance gap without increasing the computation cost. For the DId task, we typically use a dataset of sentences annotated with variety labels to fine-tune one dialect-identification model for each language cluster.Report issue for preceding element\nCombined fine-tuning (\ud835\udcae\u2112\u2062(v)subscript\ud835\udcae\u2112\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\mathcal{L}% }(v)}caligraphic_S start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT ( italic_v )): Unlike in the previous two methods, where each training set contains data from a single language cluster, we fine-tune our baseline question-answering (EQA and MRC) models using the SD-QA [CITATION] and Belebele [CITATION] datasets respectively, both of which contain training data in multiple standard varieties only and test data in other varieties. The SD-QA training data (\u2112\u2112\\mathcal{L}caligraphic_L in the notation) contains questions in 9 standard varieties (\u2112={eng, ara, ben, fin, ind, swa, kor, rus, tel}\u2112eng, ara, ben, fin, ind, swa, kor, rus, tel\\mathcal{L}=\\{\\text{eng, ara, ben, fin, ind, swa, kor, rus, tel}\\}caligraphic_L = { eng, ara, ben, fin, ind, swa, kor, rus, tel }), while Belebele assembles data from 6 distinct multiple-choice QA datasets in standard English (\u2112={eng}\u2112eng\\mathcal{L}=\\{\\text{eng}\\}caligraphic_L = { eng }). Report issue for preceding element\nZero-shot evaluation (\ud835\udcaeeng\u2062(v)subscript\ud835\udcaeeng\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{eng}}% (v)}caligraphic_S start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( italic_v )): For certain varieties, obtaining training data even for in-cluster fine-tuning can be a challenge. Fortunately, English training data is always available for the datasets we study, so we use English to fill the gaps when we lack in-variety, in-cluster, or combined training data. At the same time, we aim to assess the feasibility of using this zero-shot cross-lingual transfer in reducing any existing performance gap across varieties. So we perform zero-shot cross-lingual transfer from English to each variety for 6 tasks in total. We only leave out those tasks such as dialect identification that explicitly require in-cluster training data.Report issue for preceding element\nIn-context learning (\ud835\udcaeicl\u2062(v)subscript\ud835\udcaeicl\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{\\text{icl}}% (v)}caligraphic_S start_POSTSUBSCRIPT icl end_POSTSUBSCRIPT ( italic_v )): When evaluating large language models, we do not fine-tune them but instead rely on prompting and in-context learning. For this, we provide instructions and 5 examples in English as exemplars, followed by a prompt for predicting the test examples. Employing Mistral 7B [CITATION], we assess the present effectiveness of a close-to-state-of-the-art LLM on language varieties. The task-specific example prompts are reported in Appendix I.Report issue for preceding element\nTo quantify the performance disparity across various resource-specific settings, language clusters and varieties, we introduce a dialect performance gap metric \ud835\udca2t\u2062(u,v)subscript\ud835\udca2\ud835\udc61\ud835\udc62\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)}caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ): the relative decrease in performance of a system fine-tuned on variety t\ud835\udc61titalic_t, tested on variety v\ud835\udc63vitalic_v compared to a baseline variety u\ud835\udc62uitalic_u:Report issue for preceding element \ud835\udca2t\u2062(u,v)=\ud835\udcaet\u2062(u)\u2212\ud835\udcaet\u2062(v)\ud835\udcaet\u2062(u)subscript\ud835\udca2\ud835\udc61\ud835\udc62\ud835\udc63subscript\ud835\udcae\ud835\udc61\ud835\udc62subscript\ud835\udcae\ud835\udc61\ud835\udc63subscript\ud835\udcae\ud835\udc61\ud835\udc62\\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0% }\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)}=% \\frac{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(u)}-{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(v)}}{{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{t}(u)}}caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ) = divide start_ARG caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u ) - caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_v ) end_ARG start_ARG caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u ) end_ARG with a special case for in-variety fine-tuning: \ud835\udca2in-variety\u2062(u,v)=\ud835\udcaeu\u2062(u)\u2212\ud835\udcaev\u2062(v)\ud835\udcaeu\u2062(u).subscript\ud835\udca2in-variety\ud835\udc62\ud835\udc63subscript\ud835\udcae\ud835\udc62\ud835\udc62subscript\ud835\udcae\ud835\udc63\ud835\udc63subscript\ud835\udcae\ud835\udc62\ud835\udc62\\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0% }\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}}(u,v)}=\\frac{{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{u}(u)}-{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{v}(v)}}{{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{S}_{u}(u)}}.caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_v ) = divide start_ARG caligraphic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_u ) - caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v ) end_ARG start_ARG caligraphic_S start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_u ) end_ARG . For the baseline score, we use either the score on the standard variety for each cluster (u=v\u00af\ud835\udc62\u00af\ud835\udc63u={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}italic_u = over\u00af start_ARG italic_v end_ARG), or, in the zero-shot setting, the score on the language used for fine-tuning, namely English (u=t=eng\ud835\udc62\ud835\udc61engu=t=\\text{eng}italic_u = italic_t = eng). Rather than computing an absolute gap, we opt for a relative gap (i.e., dividing by the baseline score). We also indicate whether the training setting is zero-shot (t=eng\ud835\udc61engt=\\text{eng}italic_t = eng) or fine-tuned on in-variety, in-cluster, or assembled data. Putting all these together, we compute the following three variations of dialectal inequality.Report issue for preceding element\n1. \ud835\udca2eng\u2062(eng,v)subscript\ud835\udca2engeng\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% (\\text{eng},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ): We calculate this metric to get a comprehensive measurement of global disparity across all varieties in a resource-limited environment (zero-shot transfer from English).Report issue for preceding element 2. \ud835\udca2eng\u2062(v\u00af,v)subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% ({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ): Using this variation, we keep the setting fixed as zero-shot and calculate the gap between the representative variety and any other variety.Report issue for preceding element 3. \ud835\udca2t\u2062(v\u00af,v)subscript\ud835\udca2\ud835\udc61\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}({\\color[% rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)}caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ): The two aforementioned metrics shed light on the extent of the variety performance gap in a resource-limited setting. To gain a more comprehensive perspective, we additionally compute another metric, this time utilizing the availability of resources. The computation approach remains as straightforward as before. We just use fine-tuning on a variety t\ud835\udc61titalic_t instead of zero-shot transfer from Standard English: t=in-variety\ud835\udc61in-varietyt=\\text{in-{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}italic_t = in- roman_variety for in-variety fine-tuning, t=v\u00af\ud835\udc61\u00af\ud835\udc63t={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}italic_t = over\u00af start_ARG italic_v end_ARG for in-cluster fine-tuning, or t\ud835\udc61titalic_t is some set of varieties for combined fine-tuning. Report issue for preceding element For all three \ud835\udca2\ud835\udca2{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}}caligraphic_G metrics, we compute them at the variety level and then average them at the cluster level:Report issue for preceding element \ud835\udca2t\u2062(u,C)subscript\ud835\udca2\ud835\udc61\ud835\udc62\ud835\udc36\\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0% }\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,C)}caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_C ) =1|C|\u2062\u2211v\u2208C\ud835\udca2t\u2062(u,v)absent1\ud835\udc36subscript\ud835\udc63\ud835\udc36subscript\ud835\udca2\ud835\udc61\ud835\udc62\ud835\udc63\\displaystyle=\\frac{1}{|C|}\\sum_{v\\in C}{\\color[rgb]{0,0,0}\\definecolor[named]% {pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}% \\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}(u,v)}= divide start_ARG 1 end_ARG start_ARG | italic_C | end_ARG \u2211 start_POSTSUBSCRIPT italic_v \u2208 italic_C end_POSTSUBSCRIPT caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_u , italic_v ) \ud835\udca2in-variety\u2062(u,C)subscript\ud835\udca2in-variety\ud835\udc62\ud835\udc36\\displaystyle{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0% }\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{% \\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}}(u,C)}caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_C ) =1|C|\u2062\u2211v\u2208C\ud835\udca2in-variety\u2062(u,v).absent1\ud835\udc36subscript\ud835\udc63\ud835\udc36subscript\ud835\udca2in-variety\ud835\udc62\ud835\udc63\\displaystyle=\\frac{1}{|C|}\\sum_{v\\in C}{\\color[rgb]{0,0,0}\\definecolor[named]% {pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}% \\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{in-{\\color[rgb]{0,0,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}% \\pgfsys@color@gray@fill{0}variety}}}(u,v)}.= divide start_ARG 1 end_ARG start_ARG | italic_C | end_ARG \u2211 start_POSTSUBSCRIPT italic_v \u2208 italic_C end_POSTSUBSCRIPT caligraphic_G start_POSTSUBSCRIPT in-variety end_POSTSUBSCRIPT ( italic_u , italic_v ) .\n\ud835\udca2eng\u2062(eng,v)subscript\ud835\udca2engeng\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% (\\text{eng},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ): We calculate this metric to get a comprehensive measurement of global disparity across all varieties in a resource-limited environment (zero-shot transfer from English).Report issue for preceding element\n\ud835\udca2eng\u2062(v\u00af,v)subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% ({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ): Using this variation, we keep the setting fixed as zero-shot and calculate the gap between the representative variety and any other variety.Report issue for preceding element\n\ud835\udca2t\u2062(v\u00af,v)subscript\ud835\udca2\ud835\udc61\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{t}({\\color[% rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)}caligraphic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ): The two aforementioned metrics shed light on the extent of the variety performance gap in a resource-limited setting. To gain a more comprehensive perspective, we additionally compute another metric, this time utilizing the availability of resources. The computation approach remains as straightforward as before. We just use fine-tuning on a variety t\ud835\udc61titalic_t instead of zero-shot transfer from Standard English: t=in-variety\ud835\udc61in-varietyt=\\text{in-{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}variety}}italic_t = in- roman_variety for in-variety fine-tuning, t=v\u00af\ud835\udc61\u00af\ud835\udc63t={\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}}italic_t = over\u00af start_ARG italic_v end_ARG for in-cluster fine-tuning, or t\ud835\udc61titalic_t is some set of varieties for combined fine-tuning. Report issue for preceding element",
    "citations": [
      {
        "tag": "NLLB Team et\u00a0al. (2022)",
        "title": "No Language Left Behind: Scaling human-centered machine translation.",
        "authors": "NLLB Team, Marta\u00a0R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al\u00a0Youngblood, Bapi Akula, Loic Barrault, Gabriel\u00a0Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik\u00a0Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip\u00a0Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",
        "journal": "arXiv:2207.04672."
      },
      {
        "tag": "Jiang et\u00a0al. (2023)",
        "title": "Mistral 7B.",
        "authors": "Albert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.",
        "journal": "arXiv:2310.06825."
      },
      {
        "tag": "Conneau et\u00a0al. (2020)",
        "title": "Unsupervised cross-lingual representation learning at scale.",
        "authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.",
        "journal": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Faisal et\u00a0al. (2021)",
        "title": "SD-QA: Spoken dialectal question answering for the real world.",
        "authors": "Fahim Faisal, Sharlina Keshava, Md\u00a0Mahfuz\u00a0Ibn Alam, and Antonios Anastasopoulos. 2021.",
        "journal": "InFindings of the Association for Computational Linguistics: EMNLP 2021, pages 3296\u20133315, Punta Cana, Dominican Republic. Association for Computational Linguistics."
      },
      {
        "tag": "Bandarkar et\u00a0al. (2023)",
        "title": "The Belebele benchmark: a parallel reading comprehension dataset in 122 language variants.",
        "authors": "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya\u00a0Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023.",
        "journal": "arXiv preprint arXiv:2308.16884."
      },
      {
        "tag": "Alam et\u00a0al. (2023)",
        "title": "CODET: A benchmark for contrastive dialectal evaluation of machine translation.",
        "authors": "Md\u00a0Mahfuz\u00a0Ibn Alam, Sina Ahmadi, and Antonios Anastasopoulos. 2023.",
        "journal": ""
      },
      {
        "tag": "Devlin et\u00a0al. (2019)",
        "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
        "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
        "journal": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics."
      }
    ]
  },
  "S4": {
    "title": "4Results",
    "text": "We, first of all, discuss results by highlighting the highest possible score per variety , aka the maximum obtainable evaluation scores regardless of evaluation method or training data. Next, we extend our discussion further by reporting the existing dialectal disparity across clusters and varieties . Report issue for preceding element\nHere we provide key findings from our evaluation on each task.\nA task-specific summary is reported in Table 3 . Detailed results comprising all tasks, models, language clusters and varieties are reported in Tables 10 to 20 in Appendix E . Report issue for preceding element\nWe present visualizations for the task-specific maximum scores. We show the one for Dependency Parsing in Fig. 3 , where we observe that low-resource varieties from language clusters such as Tupi-Guarani (indigenous South American cluster ), Saami and Komi (low-resource Uralic language clusters ) have the lowest performance compared to Standard English and other closely related Germanic clusters . These low-resource varieties are also not included in the pretraining stage of our base language models (eg. mBERT). Furthermore, this trend is evident across all three structured prediction tasks. On the other hand, high-resource Indo-European languages such as Portuguese, French, and Norwegian usually perform better. Report issue for preceding element\nFor DId and SA, we generally collate different datasets for each language cluster and therefore, report the comparative classification results together. As a result, the locality level (e.g. city/region/country) also varies across cluster .\nFor example, we report city-level DId results for Arabic and High German but country-level results for Portuguese, Spanish and English. In the case of SA, we have region/country-level results for Arabic varieties . For TC and NLI, we have the same set of clusters and varieties . However, we only report the zero-shot transfer performance from Standard English for NLI using the newly created translate-test NLI dataset. Report issue for preceding element\nFor TC and NLI, we observe the largest in-cluster disparity in the Kurdish cluster, with Northern Kurdish outperforming all others. The Sotho varieties consistently perform significantly lower compared to other clusters .\nFor all three sequence classification tasks, we generally find the Chinese cluster performing on par with high-resource Latin counterparts. Report issue for preceding element\nWe generally do not see large gaps in performance within varieties in each language cluster . In EQA zero-shot experiments, English and its varieties have the highest performance overall and Korean varieties score the lowest. Combined fine-tuning boosts performance on all language clusters except in English. For MRC the performance threshold peaks at 53.4 for Standard English and the minimum we get is 29 for Southern Sotho. More detailed results are presented in Tables 19 and 15 . Report issue for preceding element\nThe performance gap here varies widely across and within language varieties. Performance is similar within the Swiss-German cluster , with higher performance (see Figure\u00a05 )\nacross regions in Northern Switzerland, which is geographically closer to Germany. The performance gap for Norwegian dialects (Figure 9(b) ) is surprising as we perform zero-shot transfer from Norwegian Nynorsk (a Western dialect) but obtain better performance on the Eastern dialect.\nWithin Arabic, Riyadh is the highest performer while Sfax performs the worst. For the Bengali cluster , Jessore has the highest performance \u2013this is not surprising since it is one of the dialects from which standard Bengali originated Alam et\u00a0al. ( 2023 ) . The Ethiopian variety of Tigrinya exhibits a higher performance than the Eritrean one, even though Tigrinya is more commonly spoken in Eritrea 5 5 5 https://en.wikipedia.org/wiki/Tigrinya_language .\nAmongst the clusters within the Basque cluster, Barkoxe and Maule have the lowest score while Azkaine scores the highest. Report issue for preceding element\nIn Fig. 4 , we plot the zero-shot dialectal gap for three tasks. In the x \ud835\udc65 x italic_x -axis, we report the aggregated cluster-level gap \ud835\udca2 eng \u2062 ( eng , v ) subscript \ud835\udca2 eng eng \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n(\\text{eng},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ) , compared against the fine-tuning variety (standard English) while in y \ud835\udc66 y italic_y -axis we report \ud835\udca2 eng \u2062 ( v \u00af , v ) subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc63 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ) , the gap compared against the representative variety of a cluster . In an ideal scenario, we would want both of these gap values to be close to zero. However, this is certainly not the case. The general observed trend is that the low-resource clusters have higher gaps of both \ud835\udca2 eng \u2062 ( eng , C ) subscript \ud835\udca2 eng eng \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n(\\text{eng},C)} caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_C ) and \ud835\udca2 eng \u2062 ( v \u00af , C ) subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc36 \\mathcal{G}_{\\text{eng}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor%\n}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C) caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) , whereas high-resource Germanic and Sinitic language clusters consistently exhibit low dialectal gaps. That said, certain specific high-resource varieties , such as Standard German and its dialectal counterparts like Swiss German, showcase significant within- cluster dialectal gaps ( Fig. 3(a) ). Report issue for preceding element\nWe primarily report dialectal gaps using zero-shot transfer because the finetuning data available across task and cluster is very disproportionate. Often the in- cluster / variety data is not good enough in terms of data quality and quantity. For example, we have 37 varieties in 13 clusters for dependency parsing but out of these, only 20 varieties have data available for in- variety fine-tuning. This lacking becomes more apparent when we compare the statistics of two types of within-cluster dialectal gaps: zero-shot \ud835\udca2 eng \u2062 ( v \u00af , C ) subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc36 \\mathcal{G}_{\\text{eng}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor%\n}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C) caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) against fine-tuning \ud835\udca2 v \u00af / in-variety \u2062 ( v \u00af , C ) subscript \ud835\udca2 \u00af \ud835\udc63 in-variety \u00af \ud835\udc63 \ud835\udc36 \\mathcal{G}_{\\bar{v}/\\text{in-variety}}({\\color[rgb]{0,0,0}\\definecolor[named]%\n{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}%\n\\pgfsys@color@gray@fill{0}\\bar{v}},C) caligraphic_G start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG / in-variety end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) in Table 4 . In general, the within- cluster dialectal disparity is smaller for zero-shot transfer (i.e. \ud835\udca2 eng \u2062 ( v \u00af , C ) \u2264 | \ud835\udca2 v \u00af / in-variety \u2062 ( v \u00af , C ) | subscript \ud835\udca2 eng \u00af \ud835\udc63 \ud835\udc36 subscript \ud835\udca2 \u00af \ud835\udc63 in-variety \u00af \ud835\udc63 \ud835\udc36 {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}%\n({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)}\\leq|{\\color%\n[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\bar{v}/%\n\\text{in-variety}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}%\n{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)}| caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) \u2264 | caligraphic_G start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG / in-variety end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) | ). Here, the in- cluster / variety fine-tuning results in a higher performance deviation primarily due to the inconsistent variety -specific finetuning data quality. Report issue for preceding element",
    "masked_text": "We, first of all, discuss results by highlighting the highest possible score per variety, aka the maximum obtainable evaluation scores regardless of evaluation method or training data. Next, we extend our discussion further by reporting the existing dialectal disparity across clusters and varieties.Report issue for preceding element\nHere we provide key findings from our evaluation on each task. A task-specific summary is reported in Table 3. Detailed results comprising all tasks, models, language clusters and varieties are reported in Tables 10 to 20 in Appendix E. Report issue for preceding element\nWe present visualizations for the task-specific maximum scores. We show the one for Dependency Parsing in Fig. 3, where we observe that low-resource varieties from language clusters such as Tupi-Guarani (indigenous South American cluster), Saami and Komi (low-resource Uralic language clusters) have the lowest performance compared to Standard English and other closely related Germanic clusters. These low-resource varieties are also not included in the pretraining stage of our base language models (eg. mBERT). Furthermore, this trend is evident across all three structured prediction tasks. On the other hand, high-resource Indo-European languages such as Portuguese, French, and Norwegian usually perform better.Report issue for preceding element\nFor DId and SA, we generally collate different datasets for each language cluster and therefore, report the comparative classification results together. As a result, the locality level (e.g. city/region/country) also varies across cluster. For example, we report city-level DId results for Arabic and High German but country-level results for Portuguese, Spanish and English. In the case of SA, we have region/country-level results for Arabic varieties. For TC and NLI, we have the same set of clusters and varieties. However, we only report the zero-shot transfer performance from Standard English for NLI using the newly created translate-test NLI dataset.Report issue for preceding element\nFor TC and NLI, we observe the largest in-cluster disparity in the Kurdish cluster, with Northern Kurdish outperforming all others. The Sotho varieties consistently perform significantly lower compared to other clusters. For all three sequence classification tasks, we generally find the Chinese cluster performing on par with high-resource Latin counterparts. Report issue for preceding element\nWe generally do not see large gaps in performance within varieties in each language cluster. In EQA zero-shot experiments, English and its varieties have the highest performance overall and Korean varieties score the lowest. Combined fine-tuning boosts performance on all language clusters except in English. For MRC the performance threshold peaks at 53.4 for Standard English and the minimum we get is 29 for Southern Sotho. More detailed results are presented in Tables 19 and 15.Report issue for preceding element\nThe performance gap here varies widely across and within language varieties. Performance is similar within the Swiss-German cluster, with higher performance (see Figure 5) across regions in Northern Switzerland, which is geographically closer to Germany. The performance gap for Norwegian dialects (Figure 9(b)) is surprising as we perform zero-shot transfer from Norwegian Nynorsk (a Western dialect) but obtain better performance on the Eastern dialect. Within Arabic, Riyadh is the highest performer while Sfax performs the worst. For the Bengali cluster, Jessore has the highest performance \u2013this is not surprising since it is one of the dialects from which standard Bengali originated [CITATION]. The Ethiopian variety of Tigrinya exhibits a higher performance than the Eritrean one, even though Tigrinya is more commonly spoken in Eritrea 555https://en.wikipedia.org/wiki/Tigrinya_language. Amongst the clusters within the Basque cluster, Barkoxe and Maule have the lowest score while Azkaine scores the highest.Report issue for preceding element\nIn Fig. 4, we plot the zero-shot dialectal gap for three tasks. In the x\ud835\udc65xitalic_x-axis, we report the aggregated cluster-level gap \ud835\udca2eng\u2062(eng,v)subscript\ud835\udca2engeng\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% (\\text{eng},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_v ), compared against the fine-tuning variety (standard English) while in y\ud835\udc66yitalic_y-axis we report \ud835\udca2eng\u2062(v\u00af,v)subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc63{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% ({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},v)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_v ), the gap compared against the representative variety of a cluster. In an ideal scenario, we would want both of these gap values to be close to zero. However, this is certainly not the case. The general observed trend is that the low-resource clusters have higher gaps of both \ud835\udca2eng\u2062(eng,C)subscript\ud835\udca2engeng\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% (\\text{eng},C)}caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( eng , italic_C ) and \ud835\udca2eng\u2062(v\u00af,C)subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc36\\mathcal{G}_{\\text{eng}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor% }{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ), whereas high-resource Germanic and Sinitic language clusters consistently exhibit low dialectal gaps. That said, certain specific high-resource varieties, such as Standard German and its dialectal counterparts like Swiss German, showcase significant within-cluster dialectal gaps (Fig. 3(a)). Report issue for preceding element\nWe primarily report dialectal gaps using zero-shot transfer because the finetuning data available across task and cluster is very disproportionate. Often the in-cluster/variety data is not good enough in terms of data quality and quantity. For example, we have 37 varieties in 13 clusters for dependency parsing but out of these, only 20 varieties have data available for in-variety fine-tuning. This lacking becomes more apparent when we compare the statistics of two types of within-cluster dialectal gaps: zero-shot \ud835\udca2eng\u2062(v\u00af,C)subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc36\\mathcal{G}_{\\text{eng}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor% }{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) against fine-tuning \ud835\udca2v\u00af/in-variety\u2062(v\u00af,C)subscript\ud835\udca2\u00af\ud835\udc63in-variety\u00af\ud835\udc63\ud835\udc36\\mathcal{G}_{\\bar{v}/\\text{in-variety}}({\\color[rgb]{0,0,0}\\definecolor[named]% {pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}% \\pgfsys@color@gray@fill{0}\\bar{v}},C)caligraphic_G start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG / in-variety end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) in Table 4. In general, the within-cluster dialectal disparity is smaller for zero-shot transfer (i.e. \ud835\udca2eng\u2062(v\u00af,C)\u2264|\ud835\udca2v\u00af/in-variety\u2062(v\u00af,C)|subscript\ud835\udca2eng\u00af\ud835\udc63\ud835\udc36subscript\ud835\udca2\u00af\ud835\udc63in-variety\u00af\ud835\udc63\ud835\udc36{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\text{eng}}% ({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)}\\leq|{\\color% [rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mathcal{G}_{\\bar{v}/% \\text{in-variety}}({\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}% {0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\bar{v}},C)}|caligraphic_G start_POSTSUBSCRIPT eng end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) \u2264 | caligraphic_G start_POSTSUBSCRIPT over\u00af start_ARG italic_v end_ARG / in-variety end_POSTSUBSCRIPT ( over\u00af start_ARG italic_v end_ARG , italic_C ) |). Here, the in-cluster/variety fine-tuning results in a higher performance deviation primarily due to the inconsistent variety-specific finetuning data quality.Report issue for preceding element",
    "citations": [
      {
        "tag": "Alam et\u00a0al. (2023)",
        "title": "CODET: A benchmark for contrastive dialectal evaluation of machine translation.",
        "authors": "Md\u00a0Mahfuz\u00a0Ibn Alam, Sina Ahmadi, and Antonios Anastasopoulos. 2023.",
        "journal": ""
      }
    ]
  },
  "S5": {
    "title": "5Discussion",
    "text": "The highest-performing varieties are mostly standard high-resource languages and a few high-resource dialects (Norwegian dialects) whereas, the majority of the lowest-performing language variants are low-resourced varieties . This clear distinction of language varieties points towards the large existence of in- cluster dialectal gaps. Furthermore, this finding correlates with language script differences. We observe that 77.2% top-10 varieties in terms of maximum obtainable score are written with Latin script. Another finding is the performance instability of\nlow-resource varieties across tasks. For instance, Old Guarani performs better in DEP parsing whereas, Mby\u00e1 Guarani (Paraguay) surpasses it in POS tagging even though the dataset remains the same (i.e. UD). For more detailed comparisons, in Appendix Table 21 , we report the top-10 highest and lowest-scoring varieties across different tasks. Report issue for preceding element\nIn Table 5 , we compare the baseline multilingual models. mBERT was comparatively easier to train than XLM-R using the default learning rates reported in the earlier task experiments. Often for tasks such as MRC, we needed to tune hyperparameters (e.g. learning rate, max. sequence length) in case of XLM-R. However, once we identify a hyperparameter configuration that converges in a zero-shot setting, we use the same to train all the language-variant training for that specific task. As a result, for some low-resource languages, XLM-R does not converge compared to mBERT. For example, XLM-R dependency parsing UAS score for Norwegian-NynorskLIA is 56.08 (zeroshot) and 8.25 (in- variety FT) whereas, we get 78.39 for in- variety mBERT fine-tuning. We suspect this hyperparameter tuning issue is one of the contributing factors toward a lower winning rate of XLM-R in few-shot / fine-tuning settings. However, this could be improved further with an extensive parameter grid search and settings specifically tailored for each language cluster . Report issue for preceding element\nLow-resource varieties written in Latin script receive greater benefit in zero-shot because of effective transfer from high-resource Standard English.\nWith the presence of in- cluster / variety finetuning data, we effectively diminish this script effect to some extent. For example in the Hindi cluster, Fiji performs better than its Latin non-standard counterparts with in- cluster finetuning for NER ( Table 12 ). In summary, if the standard variety of a cluster is non-latin but high-resource, then the success rate of in- cluster / variety fine-tuning tends to be higher. However, where all dialects are low-resource, Latin script varieties utilizing zero-shot transfer, eventually surpass others in the performance hierarchy. As an example, we report the zero-shot NER instances where the low-resource varieties perform better than the representative ones in Appendix Table 22 (most of these use Latin script). Report issue for preceding element\nFor SA and EQA tasks, we have in-context learning results using the Mistral7B LLM. Comparing the performance against zero-shot and fine-tuning using our encoder-based models, we find dialect performance of LLM is better than zero-shot transfer but falls behind the finetuned results.\nOn top of that, data contamination Ahuja et\u00a0al. ( 2023 ) during LLM evaluation is another existing issue while considering these few available dialectal resources. Creating translation-based comparable data might be a solution to perform a fair benchmarking of LLMs on low-resource varieties . Report issue for preceding element\nWe also report the cluster -level population-weighted average (i.e. demographic utility ) which rewards a system more when it provides increased linguistic utility (eg. raw F1 score) for varieties , spoken by a larger population compared to varieties spoken by a smaller population. Alone, this metric could be misleading if we consider the fact that the performance gap among all varieties from a particular cluster should be minimal.\nOn the other hand, solely looking into the linguistic utility average does not give a clear picture either (e.g. often overshadows the larger performance deviation of certain varieties having extreme scores). So for all clusters and tasks, we report the linguistic utility average as well as the demographic utility average, the minimum score of a cluster , and the standard deviation in Tables 23 to 30 . Report issue for preceding element",
    "masked_text": "The highest-performing varieties are mostly standard high-resource languages and a few high-resource dialects (Norwegian dialects) whereas, the majority of the lowest-performing language variants are low-resourced varieties. This clear distinction of language varieties points towards the large existence of in-cluster dialectal gaps. Furthermore, this finding correlates with language script differences. We observe that 77.2% top-10 varieties in terms of maximum obtainable score are written with Latin script. Another finding is the performance instability of low-resource varieties across tasks. For instance, Old Guarani performs better in DEP parsing whereas, Mby\u00e1 Guarani (Paraguay) surpasses it in POS tagging even though the dataset remains the same (i.e. UD). For more detailed comparisons, in Appendix Table 21, we report the top-10 highest and lowest-scoring varieties across different tasks.Report issue for preceding element\nIn Table 5, we compare the baseline multilingual models. mBERT was comparatively easier to train than XLM-R using the default learning rates reported in the earlier task experiments. Often for tasks such as MRC, we needed to tune hyperparameters (e.g. learning rate, max. sequence length) in case of XLM-R. However, once we identify a hyperparameter configuration that converges in a zero-shot setting, we use the same to train all the language-variant training for that specific task. As a result, for some low-resource languages, XLM-R does not converge compared to mBERT. For example, XLM-R dependency parsing UAS score for Norwegian-NynorskLIA is 56.08 (zeroshot) and 8.25 (in-variety FT) whereas, we get 78.39 for in-variety mBERT fine-tuning. We suspect this hyperparameter tuning issue is one of the contributing factors toward a lower winning rate of XLM-R in few-shot / fine-tuning settings. However, this could be improved further with an extensive parameter grid search and settings specifically tailored for each language cluster.Report issue for preceding element\nLow-resource varieties written in Latin script receive greater benefit in zero-shot because of effective transfer from high-resource Standard English. With the presence of in-cluster/variety finetuning data, we effectively diminish this script effect to some extent. For example in the Hindi cluster, Fiji performs better than its Latin non-standard counterparts with in-cluster finetuning for NER (Table 12). In summary, if the standard variety of a cluster is non-latin but high-resource, then the success rate of in-cluster/variety fine-tuning tends to be higher. However, where all dialects are low-resource, Latin script varieties utilizing zero-shot transfer, eventually surpass others in the performance hierarchy. As an example, we report the zero-shot NER instances where the low-resource varieties perform better than the representative ones in Appendix Table 22 (most of these use Latin script).Report issue for preceding element\nFor SA and EQA tasks, we have in-context learning results using the Mistral7B LLM. Comparing the performance against zero-shot and fine-tuning using our encoder-based models, we find dialect performance of LLM is better than zero-shot transfer but falls behind the finetuned results. On top of that, data contamination [CITATION] during LLM evaluation is another existing issue while considering these few available dialectal resources. Creating translation-based comparable data might be a solution to perform a fair benchmarking of LLMs on low-resource varieties.Report issue for preceding element\nWe also report the cluster-level population-weighted average (i.e. demographic utility) which rewards a system more when it provides increased linguistic utility (eg. raw F1 score) for varieties, spoken by a larger population compared to varieties spoken by a smaller population. Alone, this metric could be misleading if we consider the fact that the performance gap among all varieties from a particular cluster should be minimal. On the other hand, solely looking into the linguistic utility average does not give a clear picture either (e.g. often overshadows the larger performance deviation of certain varieties having extreme scores). So for all clusters and tasks, we report the linguistic utility average as well as the demographic utility average, the minimum score of a cluster, and the standard deviation in Tables 23 to 30.Report issue for preceding element",
    "citations": [
      {
        "tag": "Ahuja et\u00a0al. (2023)",
        "title": "Megaverse: Benchmarking large language models across languages, modalities, models and tasks.",
        "authors": "Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. 2023.",
        "journal": ""
      }
    ]
  },
  "S6": {
    "title": "6Conclusion",
    "text": "We propose DialectBench , the first-ever inclusive Dialectal NLP benchmark reporting performance evaluation and disparity across standard and non-standard varieties .\nThis is one step towards the effort of bringing more and more language under the paradigm of NLP technology. We would like to further improve the benchmark, constructing high-quality comparable data and expanding the task coverage to speech-based NLP technologies. Report issue for preceding element",
    "masked_text": "We propose DialectBench, the first-ever inclusive Dialectal NLP benchmark reporting performance evaluation and disparity across standard and non-standard varieties. This is one step towards the effort of bringing more and more language under the paradigm of NLP technology. We would like to further improve the benchmark, constructing high-quality comparable data and expanding the task coverage to speech-based NLP technologies.Report issue for preceding element",
    "citations": []
  },
  "Sx1": {
    "title": "Limitations",
    "text": "The data quality and quantity, variety coverage vary significantly across tasks because of data scarcity issues. We avoid full-scale llm-evaluation consciously because of the uncertain data-contamination issue and their well-known lower performance threshold compared to smaller masked-language-modeling-based fine-tuned models. In addition, we focus on text-based NLP tasks for this current iteration. Moreover, we do not claim the representative varieties of each language clusters to be any kind of superior or standardized forms over the other varieties . These varieties are chosen to perform a well-informed comparison among the perceived well-resourced linguistic variety and its counterparts having lesser data availability. At the same time, the mutual intelligibility and phylogenetic similarity of the similar cluster varieties also vary across cluster and this was not selected in a numerically quantifiable manner. Report issue for preceding element\nTo further improve the evaluation fairness of the current version of DialectBench , we need (i) Parallel corpus utilization to prepare task-specific data (ii) Translation-based task data generation to perform comparable analysis (iii) Quantifying the resource-supply and demand as well as population-coverage Song et\u00a0al. ( 2023 ) to identify where a variety stands in the global landscape of linguistic utility . Here, we have accumulated data for diverse varieties across tasks that vary significantly in terms of quality, example count, and domain. However, to perform a perfectly fair comparison of dialectal inequality, we should consider high-quality comparable data (e.g. parallel corpus, similar varieties across tasks) which is not available at this point. Report issue for preceding element\nDespite our best effort, this benchmark does not include every one of the already published task-specific dialectal datasets. So, our next steps on this project involve hosting the benchmark on the website that displays the current statistics of the datasets in DialectBench . We will also encourage researchers to add new and existing datasets for tasks, language clusters that might be currently missing alongside the respective baselines. Report issue for preceding element\nOur study encompasses a large set of evaluation result tables, their corresponding visualizations and findings analysis. Due to space limitations, we have to move the detailed reports ( Appendix E ) and the rest of the visualizations ( Appendix D ) in the Appendix. To better assist, we include an Appendix Table of Content ( Table 6 ) at the introductory section of Appendix ( Appendix ). Report issue for preceding element",
    "masked_text": "The data quality and quantity, variety coverage vary significantly across tasks because of data scarcity issues. We avoid full-scale llm-evaluation consciously because of the uncertain data-contamination issue and their well-known lower performance threshold compared to smaller masked-language-modeling-based fine-tuned models. In addition, we focus on text-based NLP tasks for this current iteration. Moreover, we do not claim the representative varieties of each language clusters to be any kind of superior or standardized forms over the other varieties. These varieties are chosen to perform a well-informed comparison among the perceived well-resourced linguistic variety and its counterparts having lesser data availability. At the same time, the mutual intelligibility and phylogenetic similarity of the similar cluster varieties also vary across cluster and this was not selected in a numerically quantifiable manner.Report issue for preceding element\nTo further improve the evaluation fairness of the current version of DialectBench, we need (i) Parallel corpus utilization to prepare task-specific data (ii) Translation-based task data generation to perform comparable analysis (iii) Quantifying the resource-supply and demand as well as population-coverage [CITATION] to identify where a variety stands in the global landscape of linguistic utility. Here, we have accumulated data for diverse varieties across tasks that vary significantly in terms of quality, example count, and domain. However, to perform a perfectly fair comparison of dialectal inequality, we should consider high-quality comparable data (e.g. parallel corpus, similar varieties across tasks) which is not available at this point.Report issue for preceding element\nDespite our best effort, this benchmark does not include every one of the already published task-specific dialectal datasets. So, our next steps on this project involve hosting the benchmark on the website that displays the current statistics of the datasets in DialectBench. We will also encourage researchers to add new and existing datasets for tasks, language clusters that might be currently missing alongside the respective baselines.Report issue for preceding element\nOur study encompasses a large set of evaluation result tables, their corresponding visualizations and findings analysis. Due to space limitations, we have to move the detailed reports (Appendix E) and the rest of the visualizations (Appendix D) in the Appendix. To better assist, we include an Appendix Table of Content (Table 6) at the introductory section of Appendix (Appendix). Report issue for preceding element",
    "citations": [
      {
        "tag": "Song et\u00a0al. (2023)",
        "title": "GlobalBench: A benchmark for global progress in natural language processing.",
        "authors": "Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta\u00a0Indra Winata, Alham\u00a0Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, and Graham Neubig. 2023.",
        "journal": ""
      }
    ]
  },
  "Sx2": {
    "title": "Ethics Statement",
    "text": "This work is a compilation of existent dialectal datasets across different tasks, including structured prediction and generative tasks. Our experiments do not particularly optimize for the best model performance of these tasks. Therefore we acknowledge that for some of the tasks, the baseline models might not be robust enough to handle dialectal text hence resulting in wrong predictions and generations. We believe that this underscores the need for building models robust to different language variations and future work should focus on this. Report issue for preceding element",
    "masked_text": "This work is a compilation of existent dialectal datasets across different tasks, including structured prediction and generative tasks. Our experiments do not particularly optimize for the best model performance of these tasks. Therefore we acknowledge that for some of the tasks, the baseline models might not be robust enough to handle dialectal text hence resulting in wrong predictions and generations. We believe that this underscores the need for building models robust to different language variations and future work should focus on this.Report issue for preceding element",
    "citations": []
  }
}