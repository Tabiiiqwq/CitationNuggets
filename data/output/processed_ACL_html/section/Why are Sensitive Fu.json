{
  "S1": {
    "title": "1Introduction",
    "text": "Given dramatic advances in machine learning applications powered by transformer models, there has been substantial interest in understanding which functions are easier or harder to learn and represent using transformers.\nEmpirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking.\nFor instance, Abbe et\u00a0al. ( 2023 ) experimentally argued that heldout generalization is biased towards low-degree polynomials and Bhattamishra et\u00a0al. ( 2023 ) provided empirical evidence that transformers prefer to represent functions of low sensitivity , that is, functions that do not strongly depend on many input bits.\nPerhaps the most prominent example of such learning biases is a consistent difficulty in learning the PARITY function, mapping bitstrings to their parity.\nThis function is extremely sensitive, in the sense that flipping any bit flips the string\u2019s parity.\nEmpirical studies have consistently found that training transformers to compute parities is difficult, and that solutions for shorter inputs do not generalize to longer inputs (e.g. Bhattamishra et\u00a0al., 2020 ; Chiang and Cholak, 2022 ; Del\u00e9tang et\u00a0al., 2023 ; Ruoss et\u00a0al., 2023 ) .\nThis stands in stark contrast to previously-popular reccurent models which easily fit PARITY with correct length generalization (Bhattamishra et\u00a0al., 2020 ) . Report issue for preceding element\nWhile a substantial amount of theoetical work has considered both the learnability (e.g. Edelman et\u00a0al., 2022 ; Ahn et\u00a0al., 2023 ) and the expressiveness of transformers (e.g. Yun et\u00a0al., 2019 ; Hahn, 2020 ; Yao et\u00a0al., 2021 ; Hao et\u00a0al., 2022 ; Merrill et\u00a0al., 2022 ; Merrill and Sabharwal, 2023b ; Chiang et\u00a0al., 2023 ; Strobl et\u00a0al., 2023 ; Strobl, 2023 ; Angluin et\u00a0al., 2023 ) , existing theoretical studies do not consistently explain such learning biases. Hahn ( 2020 ) proved that, under two formal models of self-attention, no transformer can express PARITY at all input lengths.\nHowever, various other formal results showed that slightly relaxed assumptions about the transformer architecture resolved such expressiveness limitations.\nMost notably, Chiang and Cholak ( 2022 ) found that layer norm, by breaking the Lipschitz assumption used in Hahn ( 2020 ) \u2019s Theorem 2, allows expressing PARITY in principle.\nSimultaneously, they empirically confirmed that such a solution could not be practically found via (S)GD training.\nVarious other formal models of transformers (e.g. Weiss et\u00a0al., 2021 ; Merrill and Sabharwal, 2023b , c ; Strobl, 2023 ) can also express PARITY despite its empirical difficulty.\nAs already concluded by Chiang and Cholak ( 2022 ) , these findings highlight a disconnect between expressive capacity and learnability: not all functions which transformers may express in principle are also learnt efficiently. Evidently, existing expressiveness theory for transformers is not able to consistently account for the practical learnability of problems under gradient descent. Report issue for preceding element\nSome prior work has studied the learnability of problems for transformers.\nFor example, Edelman et\u00a0al. ( 2022 ) bound the statistical capacity of the transformer architecture, showing that on those functions that transformers prefer to represent, they can generalize with good sample efficiency.\nNotably, they found that sparse parities could indeed be learned well by transformers.\nHowever, this result does not prove that PARITY, or other highly sensitive functions, are hard to learn, as that technique does not provide a direct characterization of which functions transformers prefer to represent.\nOther work has studied simplified setups such as linear attention (e.g. Ahn et\u00a0al., 2023 ) or individual attention layers (e.g. Sanford et\u00a0al., 2023 ) . Report issue for preceding element\nHere, we provide results that have direct bearing on the learnability of PARITY and other sensitive functions, characterizing the loss landscape of transformers in terms of input-space sensitivity.\nWe formally prove that, for the transformer architecture, parameter settings achieving high sensitivity in input space are necessarily brittle, so that close neighbors in parameter space will usually define different (typically much less sensitive) functions when inputs are long.\nAs a consequence, transformers fitting high-sensitivity functions must inhabit very steep minima.\nWe argue that this explains both difficulty in training and length generalization for PARITY (observed by Bhattamishra et\u00a0al. ( 2020 ); Del\u00e9tang et\u00a0al. ( 2023 ); Ruoss et\u00a0al. ( 2023 ) ), and a low-sensitivity and low-degree bias in random initialization and generalization (observed by Abbe et\u00a0al. ( 2023 ); Bhattamishra et\u00a0al. ( 2023 ) ). Report issue for preceding element\nWhile unique hard attention provably cannot represent PARITY (Hahn, 2020 ; Hao et\u00a0al., 2022 ; Angluin et\u00a0al., 2023 ) , more realistic upper bounds accounting for soft attention (Weiss et\u00a0al., 2021 ; Merrill and Sabharwal, 2023b , c ; Strobl, 2023 ; Chiang and Cholak, 2022 ) leave the hardness of sensitive functions unexplained.\nNot only does PARITY have transformers (Chiang and Cholak, 2022 ) , but it can also be easily represented in formalisms that have been suggested to meaningfully upper-bound the abilities of various formal models of soft-attention: 1 1 1 Zhou et\u00a0al. ( 2023 ) suggest that PARITY may not be representable in the RASP-L model, though the expressiveness of RASP-L is not well understood. Report issue for preceding element\nSimple representations for PARITY, valid across all input lengths, exist in RASP (Weiss et\u00a0al., 2021 ) , uniform circuits with majority gates (Merrill and Sabharwal, 2023c ; Strobl, 2023 ) , and FO[M] (Merrill and Sabharwal, 2023b ) . Report issue for preceding element\nWe prove this in Appendix A .\nThus, existing expressiveness bounds do not account for the difficulty that transformers encounter in learning sensitive functions, in particular given that previously-popular recurrent models do not encounter this difficulty.\nAnother family of results consists of Lipschitzness bounds (Hahn, 2020 ; Li et\u00a0al., 2023 ) , which bound the influence that any individual input bit has on the output of a transformer.\nThese turn out to underpredict the abilities of transformers: Report issue for preceding element\nBy results of Hahn ( 2020 ); Li et\u00a0al. ( 2023 ) , the following holds:\nConsider a transformer without layer norm.\nIf x , x \u2032 \u2208 { \u00b1 1 } n \ud835\udc65 superscript \ud835\udc65 \u2032 superscript plus-or-minus 1 \ud835\udc5b x,x^{\\prime}\\in\\{\\pm 1\\}^{n} italic_x , italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT differ only in the i \ud835\udc56 i italic_i -th bit, then at any other position j \u2260 i \ud835\udc57 \ud835\udc56 j\\neq i italic_j \u2260 italic_i , the output of a transformer differs only by \ud835\udcaa \u2062 ( 1 n ) \ud835\udcaa 1 \ud835\udc5b \\mathcal{O}(\\frac{1}{n}) caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ) . Report issue for preceding element\nThis accounts for the difficulty of learning PARITY.\nBut the bound suggests even simple sparse functions, such as FIRST (the language 1 \u2062 ( 0 | 1 ) \u2217 1 superscript conditional 0 1 1(0|1)^{*} 1 ( 0 | 1 ) start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) to be difficult, but transformers learn these well (Bhattamishra et\u00a0al., 2023 ; Edelman et\u00a0al., 2022 ) . Indeed, Chiang and Cholak ( 2022 ) note that the bound is overcome by layer norm or input-length-dependent scaling of attention logits, which enable modeling of sparse functions. Report issue for preceding element\nWe will show that the observed low-sensitivity bias can be understood in terms of the loss landscape : while transformers can express highly sensitive functions, such transformers are isolated in parameter space, and minima interpolating a sensitive function are very sharp.\nIndeed, we prove that tiny perturbations of a highly sensitive transformer tend to define, when inputs are sufficiently long, very different functions with much lower sensitivity. Report issue for preceding element",
    "masked_text": "Given dramatic advances in machine learning applications powered by transformer models, there has been substantial interest in understanding which functions are easier or harder to learn and represent using transformers. Empirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking. For instance, [CITATION] experimentally argued that heldout generalization is biased towards low-degree polynomials and [CITATION] provided empirical evidence that transformers prefer to represent functions of low sensitivity, that is, functions that do not strongly depend on many input bits. Perhaps the most prominent example of such learning biases is a consistent difficulty in learning the PARITY function, mapping bitstrings to their parity. This function is extremely sensitive, in the sense that flipping any bit flips the string\u2019s parity. Empirical studies have consistently found that training transformers to compute parities is difficult, and that solutions for shorter inputs do not generalize to longer inputs [CITATION]. This stands in stark contrast to previously-popular reccurent models which easily fit PARITY with correct length generalization [CITATION].Report issue for preceding element\nWhile a substantial amount of theoetical work has considered both the learnability [CITATION] and the expressiveness of transformers [CITATION], existing theoretical studies do not consistently explain such learning biases. [CITATION] proved that, under two formal models of self-attention, no transformer can express PARITY at all input lengths. However, various other formal results showed that slightly relaxed assumptions about the transformer architecture resolved such expressiveness limitations. Most notably, [CITATION] found that layer norm, by breaking the Lipschitz assumption used in [CITATION]\u2019s Theorem 2, allows expressing PARITY in principle. Simultaneously, they empirically confirmed that such a solution could not be practically found via (S)GD training. Various other formal models of transformers [CITATION] can also express PARITY despite its empirical difficulty. As already concluded by [CITATION], these findings highlight a disconnect between expressive capacity and learnability: not all functions which transformers may express in principle are also learnt efficiently. Evidently, existing expressiveness theory for transformers is not able to consistently account for the practical learnability of problems under gradient descent.Report issue for preceding element\nSome prior work has studied the learnability of problems for transformers. For example, [CITATION] bound the statistical capacity of the transformer architecture, showing that on those functions that transformers prefer to represent, they can generalize with good sample efficiency. Notably, they found that sparse parities could indeed be learned well by transformers. However, this result does not prove that PARITY, or other highly sensitive functions, are hard to learn, as that technique does not provide a direct characterization of which functions transformers prefer to represent. Other work has studied simplified setups such as linear attention [CITATION] or individual attention layers [CITATION].Report issue for preceding element\nHere, we provide results that have direct bearing on the learnability of PARITY and other sensitive functions, characterizing the loss landscape of transformers in terms of input-space sensitivity. We formally prove that, for the transformer architecture, parameter settings achieving high sensitivity in input space are necessarily brittle, so that close neighbors in parameter space will usually define different (typically much less sensitive) functions when inputs are long. As a consequence, transformers fitting high-sensitivity functions must inhabit very steep minima. We argue that this explains both difficulty in training and length generalization for PARITY (observed by [CITATION]), and a low-sensitivity and low-degree bias in random initialization and generalization (observed by [CITATION]).Report issue for preceding element\nWhile unique hard attention provably cannot represent PARITY [CITATION], more realistic upper bounds accounting for soft attention [CITATION] leave the hardness of sensitive functions unexplained. Not only does PARITY have transformers [CITATION], but it can also be easily represented in formalisms that have been suggested to meaningfully upper-bound the abilities of various formal models of soft-attention:111 [CITATION] suggest that PARITY may not be representable in the RASP-L model, though the expressiveness of RASP-L is not well understood.Report issue for preceding element\nSimple representations for PARITY, valid across all input lengths, exist in RASP [CITATION], uniform circuits with majority gates [CITATION], and FO[M] [CITATION].Report issue for preceding element\nWe prove this in Appendix A. Thus, existing expressiveness bounds do not account for the difficulty that transformers encounter in learning sensitive functions, in particular given that previously-popular recurrent models do not encounter this difficulty. Another family of results consists of Lipschitzness bounds [CITATION], which bound the influence that any individual input bit has on the output of a transformer. These turn out to underpredict the abilities of transformers:Report issue for preceding element\nBy results of [CITATION], the following holds: Consider a transformer without layer norm. If x,x\u2032\u2208{\u00b11}n\ud835\udc65superscript\ud835\udc65\u2032superscriptplus-or-minus1\ud835\udc5bx,x^{\\prime}\\in\\{\\pm 1\\}^{n}italic_x , italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT differ only in the i\ud835\udc56iitalic_i-th bit, then at any other position j\u2260i\ud835\udc57\ud835\udc56j\\neq iitalic_j \u2260 italic_i, the output of a transformer differs only by \ud835\udcaa\u2062(1n)\ud835\udcaa1\ud835\udc5b\\mathcal{O}(\\frac{1}{n})caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ).Report issue for preceding element\nThis accounts for the difficulty of learning PARITY. But the bound suggests even simple sparse functions, such as FIRST (the language 1\u2062(0|1)\u22171superscriptconditional011(0|1)^{*}1 ( 0 | 1 ) start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT) to be difficult, but transformers learn these well [CITATION]. Indeed, [CITATION] note that the bound is overcome by layer norm or input-length-dependent scaling of attention logits, which enable modeling of sparse functions.Report issue for preceding element\nWe will show that the observed low-sensitivity bias can be understood in terms of the loss landscape: while transformers can express highly sensitive functions, such transformers are isolated in parameter space, and minima interpolating a sensitive function are very sharp. Indeed, we prove that tiny perturbations of a highly sensitive transformer tend to define, when inputs are sufficiently long, very different functions with much lower sensitivity.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhou et\u00a0al. (2023)",
        "title": "What algorithms can transformers learn? a study in length\ngeneralization.",
        "authors": "Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh\nSusskind, Samy Bengio, and Preetum Nakkiran. 2023.",
        "journal": "arXiv preprint arXiv:2310.16028."
      },
      {
        "tag": "Sanford et\u00a0al. (2023)",
        "title": "Representational\nstrengths and limitations of transformers.",
        "authors": "Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2023.",
        "journal": "CoRR, abs/2306.02896."
      },
      {
        "tag": "Strobl et\u00a0al. (2023)",
        "title": "Transformers as\nrecognizers of formal languages: A survey on expressivity.",
        "authors": "Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. 2023.",
        "journal": "CoRR, abs/2311.00208."
      },
      {
        "tag": "Merrill and Sabharwal (2023c)",
        "title": "The parallelism tradeoff: Limitations of log-precision transformers.",
        "authors": "William Merrill and Ashish Sabharwal. 2023c.",
        "journal": "Transactions of the Association for Computational Linguistics,\n11:531\u2013545."
      },
      {
        "tag": "Ruoss et\u00a0al. (2023)",
        "title": "Randomized positional\nencodings boost length generalization of transformers.",
        "authors": "Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert\nCsord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. 2023.",
        "journal": ""
      },
      {
        "tag": "Angluin et\u00a0al. (2023)",
        "title": "Masked hard-attention transformers and boolean rasp recognize exactly\nthe star-free languages.",
        "authors": "Dana Angluin, David Chiang, and Andy Yang. 2023.",
        "journal": "arXiv preprint arXiv:2310.13897."
      },
      {
        "tag": "Merrill and Sabharwal (2023b)",
        "title": "A logic for expressing log-precision transformers.",
        "authors": "William Merrill and Ashish Sabharwal. 2023b.",
        "journal": "InThirty-seventh Conference on Neural Information Processing\nSystems."
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2020)",
        "title": "On the\nability and limitations of transformers to recognize formal languages.",
        "authors": "Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020.",
        "journal": "InProceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online, November 16-20, 2020,\npages 7096\u20137116. Association for Computational Linguistics."
      },
      {
        "tag": "Yao et\u00a0al. (2021)",
        "title": "Self-attention\nnetworks can process bounded hierarchical languages.",
        "authors": "Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021.",
        "journal": "InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers). Association for\nComputational Linguistics."
      },
      {
        "tag": "Merrill et\u00a0al. (2022)",
        "title": "Saturated transformers are constant-depth threshold circuits.",
        "authors": "William Merrill, Ashish Sabharwal, and Noah\u00a0A. Smith. 2022.",
        "journal": "Trans. Assoc. Comput. Linguistics, 10:843\u2013856."
      },
      {
        "tag": "Edelman et\u00a0al. (2022)",
        "title": "Inductive biases and variable creation in self-attention mechanisms.",
        "authors": "Benjamin\u00a0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages\n5793\u20135831. PMLR."
      },
      {
        "tag": "Hao et\u00a0al. (2022)",
        "title": "Formal language recognition by hard attention transformers:\nPerspectives from circuit complexity.",
        "authors": "Yiding Hao, Dana Angluin, and Robert Frank. 2022.",
        "journal": "Transactions of the Association for Computational Linguistics,\n10:800\u2013810."
      },
      {
        "tag": "Hahn (2020)",
        "title": "Theoretical limitations of self-attention in neural sequence models.",
        "authors": "Michael Hahn. 2020.",
        "journal": "Transactions of the Association for Computational Linguistics,\n8:156\u2013171."
      },
      {
        "tag": "Li et\u00a0al. (2023)",
        "title": "Transformers\nas algorithms: Generalization and stability in in-context learning.",
        "authors": "Yingcong Li, Muhammed\u00a0Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.\n2023.",
        "journal": "InProceedings of the 40th International Conference on Machine\nLearning, volume 202 ofProceedings of Machine Learning Research,\npages 19565\u201319594. PMLR."
      },
      {
        "tag": "Chiang et\u00a0al. (2023)",
        "title": "Tighter\nbounds on the expressivity of transformer encoders.",
        "authors": "David Chiang, Peter Cholak, and Anand Pillay. 2023.",
        "journal": "InInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\nMachine Learning Research, pages 5544\u20135562. PMLR."
      },
      {
        "tag": "Weiss et\u00a0al. (2021)",
        "title": "Thinking like transformers.",
        "authors": "Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.",
        "journal": "InInternational Conference on Machine Learning, pages\n11080\u201311090. PMLR."
      },
      {
        "tag": "Abbe et\u00a0al. (2023)",
        "title": "Generalization on the unseen, logic reasoning and degree curriculum.",
        "authors": "Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. 2023.",
        "journal": "InInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\nMachine Learning Research, pages 31\u201360. PMLR."
      },
      {
        "tag": "Ahn et\u00a0al. (2023)",
        "title": "Linear attention is (maybe)\nall you need (to understand transformer optimization).",
        "authors": "Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit\nSra. 2023.",
        "journal": ""
      },
      {
        "tag": "Yun et\u00a0al. (2019)",
        "title": "Are transformers universal\napproximators of sequence-to-sequence functions?",
        "authors": "Chulhee Yun, Srinadh Bhojanapalli, Ankit\u00a0Singh Rawat, Sashank\u00a0J. Reddi, and\nSanjiv Kumar. 2019.",
        "journal": ""
      },
      {
        "tag": "Strobl (2023)",
        "title": "Average-hard\nattention transformers are constant-depth uniform threshold circuits.",
        "authors": "Lena Strobl. 2023.",
        "journal": "CoRR, abs/2308.03212."
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2023)",
        "title": "Simplicity\nbias in transformers and their ability to learn sparse boolean functions.",
        "authors": "Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. 2023.",
        "journal": "InProceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 5767\u20135791. Association for Computational\nLinguistics."
      },
      {
        "tag": "Chiang and Cholak (2022)",
        "title": "Overcoming a\ntheoretical limitation of self-attention.",
        "authors": "David Chiang and Peter Cholak. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 7654\u20137664. Association for Computational\nLinguistics."
      },
      {
        "tag": "Del\u00e9tang et\u00a0al. (2023)",
        "title": "Neural networks\nand the chomsky hierarchy.",
        "authors": "Gr\u00e9goire Del\u00e9tang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein,\nLi\u00a0Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel\nVeness, and Pedro\u00a0A. Ortega. 2023.",
        "journal": ""
      }
    ]
  },
  "S2": {
    "title": "2Model of Transformers",
    "text": "We will focus on boolean functions.\nFollowing the conventions in the Analysis of Boolean Functions literature (O\u2019Donnell, 2014 ) in modeling bitstrings as elements of { \u2212 1 , 1 } n superscript 1 1 \ud835\udc5b \\{-1,1\\}^{n} { - 1 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , we assume the alphabet \u03a3 = { \u2212 1 , 1 } \u03a3 1 1 \\Sigma=\\{-1,1\\} roman_\u03a3 = { - 1 , 1 } , with word embeddings e \u2062 ( \u2212 1 ) , e \u2062 ( + 1 ) \u2208 \u211d d \ud835\udc52 1 \ud835\udc52 1 superscript \u211d \ud835\udc51 e(-1),e(+1)\\in\\mathbb{R}^{d} italic_e ( - 1 ) , italic_e ( + 1 ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\nThere further are positional encodings p 1 , p 2 , p 3 , \u22ef \u2208 \u211d d subscript \ud835\udc5d 1 subscript \ud835\udc5d 2 subscript \ud835\udc5d 3 \u22ef superscript \u211d \ud835\udc51 p_{1},p_{2},p_{3},\\dots\\in\\mathbb{R}^{d} italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u22ef \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\nAt the zero-th layer, token and positional encodings are added: y i ( 0 ) := e \u2062 ( x i ) + p i assign superscript subscript \ud835\udc66 \ud835\udc56 0 \ud835\udc52 subscript \ud835\udc65 \ud835\udc56 subscript \ud835\udc5d \ud835\udc56 y_{i}^{(0)}:=e(x_{i})+p_{i} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT := italic_e ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( i = 1 , \u2026 , n \ud835\udc56 1 \u2026 \ud835\udc5b i=1,\\dots,n italic_i = 1 , \u2026 , italic_n ), where x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the input string. Report issue for preceding element\nA transformer has a fixed number L \ud835\udc3f L italic_L of layers ; the activations y i ( k ) \u2208 \u211d d superscript subscript \ud835\udc66 \ud835\udc56 \ud835\udc58 superscript \u211d \ud835\udc51 y_{i}^{(k)}\\in\\mathbb{R}^{d} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT at position i \ud835\udc56 i italic_i of the k \ud835\udc58 k italic_k -th layer ( k = 1 , \u2026 , L \ud835\udc58 1 \u2026 \ud835\udc3f k=1,\\dots,L italic_k = 1 , \u2026 , italic_L ) are defined as follows.\nEach layer has a set of H \ud835\udc3b H italic_H attention heads ; we first compute attention scores for the h \u210e h italic_h -th head: Report issue for preceding element a i , j ( k , h ) = superscript subscript \ud835\udc4e \ud835\udc56 \ud835\udc57 \ud835\udc58 \u210e absent \\displaystyle a_{i,j}^{(k,h)}= italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = ( K k , h \u2062 y j ( k \u2212 1 ) ) T \u2062 Q k , h \u2062 y i ( k \u2212 1 ) superscript subscript \ud835\udc3e \ud835\udc58 \u210e superscript subscript \ud835\udc66 \ud835\udc57 \ud835\udc58 1 \ud835\udc47 subscript \ud835\udc44 \ud835\udc58 \u210e superscript subscript \ud835\udc66 \ud835\udc56 \ud835\udc58 1 \\displaystyle(K_{k,h}y_{j}^{(k-1)})^{T}Q_{k,h}y_{i}^{(k-1)} ( italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT a ^ i , j ( k , h ) = superscript subscript ^ \ud835\udc4e \ud835\udc56 \ud835\udc57 \ud835\udc58 \u210e absent \\displaystyle\\widehat{a}_{i,j}^{(k,h)}= over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = exp \u2061 ( a i , j ( k , h ) ) \u2211 s a i , s ( k , h ) superscript subscript \ud835\udc4e \ud835\udc56 \ud835\udc57 \ud835\udc58 \u210e subscript \ud835\udc60 superscript subscript \ud835\udc4e \ud835\udc56 \ud835\udc60 \ud835\udc58 \u210e \\displaystyle\\frac{\\exp(a_{i,j}^{(k,h)})}{\\sum_{s}a_{i,s}^{(k,h)}} divide start_ARG roman_exp ( italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT end_ARG where K k , h subscript \ud835\udc3e \ud835\udc58 \u210e K_{k,h} italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (\u201ckey\u201d), Q k , h subscript \ud835\udc44 \ud835\udc58 \u210e Q_{k,h} italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (\u201cquery\u201d) are \u2208 \u211d d \u00d7 d absent superscript \u211d \ud835\udc51 \ud835\udc51 \\in\\mathbb{R}^{d\\times d} \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d end_POSTSUPERSCRIPT .\nThe activation of the head is computed by weighting according to attention weights a ^ i , j ( k , h ) superscript subscript ^ \ud835\udc4e \ud835\udc56 \ud835\udc57 \ud835\udc58 \u210e \\hat{a}_{i,j}^{(k,h)} over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT , and applying a linear transformation V \ud835\udc49 V italic_V (\u201cvalue\u201d): Report issue for preceding element b i , h ( k ) = \u2211 j = 1 n a ^ i , j ( k , h ) \u2062 V k , h \u2062 y j ( k \u2212 1 ) superscript subscript \ud835\udc4f \ud835\udc56 \u210e \ud835\udc58 superscript subscript \ud835\udc57 1 \ud835\udc5b superscript subscript ^ \ud835\udc4e \ud835\udc56 \ud835\udc57 \ud835\udc58 \u210e subscript \ud835\udc49 \ud835\udc58 \u210e superscript subscript \ud835\udc66 \ud835\udc57 \ud835\udc58 1 b_{i,h}^{(k)}=\\sum_{j=1}^{n}\\hat{a}_{i,j}^{(k,h)}V_{k,h}y_{j}^{(k-1)} italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT (1) The per-position activations are then computed as Report issue for preceding element Y i ( k ) := f M \u2062 L \u2062 P \u2062 ( y i ( k \u2212 1 ) + \u2211 h = 1 H b i , h ( k ) ) assign superscript subscript \ud835\udc4c \ud835\udc56 \ud835\udc58 superscript \ud835\udc53 \ud835\udc40 \ud835\udc3f \ud835\udc43 superscript subscript \ud835\udc66 \ud835\udc56 \ud835\udc58 1 superscript subscript \u210e 1 \ud835\udc3b superscript subscript \ud835\udc4f \ud835\udc56 \u210e \ud835\udc58 Y_{i}^{(k)}:=f^{MLP}\\left(y_{i}^{(k-1)}+\\sum_{h=1}^{H}b_{i,h}^{(k)}\\right) italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (2) where f M \u2062 L \u2062 P superscript \ud835\udc53 \ud835\udc40 \ud835\udc3f \ud835\udc43 f^{MLP} italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT is a one-layer MLP with a skip-connection.\nTransformers additionally implement layer norm (Ba et\u00a0al., 2016 ) : Report issue for preceding element L \u2062 a \u2062 y \u2062 e \u2062 r \u2062 N \u2062 o \u2062 r \u2062 m \u2062 ( y ) := y \u2212 m \u2062 e \u2062 a \u2062 n \u2062 ( y ) \u03c3 2 \u2062 ( y ) + \u03f5 assign \ud835\udc3f \ud835\udc4e \ud835\udc66 \ud835\udc52 \ud835\udc5f \ud835\udc41 \ud835\udc5c \ud835\udc5f \ud835\udc5a \ud835\udc66 \ud835\udc66 \ud835\udc5a \ud835\udc52 \ud835\udc4e \ud835\udc5b \ud835\udc66 superscript \ud835\udf0e 2 \ud835\udc66 italic-\u03f5 LayerNorm(y):=\\frac{y-mean(y)}{\\sqrt{\\sigma^{2}(y)+\\epsilon}} italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) := divide start_ARG italic_y - italic_m italic_e italic_a italic_n ( italic_y ) end_ARG start_ARG square-root start_ARG italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_y ) + italic_\u03f5 end_ARG end_ARG (3) where \u03f5 \u2265 0 italic-\u03f5 0 \\epsilon\\geq 0 italic_\u03f5 \u2265 0 is a hyperparameter ensuring numerical stability, and \u03c3 2 \u2062 ( \u22c5 ) superscript \ud835\udf0e 2 \u22c5 \\sigma^{2}(\\cdot) italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u22c5 ) denotes the variance.\nBy design, \u2016 L \u2062 a \u2062 y \u2062 e \u2062 r \u2062 N \u2062 o \u2062 r \u2062 m \u2062 ( y ) \u2016 2 \u2264 d subscript norm \ud835\udc3f \ud835\udc4e \ud835\udc66 \ud835\udc52 \ud835\udc5f \ud835\udc41 \ud835\udc5c \ud835\udc5f \ud835\udc5a \ud835\udc66 2 \ud835\udc51 \\|LayerNorm(y)\\|_{2}\\leq\\sqrt{d} \u2225 italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2264 square-root start_ARG italic_d end_ARG , with equality at \u03f5 = 0 italic-\u03f5 0 \\epsilon=0 italic_\u03f5 = 0 .\nTransformer variants differ in where exactly layer norm is applied (e.g. Takase et\u00a0al., 2022 ) ; we here assume for notational simplicity that layer norm applies after the MLP, but the details are irrelevant to our results, provided layer norm applies at least once. We thus set: Report issue for preceding element y i ( k ) := L \u2062 a \u2062 y \u2062 e \u2062 r \u2062 N \u2062 o \u2062 r \u2062 m \u2062 ( Y i ( k ) ) assign superscript subscript \ud835\udc66 \ud835\udc56 \ud835\udc58 \ud835\udc3f \ud835\udc4e \ud835\udc66 \ud835\udc52 \ud835\udc5f \ud835\udc41 \ud835\udc5c \ud835\udc5f \ud835\udc5a superscript subscript \ud835\udc4c \ud835\udc56 \ud835\udc58 y_{i}^{(k)}:=LayerNorm\\left(Y_{i}^{(k)}\\right) italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (4) Of key importance will be the the normalization factor: Report issue for preceding element N i ( k ) := 1 \u03c3 2 \u2062 ( Y i ( k ) ) + \u03f5 assign superscript subscript \ud835\udc41 \ud835\udc56 \ud835\udc58 1 superscript \ud835\udf0e 2 superscript subscript \ud835\udc4c \ud835\udc56 \ud835\udc58 italic-\u03f5 N_{i}^{(k)}:=\\frac{1}{\\sqrt{\\sigma^{2}(Y_{i}^{(k)})+\\epsilon}} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) + italic_\u03f5 end_ARG end_ARG (5) Our theoretical results will link N i ( k ) superscript subscript \ud835\udc41 \ud835\udc56 \ud835\udc58 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT both to input-space sensitivity and parameter-space sharpness:\nWe will find that large values of N i ( k ) superscript subscript \ud835\udc41 \ud835\udc56 \ud835\udc58 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT can increase expressive capacity, but at the price of increased brittleness. Report issue for preceding element\nFinally, we assume that predictions are made by T := v o \u2062 u \u2062 t T \u22c5 y n ( L ) assign \ud835\udc47 \u22c5 superscript subscript \ud835\udc63 \ud835\udc5c \ud835\udc62 \ud835\udc61 \ud835\udc47 superscript subscript \ud835\udc66 \ud835\udc5b \ud835\udc3f T:=v_{out}^{T}\\cdot y_{n}^{(L)} italic_T := italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u22c5 italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT for some parameter v o \u2062 u \u2062 t \u2208 \u211d d subscript \ud835\udc63 \ud835\udc5c \ud835\udc62 \ud835\udc61 superscript \u211d \ud835\udc51 v_{out}\\in\\mathbb{R}^{d} italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\nThroughout, we will add the input string x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT as an argument when needed for disambiguation, e.g., writing T \u2062 ( x ) \ud835\udc47 \ud835\udc65 T(x) italic_T ( italic_x ) for the overall prediction made on x \ud835\udc65 x italic_x . Report issue for preceding element",
    "masked_text": "We will focus on boolean functions. Following the conventions in the Analysis of Boolean Functions literature [CITATION] in modeling bitstrings as elements of {\u22121,1}nsuperscript11\ud835\udc5b\\{-1,1\\}^{n}{ - 1 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we assume the alphabet \u03a3={\u22121,1}\u03a311\\Sigma=\\{-1,1\\}roman_\u03a3 = { - 1 , 1 }, with word embeddings e\u2062(\u22121),e\u2062(+1)\u2208\u211dd\ud835\udc521\ud835\udc521superscript\u211d\ud835\udc51e(-1),e(+1)\\in\\mathbb{R}^{d}italic_e ( - 1 ) , italic_e ( + 1 ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. There further are positional encodings p1,p2,p3,\u22ef\u2208\u211ddsubscript\ud835\udc5d1subscript\ud835\udc5d2subscript\ud835\udc5d3\u22efsuperscript\u211d\ud835\udc51p_{1},p_{2},p_{3},\\dots\\in\\mathbb{R}^{d}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u22ef \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. At the zero-th layer, token and positional encodings are added: yi(0):=e\u2062(xi)+piassignsuperscriptsubscript\ud835\udc66\ud835\udc560\ud835\udc52subscript\ud835\udc65\ud835\udc56subscript\ud835\udc5d\ud835\udc56y_{i}^{(0)}:=e(x_{i})+p_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT := italic_e ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (i=1,\u2026,n\ud835\udc561\u2026\ud835\udc5bi=1,\\dots,nitalic_i = 1 , \u2026 , italic_n), where x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the input string.Report issue for preceding element\nA transformer has a fixed number L\ud835\udc3fLitalic_L of layers; the activations yi(k)\u2208\u211ddsuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc58superscript\u211d\ud835\udc51y_{i}^{(k)}\\in\\mathbb{R}^{d}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT at position i\ud835\udc56iitalic_i of the k\ud835\udc58kitalic_k-th layer (k=1,\u2026,L\ud835\udc581\u2026\ud835\udc3fk=1,\\dots,Litalic_k = 1 , \u2026 , italic_L) are defined as follows. Each layer has a set of H\ud835\udc3bHitalic_H attention heads; we first compute attention scores for the h\u210ehitalic_h-th head:Report issue for preceding element ai,j(k,h)=superscriptsubscript\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc58\u210eabsent\\displaystyle a_{i,j}^{(k,h)}=italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = (Kk,h\u2062yj(k\u22121))T\u2062Qk,h\u2062yi(k\u22121)superscriptsubscript\ud835\udc3e\ud835\udc58\u210esuperscriptsubscript\ud835\udc66\ud835\udc57\ud835\udc581\ud835\udc47subscript\ud835\udc44\ud835\udc58\u210esuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc581\\displaystyle(K_{k,h}y_{j}^{(k-1)})^{T}Q_{k,h}y_{i}^{(k-1)}( italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT a^i,j(k,h)=superscriptsubscript^\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc58\u210eabsent\\displaystyle\\widehat{a}_{i,j}^{(k,h)}=over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = exp\u2061(ai,j(k,h))\u2211sai,s(k,h)superscriptsubscript\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc58\u210esubscript\ud835\udc60superscriptsubscript\ud835\udc4e\ud835\udc56\ud835\udc60\ud835\udc58\u210e\\displaystyle\\frac{\\exp(a_{i,j}^{(k,h)})}{\\sum_{s}a_{i,s}^{(k,h)}}divide start_ARG roman_exp ( italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT end_ARG where Kk,hsubscript\ud835\udc3e\ud835\udc58\u210eK_{k,h}italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (\u201ckey\u201d), Qk,hsubscript\ud835\udc44\ud835\udc58\u210eQ_{k,h}italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (\u201cquery\u201d) are \u2208\u211dd\u00d7dabsentsuperscript\u211d\ud835\udc51\ud835\udc51\\in\\mathbb{R}^{d\\times d}\u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d end_POSTSUPERSCRIPT. The activation of the head is computed by weighting according to attention weights a^i,j(k,h)superscriptsubscript^\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc58\u210e\\hat{a}_{i,j}^{(k,h)}over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT, and applying a linear transformation V\ud835\udc49Vitalic_V (\u201cvalue\u201d):Report issue for preceding element bi,h(k)=\u2211j=1na^i,j(k,h)\u2062Vk,h\u2062yj(k\u22121)superscriptsubscript\ud835\udc4f\ud835\udc56\u210e\ud835\udc58superscriptsubscript\ud835\udc571\ud835\udc5bsuperscriptsubscript^\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc58\u210esubscript\ud835\udc49\ud835\udc58\u210esuperscriptsubscript\ud835\udc66\ud835\udc57\ud835\udc581b_{i,h}^{(k)}=\\sum_{j=1}^{n}\\hat{a}_{i,j}^{(k,h)}V_{k,h}y_{j}^{(k-1)}italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT (1) The per-position activations are then computed asReport issue for preceding element Yi(k):=fM\u2062L\u2062P\u2062(yi(k\u22121)+\u2211h=1Hbi,h(k))assignsuperscriptsubscript\ud835\udc4c\ud835\udc56\ud835\udc58superscript\ud835\udc53\ud835\udc40\ud835\udc3f\ud835\udc43superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc581superscriptsubscript\u210e1\ud835\udc3bsuperscriptsubscript\ud835\udc4f\ud835\udc56\u210e\ud835\udc58Y_{i}^{(k)}:=f^{MLP}\\left(y_{i}^{(k-1)}+\\sum_{h=1}^{H}b_{i,h}^{(k)}\\right)italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (2) where fM\u2062L\u2062Psuperscript\ud835\udc53\ud835\udc40\ud835\udc3f\ud835\udc43f^{MLP}italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT is a one-layer MLP with a skip-connection. Transformers additionally implement layer norm [CITATION]:Report issue for preceding element L\u2062a\u2062y\u2062e\u2062r\u2062N\u2062o\u2062r\u2062m\u2062(y):=y\u2212m\u2062e\u2062a\u2062n\u2062(y)\u03c32\u2062(y)+\u03f5assign\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc66\ud835\udc66\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b\ud835\udc66superscript\ud835\udf0e2\ud835\udc66italic-\u03f5LayerNorm(y):=\\frac{y-mean(y)}{\\sqrt{\\sigma^{2}(y)+\\epsilon}}italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) := divide start_ARG italic_y - italic_m italic_e italic_a italic_n ( italic_y ) end_ARG start_ARG square-root start_ARG italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_y ) + italic_\u03f5 end_ARG end_ARG (3) where \u03f5\u22650italic-\u03f50\\epsilon\\geq 0italic_\u03f5 \u2265 0 is a hyperparameter ensuring numerical stability, and \u03c32\u2062(\u22c5)superscript\ud835\udf0e2\u22c5\\sigma^{2}(\\cdot)italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( \u22c5 ) denotes the variance. By design, \u2016L\u2062a\u2062y\u2062e\u2062r\u2062N\u2062o\u2062r\u2062m\u2062(y)\u20162\u2264dsubscriptnorm\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a\ud835\udc662\ud835\udc51\\|LayerNorm(y)\\|_{2}\\leq\\sqrt{d}\u2225 italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2264 square-root start_ARG italic_d end_ARG, with equality at \u03f5=0italic-\u03f50\\epsilon=0italic_\u03f5 = 0. Transformer variants differ in where exactly layer norm is applied [CITATION]; we here assume for notational simplicity that layer norm applies after the MLP, but the details are irrelevant to our results, provided layer norm applies at least once. We thus set:Report issue for preceding element yi(k):=L\u2062a\u2062y\u2062e\u2062r\u2062N\u2062o\u2062r\u2062m\u2062(Yi(k))assignsuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc58\ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5asuperscriptsubscript\ud835\udc4c\ud835\udc56\ud835\udc58y_{i}^{(k)}:=LayerNorm\\left(Y_{i}^{(k)}\\right)italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (4) Of key importance will be the the normalization factor:Report issue for preceding element Ni(k):=1\u03c32\u2062(Yi(k))+\u03f5assignsuperscriptsubscript\ud835\udc41\ud835\udc56\ud835\udc581superscript\ud835\udf0e2superscriptsubscript\ud835\udc4c\ud835\udc56\ud835\udc58italic-\u03f5N_{i}^{(k)}:=\\frac{1}{\\sqrt{\\sigma^{2}(Y_{i}^{(k)})+\\epsilon}}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) + italic_\u03f5 end_ARG end_ARG (5) Our theoretical results will link Ni(k)superscriptsubscript\ud835\udc41\ud835\udc56\ud835\udc58N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT both to input-space sensitivity and parameter-space sharpness: We will find that large values of Ni(k)superscriptsubscript\ud835\udc41\ud835\udc56\ud835\udc58N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT can increase expressive capacity, but at the price of increased brittleness.Report issue for preceding element\nFinally, we assume that predictions are made by T:=vo\u2062u\u2062tT\u22c5yn(L)assign\ud835\udc47\u22c5superscriptsubscript\ud835\udc63\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc47superscriptsubscript\ud835\udc66\ud835\udc5b\ud835\udc3fT:=v_{out}^{T}\\cdot y_{n}^{(L)}italic_T := italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u22c5 italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT for some parameter vo\u2062u\u2062t\u2208\u211ddsubscript\ud835\udc63\ud835\udc5c\ud835\udc62\ud835\udc61superscript\u211d\ud835\udc51v_{out}\\in\\mathbb{R}^{d}italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Throughout, we will add the input string x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT as an argument when needed for disambiguation, e.g., writing T\u2062(x)\ud835\udc47\ud835\udc65T(x)italic_T ( italic_x ) for the overall prediction made on x\ud835\udc65xitalic_x.Report issue for preceding element",
    "citations": [
      {
        "tag": "O\u2019Donnell (2014)",
        "title": "Analysis of Boolean Functions.",
        "authors": "Ryan O\u2019Donnell. 2014.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Ba et\u00a0al. (2016)",
        "title": "Layer normalization.",
        "authors": "Lei\u00a0Jimmy Ba, Jamie\u00a0Ryan Kiros, and Geoffrey\u00a0E. Hinton. 2016.",
        "journal": "CoRR, abs/1607.06450."
      },
      {
        "tag": "Takase et\u00a0al. (2022)",
        "title": "On layer normalizations and residual connections in transformers.",
        "authors": "Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. 2022.",
        "journal": "arXiv preprint arXiv:2206.00330."
      }
    ]
  },
  "S3": {
    "title": "3Average Sensitivity",
    "text": "Our results are centered around average sensitivity , a simple but foundational complexity metric for functions on the Boolean cube (e.g. Kahn et\u00a0al., 1988 ; De\u00a0Wolf, 2008 ; O\u2019Donnell, 2014 ) : Report issue for preceding element\nFor a bitstring x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and a function f : { \u00b1 1 } n \u2192 \u211d : \ud835\udc53 \u2192 superscript plus-or-minus 1 \ud835\udc5b \u211d f:\\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R} italic_f : { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R , we write Report issue for preceding element s \u2062 ( x , f ) := 1 4 \u2062 \u2211 i = 1 n | f \u2062 ( x ) \u2212 f \u2062 ( x \u2295 i ) | 2 assign \ud835\udc60 \ud835\udc65 \ud835\udc53 1 4 superscript subscript \ud835\udc56 1 \ud835\udc5b superscript \ud835\udc53 \ud835\udc65 \ud835\udc53 superscript \ud835\udc65 direct-sum \ud835\udc56 2 s(x,f):=\\frac{1}{4}\\sum_{i=1}^{n}|f(x)-f(x^{{}^{\\oplus i}})|^{2} italic_s ( italic_x , italic_f ) := divide start_ARG 1 end_ARG start_ARG 4 end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_f ( italic_x ) - italic_f ( italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2295 italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (6) where x \u2295 i superscript \ud835\udc65 direct-sum \ud835\udc56 x^{{}^{\\oplus i}} italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2295 italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT denotes the bitstring x \ud835\udc65 x italic_x with the i \ud835\udc56 i italic_i -th bit flipped.\nThe average sensitivity for inputs of length n \ud835\udc5b n italic_n is Report issue for preceding element a \u2062 s n \u2062 ( f ) := 1 2 n \u2062 \u2211 x \u2208 { \u00b1 1 } n s \u2062 ( x , f ) assign \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 1 superscript 2 \ud835\udc5b subscript \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b \ud835\udc60 \ud835\udc65 \ud835\udc53 as_{n}(f):=\\frac{1}{2^{n}}\\sum_{x\\in\\{\\pm 1\\}^{n}}s(x,f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) := divide start_ARG 1 end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_s ( italic_x , italic_f ) (7)\nIf f \ud835\udc53 f italic_f maps to { \u00b1 1 } plus-or-minus 1 \\{\\pm 1\\} { \u00b1 1 } , then s \u2062 ( x , f ) \ud835\udc60 \ud835\udc65 \ud835\udc53 s(x,f) italic_s ( italic_x , italic_f ) is the number of Hamming neighbors of x \ud835\udc65 x italic_x on which f \ud835\udc53 f italic_f flips.\nThis definition of a \u2062 s n \u2062 ( f ) \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 as_{n}(f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) corresponds to the \u201ctotal influence\u201d from O\u2019Donnell ( 2014 , Def. 2.27) .\nWe explicitly define average sensitivity relative to input length n \ud835\udc5b n italic_n , as we will investigate the behavior of transformers performing a single function f \ud835\udc53 f italic_f across varying input lengths.\nThe use of squared distances, rather than simple absolute distances, ensures that results about a \u2062 s n \u2062 ( f ) \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 as_{n}(f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) transfer to results about degree profiles (Abbe et\u00a0al., 2023 ) , which we will later investigate (Eq. 17 ). Report issue for preceding element\nAverage sensitivity is a general complexity metric with wide-ranging applications in theoretical computer science (e.g. Jukna, 2012 ) .\nIt is closely linked to the Fourier analysis on the Boolean cube (De\u00a0Wolf, 2008 ) , and is an average-case version of a family of sensitivity measures, closely related to other natural metrics such as decision tree depth and polynomial degree (Hatami et\u00a0al., 2010 ) .\nBoth average sensitivity itself (Bhattamishra et\u00a0al., 2023 ) and the Fourier structure (Abbe et\u00a0al., 2023 ) have been empirically linked to transformers\u2019 generalization behavior. We will ground these empirical findings by relating average sensitivity to loss landscapes for the transformer architecture. Report issue for preceding element\nOur theoretical results will apply to general functions on the Boolean cube.\nIn order to ground these, we will illustrate transformers\u2019 low-sensitivity bias at the example of a few natural functions which have played a role in the theoretical literature of transformers or are otherwise illustrative of variability in average sensitivity. Report issue for preceding element\nPARITY indicates whether the number of ones in a bitstring is even (output 1) or odd (output -1); over the input space { \u00b1 1 } n superscript plus-or-minus 1 \ud835\udc5b \\{\\pm 1\\}^{n} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and output space { \u00b1 1 } plus-or-minus 1 \\{\\pm 1\\} { \u00b1 1 } , it can be formally defined as the map x \u21a6 \u220f i = 1 n x i maps-to \ud835\udc65 superscript subscript product \ud835\udc56 1 \ud835\udc5b subscript \ud835\udc65 \ud835\udc56 x\\mapsto\\prod_{i=1}^{n}x_{i} italic_x \u21a6 \u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .\nAs flipping any input bit flips the output, a \u2062 s n \u2062 ( f ) = n \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 \ud835\udc5b as_{n}(f)=n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n .\nAs described above, this function can in principle be represented by transformers, but has empirically been found to be very hard to learn. Report issue for preceding element\nMAJORITY maps x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to 1 if # \u2062 { i : x i = 1 } > n / 2 # conditional-set \ud835\udc56 subscript \ud835\udc65 \ud835\udc56 1 \ud835\udc5b 2 \\#\\{i:x_{i}=1\\}>n/2 # { italic_i : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 } > italic_n / 2 , and \u2212 1 1 -1 - 1 else.\nTransformers show good length generalization (Merrill et\u00a0al., 2022 ; Zhou et\u00a0al., 2023 ) .\nIt is known that a \u2062 s n \u2062 ( f ) = \u0398 \u2062 ( n ) \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 \u0398 \ud835\udc5b as_{n}(f)=\\Theta(\\sqrt{n}) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = roman_\u0398 ( square-root start_ARG italic_n end_ARG ) (O\u2019Donnell, 2014 , Ex. 2.22) .\nHowever, s \u2062 ( f , x ) = n \ud835\udc60 \ud835\udc53 \ud835\udc65 \ud835\udc5b s(f,x)=n italic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x \ud835\udc65 x italic_x are almost fully balanced. Report issue for preceding element\nFIRST maps x \ud835\udc65 x italic_x to its first bit, x 1 subscript \ud835\udc65 1 x_{1} italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\nAs only the first bit matters, a \u2062 s n \u2062 ( f ) = 1 \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 1 as_{n}(f)=1 italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1 .\nIt is a simple example of a sparse function; more generally, a k \ud835\udc58 k italic_k -PARITY is a restriction of the PARITY function to only k \ud835\udc58 k italic_k inputs, where k \ud835\udc58 k italic_k is a constant.\nTransformers learn such sparse functions well (Bhattamishra et\u00a0al., 2023 ; Edelman et\u00a0al., 2022 ) . Report issue for preceding element\nMEAN maps x \u21a6 1 n \u2062 \u2211 i = 1 n x i \u2208 [ \u2212 1 , 1 ] maps-to \ud835\udc65 1 \ud835\udc5b superscript subscript \ud835\udc56 1 \ud835\udc5b subscript \ud835\udc65 \ud835\udc56 1 1 x\\mapsto\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\in[-1,1] italic_x \u21a6 divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 [ - 1 , 1 ] . We have a \u2062 s n \u2062 ( f ) = 1 n \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 1 \ud835\udc5b as_{n}(f)=\\frac{1}{n} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG . Report issue for preceding element\nThe PARITY function, when varying the number of bits considered, is a universal basis for Boolean functions, in the sense that any function { \u00b1 1 } n \u2192 \u211d \u2192 superscript plus-or-minus 1 \ud835\udc5b \u211d \\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R can be represented as a linear combination of parities applying to different subsets of { x 1 , \u2026 , x n } subscript \ud835\udc65 1 \u2026 subscript \ud835\udc65 \ud835\udc5b \\{x_{1},\\dots,x_{n}\\} { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } .\nFunctions are more sensitive when parities applying to larger subsets appear in this representation.\nWe will investigate this connection further below. Report issue for preceding element",
    "masked_text": "Our results are centered around average sensitivity, a simple but foundational complexity metric for functions on the Boolean cube [CITATION]:Report issue for preceding element\nFor a bitstring x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and a function f:{\u00b11}n\u2192\u211d:\ud835\udc53\u2192superscriptplus-or-minus1\ud835\udc5b\u211df:\\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R}italic_f : { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R, we writeReport issue for preceding element s\u2062(x,f):=14\u2062\u2211i=1n|f\u2062(x)\u2212f\u2062(x\u2295i)|2assign\ud835\udc60\ud835\udc65\ud835\udc5314superscriptsubscript\ud835\udc561\ud835\udc5bsuperscript\ud835\udc53\ud835\udc65\ud835\udc53superscript\ud835\udc65direct-sum\ud835\udc562s(x,f):=\\frac{1}{4}\\sum_{i=1}^{n}|f(x)-f(x^{{}^{\\oplus i}})|^{2}italic_s ( italic_x , italic_f ) := divide start_ARG 1 end_ARG start_ARG 4 end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_f ( italic_x ) - italic_f ( italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2295 italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (6) where x\u2295isuperscript\ud835\udc65direct-sum\ud835\udc56x^{{}^{\\oplus i}}italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2295 italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT denotes the bitstring x\ud835\udc65xitalic_x with the i\ud835\udc56iitalic_i-th bit flipped. The average sensitivity for inputs of length n\ud835\udc5bnitalic_n isReport issue for preceding element a\u2062sn\u2062(f):=12n\u2062\u2211x\u2208{\u00b11}ns\u2062(x,f)assign\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc531superscript2\ud835\udc5bsubscript\ud835\udc65superscriptplus-or-minus1\ud835\udc5b\ud835\udc60\ud835\udc65\ud835\udc53as_{n}(f):=\\frac{1}{2^{n}}\\sum_{x\\in\\{\\pm 1\\}^{n}}s(x,f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) := divide start_ARG 1 end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_s ( italic_x , italic_f ) (7)\nIf f\ud835\udc53fitalic_f maps to {\u00b11}plus-or-minus1\\{\\pm 1\\}{ \u00b1 1 }, then s\u2062(x,f)\ud835\udc60\ud835\udc65\ud835\udc53s(x,f)italic_s ( italic_x , italic_f ) is the number of Hamming neighbors of x\ud835\udc65xitalic_x on which f\ud835\udc53fitalic_f flips. This definition of a\u2062sn\u2062(f)\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53as_{n}(f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) corresponds to the \u201ctotal influence\u201d from [CITATION]. We explicitly define average sensitivity relative to input length n\ud835\udc5bnitalic_n, as we will investigate the behavior of transformers performing a single function f\ud835\udc53fitalic_f across varying input lengths. The use of squared distances, rather than simple absolute distances, ensures that results about a\u2062sn\u2062(f)\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53as_{n}(f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) transfer to results about degree profiles [CITATION], which we will later investigate (Eq. 17).Report issue for preceding element\nAverage sensitivity is a general complexity metric with wide-ranging applications in theoretical computer science [CITATION]. It is closely linked to the Fourier analysis on the Boolean cube [CITATION], and is an average-case version of a family of sensitivity measures, closely related to other natural metrics such as decision tree depth and polynomial degree [CITATION]. Both average sensitivity itself [CITATION] and the Fourier structure [CITATION] have been empirically linked to transformers\u2019 generalization behavior. We will ground these empirical findings by relating average sensitivity to loss landscapes for the transformer architecture.Report issue for preceding element\nOur theoretical results will apply to general functions on the Boolean cube. In order to ground these, we will illustrate transformers\u2019 low-sensitivity bias at the example of a few natural functions which have played a role in the theoretical literature of transformers or are otherwise illustrative of variability in average sensitivity.Report issue for preceding element\nPARITY indicates whether the number of ones in a bitstring is even (output 1) or odd (output -1); over the input space {\u00b11}nsuperscriptplus-or-minus1\ud835\udc5b\\{\\pm 1\\}^{n}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and output space {\u00b11}plus-or-minus1\\{\\pm 1\\}{ \u00b1 1 }, it can be formally defined as the map x\u21a6\u220fi=1nximaps-to\ud835\udc65superscriptsubscriptproduct\ud835\udc561\ud835\udc5bsubscript\ud835\udc65\ud835\udc56x\\mapsto\\prod_{i=1}^{n}x_{i}italic_x \u21a6 \u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. As flipping any input bit flips the output, a\u2062sn\u2062(f)=n\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53\ud835\udc5bas_{n}(f)=nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n. As described above, this function can in principle be represented by transformers, but has empirically been found to be very hard to learn.Report issue for preceding element\nMAJORITY maps x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to 1 if #\u2062{i:xi=1}>n/2#conditional-set\ud835\udc56subscript\ud835\udc65\ud835\udc561\ud835\udc5b2\\#\\{i:x_{i}=1\\}>n/2# { italic_i : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 } > italic_n / 2, and \u221211-1- 1 else. Transformers show good length generalization [CITATION]. It is known that a\u2062sn\u2062(f)=\u0398\u2062(n)\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53\u0398\ud835\udc5bas_{n}(f)=\\Theta(\\sqrt{n})italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = roman_\u0398 ( square-root start_ARG italic_n end_ARG ) [CITATION]. However, s\u2062(f,x)=n\ud835\udc60\ud835\udc53\ud835\udc65\ud835\udc5bs(f,x)=nitalic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x\ud835\udc65xitalic_x are almost fully balanced.Report issue for preceding element\nFIRST maps x\ud835\udc65xitalic_x to its first bit, x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. As only the first bit matters, a\u2062sn\u2062(f)=1\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc531as_{n}(f)=1italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1. It is a simple example of a sparse function; more generally, a k\ud835\udc58kitalic_k-PARITY is a restriction of the PARITY function to only k\ud835\udc58kitalic_k inputs, where k\ud835\udc58kitalic_k is a constant. Transformers learn such sparse functions well [CITATION].Report issue for preceding element\nMEAN maps x\u21a61n\u2062\u2211i=1nxi\u2208[\u22121,1]maps-to\ud835\udc651\ud835\udc5bsuperscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc65\ud835\udc5611x\\mapsto\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\in[-1,1]italic_x \u21a6 divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 [ - 1 , 1 ]. We have a\u2062sn\u2062(f)=1n\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc531\ud835\udc5bas_{n}(f)=\\frac{1}{n}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG.Report issue for preceding element\nThe PARITY function, when varying the number of bits considered, is a universal basis for Boolean functions, in the sense that any function {\u00b11}n\u2192\u211d\u2192superscriptplus-or-minus1\ud835\udc5b\u211d\\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R can be represented as a linear combination of parities applying to different subsets of {x1,\u2026,xn}subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5b\\{x_{1},\\dots,x_{n}\\}{ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Functions are more sensitive when parities applying to larger subsets appear in this representation. We will investigate this connection further below.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhou et\u00a0al. (2023)",
        "title": "What algorithms can transformers learn? a study in length\ngeneralization.",
        "authors": "Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh\nSusskind, Samy Bengio, and Preetum Nakkiran. 2023.",
        "journal": "arXiv preprint arXiv:2310.16028."
      },
      {
        "tag": "De\u00a0Wolf (2008)",
        "title": "A brief introduction to fourier analysis on the boolean cube.",
        "authors": "Ronald De\u00a0Wolf. 2008.",
        "journal": "Theory of Computing, pages 1\u201320."
      },
      {
        "tag": "Merrill et\u00a0al. (2022)",
        "title": "Saturated transformers are constant-depth threshold circuits.",
        "authors": "William Merrill, Ashish Sabharwal, and Noah\u00a0A. Smith. 2022.",
        "journal": "Trans. Assoc. Comput. Linguistics, 10:843\u2013856."
      },
      {
        "tag": "Abbe et\u00a0al. (2023)",
        "title": "Generalization on the unseen, logic reasoning and degree curriculum.",
        "authors": "Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. 2023.",
        "journal": "InInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\nMachine Learning Research, pages 31\u201360. PMLR."
      },
      {
        "tag": "Edelman et\u00a0al. (2022)",
        "title": "Inductive biases and variable creation in self-attention mechanisms.",
        "authors": "Benjamin\u00a0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages\n5793\u20135831. PMLR."
      },
      {
        "tag": "Jukna (2012)",
        "title": "Boolean Function Complexity: Advances and Frontiers.",
        "authors": "Stasys Jukna. 2012.",
        "journal": ""
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2023)",
        "title": "Simplicity\nbias in transformers and their ability to learn sparse boolean functions.",
        "authors": "Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. 2023.",
        "journal": "InProceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 5767\u20135791. Association for Computational\nLinguistics."
      },
      {
        "tag": "O\u2019Donnell (2014)",
        "title": "Analysis of Boolean Functions.",
        "authors": "Ryan O\u2019Donnell. 2014.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Hatami et\u00a0al. (2010)",
        "title": "Variations on the sensitivity conjecture.",
        "authors": "Pooya Hatami, Raghav Kulkarni, and Denis Pankratov. 2010.",
        "journal": "Theory of Computing, 4:1\u201327."
      },
      {
        "tag": "Kahn et\u00a0al. (1988)",
        "title": "The influence of variables on boolean functions.",
        "authors": "J.\u00a0Kahn, G.\u00a0Kalai, and N.\u00a0Linial. 1988.",
        "journal": "In[Proceedings 1988] 29th Annual Symposium on Foundations of\nComputer Science, pages 68\u201380."
      }
    ]
  },
  "S4": {
    "title": "4Lower Bounds for Sensitive Functions",
    "text": "We first prove that representing sensitive functions with transformers requires large parameter norms and, when inputs get longer and longer, highly unbounded normalization factors N i ( k ) superscript subscript \ud835\udc41 \ud835\udc56 \ud835\udc58 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT in layer norm ( 5 ).\nWe start from the global Lipschitzness bounds developed by Hahn ( 2020 ); Edelman et\u00a0al. ( 2022 ); Li et\u00a0al. ( 2023 ) , but obtain more fine-grained average-case and high-probability bounds. These will then form the basis of our characterization of loss landscape sharpness around sensitive transformers.\nOur bounds will include a constant C \ud835\udc36 C italic_C that is the product of Report issue for preceding element exp \u2061 ( 4 \u2062 d \u2062 max h \u2062 \u2211 i = 2 L \u2016 K i , h T \u2062 Q i , h \u2016 2 ) 4 \ud835\udc51 subscript \u210e superscript subscript \ud835\udc56 2 \ud835\udc3f subscript norm superscript subscript \ud835\udc3e \ud835\udc56 \u210e \ud835\udc47 subscript \ud835\udc44 \ud835\udc56 \u210e 2 \\exp\\left(4d\\max_{h}\\sum_{i=2}^{L}\\|K_{i,h}^{T}Q_{i,h}\\|_{2}\\right) roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT \u2225 italic_K start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (8) and a term polynomial in H \ud835\udc3b H italic_H , d \ud835\udc51 d italic_d , the spectral norms of all parameter matrices appearing in the transformer, and the maximum norm of any positional or word embedding.\nSee ( 32 ) in the Appendix for formal definition. Report issue for preceding element\nExisting Lipschitzness bounds (Fact 2 ) imply a bound along the lines of 2 2 2 Lipschitzness bounds by Edelman et\u00a0al. ( 2022 ) are not directly applicable here, as these authors consider Lipschitzness in the parameter space , not the effect of changes in the input space as Theorem 3. See also Appendix, Remark 10 . Report issue for preceding element s \u2062 ( f , x ) \u2264 C \u2062 exp \u2061 ( 4 \u2062 d \u2062 max h \u2061 \u2016 K 1 , h T \u2062 Q 1 , h \u2016 2 ) \u03f5 L / 2 \ud835\udc60 \ud835\udc53 \ud835\udc65 \ud835\udc36 4 \ud835\udc51 subscript \u210e subscript norm superscript subscript \ud835\udc3e 1 \u210e \ud835\udc47 subscript \ud835\udc44 1 \u210e 2 superscript italic-\u03f5 \ud835\udc3f 2 s(f,x)\\leq\\frac{C\\exp(4d\\max_{h}\\|K_{1,h}^{T}Q_{1,h}\\|_{2})}{\\epsilon^{L/2}} italic_s ( italic_f , italic_x ) \u2264 divide start_ARG italic_C roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2225 italic_K start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_ARG start_ARG italic_\u03f5 start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (9) uniformly for each x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . Li et\u00a0al. ( 2023 ) noted that the exponential dependency on the spectral norm of the key-query matrix is unavoidable for a global Lipschitzness bound.\nOur first result is that this dependency can be eliminated at the input layer for the vast majority of inputs, at the cost of a logarithmic factor, leading to a bound of the form (assuming \u03f5 > 0 italic-\u03f5 0 \\epsilon>0 italic_\u03f5 > 0 in ( 3 )) Report issue for preceding element s \u2062 ( f , x ) \u2264 C \u2062 log \u2061 n \u03f5 L / 2 \ud835\udc60 \ud835\udc53 \ud835\udc65 \ud835\udc36 \ud835\udc5b superscript italic-\u03f5 \ud835\udc3f 2 s(f,x)\\leq C\\frac{\\log{n}}{\\epsilon^{L/2}} italic_s ( italic_f , italic_x ) \u2264 italic_C divide start_ARG roman_log italic_n end_ARG start_ARG italic_\u03f5 start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (10) for 1 \u2212 H \u2062 n \u2212 2 1 \ud835\udc3b superscript \ud835\udc5b 2 1-Hn^{-2} 1 - italic_H italic_n start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT of inputs.\nWe show this using a concentration bound argument, combining a Chernoff bound applying to each attention weight individually, with a union bound over all attention weights (Appendix, Lemma 11 ).\nNext, we address the role of layer norm. Chiang and Cholak ( 2022 ) showed that, at \u03f5 \u2192 0 \u2192 italic-\u03f5 0 \\epsilon\\rightarrow 0 italic_\u03f5 \u2192 0 in ( 3 ), layer norm enables transformers to represent PARITY. Lipschitzness bounds in terms of \u03f5 italic-\u03f5 \\epsilon italic_\u03f5 ( 10 ) cease being meaningful in this limit.\nWe thus study the layer-norm induced blowup in more detail.\nIn each layer, we consider the maximum blowup, given an input string x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : Report issue for preceding element \u03c4 ( k ) \u2062 ( x ) := max w = 1 , \u2026 , n \u2061 { 1 + N w ( k ) \u2062 ( x ) } assign superscript \ud835\udf0f \ud835\udc58 \ud835\udc65 subscript \ud835\udc64 1 \u2026 \ud835\udc5b 1 superscript subscript \ud835\udc41 \ud835\udc64 \ud835\udc58 \ud835\udc65 \\tau^{(k)}(x):=\\max_{w=1,\\dots,n}\\{1+N_{w}^{(k)}(x)\\} italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) := roman_max start_POSTSUBSCRIPT italic_w = 1 , \u2026 , italic_n end_POSTSUBSCRIPT { 1 + italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) } (11) The addition of 1 is for technical reasons (Appendix, Lemma 16 ); it has little impact in the cases relevant to our results, which will be when \u03c4 ( k ) \u2062 ( x ) = \u03c9 \u2062 ( 1 ) superscript \ud835\udf0f \ud835\udc58 \ud835\udc65 \ud835\udf14 1 \\tau^{(k)}(x)=\\omega(1) italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) = italic_\u03c9 ( 1 ) .\nWe then write Blowup \u2061 ( x ) := \u220f k = 1 L \u03c4 ( k ) \u2062 ( x ) assign Blowup \ud835\udc65 superscript subscript product \ud835\udc58 1 \ud835\udc3f superscript \ud835\udf0f \ud835\udc58 \ud835\udc65 \\operatorname{Blowup}(x):=\\prod_{k=1}^{L}\\tau^{(k)}(x) roman_Blowup ( italic_x ) := \u220f start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) , an upper bound on the product of the successive layer-norm-induced blowups when going from the input to the output.\nWhen \u03f5 = 0 italic-\u03f5 0 \\epsilon=0 italic_\u03f5 = 0 in ( 3 ), Blowup \u2061 ( x ) Blowup \ud835\udc65 \\operatorname{Blowup}(x) roman_Blowup ( italic_x ) can be arbitrarily large.\nForeshadowing Theorem 6 , we will find that large values of Blowup \u2061 ( x ) Blowup \ud835\udc65 \\operatorname{Blowup}(x) roman_Blowup ( italic_x ) create very sharp minima. Report issue for preceding element\nOur first theorem localizes the layer norm blowup to the Hamming neighborhoods of sensitive inputs: Report issue for preceding element\nConsider a transformer with layer norm at arbitrary \u03f5 \u2265 0 italic-\u03f5 0 \\epsilon\\geq 0 italic_\u03f5 \u2265 0 .\nWith probability 1 \u2212 H n 2 1 \ud835\udc3b superscript \ud835\udc5b 2 1-\\frac{H}{n^{2}} 1 - divide start_ARG italic_H end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG over the choice of x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , we have Report issue for preceding element s \u2062 ( f , x ) C \u2062 n \u2062 log \u2061 n \u2264 Blowup ( x ) 2 + 1 n \u2211 i = 1 n Blowup ( x \u2295 i ) 2 \\frac{s(f,x)}{C\\sqrt{n\\log n}}\\leq\\operatorname{Blowup}(x)^{2}+\\frac{1}{n}\\sum%\n_{i=1}^{n}\\operatorname{Blowup}(x^{\\oplus i})^{2} divide start_ARG italic_s ( italic_f , italic_x ) end_ARG start_ARG italic_C square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG \u2264 roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_Blowup ( italic_x start_POSTSUPERSCRIPT \u2295 italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (12)\nThe proof is in Appendix B.5 .\nThis permits us to state a bound on average sensitivity, in terms of the average layer norm blowup: Report issue for preceding element\nConsider a transformer with layer norm at arbitrary \u03f5 \u2265 0 italic-\u03f5 0 \\epsilon\\geq 0 italic_\u03f5 \u2265 0 .\nThen Report issue for preceding element C \u22c5 \ud835\udd3c [ Blowup ( x ) 2 ] \u2265 a \u2062 s n \u2062 ( f ) n \u2062 log \u2061 n \u2212 H n C\\cdot\\mathbb{E}[\\operatorname{Blowup}(x)^{2}]\\geq\\frac{as_{n}(f)}{\\sqrt{n\\log%\n{n}}}-\\frac{H}{n} italic_C \u22c5 blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] \u2265 divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG - divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG (13) where the expectation is over the uniform distribution over x \u2208 { \u00b1 1 } n \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b x\\in\\{\\pm 1\\}^{n} italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . Report issue for preceding element\nThe proof is in Appendix B.6 .\nNote that H n \ud835\udc3b \ud835\udc5b \\frac{H}{n} divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG is small when n \ud835\udc5b n italic_n is large, and the bound is dominated by a \u2062 s n \u2062 ( f ) n \u2062 log \u2061 n \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 \ud835\udc5b \ud835\udc5b \\frac{as_{n}(f)}{\\sqrt{n\\log{n}}} divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG .\nThis result thus shows a tradeoff between parameters and LN blowup: at least one of them needs to be large to represent a sensitive function. When the sensitivity depends on the input length and grows faster than n \u2062 log \u2061 n \ud835\udc5b \ud835\udc5b \\sqrt{n\\log n} square-root start_ARG italic_n roman_log italic_n end_ARG , this tradeoff changes with the input length, requiring larger parameters or larger layer norm blowup as the input length increases. Report issue for preceding element\nLet us investigate the implications for the functions introduced in Section above.\nFor PARITY, a \u2062 s n \u2062 ( f ) = n \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 \ud835\udc5b as_{n}(f)=n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n , and\n( 13 ) predicts Report issue for preceding element C \u22c5 \ud835\udd3c [ Blowup ( x ) 2 ] = \u03a9 ( n log \u2061 n ) = \u03c9 ( 1 ) C\\cdot\\mathbb{E}[\\operatorname{Blowup}(x)^{2}]=\\Omega\\left(\\frac{\\sqrt{n}}{%\n\\log{n}}\\right)=\\omega(1) italic_C \u22c5 blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = roman_\u03a9 ( divide start_ARG square-root start_ARG italic_n end_ARG end_ARG start_ARG roman_log italic_n end_ARG ) = italic_\u03c9 ( 1 ) (14) showing that, for fixed parameters, the layer norm blowup needs to increase as the input length increases.\nFor the other functions, the bound is O \u2062 ( 1 ) \ud835\udc42 1 O(1) italic_O ( 1 ) :\nFor FIRST, s \u2062 ( f , x ) = a \u2062 s n \u2062 ( f ) = 1 \ud835\udc60 \ud835\udc53 \ud835\udc65 \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 1 s(f,x)=as_{n}(f)=1 italic_s ( italic_f , italic_x ) = italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1 , and the RHS of ( 13 ) is O \u2062 ( 1 ) \ud835\udc42 1 O(1) italic_O ( 1 ) .\nIndeed, sparse functions can be modeled well by a family of transformers where the logits in the input layer scale with log \u2061 n \ud835\udc5b \\log{n} roman_log italic_n (Edelman et\u00a0al., 2022 ; Chiang and Cholak, 2022 ) .\nUnlike the prior Lipschitzness bounds ( 9 ), these scaled logits do not contribute to C \ud835\udc36 C italic_C \u2013 our new bound is thus consistent with the ease with which transformers learn sparse functions.\nFor MEAN, s \u2062 ( f , x ) \u223c 1 n 2 similar-to \ud835\udc60 \ud835\udc53 \ud835\udc65 1 superscript \ud835\udc5b 2 s(f,x)\\sim\\frac{1}{n^{2}} italic_s ( italic_f , italic_x ) \u223c divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ; again no blowup is predicted.\nFor MAJORITY, as a \u2062 s n \u2062 ( f ) \u223c n similar-to \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 \ud835\udc5b as_{n}(f)\\sim\\sqrt{n} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) \u223c square-root start_ARG italic_n end_ARG , no nontrivial average blowup is predicted.\nHowever, s \u2062 ( f , x ) = n \ud835\udc60 \ud835\udc53 \ud835\udc65 \ud835\udc5b s(f,x)=n italic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x \ud835\udc65 x italic_x are almost fully balanced; on such strings or their Hamming neighbors, ( 12 ) predicts B \u2062 l \u2062 o \u2062 w \u2062 u \u2062 p = \u03a9 \u2062 ( n 1 / 4 ) \ud835\udc35 \ud835\udc59 \ud835\udc5c \ud835\udc64 \ud835\udc62 \ud835\udc5d \u03a9 superscript \ud835\udc5b 1 4 Blowup=\\Omega(n^{1/4}) italic_B italic_l italic_o italic_w italic_u italic_p = roman_\u03a9 ( italic_n start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ) . Report issue for preceding element",
    "masked_text": "We first prove that representing sensitive functions with transformers requires large parameter norms and, when inputs get longer and longer, highly unbounded normalization factors Ni(k)superscriptsubscript\ud835\udc41\ud835\udc56\ud835\udc58N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT in layer norm (5). We start from the global Lipschitzness bounds developed by [CITATION], but obtain more fine-grained average-case and high-probability bounds. These will then form the basis of our characterization of loss landscape sharpness around sensitive transformers. Our bounds will include a constant C\ud835\udc36Citalic_C that is the product ofReport issue for preceding element exp\u2061(4\u2062d\u2062maxh\u2062\u2211i=2L\u2016Ki,hT\u2062Qi,h\u20162)4\ud835\udc51subscript\u210esuperscriptsubscript\ud835\udc562\ud835\udc3fsubscriptnormsuperscriptsubscript\ud835\udc3e\ud835\udc56\u210e\ud835\udc47subscript\ud835\udc44\ud835\udc56\u210e2\\exp\\left(4d\\max_{h}\\sum_{i=2}^{L}\\|K_{i,h}^{T}Q_{i,h}\\|_{2}\\right)roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT \u2225 italic_K start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (8) and a term polynomial in H\ud835\udc3bHitalic_H, d\ud835\udc51ditalic_d, the spectral norms of all parameter matrices appearing in the transformer, and the maximum norm of any positional or word embedding. See (32) in the Appendix for formal definition.Report issue for preceding element\nExisting Lipschitzness bounds (Fact 2) imply a bound along the lines of222Lipschitzness bounds by [CITATION] are not directly applicable here, as these authors consider Lipschitzness in the parameter space, not the effect of changes in the input space as Theorem 3. See also Appendix, Remark 10. Report issue for preceding element s\u2062(f,x)\u2264C\u2062exp\u2061(4\u2062d\u2062maxh\u2061\u2016K1,hT\u2062Q1,h\u20162)\u03f5L/2\ud835\udc60\ud835\udc53\ud835\udc65\ud835\udc364\ud835\udc51subscript\u210esubscriptnormsuperscriptsubscript\ud835\udc3e1\u210e\ud835\udc47subscript\ud835\udc441\u210e2superscriptitalic-\u03f5\ud835\udc3f2s(f,x)\\leq\\frac{C\\exp(4d\\max_{h}\\|K_{1,h}^{T}Q_{1,h}\\|_{2})}{\\epsilon^{L/2}}italic_s ( italic_f , italic_x ) \u2264 divide start_ARG italic_C roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2225 italic_K start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_ARG start_ARG italic_\u03f5 start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (9) uniformly for each x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. [CITATION] noted that the exponential dependency on the spectral norm of the key-query matrix is unavoidable for a global Lipschitzness bound. Our first result is that this dependency can be eliminated at the input layer for the vast majority of inputs, at the cost of a logarithmic factor, leading to a bound of the form (assuming \u03f5>0italic-\u03f50\\epsilon>0italic_\u03f5 > 0 in (3))Report issue for preceding element s\u2062(f,x)\u2264C\u2062log\u2061n\u03f5L/2\ud835\udc60\ud835\udc53\ud835\udc65\ud835\udc36\ud835\udc5bsuperscriptitalic-\u03f5\ud835\udc3f2s(f,x)\\leq C\\frac{\\log{n}}{\\epsilon^{L/2}}italic_s ( italic_f , italic_x ) \u2264 italic_C divide start_ARG roman_log italic_n end_ARG start_ARG italic_\u03f5 start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (10) for 1\u2212H\u2062n\u221221\ud835\udc3bsuperscript\ud835\udc5b21-Hn^{-2}1 - italic_H italic_n start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT of inputs. We show this using a concentration bound argument, combining a Chernoff bound applying to each attention weight individually, with a union bound over all attention weights (Appendix, Lemma 11). Next, we address the role of layer norm. [CITATION] showed that, at \u03f5\u21920\u2192italic-\u03f50\\epsilon\\rightarrow 0italic_\u03f5 \u2192 0 in (3), layer norm enables transformers to represent PARITY. Lipschitzness bounds in terms of \u03f5italic-\u03f5\\epsilonitalic_\u03f5 (10) cease being meaningful in this limit. We thus study the layer-norm induced blowup in more detail. In each layer, we consider the maximum blowup, given an input string x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT:Report issue for preceding element \u03c4(k)\u2062(x):=maxw=1,\u2026,n\u2061{1+Nw(k)\u2062(x)}assignsuperscript\ud835\udf0f\ud835\udc58\ud835\udc65subscript\ud835\udc641\u2026\ud835\udc5b1superscriptsubscript\ud835\udc41\ud835\udc64\ud835\udc58\ud835\udc65\\tau^{(k)}(x):=\\max_{w=1,\\dots,n}\\{1+N_{w}^{(k)}(x)\\}italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) := roman_max start_POSTSUBSCRIPT italic_w = 1 , \u2026 , italic_n end_POSTSUBSCRIPT { 1 + italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) } (11) The addition of 1 is for technical reasons (Appendix, Lemma 16); it has little impact in the cases relevant to our results, which will be when \u03c4(k)\u2062(x)=\u03c9\u2062(1)superscript\ud835\udf0f\ud835\udc58\ud835\udc65\ud835\udf141\\tau^{(k)}(x)=\\omega(1)italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) = italic_\u03c9 ( 1 ). We then write Blowup\u2061(x):=\u220fk=1L\u03c4(k)\u2062(x)assignBlowup\ud835\udc65superscriptsubscriptproduct\ud835\udc581\ud835\udc3fsuperscript\ud835\udf0f\ud835\udc58\ud835\udc65\\operatorname{Blowup}(x):=\\prod_{k=1}^{L}\\tau^{(k)}(x)roman_Blowup ( italic_x ) := \u220f start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_\u03c4 start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ), an upper bound on the product of the successive layer-norm-induced blowups when going from the input to the output. When \u03f5=0italic-\u03f50\\epsilon=0italic_\u03f5 = 0 in (3), Blowup\u2061(x)Blowup\ud835\udc65\\operatorname{Blowup}(x)roman_Blowup ( italic_x ) can be arbitrarily large. Foreshadowing Theorem 6, we will find that large values of Blowup\u2061(x)Blowup\ud835\udc65\\operatorname{Blowup}(x)roman_Blowup ( italic_x ) create very sharp minima.Report issue for preceding element\nOur first theorem localizes the layer norm blowup to the Hamming neighborhoods of sensitive inputs:Report issue for preceding element\nConsider a transformer with layer norm at arbitrary \u03f5\u22650italic-\u03f50\\epsilon\\geq 0italic_\u03f5 \u2265 0. With probability 1\u2212Hn21\ud835\udc3bsuperscript\ud835\udc5b21-\\frac{H}{n^{2}}1 - divide start_ARG italic_H end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG over the choice of x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we haveReport issue for preceding element s\u2062(f,x)C\u2062n\u2062log\u2061n\u2264Blowup(x)2+1n\u2211i=1nBlowup(x\u2295i)2\\frac{s(f,x)}{C\\sqrt{n\\log n}}\\leq\\operatorname{Blowup}(x)^{2}+\\frac{1}{n}\\sum% _{i=1}^{n}\\operatorname{Blowup}(x^{\\oplus i})^{2}divide start_ARG italic_s ( italic_f , italic_x ) end_ARG start_ARG italic_C square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG \u2264 roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_Blowup ( italic_x start_POSTSUPERSCRIPT \u2295 italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (12)\nThe proof is in Appendix B.5. This permits us to state a bound on average sensitivity, in terms of the average layer norm blowup:Report issue for preceding element\nConsider a transformer with layer norm at arbitrary \u03f5\u22650italic-\u03f50\\epsilon\\geq 0italic_\u03f5 \u2265 0. ThenReport issue for preceding element C\u22c5\ud835\udd3c[Blowup(x)2]\u2265a\u2062sn\u2062(f)n\u2062log\u2061n\u2212HnC\\cdot\\mathbb{E}[\\operatorname{Blowup}(x)^{2}]\\geq\\frac{as_{n}(f)}{\\sqrt{n\\log% {n}}}-\\frac{H}{n}italic_C \u22c5 blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] \u2265 divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG - divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG (13) where the expectation is over the uniform distribution over x\u2208{\u00b11}n\ud835\udc65superscriptplus-or-minus1\ud835\udc5bx\\in\\{\\pm 1\\}^{n}italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.Report issue for preceding element\nThe proof is in Appendix B.6. Note that Hn\ud835\udc3b\ud835\udc5b\\frac{H}{n}divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG is small when n\ud835\udc5bnitalic_n is large, and the bound is dominated by a\u2062sn\u2062(f)n\u2062log\u2061n\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53\ud835\udc5b\ud835\udc5b\\frac{as_{n}(f)}{\\sqrt{n\\log{n}}}divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG. This result thus shows a tradeoff between parameters and LN blowup: at least one of them needs to be large to represent a sensitive function. When the sensitivity depends on the input length and grows faster than n\u2062log\u2061n\ud835\udc5b\ud835\udc5b\\sqrt{n\\log n}square-root start_ARG italic_n roman_log italic_n end_ARG, this tradeoff changes with the input length, requiring larger parameters or larger layer norm blowup as the input length increases.Report issue for preceding element\nLet us investigate the implications for the functions introduced in Section above. For PARITY, a\u2062sn\u2062(f)=n\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53\ud835\udc5bas_{n}(f)=nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n, and (13) predictsReport issue for preceding element C\u22c5\ud835\udd3c[Blowup(x)2]=\u03a9(nlog\u2061n)=\u03c9(1)C\\cdot\\mathbb{E}[\\operatorname{Blowup}(x)^{2}]=\\Omega\\left(\\frac{\\sqrt{n}}{% \\log{n}}\\right)=\\omega(1)italic_C \u22c5 blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = roman_\u03a9 ( divide start_ARG square-root start_ARG italic_n end_ARG end_ARG start_ARG roman_log italic_n end_ARG ) = italic_\u03c9 ( 1 ) (14) showing that, for fixed parameters, the layer norm blowup needs to increase as the input length increases. For the other functions, the bound is O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ): For FIRST, s\u2062(f,x)=a\u2062sn\u2062(f)=1\ud835\udc60\ud835\udc53\ud835\udc65\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc531s(f,x)=as_{n}(f)=1italic_s ( italic_f , italic_x ) = italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1, and the RHS of (13) is O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ). Indeed, sparse functions can be modeled well by a family of transformers where the logits in the input layer scale with log\u2061n\ud835\udc5b\\log{n}roman_log italic_n [CITATION]. Unlike the prior Lipschitzness bounds (9), these scaled logits do not contribute to C\ud835\udc36Citalic_C \u2013 our new bound is thus consistent with the ease with which transformers learn sparse functions. For MEAN, s\u2062(f,x)\u223c1n2similar-to\ud835\udc60\ud835\udc53\ud835\udc651superscript\ud835\udc5b2s(f,x)\\sim\\frac{1}{n^{2}}italic_s ( italic_f , italic_x ) \u223c divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG; again no blowup is predicted. For MAJORITY, as a\u2062sn\u2062(f)\u223cnsimilar-to\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53\ud835\udc5bas_{n}(f)\\sim\\sqrt{n}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) \u223c square-root start_ARG italic_n end_ARG, no nontrivial average blowup is predicted. However, s\u2062(f,x)=n\ud835\udc60\ud835\udc53\ud835\udc65\ud835\udc5bs(f,x)=nitalic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x\ud835\udc65xitalic_x are almost fully balanced; on such strings or their Hamming neighbors, (12) predicts B\u2062l\u2062o\u2062w\u2062u\u2062p=\u03a9\u2062(n1/4)\ud835\udc35\ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc62\ud835\udc5d\u03a9superscript\ud835\udc5b14Blowup=\\Omega(n^{1/4})italic_B italic_l italic_o italic_w italic_u italic_p = roman_\u03a9 ( italic_n start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ).Report issue for preceding element",
    "citations": [
      {
        "tag": "Chiang and Cholak (2022)",
        "title": "Overcoming a\ntheoretical limitation of self-attention.",
        "authors": "David Chiang and Peter Cholak. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 7654\u20137664. Association for Computational\nLinguistics."
      },
      {
        "tag": "Hahn (2020)",
        "title": "Theoretical limitations of self-attention in neural sequence models.",
        "authors": "Michael Hahn. 2020.",
        "journal": "Transactions of the Association for Computational Linguistics,\n8:156\u2013171."
      },
      {
        "tag": "Edelman et\u00a0al. (2022)",
        "title": "Inductive biases and variable creation in self-attention mechanisms.",
        "authors": "Benjamin\u00a0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages\n5793\u20135831. PMLR."
      },
      {
        "tag": "Li et\u00a0al. (2023)",
        "title": "Transformers\nas algorithms: Generalization and stability in in-context learning.",
        "authors": "Yingcong Li, Muhammed\u00a0Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.\n2023.",
        "journal": "InProceedings of the 40th International Conference on Machine\nLearning, volume 202 ofProceedings of Machine Learning Research,\npages 19565\u201319594. PMLR."
      }
    ]
  },
  "S5": {
    "title": "5Sensitive Transformers are Brittle",
    "text": "Leveraging Corollary 5 , we now show that transformers expressing sensitive functions must be very sensitive to small perturbations of model parameters.\nThat is, sensitivity in input space entails sensitivity in parameter space .\nAn important consequence is that, for any highly sensitive function, any interpolating minima will be very sharp.\nThis property is nontrivial, as seen from the fact that it disappears when adding a scratchpad (see below). Report issue for preceding element\nGiven a parameter vector \u03b8 \ud835\udf03 \\theta italic_\u03b8 defining the transformer T \u03b8 subscript \ud835\udc47 \ud835\udf03 T_{\\theta} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT , the average direction sharpness is (e.g. Wen et\u00a0al., 2022 ; Jiang et\u00a0al., 2020 ; Foret et\u00a0al., 2020 ) Report issue for preceding element L \u03c1 , n \u2062 ( T ) := \ud835\udd3c x \u2208 { \u00b1 1 } n \u2062 \ud835\udd3c \u2016 \u0394 \u2016 2 = \u03c1 \u2062 ( T \u03b8 + \u0394 \u2062 ( x ) \u2212 T \u03b8 \u2062 ( x ) ) 2 assign subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b \ud835\udc47 subscript \ud835\udd3c \ud835\udc65 superscript plus-or-minus 1 \ud835\udc5b subscript \ud835\udd3c subscript norm \u0394 2 \ud835\udf0c superscript subscript \ud835\udc47 \ud835\udf03 \u0394 \ud835\udc65 subscript \ud835\udc47 \ud835\udf03 \ud835\udc65 2 L_{\\rho,n}(T):=\\mathbb{E}_{x\\in\\{\\pm 1\\}^{n}}\\mathbb{E}_{\\|\\Delta\\|_{2}=\\rho}(%\nT_{\\theta+\\Delta}(x)-T_{\\theta}(x))^{2} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ) := blackboard_E start_POSTSUBSCRIPT italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT \u2225 roman_\u0394 \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_\u03c1 end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 + roman_\u0394 end_POSTSUBSCRIPT ( italic_x ) - italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (15) where the second expectation is over the radius- \u03c1 \ud835\udf0c \\rho italic_\u03c1 sphere in the space of parameter vectors. L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT quantifies the change in T \u03b8 subscript \ud835\udc47 \ud835\udf03 T_{\\theta} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT when perturbing \u03b8 \ud835\udf03 \\theta italic_\u03b8 by a size- \u03c1 \ud835\udf0c \\rho italic_\u03c1 vector in a random direction.\nLower bounds on L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT (Theorem 6 ) immediately entail lower bounds on other common sharpness measures, such as worst-case sharpness, and (in the limit \u03c1 \u2192 0 \u2192 \ud835\udf0c 0 \\rho\\rightarrow 0 italic_\u03c1 \u2192 0 ) the trace of the loss function\u2019s Hessian at \u03b8 \ud835\udf03 \\theta italic_\u03b8 . Report issue for preceding element\nIn defining the parameter vector \u03b8 \ud835\udf03 \\theta italic_\u03b8 in ( 15 ), we exclude positional encodings, both because they are often frozen, and because their number depends on n \ud835\udc5b n italic_n , hindering fair comparison across n \ud835\udc5b n italic_n . Report issue for preceding element\nOur next result lower-bounds L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in terms of the average sensitivity, provided the transformer is sufficiently wide in relation to its depth: Report issue for preceding element\nLet T \u03b8 subscript \ud835\udc47 \ud835\udf03 T_{\\theta} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT be a transformer where d > 12 \u2062 L \ud835\udc51 12 \ud835\udc3f d>12L italic_d > 12 italic_L .\nAssume T \u03b8 \u2062 ( x ) \u2208 [ \u2212 1 , 1 ] subscript \ud835\udc47 \ud835\udf03 \ud835\udc65 1 1 T_{\\theta}(x)\\in[-1,1] italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) \u2208 [ - 1 , 1 ] for each x \ud835\udc65 x italic_x .\nThen: Report issue for preceding element lim \u03c1 \u2192 0 lim inf n \u2192 \u221e L \u03c1 , n \u2062 ( T ) \u2265 lim inf n \u2192 \u221e a \u2062 s n \u2062 ( T \u03b8 ) 2 \u2062 n \u2212 L \u2062 exp \u2061 ( \u2212 \u03a9 \u2062 ( d ) ) subscript \u2192 \ud835\udf0c 0 subscript limit-infimum \u2192 \ud835\udc5b subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b \ud835\udc47 subscript limit-infimum \u2192 \ud835\udc5b \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b subscript \ud835\udc47 \ud835\udf03 2 \ud835\udc5b \ud835\udc3f \u03a9 \ud835\udc51 \\lim_{\\rho\\rightarrow 0}\\liminf_{n\\rightarrow\\infty}L_{\\rho,n}(T)\\geq\\liminf_{%\nn\\rightarrow\\infty}\\frac{as_{n}(T_{\\theta})}{2n}-L\\exp(-\\Omega(d)) roman_lim start_POSTSUBSCRIPT italic_\u03c1 \u2192 0 end_POSTSUBSCRIPT lim inf start_POSTSUBSCRIPT italic_n \u2192 \u221e end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ) \u2265 lim inf start_POSTSUBSCRIPT italic_n \u2192 \u221e end_POSTSUBSCRIPT divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_n end_ARG - italic_L roman_exp ( - roman_\u03a9 ( italic_d ) ) (16) Here, \u201c \u03a9 \u2062 ( d ) \u03a9 \ud835\udc51 \\Omega(d) roman_\u03a9 ( italic_d ) \u201d scales positively with d \ud835\udc51 d italic_d .\nIf T \u03b8 subscript \ud835\udc47 \ud835\udf03 T_{\\theta} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT has Boolean outputs ( T \u03b8 \u2062 ( x ) \u2208 { \u00b1 1 } subscript \ud835\udc47 \ud835\udf03 \ud835\udc65 plus-or-minus 1 T_{\\theta}(x)\\in\\{\\pm 1\\} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) \u2208 { \u00b1 1 } ),\nthen the factor \u201c2\u201d can be eliminated from ( 16 ). Report issue for preceding element\nInformally, this theorem says that, when a \u2062 s n \u2062 ( T \u03b8 ) \u223c n similar-to \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b subscript \ud835\udc47 \ud835\udf03 \ud835\udc5b as_{n}(T_{\\theta})\\sim n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) \u223c italic_n , then even tiny perturbations to the parameters will, in expectation, lead to a substantial change in predictions on long inputs.\nThis means that, as the input gets longer, the Hessian of the mean-squared loss at the minimizer fitting a sensitive function has unboundedly large entries. Report issue for preceding element\nSee Appendix C for the proof.\nThe key idea of the proof is that, if T \u03b8 subscript \ud835\udc47 \ud835\udf03 T_{\\theta} italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT is very sensitive in input space, small perturbations to the parameters usually lead to a large drop in sensitivity when n \ud835\udc5b n italic_n is large, because they lead to large changes in the layer-norm induced blowup that is needed to represent high-sensitivity functions.\nAs a consequence, transformers computing sensitive functions are isolated in parameter space. Report issue for preceding element\nThe theorem applies when d \ud835\udc51 d italic_d is substantially larger than L \ud835\udc3f L italic_L , as is indeed true of typical transformers (e.g., LLaMa 7B has d = 4096 \ud835\udc51 4096 d=4096 italic_d = 4096 and L = 32 \ud835\udc3f 32 L=32 italic_L = 32 ).\nThe convergence of the limits is slower when the parameter-norms, as summarized by C \ud835\udc36 C italic_C , are larger, as larger parameters can increase sensitivity.\nHowever, remarkably, for any fixed transformer, C \ud835\udc36 C italic_C becomes irrelevant for L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the limit where n \u2192 \u221e \u2192 \ud835\udc5b n\\rightarrow\\infty italic_n \u2192 \u221e ( 16 ). Report issue for preceding element\nWhile we stated Theorem 6 for an individual transformer, the same statement holds for families of transformers where weights may depend on n \ud835\udc5b n italic_n , as long as C \ud835\udc36 C italic_C remains bounded.\nA consequence is that scaling attention logits with log \u2061 n \ud835\udc5b \\log n roman_log italic_n (used to represent sparse functions by Edelman et\u00a0al. ( 2022 ); Chiang and Cholak ( 2022 ) ) will, at least in the input layer, not mitigate the difficulty of sensitive functions. Report issue for preceding element",
    "masked_text": "Leveraging Corollary 5, we now show that transformers expressing sensitive functions must be very sensitive to small perturbations of model parameters. That is, sensitivity in input space entails sensitivity in parameter space. An important consequence is that, for any highly sensitive function, any interpolating minima will be very sharp. This property is nontrivial, as seen from the fact that it disappears when adding a scratchpad (see below).Report issue for preceding element\nGiven a parameter vector \u03b8\ud835\udf03\\thetaitalic_\u03b8 defining the transformer T\u03b8subscript\ud835\udc47\ud835\udf03T_{\\theta}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, the average direction sharpness is [CITATION]Report issue for preceding element L\u03c1,n\u2062(T):=\ud835\udd3cx\u2208{\u00b11}n\u2062\ud835\udd3c\u2016\u0394\u20162=\u03c1\u2062(T\u03b8+\u0394\u2062(x)\u2212T\u03b8\u2062(x))2assignsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5b\ud835\udc47subscript\ud835\udd3c\ud835\udc65superscriptplus-or-minus1\ud835\udc5bsubscript\ud835\udd3csubscriptnorm\u03942\ud835\udf0csuperscriptsubscript\ud835\udc47\ud835\udf03\u0394\ud835\udc65subscript\ud835\udc47\ud835\udf03\ud835\udc652L_{\\rho,n}(T):=\\mathbb{E}_{x\\in\\{\\pm 1\\}^{n}}\\mathbb{E}_{\\|\\Delta\\|_{2}=\\rho}(% T_{\\theta+\\Delta}(x)-T_{\\theta}(x))^{2}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ) := blackboard_E start_POSTSUBSCRIPT italic_x \u2208 { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT \u2225 roman_\u0394 \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_\u03c1 end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 + roman_\u0394 end_POSTSUBSCRIPT ( italic_x ) - italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (15) where the second expectation is over the radius-\u03c1\ud835\udf0c\\rhoitalic_\u03c1 sphere in the space of parameter vectors. L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT quantifies the change in T\u03b8subscript\ud835\udc47\ud835\udf03T_{\\theta}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT when perturbing \u03b8\ud835\udf03\\thetaitalic_\u03b8 by a size-\u03c1\ud835\udf0c\\rhoitalic_\u03c1 vector in a random direction. Lower bounds on L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT (Theorem 6) immediately entail lower bounds on other common sharpness measures, such as worst-case sharpness, and (in the limit \u03c1\u21920\u2192\ud835\udf0c0\\rho\\rightarrow 0italic_\u03c1 \u2192 0) the trace of the loss function\u2019s Hessian at \u03b8\ud835\udf03\\thetaitalic_\u03b8.Report issue for preceding element\nIn defining the parameter vector \u03b8\ud835\udf03\\thetaitalic_\u03b8 in (15), we exclude positional encodings, both because they are often frozen, and because their number depends on n\ud835\udc5bnitalic_n, hindering fair comparison across n\ud835\udc5bnitalic_n.Report issue for preceding element\nOur next result lower-bounds L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in terms of the average sensitivity, provided the transformer is sufficiently wide in relation to its depth:Report issue for preceding element\nLet T\u03b8subscript\ud835\udc47\ud835\udf03T_{\\theta}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT be a transformer where d>12\u2062L\ud835\udc5112\ud835\udc3fd>12Litalic_d > 12 italic_L. Assume T\u03b8\u2062(x)\u2208[\u22121,1]subscript\ud835\udc47\ud835\udf03\ud835\udc6511T_{\\theta}(x)\\in[-1,1]italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) \u2208 [ - 1 , 1 ] for each x\ud835\udc65xitalic_x. Then:Report issue for preceding element lim\u03c1\u21920lim infn\u2192\u221eL\u03c1,n\u2062(T)\u2265lim infn\u2192\u221ea\u2062sn\u2062(T\u03b8)2\u2062n\u2212L\u2062exp\u2061(\u2212\u03a9\u2062(d))subscript\u2192\ud835\udf0c0subscriptlimit-infimum\u2192\ud835\udc5bsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5b\ud835\udc47subscriptlimit-infimum\u2192\ud835\udc5b\ud835\udc4esubscript\ud835\udc60\ud835\udc5bsubscript\ud835\udc47\ud835\udf032\ud835\udc5b\ud835\udc3f\u03a9\ud835\udc51\\lim_{\\rho\\rightarrow 0}\\liminf_{n\\rightarrow\\infty}L_{\\rho,n}(T)\\geq\\liminf_{% n\\rightarrow\\infty}\\frac{as_{n}(T_{\\theta})}{2n}-L\\exp(-\\Omega(d))roman_lim start_POSTSUBSCRIPT italic_\u03c1 \u2192 0 end_POSTSUBSCRIPT lim inf start_POSTSUBSCRIPT italic_n \u2192 \u221e end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ) \u2265 lim inf start_POSTSUBSCRIPT italic_n \u2192 \u221e end_POSTSUBSCRIPT divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_n end_ARG - italic_L roman_exp ( - roman_\u03a9 ( italic_d ) ) (16) Here, \u201c\u03a9\u2062(d)\u03a9\ud835\udc51\\Omega(d)roman_\u03a9 ( italic_d )\u201d scales positively with d\ud835\udc51ditalic_d. If T\u03b8subscript\ud835\udc47\ud835\udf03T_{\\theta}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT has Boolean outputs (T\u03b8\u2062(x)\u2208{\u00b11}subscript\ud835\udc47\ud835\udf03\ud835\udc65plus-or-minus1T_{\\theta}(x)\\in\\{\\pm 1\\}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) \u2208 { \u00b1 1 }), then the factor \u201c2\u201d can be eliminated from (16).Report issue for preceding element\nInformally, this theorem says that, when a\u2062sn\u2062(T\u03b8)\u223cnsimilar-to\ud835\udc4esubscript\ud835\udc60\ud835\udc5bsubscript\ud835\udc47\ud835\udf03\ud835\udc5bas_{n}(T_{\\theta})\\sim nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) \u223c italic_n, then even tiny perturbations to the parameters will, in expectation, lead to a substantial change in predictions on long inputs. This means that, as the input gets longer, the Hessian of the mean-squared loss at the minimizer fitting a sensitive function has unboundedly large entries.Report issue for preceding element\nSee Appendix C for the proof. The key idea of the proof is that, if T\u03b8subscript\ud835\udc47\ud835\udf03T_{\\theta}italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT is very sensitive in input space, small perturbations to the parameters usually lead to a large drop in sensitivity when n\ud835\udc5bnitalic_n is large, because they lead to large changes in the layer-norm induced blowup that is needed to represent high-sensitivity functions. As a consequence, transformers computing sensitive functions are isolated in parameter space.Report issue for preceding element\nThe theorem applies when d\ud835\udc51ditalic_d is substantially larger than L\ud835\udc3fLitalic_L, as is indeed true of typical transformers (e.g., LLaMa 7B has d=4096\ud835\udc514096d=4096italic_d = 4096 and L=32\ud835\udc3f32L=32italic_L = 32). The convergence of the limits is slower when the parameter-norms, as summarized by C\ud835\udc36Citalic_C, are larger, as larger parameters can increase sensitivity. However, remarkably, for any fixed transformer, C\ud835\udc36Citalic_C becomes irrelevant for L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the limit where n\u2192\u221e\u2192\ud835\udc5bn\\rightarrow\\inftyitalic_n \u2192 \u221e (16).Report issue for preceding element\nWhile we stated Theorem 6 for an individual transformer, the same statement holds for families of transformers where weights may depend on n\ud835\udc5bnitalic_n, as long as C\ud835\udc36Citalic_C remains bounded. A consequence is that scaling attention logits with log\u2061n\ud835\udc5b\\log nroman_log italic_n (used to represent sparse functions by [CITATION]) will, at least in the input layer, not mitigate the difficulty of sensitive functions.Report issue for preceding element",
    "citations": [
      {
        "tag": "Wen et\u00a0al. (2022)",
        "title": "How does\nsharpness-aware minimization minimize sharpness?",
        "authors": "Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. 2022.",
        "journal": "CoRR, abs/2211.05729."
      },
      {
        "tag": "Jiang et\u00a0al. (2020)",
        "title": "Fantastic\ngeneralization measures and where to find them.",
        "authors": "Yiding Jiang, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy\nBengio. 2020.",
        "journal": "InInternational Conference on Learning Representations."
      },
      {
        "tag": "Foret et\u00a0al. (2020)",
        "title": "Sharpness-aware minimization for efficiently improving\ngeneralization.",
        "authors": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2020.",
        "journal": "arXiv preprint arXiv:2010.01412."
      },
      {
        "tag": "Chiang and Cholak (2022)",
        "title": "Overcoming a\ntheoretical limitation of self-attention.",
        "authors": "David Chiang and Peter Cholak. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 7654\u20137664. Association for Computational\nLinguistics."
      },
      {
        "tag": "Edelman et\u00a0al. (2022)",
        "title": "Inductive biases and variable creation in self-attention mechanisms.",
        "authors": "Benjamin\u00a0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages\n5793\u20135831. PMLR."
      }
    ]
  },
  "S6": {
    "title": "6Implications",
    "text": "We discuss how Theorem 6 unifies a range of diverse empirical findings about the behavior of transformers. Report issue for preceding element\nFor PARITY, L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT converges to 1 1 1 1 for large d \ud835\udc51 d italic_d , showing that arbitrarily small perturbations to a transformer computing PARITY will lead to a high loss for sufficiently long inputs.\nPreviously, Chiang and Cholak ( 2022 ) noted that, for their hand-constructed transformer representing PARITY, small changes to a specific parameter led to a large increase in loss, suggesting that this made the solution impossible to reach with SGD.\nTheorem 6 shows that this phenomenon is unavoidable for any transformer representing a high-sensitivity function.\nFor functions with sublinear average sensitivity, Theorem 6 entails no nontrivial lower bound on sharpness, and no such phenomenon is predicted. Report issue for preceding element\nA key step in Theorem 6 is to show that s \u2062 ( T \u03b8 + \u0394 , x ) \ud835\udc60 subscript \ud835\udc47 \ud835\udf03 \u0394 \ud835\udc65 s(T_{\\theta+\\Delta},x) italic_s ( italic_T start_POSTSUBSCRIPT italic_\u03b8 + roman_\u0394 end_POSTSUBSCRIPT , italic_x ) is bounded with very high probability over the choice of \u0394 \u0394 \\Delta roman_\u0394 (Appendix, Eq. ( 70 )); this immediately entails that high-sensitivity transformers can only inhabit a small volume in parameter space.\nThis explains why randomly initialized transformers empirically show low average sensitivity, more so than recurrent networks (Bhattamishra et\u00a0al., 2023 ) . Report issue for preceding element\nAn important corollary is that, for a sensitive function, length generalization requires exact match to the minimum: the slightest deviation from the exact minimum will, in expectation, lead to failure when inputs get sufficient long, even if the minimum itself represents a length-generalizing solution.\nThis provides, for the first time, a rigorous explanation why, despite the in-principle existence of length-generalizing transformers, transformers struggle with length generalization for PARITY (e.g. Bhattamishra et\u00a0al., 2020 ) . Report issue for preceding element\nAnother corollary is that, in expectation, the training loss landscape around an interpolating minimum places a constraint on a function\u2019s overall sensitivity, even if not a single pair of Hamming neighbors are in the training set.\nThis is because ( 16 ) remains true, on average across randomly selected training sets, if replacing the expectation over the full input space with an expectation over the training set in ( 15 ).\nThis means that, for long inputs, flat minima of the training loss generalize with bounded sensitivity.\nTo the extent that gradient-based training tends to find flatter minima (e.g. Damian et\u00a0al., 2023 ) , this provides a theoretical justification for the empirical result that transformers\u2019 generalization behavior, when trained on a Boolean function on some training subset of { \u00b1 1 } n superscript plus-or-minus 1 \ud835\udc5b \\{\\pm 1\\}^{n} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , shows a strong bias towards low average sensitivity (Bhattamishra et\u00a0al., 2023 ) . Report issue for preceding element\nAbbe et\u00a0al. ( 2023 ) proposed a min-degree bias, that is, a generalization bias towards functions that are linear combinations of functions that each depend on only a few inputs. Any function f : { \u00b1 1 } n \u2192 \u211d : \ud835\udc53 \u2192 superscript plus-or-minus 1 \ud835\udc5b \u211d f:\\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R} italic_f : { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R can be uniquely written as a linear combination of the multilinear monomials \u03c7 P \u2062 ( x ) := \u220f i \u2208 P x i assign subscript \ud835\udf12 \ud835\udc43 \ud835\udc65 subscript product \ud835\udc56 \ud835\udc43 subscript \ud835\udc65 \ud835\udc56 \\chi_{P}(x):=\\prod_{i\\in P}x_{i} italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) := \u220f start_POSTSUBSCRIPT italic_i \u2208 italic_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where P \u2286 { 1 , \u2026 , n } \ud835\udc43 1 \u2026 \ud835\udc5b P\\subseteq\\{1,\\dots,n\\} italic_P \u2286 { 1 , \u2026 , italic_n } : f \u2062 ( x ) = \u2211 P \u03bb P \u2062 \u03c7 P \u2062 ( x ) \ud835\udc53 \ud835\udc65 subscript \ud835\udc43 subscript \ud835\udf06 \ud835\udc43 subscript \ud835\udf12 \ud835\udc43 \ud835\udc65 f(x)=\\sum_{P}\\lambda_{P}\\chi_{P}(x) italic_f ( italic_x ) = \u2211 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) .\nThe coefficients \u03bb P subscript \ud835\udf06 \ud835\udc43 \\lambda_{P} italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT define the Fourier-Walsh transform of f \ud835\udc53 f italic_f .\nFor \u03c7 P subscript \ud835\udf12 \ud835\udc43 \\chi_{P} italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , both its degree as a polynomial, and its average sensitivity, are | P | \ud835\udc43 |P| | italic_P | .\nThe degree profile, as defined by Abbe et\u00a0al. ( 2023 ) , is the tuple ( d 1 , \u2026 , d n ) subscript \ud835\udc51 1 \u2026 subscript \ud835\udc51 \ud835\udc5b (d_{1},...,d_{n}) ( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) where d i = \u2211 P : | P | = i | \u03bb P | 2 subscript \ud835\udc51 \ud835\udc56 subscript : \ud835\udc43 \ud835\udc43 \ud835\udc56 superscript subscript \ud835\udf06 \ud835\udc43 2 d_{i}=\\sum_{P:|P|=i}|\\lambda_{P}|^{2} italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_P : | italic_P | = italic_i end_POSTSUBSCRIPT | italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Minimization of degree profile then refers to setting as many of the later entries to zero, and minimizing the size of the last nonzero entry. Abbe et\u00a0al. ( 2023 ) proved inductive biases towards functions with a low degree profile for a random features model and diagonal networks. Evidence in the case of transformers was limited to empirical data on three example functions.\nOur results entail prove an bound on degree profiles for the full transformer architecture, because\naverage sensitivity is a summary statistic of the degree profile (O\u2019Donnell, 2014 ) : Report issue for preceding element a \u2062 s n \u2062 ( f ) = \u2211 P \u2286 { 1 , \u2026 , n } \u03bb P 2 \u2062 | P | = \u2211 i = 0 n i \u22c5 d i \ud835\udc4e subscript \ud835\udc60 \ud835\udc5b \ud835\udc53 subscript \ud835\udc43 1 \u2026 \ud835\udc5b superscript subscript \ud835\udf06 \ud835\udc43 2 \ud835\udc43 superscript subscript \ud835\udc56 0 \ud835\udc5b \u22c5 \ud835\udc56 subscript \ud835\udc51 \ud835\udc56 as_{n}(f)=\\sum_{P\\subseteq\\{1,\\dots,n\\}}\\lambda_{P}^{2}|P|=\\sum_{i=0}^{n}i%\n\\cdot d_{i} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = \u2211 start_POSTSUBSCRIPT italic_P \u2286 { 1 , \u2026 , italic_n } end_POSTSUBSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_P | = \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_i \u22c5 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (17) Hence, functions with degree profile assigning substantial weight to degrees of order \u223c n similar-to absent \ud835\udc5b \\sim n \u223c italic_n are brittle in parameter space, corresponding to very sharp minima. Report issue for preceding element\nPARITY can be solved well with a scratchpad (Anil et\u00a0al., 2022 ; Liu et\u00a0al., 2023 ) .\nExisting theoretical accounts of the benefit of intermediate steps for transformers\u2019 expressive capacity (e.g. Merrill and Sabharwal, 2023a ; Feng et\u00a0al., 2023 ) do not account for the benefit of intermediate steps for PARITY-like problems: While the theoretical models of transformer expressivenes used in these studies do predict versions with intermediate steps to be easy, they do not predict that computing PARITY in a single step would be hard, due to Fact 1.\nThe concept of average sensitivity provides a simple explanation for the benefit of intermediate steps.\nFormally, we can consider the problem of simulating a finite automaton with state set \ud835\udcb3 \ud835\udcb3 \\mathcal{X} caligraphic_X either translating to the final state t n subscript \ud835\udc61 \ud835\udc5b {t}_{n} italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT in one go (standard), or to autoregressively translate it into a sequence of states t 1 , \u2026 , t n subscript \ud835\udc61 1 \u2026 subscript \ud835\udc61 \ud835\udc5b {t}_{1},...,{t}_{n} italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (scratchpad). Then (proof in Appendix D ): Report issue for preceding element\nSimulating an automaton with scratchpad has sensitivity \ud835\udcaa \u2062 ( 1 ) \ud835\udcaa 1 \\mathcal{O}(1) caligraphic_O ( 1 ) for each autoregressive step. Report issue for preceding element",
    "masked_text": "We discuss how Theorem 6 unifies a range of diverse empirical findings about the behavior of transformers.Report issue for preceding element\nFor PARITY, L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT converges to 1111 for large d\ud835\udc51ditalic_d, showing that arbitrarily small perturbations to a transformer computing PARITY will lead to a high loss for sufficiently long inputs. Previously, [CITATION] noted that, for their hand-constructed transformer representing PARITY, small changes to a specific parameter led to a large increase in loss, suggesting that this made the solution impossible to reach with SGD. Theorem 6 shows that this phenomenon is unavoidable for any transformer representing a high-sensitivity function. For functions with sublinear average sensitivity, Theorem 6 entails no nontrivial lower bound on sharpness, and no such phenomenon is predicted.Report issue for preceding element\nA key step in Theorem 6 is to show that s\u2062(T\u03b8+\u0394,x)\ud835\udc60subscript\ud835\udc47\ud835\udf03\u0394\ud835\udc65s(T_{\\theta+\\Delta},x)italic_s ( italic_T start_POSTSUBSCRIPT italic_\u03b8 + roman_\u0394 end_POSTSUBSCRIPT , italic_x ) is bounded with very high probability over the choice of \u0394\u0394\\Deltaroman_\u0394 (Appendix, Eq. (70)); this immediately entails that high-sensitivity transformers can only inhabit a small volume in parameter space. This explains why randomly initialized transformers empirically show low average sensitivity, more so than recurrent networks [CITATION].Report issue for preceding element\nAn important corollary is that, for a sensitive function, length generalization requires exact match to the minimum: the slightest deviation from the exact minimum will, in expectation, lead to failure when inputs get sufficient long, even if the minimum itself represents a length-generalizing solution. This provides, for the first time, a rigorous explanation why, despite the in-principle existence of length-generalizing transformers, transformers struggle with length generalization for PARITY [CITATION].Report issue for preceding element\nAnother corollary is that, in expectation, the training loss landscape around an interpolating minimum places a constraint on a function\u2019s overall sensitivity, even if not a single pair of Hamming neighbors are in the training set. This is because (16) remains true, on average across randomly selected training sets, if replacing the expectation over the full input space with an expectation over the training set in (15). This means that, for long inputs, flat minima of the training loss generalize with bounded sensitivity. To the extent that gradient-based training tends to find flatter minima [CITATION], this provides a theoretical justification for the empirical result that transformers\u2019 generalization behavior, when trained on a Boolean function on some training subset of {\u00b11}nsuperscriptplus-or-minus1\ud835\udc5b\\{\\pm 1\\}^{n}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, shows a strong bias towards low average sensitivity [CITATION].Report issue for preceding element\n[CITATION] proposed a min-degree bias, that is, a generalization bias towards functions that are linear combinations of functions that each depend on only a few inputs. Any function f:{\u00b11}n\u2192\u211d:\ud835\udc53\u2192superscriptplus-or-minus1\ud835\udc5b\u211df:\\{\\pm 1\\}^{n}\\rightarrow\\mathbb{R}italic_f : { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R can be uniquely written as a linear combination of the multilinear monomials \u03c7P\u2062(x):=\u220fi\u2208Pxiassignsubscript\ud835\udf12\ud835\udc43\ud835\udc65subscriptproduct\ud835\udc56\ud835\udc43subscript\ud835\udc65\ud835\udc56\\chi_{P}(x):=\\prod_{i\\in P}x_{i}italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) := \u220f start_POSTSUBSCRIPT italic_i \u2208 italic_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where P\u2286{1,\u2026,n}\ud835\udc431\u2026\ud835\udc5bP\\subseteq\\{1,\\dots,n\\}italic_P \u2286 { 1 , \u2026 , italic_n }: f\u2062(x)=\u2211P\u03bbP\u2062\u03c7P\u2062(x)\ud835\udc53\ud835\udc65subscript\ud835\udc43subscript\ud835\udf06\ud835\udc43subscript\ud835\udf12\ud835\udc43\ud835\udc65f(x)=\\sum_{P}\\lambda_{P}\\chi_{P}(x)italic_f ( italic_x ) = \u2211 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ). The coefficients \u03bbPsubscript\ud835\udf06\ud835\udc43\\lambda_{P}italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT define the Fourier-Walsh transform of f\ud835\udc53fitalic_f. For \u03c7Psubscript\ud835\udf12\ud835\udc43\\chi_{P}italic_\u03c7 start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT, both its degree as a polynomial, and its average sensitivity, are |P|\ud835\udc43|P|| italic_P |. The degree profile, as defined by [CITATION], is the tuple (d1,\u2026,dn)subscript\ud835\udc511\u2026subscript\ud835\udc51\ud835\udc5b(d_{1},...,d_{n})( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) where di=\u2211P:|P|=i|\u03bbP|2subscript\ud835\udc51\ud835\udc56subscript:\ud835\udc43\ud835\udc43\ud835\udc56superscriptsubscript\ud835\udf06\ud835\udc432d_{i}=\\sum_{P:|P|=i}|\\lambda_{P}|^{2}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_P : | italic_P | = italic_i end_POSTSUBSCRIPT | italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Minimization of degree profile then refers to setting as many of the later entries to zero, and minimizing the size of the last nonzero entry. [CITATION] proved inductive biases towards functions with a low degree profile for a random features model and diagonal networks. Evidence in the case of transformers was limited to empirical data on three example functions. Our results entail prove an bound on degree profiles for the full transformer architecture, because average sensitivity is a summary statistic of the degree profile [CITATION]:Report issue for preceding element a\u2062sn\u2062(f)=\u2211P\u2286{1,\u2026,n}\u03bbP2\u2062|P|=\u2211i=0ni\u22c5di\ud835\udc4esubscript\ud835\udc60\ud835\udc5b\ud835\udc53subscript\ud835\udc431\u2026\ud835\udc5bsuperscriptsubscript\ud835\udf06\ud835\udc432\ud835\udc43superscriptsubscript\ud835\udc560\ud835\udc5b\u22c5\ud835\udc56subscript\ud835\udc51\ud835\udc56as_{n}(f)=\\sum_{P\\subseteq\\{1,\\dots,n\\}}\\lambda_{P}^{2}|P|=\\sum_{i=0}^{n}i% \\cdot d_{i}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = \u2211 start_POSTSUBSCRIPT italic_P \u2286 { 1 , \u2026 , italic_n } end_POSTSUBSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_P | = \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_i \u22c5 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (17) Hence, functions with degree profile assigning substantial weight to degrees of order \u223cnsimilar-toabsent\ud835\udc5b\\sim n\u223c italic_n are brittle in parameter space, corresponding to very sharp minima.Report issue for preceding element\nPARITY can be solved well with a scratchpad [CITATION]. Existing theoretical accounts of the benefit of intermediate steps for transformers\u2019 expressive capacity [CITATION] do not account for the benefit of intermediate steps for PARITY-like problems: While the theoretical models of transformer expressivenes used in these studies do predict versions with intermediate steps to be easy, they do not predict that computing PARITY in a single step would be hard, due to Fact 1. The concept of average sensitivity provides a simple explanation for the benefit of intermediate steps. Formally, we can consider the problem of simulating a finite automaton with state set \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X either translating to the final state tnsubscript\ud835\udc61\ud835\udc5b{t}_{n}italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT in one go (standard), or to autoregressively translate it into a sequence of states t1,\u2026,tnsubscript\ud835\udc611\u2026subscript\ud835\udc61\ud835\udc5b{t}_{1},...,{t}_{n}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (scratchpad). Then (proof in Appendix D):Report issue for preceding element\nSimulating an automaton with scratchpad has sensitivity \ud835\udcaa\u2062(1)\ud835\udcaa1\\mathcal{O}(1)caligraphic_O ( 1 ) for each autoregressive step.Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2023)",
        "title": "Transformers\nlearn shortcuts to automata.",
        "authors": "Bingbin Liu, Jordan\u00a0T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.\n2023.",
        "journal": "InThe Eleventh International Conference on Learning\nRepresentations."
      },
      {
        "tag": "Feng et\u00a0al. (2023)",
        "title": "Towards revealing\nthe mystery behind chain of thought: A theoretical perspective.",
        "authors": "Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di\u00a0He, and Liwei Wang. 2023.",
        "journal": "InThirty-seventh Conference on Neural Information Processing\nSystems."
      },
      {
        "tag": "Abbe et\u00a0al. (2023)",
        "title": "Generalization on the unseen, logic reasoning and degree curriculum.",
        "authors": "Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. 2023.",
        "journal": "InInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\nMachine Learning Research, pages 31\u201360. PMLR."
      },
      {
        "tag": "Merrill and Sabharwal (2023a)",
        "title": "The expressive\npower of transformers with chain of thought.",
        "authors": "William Merrill and Ashish Sabharwal. 2023a.",
        "journal": "InNeurIPS 2023 Workshop on Mathematics of Modern Machine\nLearning."
      },
      {
        "tag": "Anil et\u00a0al. (2022)",
        "title": "Exploring length generalization in large language models.",
        "authors": "Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay\nRamasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.",
        "journal": "Advances in Neural Information Processing Systems,\n35:38546\u201338556."
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2023)",
        "title": "Simplicity\nbias in transformers and their ability to learn sparse boolean functions.",
        "authors": "Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. 2023.",
        "journal": "InProceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 5767\u20135791. Association for Computational\nLinguistics."
      },
      {
        "tag": "Damian et\u00a0al. (2023)",
        "title": "Self-stabilization:\nThe implicit bias of gradient descent at the edge of stability.",
        "authors": "Alex Damian, Eshaan Nichani, and Jason\u00a0D. Lee. 2023.",
        "journal": "InThe Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net."
      },
      {
        "tag": "Chiang and Cholak (2022)",
        "title": "Overcoming a\ntheoretical limitation of self-attention.",
        "authors": "David Chiang and Peter Cholak. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 7654\u20137664. Association for Computational\nLinguistics."
      },
      {
        "tag": "O\u2019Donnell (2014)",
        "title": "Analysis of Boolean Functions.",
        "authors": "Ryan O\u2019Donnell. 2014.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2020)",
        "title": "On the\nability and limitations of transformers to recognize formal languages.",
        "authors": "Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020.",
        "journal": "InProceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online, November 16-20, 2020,\npages 7096\u20137116. Association for Computational Linguistics."
      }
    ]
  },
  "S7": {
    "title": "7Experiments",
    "text": "We conducted experiments to test the predictions made by our theory, specifically assessing predictions regarding loss landscapes and sharpness of the minima.\nIn all experiments, we use the transformer encoder architecture, using the default implementation in PyTorch (Paszke et\u00a0al., 2019 ) . Each model is trained to fit a function f \ud835\udc53 f italic_f on a specific sequence length n \ud835\udc5b n italic_n . Each training input x \ud835\udc65 x italic_x is generated uniformly from { \u00b1 1 } n superscript plus-or-minus 1 \ud835\udc5b \\{\\pm 1\\}^{n} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , and each input bit, treated as a separate token, is embedded using learned token and positional encodings. 3 3 3 Code is available at https://github.com/lacoco-lab/sensitivity-hardness . Report issue for preceding element\nThe representation of the last token is passed to a linear layer to generate the prediction T \u03b8 \u2062 ( x ) subscript \ud835\udc47 \ud835\udf03 \ud835\udc65 T_{\\theta}(x) italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) . Parameters \u03b8 \ud835\udf03 \\theta italic_\u03b8 of the transformer are optimized for MSE loss between T \u03b8 \u2062 ( x ) subscript \ud835\udc47 \ud835\udf03 \ud835\udc65 T_{\\theta}(x) italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) and f \u2062 ( x ) \ud835\udc53 \ud835\udc65 f(x) italic_f ( italic_x ) using AdamW (Loshchilov and Hutter, 2017 ) . For full details on hyperparameters and training setup, refer to Appendix E.1 . Report issue for preceding element\nIn implementation, we assumed versions of the functions outputting to { 0 , 1 } 0 1 \\{0,1\\} { 0 , 1 } ; we rescaled sharpness values accordingly for comparability with the theory. Report issue for preceding element\nWe analyzed models using the following metrics: Report issue for preceding element 1. Parameter Norm. We compute the L2 norm of the entire model\u2019s parameter vector, excluding positional encoding matrices.\nWe discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable. Report issue for preceding element 2. LayerNorm Blowup. This metric is computed by computing the maximum normalization factor ( 5 ) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to Blowup Blowup \\operatorname{Blowup} roman_Blowup . 4 4 4 In the theory, Blowup has an additional 1+\u2026 in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B \u2062 l \u2062 o \u2062 w \u2062 u \u2062 p = \u03c9 \u2062 ( 1 ) \ud835\udc35 \ud835\udc59 \ud835\udc5c \ud835\udc64 \ud835\udc62 \ud835\udc5d \ud835\udf14 1 Blowup=\\omega(1) italic_B italic_l italic_o italic_w italic_u italic_p = italic_\u03c9 ( 1 ) . Report issue for preceding element 3. Sharpness. In order to avoid committing to any specific \u03c1 \ud835\udf0c \\rho italic_\u03c1 , we sample \u0394 \u0394 \\Delta roman_\u0394 in ( 15 ) not from the radius- \u03c1 \ud835\udf0c \\rho italic_\u03c1 -sphere, but from a mean-zero Gaussian with STD \u03c1 = 0.02 \ud835\udf0c 0.02 \\rho=0.02 italic_\u03c1 = 0.02 .\nWe estimate using N s , p subscript \ud835\udc41 \ud835\udc60 \ud835\udc5d N_{s,p} italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and N s , b subscript \ud835\udc41 \ud835\udc60 \ud835\udc4f N_{s,b} italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x \ud835\udc65 x italic_x . This provides results equivalent to L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the \u03c1 \u2192 0 \u2192 \ud835\udf0c 0 \\rho\\rightarrow 0 italic_\u03c1 \u2192 0 asymptotic, while avoiding committing to any specific \u03c1 \ud835\udf0c \\rho italic_\u03c1 . Report issue for preceding element\nParameter Norm. We compute the L2 norm of the entire model\u2019s parameter vector, excluding positional encoding matrices.\nWe discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable. Report issue for preceding element\nLayerNorm Blowup. This metric is computed by computing the maximum normalization factor ( 5 ) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to Blowup Blowup \\operatorname{Blowup} roman_Blowup . 4 4 4 In the theory, Blowup has an additional 1+\u2026 in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B \u2062 l \u2062 o \u2062 w \u2062 u \u2062 p = \u03c9 \u2062 ( 1 ) \ud835\udc35 \ud835\udc59 \ud835\udc5c \ud835\udc64 \ud835\udc62 \ud835\udc5d \ud835\udf14 1 Blowup=\\omega(1) italic_B italic_l italic_o italic_w italic_u italic_p = italic_\u03c9 ( 1 ) . Report issue for preceding element\nSharpness. In order to avoid committing to any specific \u03c1 \ud835\udf0c \\rho italic_\u03c1 , we sample \u0394 \u0394 \\Delta roman_\u0394 in ( 15 ) not from the radius- \u03c1 \ud835\udf0c \\rho italic_\u03c1 -sphere, but from a mean-zero Gaussian with STD \u03c1 = 0.02 \ud835\udf0c 0.02 \\rho=0.02 italic_\u03c1 = 0.02 .\nWe estimate using N s , p subscript \ud835\udc41 \ud835\udc60 \ud835\udc5d N_{s,p} italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and N s , b subscript \ud835\udc41 \ud835\udc60 \ud835\udc4f N_{s,b} italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x \ud835\udc65 x italic_x . This provides results equivalent to L \u03c1 , n subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b L_{\\rho,n} italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the \u03c1 \u2192 0 \u2192 \ud835\udf0c 0 \\rho\\rightarrow 0 italic_\u03c1 \u2192 0 asymptotic, while avoiding committing to any specific \u03c1 \ud835\udf0c \\rho italic_\u03c1 . Report issue for preceding element\nAs we are interested in properties of models that compute given functions, runs that did not converge (evaluation MSE higher than 10 \u2212 3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT ) were discarded. Report issue for preceding element\nIn this experiment, we train transformers to fit f \u2208 { PARITY , MAJORITY , FIRST , MEAN } \ud835\udc53 PARITY MAJORITY FIRST MEAN f\\in\\{\\text{PARITY},\\text{MAJORITY},\\text{FIRST},\\text{MEAN}\\} italic_f \u2208 { PARITY , MAJORITY , FIRST , MEAN } on sequence lengths from 4 to 30. For each function and sequence length, we retrain the model 10 times from different random initializations. Report issue for preceding element\nFor PARITY, sharpness stably increases with the input length (Figure 1 ).\nFor the other functions, whose sensitivity grows more slowly with n \ud835\udc5b n italic_n , (a) the absolute value of sharpness is orders of magnitude lower than for PARITY; (b) there is little increase with n \ud835\udc5b n italic_n .\nMore results are shown in Appendix E.2 . Report issue for preceding element\nAt any fixed input length n \ud835\udc5b n italic_n , high sensitivity can be achieved by a combination of large weights and a large LayerNorm blowup.\nBy Theorem 5 , the product of C \ud835\udc36 C italic_C and squared blowup is bounded from below with some value B n \u2062 ( f ) subscript \ud835\udc35 \ud835\udc5b \ud835\udc53 B_{n}(f) italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) . Hence, the product of C \ud835\udc36 \\sqrt{C} square-root start_ARG italic_C end_ARG and blowup is bounded with B n \u2062 ( f ) subscript \ud835\udc35 \ud835\udc5b \ud835\udc53 \\sqrt{B_{n}(f)} square-root start_ARG italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG , and the sum of 1 2 \u2062 log \u2061 C 1 2 \ud835\udc36 \\frac{1}{2}\\log C divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_C and log \u2061 Blowup Blowup \\log\\mathrm{Blowup} roman_log roman_Blowup is bounded with 1 2 \u2062 log \u2061 B n \u2062 ( f ) 1 2 subscript \ud835\udc35 \ud835\udc5b \ud835\udc53 \\frac{1}{2}\\log{B_{n}(f)} divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) .\nHowever, C \ud835\udc36 C italic_C depends exponentially on parameters, and thus we expect the parameter norm to trade off with the logarithm of the layer norm blowup.\nMoreover, for PARITY the value of B n \u2062 ( f ) subscript \ud835\udc35 \ud835\udc5b \ud835\udc53 B_{n}(f) italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) increases with n \ud835\udc5b n italic_n , and therefore sum of parameter norms and the logarithm of the blowup should also increase with n \ud835\udc5b n italic_n . Report issue for preceding element\nTo test this prediction, for each function f \ud835\udc53 f italic_f we train a set of models with varying sequence lengths n \ud835\udc5b n italic_n , weight decay and learning rate parameters. It allows us to obtain datapoints with diverse values of LN Blowup and parameter norm. Report issue for preceding element\nResults for PARITY can be seen in Figure 2 , and for other functions in Figure 9 . For all functions, there is a clear log Blowup-Parameter Norm tradeoff.\nFor PARITY, the shape of the tradeoff indeed depends on n \ud835\udc5b n italic_n , with transformers trained for high n \ud835\udc5b n italic_n located above others in the log Blowup-Parameter Norm coordinates. For other functions, dependency on n \ud835\udc5b n italic_n is not visible, at least at this range of n \ud835\udc5b n italic_n . Report issue for preceding element\nAs discussed above, Theorem 6 predicts that transformers will generalize with low sensitivity, as low-sensitivity functions will tend to have flatter minima. Report issue for preceding element\nTo test this prediction, we created random functions f := { \u00b1 1 } n \u2192 { \u00b1 1 } assign \ud835\udc53 superscript plus-or-minus 1 \ud835\udc5b \u2192 plus-or-minus 1 f:=\\{\\pm 1\\}^{n}\\rightarrow\\{\\pm 1\\} italic_f := { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 { \u00b1 1 } , and sampled random training sets from { \u00b1 1 } n superscript plus-or-minus 1 \ud835\udc5b \\{\\pm 1\\}^{n} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT of size 128/256/512. We fixed n = 10 \ud835\udc5b 10 n=10 italic_n = 10 .\nFor each f \ud835\udc53 f italic_f , we train a transformer T 1 subscript \ud835\udc47 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on the training set and use it to label the whole input space of sequences of length n \ud835\udc5b n italic_n (1024 objects in our case).\nReplicating Bhattamishra et\u00a0al. ( 2023 ) , these extrapolated functions T 1 subscript \ud835\udc47 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT have lower average sensitivity than the original functions f \ud835\udc53 f italic_f , indicating a low-sensitivity bias in generalization (Figure 4 ).\nNow, in order to directly compare the sharpness of minima corresponding to the true function f \ud835\udc53 f italic_f and the extrapolated function T 1 subscript \ud835\udc47 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , we trained new transformers to fit both functions on the entire dataset { \u00b1 1 } n superscript plus-or-minus 1 \ud835\udc5b \\{\\pm 1\\}^{n} { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , and measured the sharpness for these two new transformers. The results were averaged over 10 random functions f \ud835\udc53 f italic_f , 10 training sets per f \ud835\udc53 f italic_f and training set size, and 5 new transformers per each T 1 subscript \ud835\udc47 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\nSharpness was indeed lower for the transformer fitting the extrapolated functions than for the transformer matching the original random functions f \ud835\udc53 f italic_f .\nThis also held when measuring sharpness only on the training set (Appendix, Figure 15 ). Report issue for preceding element\nBy Theorem 7 , sensitivity of each autoregressive step when computing PARITY with scratchpad is O \u2062 ( 1 ) \ud835\udc42 1 O(1) italic_O ( 1 ) . Hence, Theorem 6 provides no nontrivial lower bound for L \u03c1 , n \u2062 ( T ) subscript \ud835\udc3f \ud835\udf0c \ud835\udc5b \ud835\udc47 L_{\\rho,n}(T) italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ) .\nWe trained an Encoder-Decoder Transformer, predicting PARITY of i \ud835\udc56 i italic_i -th substring on i \ud835\udc56 i italic_i -th autoregressive step: t i = PARITY \u2062 ( x 1 : i ) = x i \u2295 t i \u2212 1 subscript \ud835\udc61 \ud835\udc56 PARITY subscript \ud835\udc65 : 1 \ud835\udc56 direct-sum subscript \ud835\udc65 \ud835\udc56 subscript \ud835\udc61 \ud835\udc56 1 {t}_{i}=\\mathrm{PARITY}(x_{1:i})=x_{i}\\oplus{t}_{i-1} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_PARITY ( italic_x start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2295 italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ( t 0 = 0 subscript \ud835\udc61 0 0 {t}_{0}=0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 ).\nThe visual dependency between sharpness and length of input for PARITY with a scratchpad is shown in Figure 11 . Even for length around 300, sharpness is low and there is little increase with input length.\nThus, decrease in sensitivity due to the scratchpad can explain why prior work (Anil et\u00a0al., 2022 ) found that PARITY is easy for Transformers with scratchpad. Report issue for preceding element\nFigure 3 represents the evolution of loss and sharpness of Transformer models trained for PARITY with input length 25. The results are averaged across 39 converged runs. Similar curves for parameter norm and LayerNorm blowup are presented in Appendix 12 . Report issue for preceding element\nA dramatic increase in sharpness occurs at exactly the time when loss falls to 0.\nThis suggests the presence of a steep minimum in the loss landscape, and fitting PARITY requires the optimization procedure to find this minimum.\nFigure 12 (Appendix) shows further details of this process. During learning, there is a small but sharp increase in the blowup which makes the high sensitivity of the model possible, increasing the left-hand side of the inequality in Corollary 5 . Following that, non-zero weight decay drives the parameter norm down, which \u2013 by the theoretically predicted tradeoff between blowup and parameter norm \u2013 is accompanied by an exponential increase in blowup. Report issue for preceding element",
    "masked_text": "We conducted experiments to test the predictions made by our theory, specifically assessing predictions regarding loss landscapes and sharpness of the minima. In all experiments, we use the transformer encoder architecture, using the default implementation in PyTorch [CITATION]. Each model is trained to fit a function f\ud835\udc53fitalic_f on a specific sequence length n\ud835\udc5bnitalic_n. Each training input x\ud835\udc65xitalic_x is generated uniformly from {\u00b11}nsuperscriptplus-or-minus1\ud835\udc5b\\{\\pm 1\\}^{n}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and each input bit, treated as a separate token, is embedded using learned token and positional encodings.333Code is available at https://github.com/lacoco-lab/sensitivity-hardness.Report issue for preceding element\nThe representation of the last token is passed to a linear layer to generate the prediction T\u03b8\u2062(x)subscript\ud835\udc47\ud835\udf03\ud835\udc65T_{\\theta}(x)italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ). Parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 of the transformer are optimized for MSE loss between T\u03b8\u2062(x)subscript\ud835\udc47\ud835\udf03\ud835\udc65T_{\\theta}(x)italic_T start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x ) and f\u2062(x)\ud835\udc53\ud835\udc65f(x)italic_f ( italic_x ) using AdamW [CITATION]. For full details on hyperparameters and training setup, refer to Appendix E.1.Report issue for preceding element\nIn implementation, we assumed versions of the functions outputting to {0,1}01\\{0,1\\}{ 0 , 1 }; we rescaled sharpness values accordingly for comparability with the theory.Report issue for preceding element\nWe analyzed models using the following metrics:Report issue for preceding element 1. Parameter Norm. We compute the L2 norm of the entire model\u2019s parameter vector, excluding positional encoding matrices. We discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable.Report issue for preceding element 2. LayerNorm Blowup. This metric is computed by computing the maximum normalization factor (5) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to BlowupBlowup\\operatorname{Blowup}roman_Blowup.444In the theory, Blowup has an additional 1+\u2026 in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B\u2062l\u2062o\u2062w\u2062u\u2062p=\u03c9\u2062(1)\ud835\udc35\ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc62\ud835\udc5d\ud835\udf141Blowup=\\omega(1)italic_B italic_l italic_o italic_w italic_u italic_p = italic_\u03c9 ( 1 ).Report issue for preceding element 3. Sharpness. In order to avoid committing to any specific \u03c1\ud835\udf0c\\rhoitalic_\u03c1, we sample \u0394\u0394\\Deltaroman_\u0394 in (15) not from the radius-\u03c1\ud835\udf0c\\rhoitalic_\u03c1-sphere, but from a mean-zero Gaussian with STD \u03c1=0.02\ud835\udf0c0.02\\rho=0.02italic_\u03c1 = 0.02. We estimate using Ns,psubscript\ud835\udc41\ud835\udc60\ud835\udc5dN_{s,p}italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and Ns,bsubscript\ud835\udc41\ud835\udc60\ud835\udc4fN_{s,b}italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x\ud835\udc65xitalic_x. This provides results equivalent to L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the \u03c1\u21920\u2192\ud835\udf0c0\\rho\\rightarrow 0italic_\u03c1 \u2192 0 asymptotic, while avoiding committing to any specific \u03c1\ud835\udf0c\\rhoitalic_\u03c1.Report issue for preceding element\nParameter Norm. We compute the L2 norm of the entire model\u2019s parameter vector, excluding positional encoding matrices. We discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable.Report issue for preceding element\nLayerNorm Blowup. This metric is computed by computing the maximum normalization factor (5) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to BlowupBlowup\\operatorname{Blowup}roman_Blowup.444In the theory, Blowup has an additional 1+\u2026 in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B\u2062l\u2062o\u2062w\u2062u\u2062p=\u03c9\u2062(1)\ud835\udc35\ud835\udc59\ud835\udc5c\ud835\udc64\ud835\udc62\ud835\udc5d\ud835\udf141Blowup=\\omega(1)italic_B italic_l italic_o italic_w italic_u italic_p = italic_\u03c9 ( 1 ).Report issue for preceding element\nSharpness. In order to avoid committing to any specific \u03c1\ud835\udf0c\\rhoitalic_\u03c1, we sample \u0394\u0394\\Deltaroman_\u0394 in (15) not from the radius-\u03c1\ud835\udf0c\\rhoitalic_\u03c1-sphere, but from a mean-zero Gaussian with STD \u03c1=0.02\ud835\udf0c0.02\\rho=0.02italic_\u03c1 = 0.02. We estimate using Ns,psubscript\ud835\udc41\ud835\udc60\ud835\udc5dN_{s,p}italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and Ns,bsubscript\ud835\udc41\ud835\udc60\ud835\udc4fN_{s,b}italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x\ud835\udc65xitalic_x. This provides results equivalent to L\u03c1,nsubscript\ud835\udc3f\ud835\udf0c\ud835\udc5bL_{\\rho,n}italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT in the \u03c1\u21920\u2192\ud835\udf0c0\\rho\\rightarrow 0italic_\u03c1 \u2192 0 asymptotic, while avoiding committing to any specific \u03c1\ud835\udf0c\\rhoitalic_\u03c1.Report issue for preceding element\nAs we are interested in properties of models that compute given functions, runs that did not converge (evaluation MSE higher than 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT) were discarded.Report issue for preceding element\nIn this experiment, we train transformers to fit f\u2208{PARITY,MAJORITY,FIRST,MEAN}\ud835\udc53PARITYMAJORITYFIRSTMEANf\\in\\{\\text{PARITY},\\text{MAJORITY},\\text{FIRST},\\text{MEAN}\\}italic_f \u2208 { PARITY , MAJORITY , FIRST , MEAN } on sequence lengths from 4 to 30. For each function and sequence length, we retrain the model 10 times from different random initializations.Report issue for preceding element\nFor PARITY, sharpness stably increases with the input length (Figure 1). For the other functions, whose sensitivity grows more slowly with n\ud835\udc5bnitalic_n, (a) the absolute value of sharpness is orders of magnitude lower than for PARITY; (b) there is little increase with n\ud835\udc5bnitalic_n. More results are shown in Appendix E.2.Report issue for preceding element\nAt any fixed input length n\ud835\udc5bnitalic_n, high sensitivity can be achieved by a combination of large weights and a large LayerNorm blowup. By Theorem 5, the product of C\ud835\udc36Citalic_C and squared blowup is bounded from below with some value Bn\u2062(f)subscript\ud835\udc35\ud835\udc5b\ud835\udc53B_{n}(f)italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ). Hence, the product of C\ud835\udc36\\sqrt{C}square-root start_ARG italic_C end_ARG and blowup is bounded with Bn\u2062(f)subscript\ud835\udc35\ud835\udc5b\ud835\udc53\\sqrt{B_{n}(f)}square-root start_ARG italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG, and the sum of 12\u2062log\u2061C12\ud835\udc36\\frac{1}{2}\\log Cdivide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_C and log\u2061BlowupBlowup\\log\\mathrm{Blowup}roman_log roman_Blowup is bounded with 12\u2062log\u2061Bn\u2062(f)12subscript\ud835\udc35\ud835\udc5b\ud835\udc53\\frac{1}{2}\\log{B_{n}(f)}divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ). However, C\ud835\udc36Citalic_C depends exponentially on parameters, and thus we expect the parameter norm to trade off with the logarithm of the layer norm blowup. Moreover, for PARITY the value of Bn\u2062(f)subscript\ud835\udc35\ud835\udc5b\ud835\udc53B_{n}(f)italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) increases with n\ud835\udc5bnitalic_n, and therefore sum of parameter norms and the logarithm of the blowup should also increase with n\ud835\udc5bnitalic_n.Report issue for preceding element\nTo test this prediction, for each function f\ud835\udc53fitalic_f we train a set of models with varying sequence lengths n\ud835\udc5bnitalic_n, weight decay and learning rate parameters. It allows us to obtain datapoints with diverse values of LN Blowup and parameter norm.Report issue for preceding element\nResults for PARITY can be seen in Figure 2, and for other functions in Figure 9. For all functions, there is a clear log Blowup-Parameter Norm tradeoff. For PARITY, the shape of the tradeoff indeed depends on n\ud835\udc5bnitalic_n, with transformers trained for high n\ud835\udc5bnitalic_n located above others in the log Blowup-Parameter Norm coordinates. For other functions, dependency on n\ud835\udc5bnitalic_n is not visible, at least at this range of n\ud835\udc5bnitalic_n.Report issue for preceding element\nAs discussed above, Theorem 6 predicts that transformers will generalize with low sensitivity, as low-sensitivity functions will tend to have flatter minima.Report issue for preceding element\nTo test this prediction, we created random functions f:={\u00b11}n\u2192{\u00b11}assign\ud835\udc53superscriptplus-or-minus1\ud835\udc5b\u2192plus-or-minus1f:=\\{\\pm 1\\}^{n}\\rightarrow\\{\\pm 1\\}italic_f := { \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 { \u00b1 1 }, and sampled random training sets from {\u00b11}nsuperscriptplus-or-minus1\ud835\udc5b\\{\\pm 1\\}^{n}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT of size 128/256/512. We fixed n=10\ud835\udc5b10n=10italic_n = 10. For each f\ud835\udc53fitalic_f, we train a transformer T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on the training set and use it to label the whole input space of sequences of length n\ud835\udc5bnitalic_n (1024 objects in our case). Replicating [CITATION], these extrapolated functions T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT have lower average sensitivity than the original functions f\ud835\udc53fitalic_f, indicating a low-sensitivity bias in generalization (Figure 4). Now, in order to directly compare the sharpness of minima corresponding to the true function f\ud835\udc53fitalic_f and the extrapolated function T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we trained new transformers to fit both functions on the entire dataset {\u00b11}nsuperscriptplus-or-minus1\ud835\udc5b\\{\\pm 1\\}^{n}{ \u00b1 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and measured the sharpness for these two new transformers. The results were averaged over 10 random functions f\ud835\udc53fitalic_f, 10 training sets per f\ud835\udc53fitalic_f and training set size, and 5 new transformers per each T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Sharpness was indeed lower for the transformer fitting the extrapolated functions than for the transformer matching the original random functions f\ud835\udc53fitalic_f. This also held when measuring sharpness only on the training set (Appendix, Figure 15).Report issue for preceding element\nBy Theorem 7, sensitivity of each autoregressive step when computing PARITY with scratchpad is O\u2062(1)\ud835\udc421O(1)italic_O ( 1 ). Hence, Theorem 6 provides no nontrivial lower bound for L\u03c1,n\u2062(T)subscript\ud835\udc3f\ud835\udf0c\ud835\udc5b\ud835\udc47L_{\\rho,n}(T)italic_L start_POSTSUBSCRIPT italic_\u03c1 , italic_n end_POSTSUBSCRIPT ( italic_T ). We trained an Encoder-Decoder Transformer, predicting PARITY of i\ud835\udc56iitalic_i-th substring on i\ud835\udc56iitalic_i-th autoregressive step: ti=PARITY\u2062(x1:i)=xi\u2295ti\u22121subscript\ud835\udc61\ud835\udc56PARITYsubscript\ud835\udc65:1\ud835\udc56direct-sumsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc61\ud835\udc561{t}_{i}=\\mathrm{PARITY}(x_{1:i})=x_{i}\\oplus{t}_{i-1}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_PARITY ( italic_x start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2295 italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT (t0=0subscript\ud835\udc6100{t}_{0}=0italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0). The visual dependency between sharpness and length of input for PARITY with a scratchpad is shown in Figure 11. Even for length around 300, sharpness is low and there is little increase with input length. Thus, decrease in sensitivity due to the scratchpad can explain why prior work [CITATION] found that PARITY is easy for Transformers with scratchpad.Report issue for preceding element\nFigure 3 represents the evolution of loss and sharpness of Transformer models trained for PARITY with input length 25. The results are averaged across 39 converged runs. Similar curves for parameter norm and LayerNorm blowup are presented in Appendix 12.Report issue for preceding element\nA dramatic increase in sharpness occurs at exactly the time when loss falls to 0. This suggests the presence of a steep minimum in the loss landscape, and fitting PARITY requires the optimization procedure to find this minimum. Figure 12 (Appendix) shows further details of this process. During learning, there is a small but sharp increase in the blowup which makes the high sensitivity of the model possible, increasing the left-hand side of the inequality in Corollary 5. Following that, non-zero weight decay drives the parameter norm down, which \u2013 by the theoretically predicted tradeoff between blowup and parameter norm \u2013 is accompanied by an exponential increase in blowup.Report issue for preceding element",
    "citations": [
      {
        "tag": "Loshchilov and Hutter (2017)",
        "title": "Decoupled\nweight decay regularization.",
        "authors": "Ilya Loshchilov and Frank Hutter. 2017.",
        "journal": "InInternational Conference on Learning Representations."
      },
      {
        "tag": "Paszke et\u00a0al. (2019)",
        "title": "Pytorch: An imperative style, high-performance deep learning library.",
        "authors": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\nDesmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu\u00a0Fang, Junjie Bai, and Soumith\nChintala. 2019.",
        "journal": "InAdvances in Neural Information Processing Systems,\nvolume\u00a032. Curran Associates, Inc."
      },
      {
        "tag": "Bhattamishra et\u00a0al. (2023)",
        "title": "Simplicity\nbias in transformers and their ability to learn sparse boolean functions.",
        "authors": "Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. 2023.",
        "journal": "InProceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 5767\u20135791. Association for Computational\nLinguistics."
      },
      {
        "tag": "Anil et\u00a0al. (2022)",
        "title": "Exploring length generalization in large language models.",
        "authors": "Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay\nRamasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.",
        "journal": "Advances in Neural Information Processing Systems,\n35:38546\u201338556."
      }
    ]
  },
  "S8": {
    "title": "8Discussion",
    "text": "We have provided a rigorous theoretical explanation of the inductive bias towards low sensitivity observed in empirical research on transformers.\nTheorem 6 describes a fundamental inductive bias of the transformer architecture: for long inputs, fitting sensitive functions is only possible in sharp minima of the loss.\nThis holds without assumptions often made in previous work about expressive capacity, such as non-infinite precision (e.g. Merrill and Sabharwal, 2023b ; Angluin et\u00a0al., 2023 ) , hard attention (e.g. Hahn, 2020 ) , or Lipschitz-continuous variants of layer norm (Edelman et\u00a0al., 2022 ) .\nWe speculate that Theorem 6 reflects a fundamental limitation of parallelized differentiable computing with bounded depth.\nOur results show that it is overcome by scaling the number of computation steps with the input length.\nWhile we focused on functions outputting a single label; an interesting direction for future research is the extension of this theory to sequence-to-sequence transductions, for which some empirical data has been reported (e.g. Zhou et\u00a0al., 2023 ) , but theoretical understanding remains wide open. Report issue for preceding element\nDue to the relation between average sensitivity and Fourier analysis on the Boolean cube (Equation 17 ), a low-sensitivity bias can be viewed as a sequence modeling analogue of a spectral bias towards low frequencies observed in other neural architectures (e.g. Rahaman et\u00a0al., 2019 ; Fridovich-Keil et\u00a0al., 2022 ) . Report issue for preceding element\nWe note that, while our results show that sensitive transformers are very brittle, these results do not by themselves have implications for real-world generalization , as a low-sensitivity bias need not always be beneficial in real-world setups. Indeed, the relationship between sharpness and real-world generalization is not straightforward (e.g. Andriushchenko et\u00a0al., 2023 ; Kaur et\u00a0al., 2023 ) . Our theory suggests that transformers generalize well to the extent that real-world data has bounded sensitivity (e.g. Hahn et\u00a0al., 2021 ) . Report issue for preceding element",
    "masked_text": "We have provided a rigorous theoretical explanation of the inductive bias towards low sensitivity observed in empirical research on transformers. Theorem 6 describes a fundamental inductive bias of the transformer architecture: for long inputs, fitting sensitive functions is only possible in sharp minima of the loss. This holds without assumptions often made in previous work about expressive capacity, such as non-infinite precision [CITATION], hard attention [CITATION], or Lipschitz-continuous variants of layer norm [CITATION]. We speculate that Theorem 6 reflects a fundamental limitation of parallelized differentiable computing with bounded depth. Our results show that it is overcome by scaling the number of computation steps with the input length. While we focused on functions outputting a single label; an interesting direction for future research is the extension of this theory to sequence-to-sequence transductions, for which some empirical data has been reported [CITATION], but theoretical understanding remains wide open.Report issue for preceding element\nDue to the relation between average sensitivity and Fourier analysis on the Boolean cube (Equation 17), a low-sensitivity bias can be viewed as a sequence modeling analogue of a spectral bias towards low frequencies observed in other neural architectures [CITATION].Report issue for preceding element\nWe note that, while our results show that sensitive transformers are very brittle, these results do not by themselves have implications for real-world generalization, as a low-sensitivity bias need not always be beneficial in real-world setups. Indeed, the relationship between sharpness and real-world generalization is not straightforward [CITATION]. Our theory suggests that transformers generalize well to the extent that real-world data has bounded sensitivity [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Kaur et\u00a0al. (2023)",
        "title": "On the maximum hessian eigenvalue and generalization.",
        "authors": "Simran Kaur, Jeremy Cohen, and Zachary\u00a0Chase Lipton. 2023.",
        "journal": "InProceedings on, pages 51\u201365. PMLR."
      },
      {
        "tag": "Zhou et\u00a0al. (2023)",
        "title": "What algorithms can transformers learn? a study in length\ngeneralization.",
        "authors": "Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh\nSusskind, Samy Bengio, and Preetum Nakkiran. 2023.",
        "journal": "arXiv preprint arXiv:2310.16028."
      },
      {
        "tag": "Hahn et\u00a0al. (2021)",
        "title": "Sensitivity as a complexity measure for sequence classification tasks.",
        "authors": "Michael Hahn, Dan Jurafsky, and Richard Futrell. 2021.",
        "journal": "Transactions of the Association for Computational Linguistics,\n9:891\u2013908."
      },
      {
        "tag": "Rahaman et\u00a0al. (2019)",
        "title": "On the\nspectral bias of neural networks.",
        "authors": "Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred\nHamprecht, Yoshua Bengio, and Aaron Courville. 2019.",
        "journal": "InProceedings of the 36th International Conference on Machine\nLearning, volume\u00a097 ofProceedings of Machine Learning Research,\npages 5301\u20135310. PMLR."
      },
      {
        "tag": "Fridovich-Keil et\u00a0al. (2022)",
        "title": "Spectral bias in practice: The role of function frequency in\ngeneralization.",
        "authors": "Sara Fridovich-Keil, Raphael Gontijo\u00a0Lopes, and Rebecca Roelofs. 2022.",
        "journal": "InAdvances in Neural Information Processing Systems,\nvolume\u00a035, pages 7368\u20137382. Curran Associates, Inc."
      },
      {
        "tag": "Andriushchenko et\u00a0al. (2023)",
        "title": "A\nmodern look at the relationship between sharpness and generalization.",
        "authors": "Maksym Andriushchenko, Francesco Croce, Maximilian M\u00fcller, Matthias Hein,\nand Nicolas Flammarion. 2023.",
        "journal": "InInternational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\nMachine Learning Research, pages 840\u2013902. PMLR."
      },
      {
        "tag": "Angluin et\u00a0al. (2023)",
        "title": "Masked hard-attention transformers and boolean rasp recognize exactly\nthe star-free languages.",
        "authors": "Dana Angluin, David Chiang, and Andy Yang. 2023.",
        "journal": "arXiv preprint arXiv:2310.13897."
      },
      {
        "tag": "Hahn (2020)",
        "title": "Theoretical limitations of self-attention in neural sequence models.",
        "authors": "Michael Hahn. 2020.",
        "journal": "Transactions of the Association for Computational Linguistics,\n8:156\u2013171."
      },
      {
        "tag": "Merrill and Sabharwal (2023b)",
        "title": "A logic for expressing log-precision transformers.",
        "authors": "William Merrill and Ashish Sabharwal. 2023b.",
        "journal": "InThirty-seventh Conference on Neural Information Processing\nSystems."
      },
      {
        "tag": "Edelman et\u00a0al. (2022)",
        "title": "Inductive biases and variable creation in self-attention mechanisms.",
        "authors": "Benjamin\u00a0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages\n5793\u20135831. PMLR."
      }
    ]
  },
  "S9": {
    "title": "9Conclusion",
    "text": "We have proven that, under the transformer architecture, high sensitivity in input space can only be achieved in very sharp minima.\nEmpirical results confirm the predictions of the theory.\nTaken together, our results explain a diverse set of empirical observations about transformers not explained by previous theoretical work.\nThey suggest shifting theoretical research from in-principle expressiveness considerations to studying quantitative bounds and the shape of the loss landscape in order to understand the abilities of transformers. Report issue for preceding element",
    "masked_text": "We have proven that, under the transformer architecture, high sensitivity in input space can only be achieved in very sharp minima. Empirical results confirm the predictions of the theory. Taken together, our results explain a diverse set of empirical observations about transformers not explained by previous theoretical work. They suggest shifting theoretical research from in-principle expressiveness considerations to studying quantitative bounds and the shape of the loss landscape in order to understand the abilities of transformers.Report issue for preceding element",
    "citations": []
  },
  "Sx1": {
    "title": "Limitations",
    "text": "A limitation of our results is that the theoretical results are asymptotic, providing statements about the limit of very long input sequences. Providing more quantitative bounds that tightly characterize finite-length behavior is an interesting problem for future research. Report issue for preceding element\nA second limitation is that we only target functions outputting a single value. Operationalizing sensitivity-like metrics for sequence-to-sequence functions may be required in order to expand the theory to such functions. Report issue for preceding element\nThird, our results apply to transformer encoders. It remains open if transformer decoders, with causal attention masking, face a different set of limitations than we have shown here in the absence of masking. Report issue for preceding element\nFourth, our theoretical results concern the loss landscape, not the training dynamics itself. Further technical advances may be needed to directly prove corresponding results for training dynamics. Report issue for preceding element",
    "masked_text": "A limitation of our results is that the theoretical results are asymptotic, providing statements about the limit of very long input sequences. Providing more quantitative bounds that tightly characterize finite-length behavior is an interesting problem for future research.Report issue for preceding element\nA second limitation is that we only target functions outputting a single value. Operationalizing sensitivity-like metrics for sequence-to-sequence functions may be required in order to expand the theory to such functions.Report issue for preceding element\nThird, our results apply to transformer encoders. It remains open if transformer decoders, with causal attention masking, face a different set of limitations than we have shown here in the absence of masking.Report issue for preceding element\nFourth, our theoretical results concern the loss landscape, not the training dynamics itself. Further technical advances may be needed to directly prove corresponding results for training dynamics.Report issue for preceding element",
    "citations": []
  },
  "Sx2": {
    "title": "Acknowledgments",
    "text": "We thank Lena Strobl, Dana Angluin, and David Chiang for useful discussion.\nWe thank David Chiang, Paul Lintilhac, and Yuval Pinter for spotting various errors in a previous version, and the anonymous ARR reviewers for useful feedback. Report issue for preceding element",
    "masked_text": "We thank Lena Strobl, Dana Angluin, and David Chiang for useful discussion. We thank David Chiang, Paul Lintilhac, and Yuval Pinter for spotting various errors in a previous version, and the anonymous ARR reviewers for useful feedback.Report issue for preceding element",
    "citations": []
  }
}