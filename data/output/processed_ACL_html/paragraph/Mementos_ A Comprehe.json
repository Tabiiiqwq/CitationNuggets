{
  "S1.p1": {
    "text": "The recent emergence of Multimodal Large Language Models (MLLMs) such as GPT-4V OpenAI ( 2023b ) and Gemini Team ( 2023 ) has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering.\nDespite the notable performance of existing MLLMs, they often suffer from hallucination (a phenomenon where MLLMs produce inaccurate descriptions of the given images) due to insufficient reasoning capabilities, generating inaccurate responses in visual inference Liu et\u00a0al. ( 2023a ); Yue et\u00a0al. ( 2023 ) .\nThus, monitoring the reasoning capability is of great importance in understanding the ability and the limitations of MLLMs and applying MLLMs in the real world.\nPrevious benchmarks, such as those in Liu et\u00a0al. ( 2023a ) and Yue et\u00a0al. ( 2023 ) , have primarily addressed evaluating reasoning in each individual image, relying on static and object-centric knowledge.\nHowever, they are insufficient to comprehensively assess the reasoning capabilities of MLLMs due to a lack of time-varying object behaviors or events. Report issue for preceding element",
    "masked_text": "The recent emergence of Multimodal Large Language Models (MLLMs) such as GPT-4V [CITATION] and Gemini [CITATION] has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering. Despite the notable performance of existing MLLMs, they often suffer from hallucination (a phenomenon where MLLMs produce inaccurate descriptions of the given images) due to insufficient reasoning capabilities, generating inaccurate responses in visual inference [CITATION]. Thus, monitoring the reasoning capability is of great importance in understanding the ability and the limitations of MLLMs and applying MLLMs in the real world. Previous benchmarks, such as those in [CITATION] and [CITATION], have primarily addressed evaluating reasoning in each individual image, relying on static and object-centric knowledge. However, they are insufficient to comprehensively assess the reasoning capabilities of MLLMs due to a lack of time-varying object behaviors or events.Report issue for preceding element",
    "citations": [
      {
        "tag": "Team (2023)",
        "title": "Gemini: A family of highly capable multimodal models.",
        "authors": "Gemini Team. 2023.",
        "journal": ""
      },
      {
        "tag": "Yue et\u00a0al. (2023)",
        "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.",
        "authors": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge\u00a0Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et\u00a0al. 2023.",
        "journal": "arXiv preprint arXiv:2311.16502."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.",
        "authors": "Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a.",
        "journal": "arXiv preprint arXiv:2310.14566."
      },
      {
        "tag": "OpenAI (2023b)",
        "title": "Gpt-4v(ision) system card.",
        "authors": "OpenAI. 2023b.",
        "journal": ""
      }
    ]
  },
  "S1.p2": {
    "text": "To investigate the capabilities of Multi-Modal Language Models (MLLMs) in dynamic reasoning across image sequences, we present a new benchmark, Mementos .\nThis benchmark focuses on the complex task of monitoring and deciphering the positional changes of objects within an image sequence, followed by the inference of behavioral patterns and logical connections among them.\nSuch an endeavor requires the interpretation of the overarching context based on time-variant visual elements, posing a greater challenge than the analysis of static scenes. Concretely, Mementos consists of 4,761 image sequences with varying episode lengths 1 1 1 Here, an episode refers to a specific event or series of events depicted in the image sequence. , encompassing diverse scenarios from everyday life, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated description of the primary objects and their behaviors within the sequence. Report issue for preceding element",
    "masked_text": "To investigate the capabilities of Multi-Modal Language Models (MLLMs) in dynamic reasoning across image sequences, we present a new benchmark, Mementos. This benchmark focuses on the complex task of monitoring and deciphering the positional changes of objects within an image sequence, followed by the inference of behavioral patterns and logical connections among them. Such an endeavor requires the interpretation of the overarching context based on time-variant visual elements, posing a greater challenge than the analysis of static scenes. Concretely, Mementos consists of 4,761 image sequences with varying episode lengths111Here, an episode refers to a specific event or series of events depicted in the image sequence., encompassing diverse scenarios from everyday life, robotics tasks, and comic-style storyboards. Each sequence is paired with a human-annotated description of the primary objects and their behaviors within the sequence.Report issue for preceding element",
    "citations": []
  },
  "S1.p3": {
    "text": "To assess the reasoning capability of MLLMs on Mementos, we employ a GPT-4 -assisted evaluation procedure:\nafter an MLLM produces a description for an image sequence,\nwe extract behavior and object keywords from both AI-generated and human-annotated descriptions using GPT-4. We then use keyword matching to assess the degree of behavioral and object hallucinations.\nTo refine the correctness of this evaluation, we have developed behavior and object synonym graphs for each domain. These graphs facilitate more precise keyword matching, ensuring a thorough and nuanced analysis of the MLLMs\u2019 reasoning abilities.\nBesides, we also provide the comparison with human evaluation to demonstrates that the GPT-4-assisted evaluation procedure is very reliable. Report issue for preceding element",
    "masked_text": "To assess the reasoning capability of MLLMs on Mementos, we employ a GPT-4-assisted evaluation procedure: after an MLLM produces a description for an image sequence, we extract behavior and object keywords from both AI-generated and human-annotated descriptions using GPT-4. We then use keyword matching to assess the degree of behavioral and object hallucinations. To refine the correctness of this evaluation, we have developed behavior and object synonym graphs for each domain. These graphs facilitate more precise keyword matching, ensuring a thorough and nuanced analysis of the MLLMs\u2019 reasoning abilities. Besides, we also provide the comparison with human evaluation to demonstrates that the GPT-4-assisted evaluation procedure is very reliable.Report issue for preceding element",
    "citations": []
  },
  "S1.p4": {
    "text": "We evaluated the reasoning proficiency of nine leading-edge MLLMs on Mementos, encompassing both black-box and open-source models.\nOur findings indicate that Mementos poses a considerable challenge to these current MLLMs.\nFor instance, as depicted in Figure 1 , GPT-4V exhibits notable behavioral and object hallucinations in various domains during image sequence reasoning.\nBehavioral hallucinations are defined as the MLLMs\u2019 erroneous interpretations or predictions of entity actions, while object hallucinations pertain to the inaccurate identification or creation of objects within the image sequences.\nNotably, behavioral hallucinations were more frequent than object hallucinations, highlighting a significant deficiency in MLLMs\u2019 capability to deduce events from image sequences. Report issue for preceding element",
    "masked_text": "We evaluated the reasoning proficiency of nine leading-edge MLLMs on Mementos, encompassing both black-box and open-source models. Our findings indicate that Mementos poses a considerable challenge to these current MLLMs. For instance, as depicted in Figure 1, GPT-4V exhibits notable behavioral and object hallucinations in various domains during image sequence reasoning. Behavioral hallucinations are defined as the MLLMs\u2019 erroneous interpretations or predictions of entity actions, while object hallucinations pertain to the inaccurate identification or creation of objects within the image sequences. Notably, behavioral hallucinations were more frequent than object hallucinations, highlighting a significant deficiency in MLLMs\u2019 capability to deduce events from image sequences.Report issue for preceding element",
    "citations": []
  },
  "S1.p5": {
    "text": "Furthermore, our research pinpoints three principal factors that lead to the reasoning failures of MLLMs: (1) the interconnectedness of object and behavioral hallucinations, (2) the impact of co-occurring behaviors, and (3) the cumulative effect of behavioral hallucinations. The objective of our proposed benchmark and analyses is to shed light on innovative approaches to augment the reasoning abilities of MLLMs and to reduce hallucinations in their subsequent advancements. Report issue for preceding element",
    "masked_text": "Furthermore, our research pinpoints three principal factors that lead to the reasoning failures of MLLMs: (1) the interconnectedness of object and behavioral hallucinations, (2) the impact of co-occurring behaviors, and (3) the cumulative effect of behavioral hallucinations. The objective of our proposed benchmark and analyses is to shed light on innovative approaches to augment the reasoning abilities of MLLMs and to reduce hallucinations in their subsequent advancements.Report issue for preceding element",
    "citations": []
  },
  "S2.p1": {
    "text": "In this section, we introduce Mementos, a novel and challenging benchmark designed to test the reasoning capability of Multimodal Large Language Model (MLLM) under sequential image input.\nInitially, we detail the data gathering and annotation methodology for Mementos, alongside an overview of its data distribution.\nSubsequently, we outline the evaluation protocol and the metric employed to assess the reasoning prowess of MLLMs in Mementos. Report issue for preceding element",
    "masked_text": "In this section, we introduce Mementos, a novel and challenging benchmark designed to test the reasoning capability of Multimodal Large Language Model (MLLM) under sequential image input. Initially, we detail the data gathering and annotation methodology for Mementos, alongside an overview of its data distribution. Subsequently, we outline the evaluation protocol and the metric employed to assess the reasoning prowess of MLLMs in Mementos.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS1.p1": {
    "text": "Mementos comprises 4,761 image sequences of varying lengths, predominantly sourced from Daily-life, Robotics, and Comics domains. Detailed statistics are provided in Table 1 . This diverse collection is instrumental in evaluating the comprehensive time-varying reasoning abilities of MLLMs. Specifically, the robotics data, closely associated with embodied AI or real-world contexts, and the comic-style storyboard data, rich in stylistic and episodic diversity in image sequences, significantly enhance the benchmark\u2019s relevance and robustness. Report issue for preceding element",
    "masked_text": "Mementos comprises 4,761 image sequences of varying lengths, predominantly sourced from Daily-life, Robotics, and Comics domains. Detailed statistics are provided in Table 1. This diverse collection is instrumental in evaluating the comprehensive time-varying reasoning abilities of MLLMs. Specifically, the robotics data, closely associated with embodied AI or real-world contexts, and the comic-style storyboard data, rich in stylistic and episodic diversity in image sequences, significantly enhance the benchmark\u2019s relevance and robustness.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS1.Px1.p1": {
    "text": "The Daily-life image sequences in Mementos are derived from video clips in the Next-QA dataset, as cited in Xiao et\u00a0al. ( 2021 ) . These sequences represent a range of everyday life scenarios. We have selectively extracted videos from the Next-QA Training set, specifically those with frame counts ranging from 400 to 2,500.\nTo balance the challenge of testing MLLMs\u2019 reasoning capabilities against the risk of losing critical information, our methodology involves retaining the first frame of each video. Subsequently, we sample one image every 100 frames. The collected images from this sampling process then form an image sequence that corresponds to the original video. This approach ensures a rigorous yet feasible evaluation of MLLMs\u2019 reasoning abilities in dynamically evolving everyday scenarios. Report issue for preceding element",
    "masked_text": "The Daily-life image sequences in Mementos are derived from video clips in the Next-QA dataset, as cited in [CITATION]. These sequences represent a range of everyday life scenarios. We have selectively extracted videos from the Next-QA Training set, specifically those with frame counts ranging from 400 to 2,500. To balance the challenge of testing MLLMs\u2019 reasoning capabilities against the risk of losing critical information, our methodology involves retaining the first frame of each video. Subsequently, we sample one image every 100 frames. The collected images from this sampling process then form an image sequence that corresponds to the original video. This approach ensures a rigorous yet feasible evaluation of MLLMs\u2019 reasoning abilities in dynamically evolving everyday scenarios.Report issue for preceding element",
    "citations": [
      {
        "tag": "Xiao et\u00a0al. (2021)",
        "title": "Next-qa: Next phase of question-answering to explaining temporal actions.",
        "authors": "Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9777\u20139786."
      }
    ]
  },
  "S2.SS1.SSS1.Px2.p1": {
    "text": "For the Robotics data, we utilized videos from various sub-datasets within Open X-Embodiment Collaboration et\u00a0al. ( 2023 ) .\nOpen X-Embodiment aggregates video datasets from multiple university laboratories, showcasing a variety of tasks performed by different robotic systems.\nWe meticulously selected sub-datasets from Open X-Embodiment that offer video resolutions exceeding 128x128 and exhibit a high degree of task diversity. From these chosen sub-datasets, a total of 1,101 videos were sampled. The precise number of videos sourced from each sub-dataset is detailed in Appendix A .\nFor video sampling, our approach varied based on the length of the videos. Videos exceeding 100 frames were processed by sampling one image every n / 20 \ud835\udc5b 20 n/20 italic_n / 20 frames, where n \ud835\udc5b n italic_n represents the total frame count of the video. Conversely, for videos with frame counts ranging from 20 to 100, we sampled one image every 5 frames. This ensures the formation of comprehensive and representative image sequences for each video, catering to the evaluation of MLLMs in diverse and complex robotic contexts. Report issue for preceding element",
    "masked_text": "For the Robotics data, we utilized videos from various sub-datasets within Open X-Embodiment [CITATION]. Open X-Embodiment aggregates video datasets from multiple university laboratories, showcasing a variety of tasks performed by different robotic systems. We meticulously selected sub-datasets from Open X-Embodiment that offer video resolutions exceeding 128x128 and exhibit a high degree of task diversity. From these chosen sub-datasets, a total of 1,101 videos were sampled. The precise number of videos sourced from each sub-dataset is detailed in Appendix A. For video sampling, our approach varied based on the length of the videos. Videos exceeding 100 frames were processed by sampling one image every n/20\ud835\udc5b20n/20italic_n / 20 frames, where n\ud835\udc5bnitalic_n represents the total frame count of the video. Conversely, for videos with frame counts ranging from 20 to 100, we sampled one image every 5 frames. This ensures the formation of comprehensive and representative image sequences for each video, catering to the evaluation of MLLMs in diverse and complex robotic contexts.Report issue for preceding element",
    "citations": [
      {
        "tag": "Collaboration et\u00a0al. (2023)",
        "title": "Open X-Embodiment: Robotic learning datasets and RT-X models.",
        "authors": "Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav\u00a0S. Sukhatme, Gautam Salhotra, Ge\u00a0Yan, Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, Heni\u00a0Ben Amor, Henrik\u00a0I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph\u00a0J. Lim, Jo\u00e3o\nSilv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Srinivasan, Lawrence\u00a0Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan\u00a0Kumar Srirama, Mohit Sharma, Moo\u00a0Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil\u00a0J Joshi, Niko Suenderhauf, Norman\u00a0Di Palo, Nur Muhammad\u00a0Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag\u00a0R Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart\u00edn-Mart\u00edn, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari,\nSuneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony\u00a0Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi\u00a0Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen\u00a0Jeff Cui. 2023.",
        "journal": "https://arxiv.org/abs/2310.08864."
      }
    ]
  },
  "S2.SS1.SSS1.Px3.p1": {
    "text": "The Comics data is composed of wordless multi-panel comics of diverse styles, curated from online sources.\nUnlike Daily-life and Robotics sections, where image sequences are uniformly extracted from video frames, the comics represent intentionally selected key moments within a narrative, manually illustrated by artists.\nThis distinction sets our dataset apart from conventional video datasets.\nIn addition to traditional comics, this category also incorporates 20 storyboards from movies reimagined in comic style.\nWe have further deconstructed these comics into individual image sequences by taking screenshots.\nThis approach enables a unique exploration of sequential visual reasoning, enhancing the diversity and complexity of the dataset for evaluating MLLMs. Report issue for preceding element",
    "masked_text": "The Comics data is composed of wordless multi-panel comics of diverse styles, curated from online sources. Unlike Daily-life and Robotics sections, where image sequences are uniformly extracted from video frames, the comics represent intentionally selected key moments within a narrative, manually illustrated by artists. This distinction sets our dataset apart from conventional video datasets. In addition to traditional comics, this category also incorporates 20 storyboards from movies reimagined in comic style. We have further deconstructed these comics into individual image sequences by taking screenshots. This approach enables a unique exploration of sequential visual reasoning, enhancing the diversity and complexity of the dataset for evaluating MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS2.p1": {
    "text": "For each image sequence in Mementos, we have meticulously annotated a ground truth description that captures the unfolding events.\nThese descriptions focus on the primary objects and their respective behaviors, where behavior refers to a verb or verb phrase associated with the object in question. Report issue for preceding element",
    "masked_text": "For each image sequence in Mementos, we have meticulously annotated a ground truth description that captures the unfolding events. These descriptions focus on the primary objects and their respective behaviors, where behavior refers to a verb or verb phrase associated with the object in question.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS2.p2": {
    "text": "For the Daily-life data, we initially employed GPT-4V(ision) OpenAI ( 2023a ) , to amalgamate and reformulate the questions and answers from the Next-QA videos into single paragraph descriptions.\nThis method significantly expedited the manual annotation process. Following this, we conducted a thorough manual review of these automated descriptions, making necessary adjustments.\nThis included rectifying inaccuracies, removing non-existent episodes, and adding missing details to ensure alignment with the actual image sequences.\nTo ensure reliability, we implemented a cross-validation step, where a separate set of annotators performed a secondary review.\nFor the Robotics and Comics categories, the annotation process was entirely manual, conducted by human annotators. These annotations were then subjected to a verification process by the authors which ensures the accuracy and consistency of the descriptions across all categories. Report issue for preceding element",
    "masked_text": "For the Daily-life data, we initially employed GPT-4V(ision) [CITATION], to amalgamate and reformulate the questions and answers from the Next-QA videos into single paragraph descriptions. This method significantly expedited the manual annotation process. Following this, we conducted a thorough manual review of these automated descriptions, making necessary adjustments. This included rectifying inaccuracies, removing non-existent episodes, and adding missing details to ensure alignment with the actual image sequences. To ensure reliability, we implemented a cross-validation step, where a separate set of annotators performed a secondary review. For the Robotics and Comics categories, the annotation process was entirely manual, conducted by human annotators. These annotations were then subjected to a verification process by the authors which ensures the accuracy and consistency of the descriptions across all categories. Report issue for preceding element",
    "citations": [
      {
        "tag": "OpenAI (2023a)",
        "title": "Gpt-4 technical report.",
        "authors": "OpenAI. 2023a.",
        "journal": ""
      }
    ]
  },
  "S2.SS1.SSS3.p1": {
    "text": "In showcasing the extensive diversity of Mementos, we present a detailed overview of the data distribution within the Mementos validation set.\nOur analysis focuses on two key dimensions: the length of the image sequence and the length of the episode.\nThe length of an image sequence is defined by the number of frames it contains, while the episode length is determined by the total number of events depicted in the sequence.\nA longer image sequence necessitates the MLLM to process a larger number of images, thereby challenging the model\u2019s capacity to manage sequences spanning broader timeframes.\nA greater episode length signifies that the image sequence encompasses more intricate scenarios. Report issue for preceding element",
    "masked_text": "In showcasing the extensive diversity of Mementos, we present a detailed overview of the data distribution within the Mementos validation set. Our analysis focuses on two key dimensions: the length of the image sequence and the length of the episode. The length of an image sequence is defined by the number of frames it contains, while the episode length is determined by the total number of events depicted in the sequence. A longer image sequence necessitates the MLLM to process a larger number of images, thereby challenging the model\u2019s capacity to manage sequences spanning broader timeframes. A greater episode length signifies that the image sequence encompasses more intricate scenarios. Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS3.Px1.p1": {
    "text": "For the image sequence length, we count the number of frames in each image sequence. As shown in Figure 2 ,\nthe majority of image sequences are between 4 and 14 frames in length.\n67.38% of image sequences contain 4 to 14 frames, yet 31.90% of sequences are composed of longer frames - more than 15 frames. Report issue for preceding element",
    "masked_text": "For the image sequence length, we count the number of frames in each image sequence. As shown in Figure 2, the majority of image sequences are between 4 and 14 frames in length. 67.38% of image sequences contain 4 to 14 frames, yet 31.90% of sequences are composed of longer frames - more than 15 frames.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.SSS3.Px2.p1": {
    "text": "To quantify the episode length within each image sequence of Mementos, we employed GPT-4 for extracting behavior keywords, specifically verbs associated with objects, from the human-annotated descriptions. This extraction was facilitated using a pre-defined manual prompt, details of which can be found in Appendix C . Following the extraction, we calculated the length of the behavior list for each image sequence.\nA lengthier behavior list signifies a more extended episode within the image sequence, which inherently poses a greater challenge for the MLLM in comprehending the entire image sequence. As illustrated in Figure 3 , a significant portion of the image sequences, particularly those from the robotics data, feature episode lengths ranging between 1 and 3. This is mainly attributed to the dominance of two-action episodes like \u2018pick up and place\u2019, \u2018move and pull open\u2019, \u2018locate and push\u2019. Meanwhile, the remaining data exhibits a normal distribution for episode lengths spanning 4 to 17. Report issue for preceding element",
    "masked_text": "To quantify the episode length within each image sequence of Mementos, we employed GPT-4 for extracting behavior keywords, specifically verbs associated with objects, from the human-annotated descriptions. This extraction was facilitated using a pre-defined manual prompt, details of which can be found in Appendix C. Following the extraction, we calculated the length of the behavior list for each image sequence. A lengthier behavior list signifies a more extended episode within the image sequence, which inherently poses a greater challenge for the MLLM in comprehending the entire image sequence. As illustrated in Figure 3, a significant portion of the image sequences, particularly those from the robotics data, feature episode lengths ranging between 1 and 3. This is mainly attributed to the dominance of two-action episodes like \u2018pick up and place\u2019, \u2018move and pull open\u2019, \u2018locate and push\u2019. Meanwhile, the remaining data exhibits a normal distribution for episode lengths spanning 4 to 17.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.p1": {
    "text": "In this section, we illustrate how to evaluate the descriptions generated by MLLMs, including the evaluation procedure and metrics. Report issue for preceding element",
    "masked_text": "In this section, we illustrate how to evaluate the descriptions generated by MLLMs, including the evaluation procedure and metrics.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.SSS0.Px1.p1": {
    "text": "As shown in Figure 4 , we use an image sequence and a pre-designed prompt together as the input for MLLMs, and generate the description aligned with the corresponding image sequence.\nNext,\nwe ask GPT-4 to extract object and behavior keywords using the AI-generated description.\nWe then match the obtained keywords with the synonym graph we built, replacing the matched keywords with the root word from the synonym graph.\nFinally, we obtain two lists of keywords: AI-generated object list and AI-generated behavior list.\nWe note that the proposed keyword extraction leveraging GPT-4 is surprisingly reliable and accurate, which is competitive with human extraction.\nPlease refer to Appendix B for a detailed comparison. Report issue for preceding element",
    "masked_text": "As shown in Figure 4, we use an image sequence and a pre-designed prompt together as the input for MLLMs, and generate the description aligned with the corresponding image sequence. Next, we ask GPT-4 to extract object and behavior keywords using the AI-generated description. We then match the obtained keywords with the synonym graph we built, replacing the matched keywords with the root word from the synonym graph. Finally, we obtain two lists of keywords: AI-generated object list and AI-generated behavior list. We note that the proposed keyword extraction leveraging GPT-4 is surprisingly reliable and accurate, which is competitive with human extraction. Please refer to Appendix B for a detailed comparison.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.SSS0.Px2.p1": {
    "text": "The synonym graph is an unilateral digraph where each edge connects two nodes representing words or phrases. For instance, given a synonym pair (pick up, lift up), an edge would be directed from \u2018lift up\u2019 to \u2018pick up\u2019.\nIn each synonym pair, the first word, originating from the human-annotated keyword list, is referred to as the root word, while the second word comes from the AI-generated keyword list.\nTo construct this synonym graph, we first use GPT-4 to extract object and behavior keywords from all human-annotated descriptions in the Val set, forming a human-annotated keyword list.\nThen, we generate descriptions on the Val set using GPT-4V, LLAVA, and Gemini and use GPT-4 to extract object and behavior keywords.\nAfter that,\nwe manually match these words with the human-annotated keyword list to identify all synonym pairs and add them as edges to the synonym graph.\nGiven a word or phrase, this synonym graph can quickly match the corresponding root word if a synonym exists in the human-annotated keyword list, completing the keyword replacement.\nFor convenience in evaluation, we maintain separate synonym graphs for objects and behaviors of different categories.\nWe make all constructed synonym graphs publicly available as open-source resources. Report issue for preceding element",
    "masked_text": "The synonym graph is an unilateral digraph where each edge connects two nodes representing words or phrases. For instance, given a synonym pair (pick up, lift up), an edge would be directed from \u2018lift up\u2019 to \u2018pick up\u2019. In each synonym pair, the first word, originating from the human-annotated keyword list, is referred to as the root word, while the second word comes from the AI-generated keyword list. To construct this synonym graph, we first use GPT-4 to extract object and behavior keywords from all human-annotated descriptions in the Val set, forming a human-annotated keyword list. Then, we generate descriptions on the Val set using GPT-4V, LLAVA, and Gemini and use GPT-4 to extract object and behavior keywords. After that, we manually match these words with the human-annotated keyword list to identify all synonym pairs and add them as edges to the synonym graph. Given a word or phrase, this synonym graph can quickly match the corresponding root word if a synonym exists in the human-annotated keyword list, completing the keyword replacement. For convenience in evaluation, we maintain separate synonym graphs for objects and behaviors of different categories. We make all constructed synonym graphs publicly available as open-source resources.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.SSS0.Px3.p1": {
    "text": "After obtaining the AI-generated object list and AI-generated behavior list for each image sequence, we utilize the corresponding human-annotated object list and human-annotated behavior list as the ground truth to calculate \u2018Recall,\u2019 \u2018Precision,\u2019 and \u2018F1 metrics\u2019 at both the object and behavior levels.\nThese metrics are used to measure the understanding capabilities of different MLLMs regarding the image sequence episode.\n\u2018Recall\u2019 reflects the accuracy of an MLLM\u2019s reasoning about episodes in an image sequence, while \u2018precision\u2019 focuses on assessing the severity of hallucinations that occur when understanding the image sequence. Report issue for preceding element",
    "masked_text": "After obtaining the AI-generated object list and AI-generated behavior list for each image sequence, we utilize the corresponding human-annotated object list and human-annotated behavior list as the ground truth to calculate \u2018Recall,\u2019 \u2018Precision,\u2019 and \u2018F1 metrics\u2019 at both the object and behavior levels. These metrics are used to measure the understanding capabilities of different MLLMs regarding the image sequence episode. \u2018Recall\u2019 reflects the accuracy of an MLLM\u2019s reasoning about episodes in an image sequence, while \u2018precision\u2019 focuses on assessing the severity of hallucinations that occur when understanding the image sequence.Report issue for preceding element",
    "citations": []
  },
  "S3.p1": {
    "text": "In our experimental section, we delve into two key questions:\n(a) We examine the reasoning capabilities of current black-box and open-source MLLMs on Mementos. Specifically, we assess the severity of object and behavioral hallucinations in these models.\n(b) We investigate the underlying causes of reasoning failures in MLLMs when interpreting image sequences. Report issue for preceding element",
    "masked_text": "In our experimental section, we delve into two key questions: (a) We examine the reasoning capabilities of current black-box and open-source MLLMs on Mementos. Specifically, we assess the severity of object and behavioral hallucinations in these models. (b) We investigate the underlying causes of reasoning failures in MLLMs when interpreting image sequences.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS1.p1": {
    "text": "We establish our baseline using 9 popular black-box and open-source MLLMs.\nThe black-box MLLMs include GPT-4V OpenAI ( 2023a ) and Gemini Team ( 2023 ) , and the open-source MLLMs are Video-LLaMA-2 Zhang et\u00a0al. ( 2023a ) , Chat-UniVi Jin et\u00a0al. ( 2023 ) , LLaVA-1.5 Liu et\u00a0al. ( 2023c ) , MiniGPT4 Zhu et\u00a0al. ( 2023 ) , MiniGPT5 Zheng et\u00a0al. ( 2023 ) , mPLUG_Owl-v2 Ye et\u00a0al. ( 2023 ) , and InstructBLIP Dai et\u00a0al. ( 2023 ) .\nAdditionally, considering that only a few open-source MLLMs are designed to process sequential images or videos (Video-LLaMA-2 and Chat-UniVi), we adapt input for other open-source MLLMs by combining all frames from an image sequence into one composite image. This input is referred to as the combined-input (c-input) setting. For black-box MLLMs and Chat-UniVi, we conduct evaluations using both the c-input setting and an alternative approach where frames from the image sequence are input sequentially, termed the sequential-input (s-input) setting. For Video-LLaMA-2, we only test the performance on s-input setting. Report issue for preceding element",
    "masked_text": "We establish our baseline using 9 popular black-box and open-source MLLMs. The black-box MLLMs include GPT-4V [CITATION] and Gemini [CITATION], and the open-source MLLMs are Video-LLaMA-2 [CITATION], Chat-UniVi [CITATION], LLaVA-1.5 [CITATION], MiniGPT4 [CITATION], MiniGPT5 [CITATION], mPLUG_Owl-v2 [CITATION], and InstructBLIP [CITATION]. Additionally, considering that only a few open-source MLLMs are designed to process sequential images or videos (Video-LLaMA-2 and Chat-UniVi), we adapt input for other open-source MLLMs by combining all frames from an image sequence into one composite image. This input is referred to as the combined-input (c-input) setting. For black-box MLLMs and Chat-UniVi, we conduct evaluations using both the c-input setting and an alternative approach where frames from the image sequence are input sequentially, termed the sequential-input (s-input) setting. For Video-LLaMA-2, we only test the performance on s-input setting.Report issue for preceding element",
    "citations": [
      {
        "tag": "Jin et\u00a0al. (2023)",
        "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding.",
        "authors": "Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li\u00a0Yuan. 2023.",
        "journal": "arXiv preprint arXiv:2311.08046."
      },
      {
        "tag": "Zheng et\u00a0al. (2023)",
        "title": "Minigpt-5: Interleaved vision-and-language generation via generative vokens.",
        "authors": "Kaizhi Zheng, Xuehai He, and Xin\u00a0Eric Wang. 2023.",
        "journal": ""
      },
      {
        "tag": "OpenAI (2023a)",
        "title": "Gpt-4 technical report.",
        "authors": "OpenAI. 2023a.",
        "journal": ""
      },
      {
        "tag": "Zhang et\u00a0al. (2023a)",
        "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding.",
        "authors": "Hang Zhang, Xin Li, and Lidong Bing. 2023a.",
        "journal": "arXiv preprint arXiv:2306.02858."
      },
      {
        "tag": "Zhu et\u00a0al. (2023)",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
        "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",
        "journal": "arXiv preprint arXiv:2304.10592."
      },
      {
        "tag": "Ye et\u00a0al. (2023)",
        "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.",
        "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi\u00a0Qian, Ji\u00a0Zhang, Fei Huang, and Jingren Zhou. 2023.",
        "journal": "arXiv preprint arXiv:2311.04257."
      },
      {
        "tag": "Team (2023)",
        "title": "Gemini: A family of highly capable multimodal models.",
        "authors": "Gemini Team. 2023.",
        "journal": ""
      },
      {
        "tag": "Dai et\u00a0al. (2023)",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
        "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng\u00a0Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.",
        "journal": ""
      },
      {
        "tag": "Liu et\u00a0al. (2023c)",
        "title": "Improved baselines with visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong\u00a0Jae Lee. 2023c.",
        "journal": ""
      }
    ]
  },
  "S3.SS1.SSS2.p1": {
    "text": "We evaluate all baseline MLLMs on Mementos and report the results in Figure 5 .\nBesides, we provide the performance comparison of each baseline method across the three different domains (Daily-life, Robotics, and Comics) in Table 2 . We summarize our findings as follows: Report issue for preceding element",
    "masked_text": "We evaluate all baseline MLLMs on Mementos and report the results in Figure 5. Besides, we provide the performance comparison of each baseline method across the three different domains (Daily-life, Robotics, and Comics) in Table 2. We summarize our findings as follows:Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS2.Px1.p1": {
    "text": "As shown in Figure 5 , except for being on par with Gemini (s-input) and LLaVA-1.5 in behavior precision, GPT-4V with s-input demonstrates the best reasoning capability compared with all other MLLMs in understanding image sequences.\nAmong open-source models, LLaVA1.5 performs the best, nearly matching or even surpassing the black-box model Gemini in object comprehension, but its ability to infer behaviors from image sequences is weaker compared to Gemini and GPT-4V.\nAlthough Video-LLaMA-2 and Chat-UniVi are designed for video understanding, they do not show an advantage over LLaVA-1.5, especially Video-LLaMA-2, which performs notably worse compared to LLaVA-1.5.\nThe weakest models in understanding image sequences are MiniGPT4 and MiniGPT5, with a significant gap in every metric compared to the other baselines.\nIt\u2019s noteworthy that under c-input setting, the performance of black-box MLLMs does not significantly differ from that of open-source MLLMs.\nLLaVA-1.5 and mPLUG_Owl-v2 meet or even exceed the black-box MLLMs on many metrics. Report issue for preceding element",
    "masked_text": "As shown in Figure 5, except for being on par with Gemini (s-input) and LLaVA-1.5 in behavior precision, GPT-4V with s-input demonstrates the best reasoning capability compared with all other MLLMs in understanding image sequences. Among open-source models, LLaVA1.5 performs the best, nearly matching or even surpassing the black-box model Gemini in object comprehension, but its ability to infer behaviors from image sequences is weaker compared to Gemini and GPT-4V. Although Video-LLaMA-2 and Chat-UniVi are designed for video understanding, they do not show an advantage over LLaVA-1.5, especially Video-LLaMA-2, which performs notably worse compared to LLaVA-1.5. The weakest models in understanding image sequences are MiniGPT4 and MiniGPT5, with a significant gap in every metric compared to the other baselines. It\u2019s noteworthy that under c-input setting, the performance of black-box MLLMs does not significantly differ from that of open-source MLLMs. LLaVA-1.5 and mPLUG_Owl-v2 meet or even exceed the black-box MLLMs on many metrics.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS2.Px2.p1": {
    "text": "We find that all MLLM methods perform significantly better on the three metrics for objects than those for behaviors.\nTaking the best-performing GPT-4V as an example, it achieves over 50% on all three object metrics, with recall even reaching 60%, indicating it can effectively recognize the main objects in an image sequence.\nHowever, for behaviors, GPT-4V scores only around 30%, with the best recall metric barely exceeding 40%.\nDespite this, GPT-4V is still the best-performing MLLM in reasoning behaviors.\nThis suggests that current MLLMs do not possess strong abilities to autonomously infer the behaviors from given sequential images, indicating the importance of our benchmark in highlighting the limitations in the reasoning abilities of MLLMs. Report issue for preceding element",
    "masked_text": "We find that all MLLM methods perform significantly better on the three metrics for objects than those for behaviors. Taking the best-performing GPT-4V as an example, it achieves over 50% on all three object metrics, with recall even reaching 60%, indicating it can effectively recognize the main objects in an image sequence. However, for behaviors, GPT-4V scores only around 30%, with the best recall metric barely exceeding 40%. Despite this, GPT-4V is still the best-performing MLLM in reasoning behaviors. This suggests that current MLLMs do not possess strong abilities to autonomously infer the behaviors from given sequential images, indicating the importance of our benchmark in highlighting the limitations in the reasoning abilities of MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS2.Px3.p1": {
    "text": "From Table 2 ,\nwe find that black-box models perform best in the robotics domain across the three domains, while open-source models show relatively better performance in the daily-life domain.\nAnalyzing each domain specifically, it is evident that in the daily-life domain, the performance of all methods, except for GPT-4V (s-input), does not vary significantly.\nThe main reason for the performance gap between open-source MLLMs and black-box MLLMs is the noticeably lower metrics of open-source models compared to black-box models in the robotics and comics domains.\nThe recall, precision, and F1 of both object and behavior for black-box MLLMs are almost more than double those of open-source models.\nWe speculate that one reason for this phenomenon is the distribution shift between Mementos and the training data of open-source MLLMs.\nThe limitations of the training data lead to weaker reasoning capability of open-source MLLMs. Report issue for preceding element",
    "masked_text": "From Table 2, we find that black-box models perform best in the robotics domain across the three domains, while open-source models show relatively better performance in the daily-life domain. Analyzing each domain specifically, it is evident that in the daily-life domain, the performance of all methods, except for GPT-4V (s-input), does not vary significantly. The main reason for the performance gap between open-source MLLMs and black-box MLLMs is the noticeably lower metrics of open-source models compared to black-box models in the robotics and comics domains. The recall, precision, and F1 of both object and behavior for black-box MLLMs are almost more than double those of open-source models. We speculate that one reason for this phenomenon is the distribution shift between Mementos and the training data of open-source MLLMs. The limitations of the training data lead to weaker reasoning capability of open-source MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p1": {
    "text": "In this section, we will provide reasons for failure reasoning results in current MLLMs, combining specific quantitative analyses and case studies.\nSince behavioral hallucination is a unique phenomenon in image sequence reasoning, and the causes of object hallucination are not significantly different from those in single image reasoning, we only present the reasons leading to behavioral hallucination in this paper.\nDue to space limitations, please refer to the Appendix D for specific case studies. The following are our main findings: Report issue for preceding element",
    "masked_text": "In this section, we will provide reasons for failure reasoning results in current MLLMs, combining specific quantitative analyses and case studies. Since behavioral hallucination is a unique phenomenon in image sequence reasoning, and the causes of object hallucination are not significantly different from those in single image reasoning, we only present the reasons leading to behavioral hallucination in this paper. Due to space limitations, please refer to the Appendix D for specific case studies. The following are our main findings: Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.SSS0.Px1.p1": {
    "text": "A key hypothesis underpinning behavioral hallucination is that incorrect object identification leads to subsequent inaccuracies in behavior identification. To test this, we evaluated the correlation coefficients between object and behavioral hallucinations across different domains for various MLLMs, as detailed in Table 3 .\nOur findings reveal that, for most MLLMs, the correlation coefficients in the three domains fluctuate between 0.1 and 0.4, suggesting a weak yet present correlation. This outcome supports the hypothesis that object hallucination contributes to behavioral hallucination to some extent.\nCase studies further reveal that after an object hallucination occurs, MLLMs tend to describe behaviors related to the hallucinated object, even if these behaviors do not exist in the image sequence.\nAs shown in Figure 6 , after recognizing a scene as a tennis court, a MLLM might describe a person playing tennis.\nInterestingly, in the robotics domain, there is a negligible correlation between object and behavioral hallucinations in black-box MLLMs. This divergence is likely because behaviors in robotics are predominantly linked to robotic arms, which these MLLMs generally identify correctly. Report issue for preceding element",
    "masked_text": "A key hypothesis underpinning behavioral hallucination is that incorrect object identification leads to subsequent inaccuracies in behavior identification. To test this, we evaluated the correlation coefficients between object and behavioral hallucinations across different domains for various MLLMs, as detailed in Table 3. Our findings reveal that, for most MLLMs, the correlation coefficients in the three domains fluctuate between 0.1 and 0.4, suggesting a weak yet present correlation. This outcome supports the hypothesis that object hallucination contributes to behavioral hallucination to some extent. Case studies further reveal that after an object hallucination occurs, MLLMs tend to describe behaviors related to the hallucinated object, even if these behaviors do not exist in the image sequence. As shown in Figure 6, after recognizing a scene as a tennis court, a MLLM might describe a person playing tennis. Interestingly, in the robotics domain, there is a negligible correlation between object and behavioral hallucinations in black-box MLLMs. This divergence is likely because behaviors in robotics are predominantly linked to robotic arms, which these MLLMs generally identify correctly.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.SSS0.Px2.p1": {
    "text": "In line with object hallucination phenomena, as noted in Li et\u00a0al. ( 2023c ) and Zhou et\u00a0al. ( 2023a ) , MLLMs demonstrate a tendency to generate behaviors that are commonly paired together when reasoning through image sequences. This proclivity exacerbates the problem of behavioral hallucination, especially in the field of robotics.\nConsider the case in Figure 1 where a robotic arm is tasked with opening a drawer by grabbing its side .\nMLLMs might erroneously depict the sequence as the arm grabbing the handle first, followed by pulling the drawer open, since grabbing the handle is a more co-occurring behavior with \u2018pull open\u2019.\nDespite the final outcome being accurately described, such errors in key details are unacceptable in robotics.\nThis issue is of particular concern given the growing inclination to utilize MLLMs as reward functions in robotic training, as discussed in recent studies Ma et\u00a0al. ( 2023 ); Sontakke et\u00a0al. ( 2023 ); Rocamonde et\u00a0al. ( 2023 ); Baumli et\u00a0al. ( 2023 ) .\nSuch subtle yet significant behavioral hallucinations can critically affect the quality of the reward function, leading to potential mislearning of behaviors in robotic systems.\nFor more detailed case studies, please refer to the Appendix D . Report issue for preceding element",
    "masked_text": "In line with object hallucination phenomena, as noted in [CITATION] and [CITATION], MLLMs demonstrate a tendency to generate behaviors that are commonly paired together when reasoning through image sequences. This proclivity exacerbates the problem of behavioral hallucination, especially in the field of robotics. Consider the case in Figure 1 where a robotic arm is tasked with opening a drawer by grabbing its side. MLLMs might erroneously depict the sequence as the arm grabbing the handle first, followed by pulling the drawer open, since grabbing the handle is a more co-occurring behavior with \u2018pull open\u2019. Despite the final outcome being accurately described, such errors in key details are unacceptable in robotics. This issue is of particular concern given the growing inclination to utilize MLLMs as reward functions in robotic training, as discussed in recent studies [CITATION]. Such subtle yet significant behavioral hallucinations can critically affect the quality of the reward function, leading to potential mislearning of behaviors in robotic systems. For more detailed case studies, please refer to the Appendix D.Report issue for preceding element",
    "citations": [
      {
        "tag": "Rocamonde et\u00a0al. (2023)",
        "title": "Vision-language models are zero-shot reward models for reinforcement learning.",
        "authors": "Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. 2023.",
        "journal": "arXiv preprint arXiv:2310.12921."
      },
      {
        "tag": "Zhou et\u00a0al. (2023a)",
        "title": "Analyzing and mitigating object hallucination in large vision-language models.",
        "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023a.",
        "journal": "arXiv preprint arXiv:2310.00754."
      },
      {
        "tag": "Sontakke et\u00a0al. (2023)",
        "title": "Roboclip: One demonstration is enough to learn robot policies.",
        "authors": "Sumedh\u00a0Anand Sontakke, Jesse Zhang, S\u00e9b Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. 2023.",
        "journal": "InThirty-seventh Conference on Neural Information Processing Systems."
      },
      {
        "tag": "Baumli et\u00a0al. (2023)",
        "title": "Vision-language models as a source of rewards.",
        "authors": "Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, et\u00a0al. 2023.",
        "journal": "arXiv preprint arXiv:2312.09187."
      },
      {
        "tag": "Li et\u00a0al. (2023c)",
        "title": "Evaluating object hallucination in large vision-language models.",
        "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2023c.",
        "journal": "arXiv preprint arXiv:2305.10355."
      },
      {
        "tag": "Ma et\u00a0al. (2023)",
        "title": "Liv: Language-image representations and rewards for robotic control.",
        "authors": "Yecheng\u00a0Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. 2023.",
        "journal": "arXiv preprint arXiv:2306.00958."
      }
    ]
  },
  "S3.SS2.SSS0.Px3.p1": {
    "text": "In machine learning, the Snowball effect is a well-documented phenomenon, referring to the progressive accumulation or intensification of errors within a system, as discussed in Asadi et\u00a0al. ( 2019 ); Zhang et\u00a0al. ( 2023b ); Wang et\u00a0al. ( 2023c ); Liu et\u00a0al. ( 2023d ) . Zhang et\u00a0al. ( 2023b ) notably highlight this phenomenon in Large Language Models.\nExperiments on Mementos reveal that the snowball effect in both behavioral and object hallucinations becomes markedly pronounced when reasoning through image sequences.\nThe temporal nature of image sequences, consisting of a series of frames rather than a solitary image, demands that MLLMs sequentially infer the narrative, frame by frame. This process makes the models susceptible to accumulating and exacerbating hallucinations if errors occur early in the sequence.\nIn our analysis, we specifically examined the trend of object and behavioral hallucination in GPT-4V and LLaVA-1.5 within the daily-life domain, correlating it with the increasing episode length of image sequences. As depicted in Figure 7 , there is a noticeable decrease in object and behavior recall for both MLLMs as the episode length extends. This trend suggests a heightened susceptibility to hallucinations and a pronounced snowball effect in MLLMs when processing image sequences with a greater array of objects and behaviors.\nDetailed case studies can be found in Appendix D . Report issue for preceding element",
    "masked_text": "In machine learning, the Snowball effect is a well-documented phenomenon, referring to the progressive accumulation or intensification of errors within a system, as discussed in [CITATION]. [CITATION] notably highlight this phenomenon in Large Language Models. Experiments on Mementos reveal that the snowball effect in both behavioral and object hallucinations becomes markedly pronounced when reasoning through image sequences. The temporal nature of image sequences, consisting of a series of frames rather than a solitary image, demands that MLLMs sequentially infer the narrative, frame by frame. This process makes the models susceptible to accumulating and exacerbating hallucinations if errors occur early in the sequence. In our analysis, we specifically examined the trend of object and behavioral hallucination in GPT-4V and LLaVA-1.5 within the daily-life domain, correlating it with the increasing episode length of image sequences. As depicted in Figure 7, there is a noticeable decrease in object and behavior recall for both MLLMs as the episode length extends. This trend suggests a heightened susceptibility to hallucinations and a pronounced snowball effect in MLLMs when processing image sequences with a greater array of objects and behaviors. Detailed case studies can be found in Appendix D.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2023b)",
        "title": "How language model hallucinations can snowball.",
        "authors": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah\u00a0A. Smith. 2023b.",
        "journal": ""
      },
      {
        "tag": "Asadi et\u00a0al. (2019)",
        "title": "Combating the compounding-error problem with a multi-step model.",
        "authors": "Kavosh Asadi, Dipendra Misra, Seungchan Kim, and Michel\u00a0L. Littman. 2019.",
        "journal": ""
      },
      {
        "tag": "Liu et\u00a0al. (2023d)",
        "title": "C-disentanglement: Discovering causally-independent generative factors under an inductive bias of confounder.",
        "authors": "Xiaoyu Liu, Jiaxin Yuan, Bang An, Yuancheng Xu, Yifan Yang, and Furong Huang. 2023d.",
        "journal": "arXiv preprint arXiv:2310.17325."
      },
      {
        "tag": "Wang et\u00a0al. (2023c)",
        "title": "Coplanner: Plan to roll out conservatively but to explore optimistically for model-based rl.",
        "authors": "Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, and Furong Huang. 2023c.",
        "journal": ""
      }
    ]
  },
  "S4.SS1.p1": {
    "text": "The advent of Multimodal Large Language Models (MLLMs) has prompted a reassessment of traditional benchmarks Lin et\u00a0al. ( 2014 ); Marino et\u00a0al. ( 2019 ); Hudson and Manning ( 2019 ) , originally conceived for Vision Language Models (VLMs). These existing benchmarks fail to sufficiently expose the robustness and hallucination issues inherent in MLLMs. Consequently, there is a growing impetus within the research community to devise more challenging benchmarks. This trend spans various domains, from question and answering (QA) reasoning Liu et\u00a0al. ( 2023a ); Yue et\u00a0al. ( 2023 ) , to optical character recognition (OCR) Liu et\u00a0al. ( 2023f ) , and extends to the study of hallucinations Wang et\u00a0al. ( 2023a ) , with benchmarks like POPE Li et\u00a0al. ( 2023c ) and Bingo Cui et\u00a0al. ( 2023 ) . Additionally, comprehensive analyses of MLLMs, such as Mmbench Liu et\u00a0al. ( 2023e ) , Mm-vet Yu et\u00a0al. ( 2023b ) , LVLM-eHub Xu et\u00a0al. ( 2023 ) , SEED Li et\u00a0al. ( 2023a ) , GAVIE Liu et\u00a0al. ( 2023b ) , and LAMM Yin et\u00a0al. ( 2023 ) , are emerging. Report issue for preceding element",
    "masked_text": "The advent of Multimodal Large Language Models (MLLMs) has prompted a reassessment of traditional benchmarks [CITATION], originally conceived for Vision Language Models (VLMs). These existing benchmarks fail to sufficiently expose the robustness and hallucination issues inherent in MLLMs. Consequently, there is a growing impetus within the research community to devise more challenging benchmarks. This trend spans various domains, from question and answering (QA) reasoning [CITATION], to optical character recognition (OCR) [CITATION], and extends to the study of hallucinations [CITATION], with benchmarks like POPE [CITATION] and Bingo [CITATION]. Additionally, comprehensive analyses of MLLMs, such as Mmbench [CITATION], Mm-vet [CITATION], LVLM-eHub [CITATION], SEED [CITATION], GAVIE [CITATION], and LAMM [CITATION], are emerging.Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2023b)",
        "title": "Aligning large multi-modal model with robust instruction tuning.",
        "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023b.",
        "journal": "arXiv preprint arXiv:2306.14565."
      },
      {
        "tag": "Hudson and Manning (2019)",
        "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering.",
        "authors": "Drew\u00a0A Hudson and Christopher\u00a0D Manning. 2019.",
        "journal": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709."
      },
      {
        "tag": "Yue et\u00a0al. (2023)",
        "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.",
        "authors": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge\u00a0Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et\u00a0al. 2023.",
        "journal": "arXiv preprint arXiv:2311.16502."
      },
      {
        "tag": "Yin et\u00a0al. (2023)",
        "title": "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.",
        "authors": "Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu\u00a0Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et\u00a0al. 2023.",
        "journal": "arXiv preprint arXiv:2306.06687."
      },
      {
        "tag": "Yu et\u00a0al. (2023b)",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities.",
        "authors": "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023b.",
        "journal": "arXiv preprint arXiv:2308.02490."
      },
      {
        "tag": "Li et\u00a0al. (2023c)",
        "title": "Evaluating object hallucination in large vision-language models.",
        "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2023c.",
        "journal": "arXiv preprint arXiv:2305.10355."
      },
      {
        "tag": "Liu et\u00a0al. (2023e)",
        "title": "Mmbench: Is your multi-modal model an all-around player?",
        "authors": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo\u00a0Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et\u00a0al. 2023e.",
        "journal": "arXiv preprint arXiv:2307.06281."
      },
      {
        "tag": "Li et\u00a0al. (2023a)",
        "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension.",
        "authors": "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023a.",
        "journal": "arXiv preprint arXiv:2307.16125."
      },
      {
        "tag": "Liu et\u00a0al. (2023f)",
        "title": "On the hidden mystery of ocr in large multimodal models.",
        "authors": "Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et\u00a0al. 2023f.",
        "journal": "arXiv preprint arXiv:2305.07895."
      },
      {
        "tag": "Lin et\u00a0al. (2014)",
        "title": "Microsoft coco: Common objects in context.",
        "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C\u00a0Lawrence Zitnick. 2014.",
        "journal": "InComputer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer."
      },
      {
        "tag": "Wang et\u00a0al. (2023a)",
        "title": "Evaluation and analysis of hallucination in large vision-language models.",
        "authors": "Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji\u00a0Zhang, Jihua Zhu, et\u00a0al. 2023a.",
        "journal": "arXiv preprint arXiv:2308.15126."
      },
      {
        "tag": "Marino et\u00a0al. (2019)",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge.",
        "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.",
        "journal": "InProceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204."
      },
      {
        "tag": "Cui et\u00a0al. (2023)",
        "title": "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.",
        "authors": "Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023.",
        "journal": "arXiv preprint arXiv:2311.03287."
      },
      {
        "tag": "Xu et\u00a0al. (2023)",
        "title": "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.",
        "authors": "Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu\u00a0Qiao, and Ping Luo. 2023.",
        "journal": "arXiv preprint arXiv:2306.09265."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.",
        "authors": "Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a.",
        "journal": "arXiv preprint arXiv:2310.14566."
      }
    ]
  },
  "S4.SS1.p2": {
    "text": "Diverging from the focus on single images in prior studies, our paper introduces a novel benchmark that employs image sequences derived from video frames or comics, specifically examining the behavioral hallucinations in MLLMs. While Chen et\u00a0al. ( 2023a ) have employed a method of uniformly sampling frames from videos to create vision QA tasks for MLLMs, our benchmark is distinct in its challenge, as it prompts MLLMs to formulate descriptions of image sequences without the guidance of questions. This approach allows for a more nuanced evaluation of behavioral hallucinations and, by extension, a more precise assessment of MLLMs\u2019 reasoning capabilities. Report issue for preceding element",
    "masked_text": "Diverging from the focus on single images in prior studies, our paper introduces a novel benchmark that employs image sequences derived from video frames or comics, specifically examining the behavioral hallucinations in MLLMs. While [CITATION] have employed a method of uniformly sampling frames from videos to create vision QA tasks for MLLMs, our benchmark is distinct in its challenge, as it prompts MLLMs to formulate descriptions of image sequences without the guidance of questions. This approach allows for a more nuanced evaluation of behavioral hallucinations and, by extension, a more precise assessment of MLLMs\u2019 reasoning capabilities.Report issue for preceding element",
    "citations": [
      {
        "tag": "Chen et\u00a0al. (2023a)",
        "title": "Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering.",
        "authors": "Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. 2023a.",
        "journal": "arXiv preprint arXiv:2311.14906."
      }
    ]
  },
  "S4.SS2.p1": {
    "text": "Hallucinations in MLLMs, akin to those in Large Language Models (LLMs) Zhang et\u00a0al. ( 2023c ); Li et\u00a0al. ( 2023b ); Zhou et\u00a0al. ( 2023c ); Chen et\u00a0al. ( 2023b ) , represent a significant challenge. In MLLMs, hallucinations are characterized by inconsistencies between the model\u2019s output and the visual content Rohrbach et\u00a0al. ( 2018 ); Wang et\u00a0al. ( 2023a ) . Recent studies have explored various aspects of hallucination in MLLMs, covering topics such as object hallucination Li et\u00a0al. ( 2023c ) , hallucination assessment in GPT-4V Cui et\u00a0al. ( 2023 ) , and knowledge hallucination Liu et\u00a0al. ( 2023a ) . Report issue for preceding element",
    "masked_text": "Hallucinations in MLLMs, akin to those in Large Language Models (LLMs) [CITATION], represent a significant challenge. In MLLMs, hallucinations are characterized by inconsistencies between the model\u2019s output and the visual content [CITATION]. Recent studies have explored various aspects of hallucination in MLLMs, covering topics such as object hallucination [CITATION], hallucination assessment in GPT-4V [CITATION], and knowledge hallucination [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Li et\u00a0al. (2023b)",
        "title": "Halueval: A large-scale hallucination evaluation benchmark for large language models.",
        "authors": "Junyi Li, Xiaoxue Cheng, Wayne\u00a0Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b.",
        "journal": "InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449\u20136464."
      },
      {
        "tag": "Zhou et\u00a0al. (2023c)",
        "title": "Explore spurious correlations at the concept level in language models for text classification.",
        "authors": "Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, and Furong Huang. 2023c.",
        "journal": "arXiv preprint arXiv:2311.08648."
      },
      {
        "tag": "Rohrbach et\u00a0al. (2018)",
        "title": "Object hallucination in image captioning.",
        "authors": "Anna Rohrbach, Lisa\u00a0Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.",
        "journal": "arXiv preprint arXiv:1809.02156."
      },
      {
        "tag": "Zhang et\u00a0al. (2023c)",
        "title": "Siren\u2019s song in the ai ocean: A survey on hallucination in large language models.",
        "authors": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu\u00a0Zhang, Yulong Chen, et\u00a0al. 2023c.",
        "journal": "arXiv preprint arXiv:2309.01219."
      },
      {
        "tag": "Li et\u00a0al. (2023c)",
        "title": "Evaluating object hallucination in large vision-language models.",
        "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne\u00a0Xin Zhao, and Ji-Rong Wen. 2023c.",
        "journal": "arXiv preprint arXiv:2305.10355."
      },
      {
        "tag": "Wang et\u00a0al. (2023a)",
        "title": "Evaluation and analysis of hallucination in large vision-language models.",
        "authors": "Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji\u00a0Zhang, Jihua Zhu, et\u00a0al. 2023a.",
        "journal": "arXiv preprint arXiv:2308.15126."
      },
      {
        "tag": "Chen et\u00a0al. (2023b)",
        "title": "Hallucination detection: Robustly discerning reliable answers in large language models.",
        "authors": "Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge\u00a0Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. 2023b.",
        "journal": "InProceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 245\u2013255."
      },
      {
        "tag": "Cui et\u00a0al. (2023)",
        "title": "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.",
        "authors": "Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023.",
        "journal": "arXiv preprint arXiv:2311.03287."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models.",
        "authors": "Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a.",
        "journal": "arXiv preprint arXiv:2310.14566."
      }
    ]
  },
  "S4.SS2.p2": {
    "text": "While there are frameworks proposed for mitigating hallucinations Zhou et\u00a0al. ( 2023a ); Wang et\u00a0al. ( 2023b ); Leng et\u00a0al. ( 2023 ); Zhou et\u00a0al. ( 2023b ); Chen et\u00a0al. ( 2023c ); Jiang et\u00a0al. ( 2023 ); Huang et\u00a0al. ( 2023 ); Yu et\u00a0al. ( 2023a ); Zhao et\u00a0al. ( 2023 ) , there is a noticeable gap in the literature regarding the study of behavioral hallucination. Behavioral hallucination refers to scenarios where the generated content contains actions that conflict with what is depicted in image sequences. Moreover, the existing body of work does not offer a dedicated metric for evaluating behavioral hallucinations, an area that warrants further exploration and development. Report issue for preceding element",
    "masked_text": "While there are frameworks proposed for mitigating hallucinations [CITATION], there is a noticeable gap in the literature regarding the study of behavioral hallucination. Behavioral hallucination refers to scenarios where the generated content contains actions that conflict with what is depicted in image sequences. Moreover, the existing body of work does not offer a dedicated metric for evaluating behavioral hallucinations, an area that warrants further exploration and development.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhou et\u00a0al. (2023a)",
        "title": "Analyzing and mitigating object hallucination in large vision-language models.",
        "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023a.",
        "journal": "arXiv preprint arXiv:2310.00754."
      },
      {
        "tag": "Zhou et\u00a0al. (2023b)",
        "title": "Scalable prompt generation for semi-supervised learning with language models.",
        "authors": "Yuhang Zhou, Suraj Maharjan, and Beiye Liu. 2023b.",
        "journal": "arXiv preprint arXiv:2302.09236."
      },
      {
        "tag": "Zhao et\u00a0al. (2023)",
        "title": "Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization.",
        "authors": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023.",
        "journal": "arXiv preprint arXiv:2311.16839."
      },
      {
        "tag": "Yu et\u00a0al. (2023a)",
        "title": "Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.",
        "authors": "Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi\u00a0Tian, and Yueting Zhuang. 2023a.",
        "journal": "arXiv preprint arXiv:2311.13614."
      },
      {
        "tag": "Chen et\u00a0al. (2023c)",
        "title": "Mitigating hallucination in visual language models with visual supervision.",
        "authors": "Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. 2023c.",
        "journal": "arXiv preprint arXiv:2311.16479."
      },
      {
        "tag": "Jiang et\u00a0al. (2023)",
        "title": "Hallucination augmented contrastive learning for multimodal large language model.",
        "authors": "Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji\u00a0Zhang, Fei Huang, and Shikun Zhang. 2023.",
        "journal": "arXiv preprint arXiv:2312.06968."
      },
      {
        "tag": "Huang et\u00a0al. (2023)",
        "title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.",
        "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023.",
        "journal": "arXiv preprint arXiv:2311.17911."
      },
      {
        "tag": "Leng et\u00a0al. (2023)",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding.",
        "authors": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.",
        "journal": "arXiv preprint arXiv:2311.16922."
      },
      {
        "tag": "Wang et\u00a0al. (2023b)",
        "title": "Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites.",
        "authors": "Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2023b.",
        "journal": "arXiv preprint arXiv:2312.01701."
      }
    ]
  },
  "S5.p1": {
    "text": "In this paper, we present Mementos, an novel and challenging benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in interpreting image sequences.\nWe conduct evaluations on nine most recent MLLMs using GPT-4-assisted evaluation procedure.\nOur findings indicate that all tested MLLMs struggle with significant behavioral and object hallucinations in generating descriptions for image sequences.\nThrough a mix of quantitative analysis and case studies, we identify three primary factors contributing to these reasoning failures in MLLMs. Report issue for preceding element",
    "masked_text": "In this paper, we present Mementos, an novel and challenging benchmark designed to assess the reasoning abilities of Multimodal Large Language Models (MLLMs) in interpreting image sequences. We conduct evaluations on nine most recent MLLMs using GPT-4-assisted evaluation procedure. Our findings indicate that all tested MLLMs struggle with significant behavioral and object hallucinations in generating descriptions for image sequences. Through a mix of quantitative analysis and case studies, we identify three primary factors contributing to these reasoning failures in MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S5.p2": {
    "text": "Looking ahead, there are three potential avenues for future research: (1) Dataset Diversification: We suggest further enriching Mementos by including a broader variety of data types. This expansion could encompass first-person navigation experiences, sequential medical CT scans, and interactive gaming data. Such diversification would provide a more comprehensive platform for evaluating MLLMs across a wider range of contexts and scenarios. (2) Evaluation Process Optimization: Another key area for development is refining the evaluation process. This involves exploring more nuanced methods to assess the reasoning capabilities of MLLMs, focusing on semantic understanding rather than relying predominantly on keyword matching. Such advancements would enable a deeper and more accurate appraisal of MLLMs\u2019 comprehension skills. (3) Hallucination Mitigation and Reasoning Enhancement: Lastly, informed by the three identified causes of reasoning failures, we propose developing targeted strategies to reduce both behavioral and object hallucinations in the future work. These methods would aim to bolster the reasoning faculties of MLLMs, making them more adept at accurately interpreting and describing complex image sequences. Report issue for preceding element",
    "masked_text": "Looking ahead, there are three potential avenues for future research: (1) Dataset Diversification: We suggest further enriching Mementos by including a broader variety of data types. This expansion could encompass first-person navigation experiences, sequential medical CT scans, and interactive gaming data. Such diversification would provide a more comprehensive platform for evaluating MLLMs across a wider range of contexts and scenarios. (2) Evaluation Process Optimization: Another key area for development is refining the evaluation process. This involves exploring more nuanced methods to assess the reasoning capabilities of MLLMs, focusing on semantic understanding rather than relying predominantly on keyword matching. Such advancements would enable a deeper and more accurate appraisal of MLLMs\u2019 comprehension skills. (3) Hallucination Mitigation and Reasoning Enhancement: Lastly, informed by the three identified causes of reasoning failures, we propose developing targeted strategies to reduce both behavioral and object hallucinations in the future work. These methods would aim to bolster the reasoning faculties of MLLMs, making them more adept at accurately interpreting and describing complex image sequences.Report issue for preceding element",
    "citations": []
  },
  "Sx1.p1": {
    "text": "Wang, Zhou, Liu, Xu, and Huang are supported by National Science Foundation NSF-IIS FAI program, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, Capital One and JP Morgan faculty fellowships. Yao thanks Center for AI Safety and Google Cloud Research Credits program for supporting our computing needs. Bansal is supported by DARPA ECOLE Program No. HR00112390060 and ONR Grant N00014-23-1-2356. Report issue for preceding element",
    "masked_text": "Wang, Zhou, Liu, Xu, and Huang are supported by National Science Foundation NSF-IIS FAI program, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, Capital One and JP Morgan faculty fellowships. Yao thanks Center for AI Safety and Google Cloud Research Credits program for supporting our computing needs. Bansal is supported by DARPA ECOLE Program No. HR00112390060 and ONR Grant N00014-23-1-2356.Report issue for preceding element",
    "citations": []
  }
}