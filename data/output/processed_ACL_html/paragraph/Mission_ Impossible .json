{
  "S1.p1": {
    "text": "Chomsky ( 2023 ) , Chomsky et\u00a0al. ( 2023 ) , Moro et\u00a0al. ( 2023 ) , and Bolhuis et\u00a0al. ( 2024 ) make very broad claims to the effect that large language models (LLMs) are equally capable of learning possible and impossible human languages. For these authors, it follows from this claim that LLMs cannot teach us anything about language, and so the claim (if true) would have significant consequences for linguistic methodology and potentially also for the viability of LLMs as the basis for robust language capabilities. Report issue for preceding element",
    "masked_text": "[CITATION], [CITATION], [CITATION], and [CITATION] make very broad claims to the effect that large language models (LLMs) are equally capable of learning possible and impossible human languages. For these authors, it follows from this claim that LLMs cannot teach us anything about language, and so the claim (if true) would have significant consequences for linguistic methodology and potentially also for the viability of LLMs as the basis for robust language capabilities.Report issue for preceding element",
    "citations": [
      {
        "tag": "Chomsky (2023)",
        "title": "Conversations with Tyler: Noam Chomsky.",
        "authors": "Noam Chomsky. 2023.",
        "journal": "Conversations with Tyler Podcast."
      },
      {
        "tag": "Moro et\u00a0al. (2023)",
        "title": "Large languages, impossible languages and human brains.",
        "authors": "Andrea Moro, Matteo Greco, and Stefano\u00a0F. Cappa. 2023.",
        "journal": "Cortex, 167:82\u201385."
      },
      {
        "tag": "Bolhuis et\u00a0al. (2024)",
        "title": "Three reasons why AI doesn\u2019t model human language.",
        "authors": "Johan\u00a0J. Bolhuis, Stephen Crain, Sandiway Fong, and Andrea Moro. 2024.",
        "journal": "Nature, 627(8004):489\u2013489."
      },
      {
        "tag": "Chomsky et\u00a0al. (2023)",
        "title": "Noam Chomsky: The false promise of ChatGPT.",
        "authors": "Noam Chomsky, Ian Roberts, and Jeffrey Watumull. 2023.",
        "journal": "The New York Times."
      }
    ]
  },
  "S1.p2": {
    "text": "These authors state this claim in absolute terms. For example, Chomsky et\u00a0al. ( 2023 ) flatly assert that LLMs \u201care incapable of distinguishing the possible from the impossible,\u201d Chomsky ( 2023 ) says this property \u201ccan\u2019t be modified,\u201d and Moro et\u00a0al. ( 2023 ) write that \u201cthe distinction between possible versus impossible languages cannot be formulated by definition for LLM.\u201d Bolhuis et\u00a0al. ( 2024 ) go so far as to claim that \u201cLLMs can produce \u2018impossible\u2019 languages [\u2026] just as well as (if not better than) natural language output.\u201d\nOne might expect such strong claims to be supported by extensive formal analysis and/or experimental evidence. However, as far as we are aware, this is not the case. The sole experimental paper cited by the above authors is Mitchell and Bowers 2020 \u2014an important and inspiring paper but not one that can resolve these questions on its own. In addition, linguists themselves do not even have an agreed upon notion of what defines the possible or the impossible languages, to say nothing of having formal results with respect to LLMs. Report issue for preceding element",
    "masked_text": "These authors state this claim in absolute terms. For example, [CITATION] flatly assert that LLMs \u201care incapable of distinguishing the possible from the impossible,\u201d [CITATION] says this property \u201ccan\u2019t be modified,\u201d and [CITATION] write that \u201cthe distinction between possible versus impossible languages cannot be formulated by definition for LLM.\u201d [CITATION] go so far as to claim that \u201cLLMs can produce \u2018impossible\u2019 languages [\u2026] just as well as (if not better than) natural language output.\u201d One might expect such strong claims to be supported by extensive formal analysis and/or experimental evidence. However, as far as we are aware, this is not the case. The sole experimental paper cited by the above authors is [CITATION]\u2014an important and inspiring paper but not one that can resolve these questions on its own. In addition, linguists themselves do not even have an agreed upon notion of what defines the possible or the impossible languages, to say nothing of having formal results with respect to LLMs.Report issue for preceding element",
    "citations": [
      {
        "tag": "Moro et\u00a0al. (2023)",
        "title": "Large languages, impossible languages and human brains.",
        "authors": "Andrea Moro, Matteo Greco, and Stefano\u00a0F. Cappa. 2023.",
        "journal": "Cortex, 167:82\u201385."
      },
      {
        "tag": "Chomsky et\u00a0al. (2023)",
        "title": "Noam Chomsky: The false promise of ChatGPT.",
        "authors": "Noam Chomsky, Ian Roberts, and Jeffrey Watumull. 2023.",
        "journal": "The New York Times."
      },
      {
        "tag": "Mitchell and Bowers (2020)",
        "title": "Priorless recurrent networks learn curiously.",
        "authors": "Jeff Mitchell and Jeffrey Bowers. 2020.",
        "journal": "InProceedings of the 28th International Conference on Computational Linguistics, pages 5147\u20135158, Barcelona, Spain (Online). International Committee on Computational Linguistics."
      },
      {
        "tag": "Bolhuis et\u00a0al. (2024)",
        "title": "Three reasons why AI doesn\u2019t model human language.",
        "authors": "Johan\u00a0J. Bolhuis, Stephen Crain, Sandiway Fong, and Andrea Moro. 2024.",
        "journal": "Nature, 627(8004):489\u2013489."
      },
      {
        "tag": "Chomsky (2023)",
        "title": "Conversations with Tyler: Noam Chomsky.",
        "authors": "Noam Chomsky. 2023.",
        "journal": "Conversations with Tyler Podcast."
      }
    ]
  },
  "S1.p3": {
    "text": "Here we provide extensive new experimental evidence to inform the claim that LLMs are equally capable of learning possible and impossible languages in the human sense. Arguably, the central challenge for such work is the fact that there is no agreed-upon way of distinguishing these two groups. We do not feel positioned ourselves to assert such a definition, so we instead offer some examples of impossible languages on a continuum of intuitive complexity ( Figure 1 ).\nSome of these examples seem intuitively impossible, such as random sentence-level shuffling of English words. Others operationalize less obvious but common claims in the linguistics literature about rules that are impossible, like those that depend on counting words. Report issue for preceding element",
    "masked_text": "Here we provide extensive new experimental evidence to inform the claim that LLMs are equally capable of learning possible and impossible languages in the human sense. Arguably, the central challenge for such work is the fact that there is no agreed-upon way of distinguishing these two groups. We do not feel positioned ourselves to assert such a definition, so we instead offer some examples of impossible languages on a continuum of intuitive complexity (Figure 1). Some of these examples seem intuitively impossible, such as random sentence-level shuffling of English words. Others operationalize less obvious but common claims in the linguistics literature about rules that are impossible, like those that depend on counting words.Report issue for preceding element",
    "citations": []
  },
  "S1.p4": {
    "text": "All of our examples are, we take it, uncontroversial instances of impossible languages. Thus, our experiments can inform the core hypotheses as follows: if LLMs learn these languages as well as they learn natural languages, then the claims of Chomsky and others are supported (for the specific class of LLMs tested). Conversely, if LLMs do not learn these languages as well as the possible ones, it would call into question those assertions. In that case, proponents of those claims ought to provide examples of impossible languages that they find more informative, which we can then evaluate using our approach to further advance the discussion. Report issue for preceding element",
    "masked_text": "All of our examples are, we take it, uncontroversial instances of impossible languages. Thus, our experiments can inform the core hypotheses as follows: if LLMs learn these languages as well as they learn natural languages, then the claims of Chomsky and others are supported (for the specific class of LLMs tested). Conversely, if LLMs do not learn these languages as well as the possible ones, it would call into question those assertions. In that case, proponents of those claims ought to provide examples of impossible languages that they find more informative, which we can then evaluate using our approach to further advance the discussion.Report issue for preceding element",
    "citations": []
  },
  "S1.p5": {
    "text": "Our experiments use GPT-2 small models Radford et\u00a0al. ( 2018 , 2019 ) ,\nand our base training corpus is the BabyLM dataset Warstadt et\u00a0al. ( 2023 ) , which we modify in various ways to implement our impossible languages.\nWhat we find is that these models indeed struggle to learn impossible languages, shown through three core experiments: Report issue for preceding element",
    "masked_text": "Our experiments use GPT-2 small models [CITATION], and our base training corpus is the BabyLM dataset [CITATION], which we modify in various ways to implement our impossible languages. What we find is that these models indeed struggle to learn impossible languages, shown through three core experiments:Report issue for preceding element",
    "citations": [
      {
        "tag": "Radford et\u00a0al. (2019)",
        "title": "Language models are unsupervised multitask learners.",
        "authors": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
        "journal": "Ms, OpenAI."
      },
      {
        "tag": "Radford et\u00a0al. (2018)",
        "title": "Improving language understanding by generative pre-training.",
        "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.",
        "journal": "Ms, OpenAI."
      },
      {
        "tag": "Warstadt et\u00a0al. (2023)",
        "title": "Call for papers \u2013 the BabyLM challenge: Sample-efficient pretraining on a developmentally plausible corpus.",
        "authors": "Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang. 2023.",
        "journal": ""
      }
    ]
  },
  "S1.p6": {
    "text": "\u2022 In Experiment 1 , we train GPT-2 models on our set of defined possible and impossible languages, measuring their learning efficiency through test set perplexities. We find that models trained on possible languages learn more efficiently , evident from lower perplexities achieved in fewer training steps. Report issue for preceding element \u2022 In Experiment 2 , we more closely examine a set of languages that exhibit count-based verb marking rules, using surprisal comparisons to target the relevant patterns. We find that GPT-2s trained on possible languages are more surprised by ungrammatical constructions, indicating that models disprefer agreement rules involving counting . Report issue for preceding element \u2022 In Experiment 3 , we dive deeper into the internal mechanisms that models may develop to learn such count-based grammar rules using causal abstraction analysis. We find that models develop natural, modular solutions to unnatural grammatical patterns . Report issue for preceding element",
    "masked_text": "\u2022 In Experiment 1, we train GPT-2 models on our set of defined possible and impossible languages, measuring their learning efficiency through test set perplexities. We find that models trained on possible languages learn more efficiently, evident from lower perplexities achieved in fewer training steps.Report issue for preceding element \u2022 In Experiment 2, we more closely examine a set of languages that exhibit count-based verb marking rules, using surprisal comparisons to target the relevant patterns. We find that GPT-2s trained on possible languages are more surprised by ungrammatical constructions, indicating that models disprefer agreement rules involving counting.Report issue for preceding element \u2022 In Experiment 3, we dive deeper into the internal mechanisms that models may develop to learn such count-based grammar rules using causal abstraction analysis. We find that models develop natural, modular solutions to unnatural grammatical patterns.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i1.p1": {
    "text": "In Experiment 1 , we train GPT-2 models on our set of defined possible and impossible languages, measuring their learning efficiency through test set perplexities. We find that models trained on possible languages learn more efficiently , evident from lower perplexities achieved in fewer training steps. Report issue for preceding element",
    "masked_text": "In Experiment 1, we train GPT-2 models on our set of defined possible and impossible languages, measuring their learning efficiency through test set perplexities. We find that models trained on possible languages learn more efficiently, evident from lower perplexities achieved in fewer training steps.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i2.p1": {
    "text": "In Experiment 2 , we more closely examine a set of languages that exhibit count-based verb marking rules, using surprisal comparisons to target the relevant patterns. We find that GPT-2s trained on possible languages are more surprised by ungrammatical constructions, indicating that models disprefer agreement rules involving counting . Report issue for preceding element",
    "masked_text": "In Experiment 2, we more closely examine a set of languages that exhibit count-based verb marking rules, using surprisal comparisons to target the relevant patterns. We find that GPT-2s trained on possible languages are more surprised by ungrammatical constructions, indicating that models disprefer agreement rules involving counting.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i3.p1": {
    "text": "In Experiment 3 , we dive deeper into the internal mechanisms that models may develop to learn such count-based grammar rules using causal abstraction analysis. We find that models develop natural, modular solutions to unnatural grammatical patterns . Report issue for preceding element",
    "masked_text": "In Experiment 3, we dive deeper into the internal mechanisms that models may develop to learn such count-based grammar rules using causal abstraction analysis. We find that models develop natural, modular solutions to unnatural grammatical patterns.Report issue for preceding element",
    "citations": []
  },
  "S1.p7": {
    "text": "Overall, our experimental results strongly challenge the claims of Chomsky and others given above, and we believe they pave the way for even deeper discussions of LLMs as models of language learning. At the same time, we recognize that models and humans exhibit fundamental differences, but the extent to which models favor or disfavor natural languages can be influenced by specific architectural decisions (as demonstrated by our findings on tokenization and positional encodings). We hope this paper initiates a new line of work that explores how different model architectures can distinguish between the possible and impossible languages. 1 1 1 The code for this paper is available at https://github.com/jkallini/mission-impossible-language-models . Report issue for preceding element",
    "masked_text": "Overall, our experimental results strongly challenge the claims of Chomsky and others given above, and we believe they pave the way for even deeper discussions of LLMs as models of language learning. At the same time, we recognize that models and humans exhibit fundamental differences, but the extent to which models favor or disfavor natural languages can be influenced by specific architectural decisions (as demonstrated by our findings on tokenization and positional encodings). We hope this paper initiates a new line of work that explores how different model architectures can distinguish between the possible and impossible languages.111The code for this paper is available at https://github.com/jkallini/mission-impossible-language-models.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.p1": {
    "text": "The notion of an impossible human language is elusive and difficult to define, in part due to a lack of consensus on which properties are universal in human language and which properties are \u201cimpossible\u201d (Comrie, 1989 ; Evans and Levinson, 2009 ; Nefdt, 2024 ) .\nFor instance, recursion , or the principle that all languages produce hierarchical syntactic structures via recursive procedures, has been claimed to be a universal property of human language (Chomsky, 1957 , 1965 , 2002 ; Hauser et\u00a0al., 2002 ) .\nHowever, the motivations for recursion have been questioned, with empirical limits on the maximum depth of nested phrases Karlsson ( 2007 ); Jin et\u00a0al. ( 2018 ) and counterevidence from at least one natural language that seems to lack embedded structures Everett ( 2012 ) . Still, if we grant that possible languages are defined by hierarchical, recursive rules, what defines the impossible languages? Moro et\u00a0al. ( 2023 ) claim that the class of impossible languages would use the \u201copposite\u201d type of rules: those based on the linear order of words. Musso et\u00a0al. ( 2003 ) provide a few concrete examples that involve counting word positions to mark features like negation and agreement, and we include languages with similar rules in our set of tested impossible languages. Report issue for preceding element",
    "masked_text": "The notion of an impossible human language is elusive and difficult to define, in part due to a lack of consensus on which properties are universal in human language and which properties are \u201cimpossible\u201d [CITATION]. For instance, recursion, or the principle that all languages produce hierarchical syntactic structures via recursive procedures, has been claimed to be a universal property of human language [CITATION]. However, the motivations for recursion have been questioned, with empirical limits on the maximum depth of nested phrases [CITATION] and counterevidence from at least one natural language that seems to lack embedded structures [CITATION]. Still, if we grant that possible languages are defined by hierarchical, recursive rules, what defines the impossible languages? [CITATION] claim that the class of impossible languages would use the \u201copposite\u201d type of rules: those based on the linear order of words. [CITATION] provide a few concrete examples that involve counting word positions to mark features like negation and agreement, and we include languages with similar rules in our set of tested impossible languages.Report issue for preceding element",
    "citations": [
      {
        "tag": "Moro et\u00a0al. (2023)",
        "title": "Large languages, impossible languages and human brains.",
        "authors": "Andrea Moro, Matteo Greco, and Stefano\u00a0F. Cappa. 2023.",
        "journal": "Cortex, 167:82\u201385."
      },
      {
        "tag": "Musso et\u00a0al. (2003)",
        "title": "Broca\u2019s area and the language instinct.",
        "authors": "Mariacristina Musso, Andrea Moro, Volkmar Glauche, Michel Rijntjes, J\u00fcrgen Reichenbach, Christian B\u00fcchel, and Cornelius Weiller. 2003.",
        "journal": "Nature Neuroscience, 6(7):774\u2013781."
      },
      {
        "tag": "Jin et\u00a0al. (2018)",
        "title": "Depth-bounding is effective: Improvements and evaluation of unsupervised PCFG induction.",
        "authors": "Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler, and Lane Schwartz. 2018.",
        "journal": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2721\u20132731, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Karlsson (2007)",
        "title": "Constraints on multiple center-embedding of clauses.",
        "authors": "Fred Karlsson. 2007.",
        "journal": "Journal of Linguistics, 43(2):365\u2013392."
      },
      {
        "tag": "Evans and Levinson (2009)",
        "title": "The myth of language universals: Language diversity and its importance for cognitive science.",
        "authors": "Nicholas Evans and Stephen\u00a0C Levinson. 2009.",
        "journal": "Behavioral and brain sciences, 32(5):429\u2013448."
      },
      {
        "tag": "Chomsky (1957)",
        "title": "Syntactic Structures.",
        "authors": "Noam Chomsky. 1957.",
        "journal": "De Gruyter Mouton, Berlin, Boston."
      },
      {
        "tag": "Everett (2012)",
        "title": "What does Pirah\u00e3 grammar have to teach us about human language and the mind?",
        "authors": "Daniel\u00a0L. Everett. 2012.",
        "journal": "WIREs Cognitive Science, 3(6):555\u2013563."
      },
      {
        "tag": "Nefdt (2024)",
        "title": "The Philosophy of Theoretical Linguistics: A Contemporary Outlook.",
        "authors": "Ryan\u00a0M. Nefdt. 2024.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Comrie (1989)",
        "title": "Language universals and linguistic typology: Syntax and morphology.",
        "authors": "Bernard Comrie. 1989.",
        "journal": "University of Chicago press."
      },
      {
        "tag": "Chomsky (2002)",
        "title": "On Nature and Language.",
        "authors": "Noam Chomsky. 2002.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Chomsky (1965)",
        "title": "Aspects of the Theory of Syntax.",
        "authors": "Noam Chomsky. 1965.",
        "journal": "The MIT Press."
      },
      {
        "tag": "Hauser et\u00a0al. (2002)",
        "title": "The faculty of language: What is it, who has it, and how did it evolve?",
        "authors": "Marc\u00a0D. Hauser, Noam Chomsky, and W.\u00a0Tecumseh Fitch. 2002.",
        "journal": "Science, 298(5598):1569\u20131579."
      }
    ]
  },
  "S2.SS1.p2": {
    "text": "It is important to also distinguish what is impossible from what is merely typologically marked, such as the word order patterns listed in Greenberg \u2019s ( 1963 ) language universals. Previous work has shown that such word order universals can arise through a language\u2019s optimization of communication efficiency, achieved by balancing complexity and ambiguity Hahn et\u00a0al. ( 2020 ); Futrell and Hahn ( 2022 ) .\nWhile our current exploration does not encompass attested languages, various impossible languages can similarly differ in their information-theoretic complexity, informing the patterns that lie at the boundary between possible and impossible. Report issue for preceding element",
    "masked_text": "It is important to also distinguish what is impossible from what is merely typologically marked, such as the word order patterns listed in [CITATION]\u2019s ( [CITATION]) language universals. Previous work has shown that such word order universals can arise through a language\u2019s optimization of communication efficiency, achieved by balancing complexity and ambiguity [CITATION]. While our current exploration does not encompass attested languages, various impossible languages can similarly differ in their information-theoretic complexity, informing the patterns that lie at the boundary between possible and impossible.Report issue for preceding element",
    "citations": [
      {
        "tag": "Greenberg (1963)",
        "title": "Some universals of grammar with particular reference to the order of meaningful elements.",
        "authors": "Joseph Greenberg. 1963.",
        "journal": "Universals of Language, pages 73\u2013113."
      },
      {
        "tag": "Hahn et\u00a0al. (2020)",
        "title": "Universals of word order reflect optimization of grammars for efficient communication.",
        "authors": "Michael Hahn, Dan Jurafsky, and Richard Futrell. 2020.",
        "journal": "Proceedings of the National Academy of Sciences, 117(5):2347\u20132353."
      },
      {
        "tag": "Futrell and Hahn (2022)",
        "title": "Information theory as a bridge between language function and language form.",
        "authors": "Richard Futrell and Michael Hahn. 2022.",
        "journal": "Frontiers in Communication, 7."
      }
    ]
  },
  "S2.SS2.p1": {
    "text": "The only work cited by Chomsky that investigates neural language models\u2019 ability to learn impossible languages is Mitchell and Bowers 2020 , which finds that recurrent neural networks (RNNs; Elman, 1990 ) trained on various unnatural language constructs, such as reversed sentences and randomized vocabularies, achieve high accuracy on a subject\u2013verb number agreement task.\nOther work turns to more recent Transformer-based language models Vaswani et\u00a0al. ( 2017 ) , observing their sensitivity to word order and phrase structure Alleman et\u00a0al. ( 2021 ); Galke et\u00a0al. ( 2023 ) as well as their surprising ability to learn from syntactic information alone Huang et\u00a0al. ( 2023 ) .\nStudies by Sinha et\u00a0al. ( 2021 ) and Abdou et\u00a0al. ( 2022 ) debate the impact of tokenization, pretraining adjustments, and positional encodings in recovering word order information from shuffled languages.\nFurther investigations into BERT\u2019s Devlin et\u00a0al. ( 2019 ) reliance on word order for grammatical role classification suggest that lexical cues alone may not always be sufficient for good performance ( Papadimitriou et\u00a0al., 2022 ; see also Hessel and Schofield, 2021 ; Pham et\u00a0al., 2021 ). Report issue for preceding element",
    "masked_text": "The only work cited by Chomsky that investigates neural language models\u2019 ability to learn impossible languages is [CITATION], which finds that recurrent neural networks (RNNs; [CITATION]) trained on various unnatural language constructs, such as reversed sentences and randomized vocabularies, achieve high accuracy on a subject\u2013verb number agreement task. Other work turns to more recent Transformer-based language models [CITATION], observing their sensitivity to word order and phrase structure [CITATION] as well as their surprising ability to learn from syntactic information alone [CITATION]. Studies by [CITATION] and [CITATION] debate the impact of tokenization, pretraining adjustments, and positional encodings in recovering word order information from shuffled languages. Further investigations into BERT\u2019s [CITATION] reliance on word order for grammatical role classification suggest that lexical cues alone may not always be sufficient for good performance ( [CITATION]; see also [CITATION]).Report issue for preceding element",
    "citations": [
      {
        "tag": "Elman (1990)",
        "title": "Finding structure in time.",
        "authors": "Jeffrey\u00a0L. Elman. 1990.",
        "journal": "Cognitive Science, 14(2):179\u2013211."
      },
      {
        "tag": "Vaswani et\u00a0al. (2017)",
        "title": "Attention is all you need.",
        "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141\u00a0ukasz Kaiser, and Illia Polosukhin. 2017.",
        "journal": "InAdvances in Neural Information Processing Systems, volume\u00a030. Curran Associates, Inc."
      },
      {
        "tag": "Papadimitriou et\u00a0al. (2022)",
        "title": "When classifying grammatical role, BERT doesn\u2019t care about word order\u2026 except when it matters.",
        "authors": "Isabel Papadimitriou, Richard Futrell, and Kyle Mahowald. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 636\u2013643, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Abdou et\u00a0al. (2022)",
        "title": "Word order does matter and shuffled language models know it.",
        "authors": "Mostafa Abdou, Vinit Ravishankar, Artur Kulmizev, and Anders S\u00f8gaard. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6907\u20136919, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Sinha et\u00a0al. (2021)",
        "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little.",
        "authors": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. 2021.",
        "journal": "InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2888\u20132913, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."
      },
      {
        "tag": "Pham et\u00a0al. (2021)",
        "title": "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks?",
        "authors": "Thang Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021.",
        "journal": "InFindings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1145\u20131160, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Alleman et\u00a0al. (2021)",
        "title": "Syntactic perturbations reveal representational correlates of hierarchical phrase structure in pretrained language models.",
        "authors": "Matteo Alleman, Jonathan Mamou, Miguel A\u00a0Del\u00a0Rio, Hanlin Tang, Yoon Kim, and SueYeon Chung. 2021.",
        "journal": "InProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 263\u2013276, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Mitchell and Bowers (2020)",
        "title": "Priorless recurrent networks learn curiously.",
        "authors": "Jeff Mitchell and Jeffrey Bowers. 2020.",
        "journal": "InProceedings of the 28th International Conference on Computational Linguistics, pages 5147\u20135158, Barcelona, Spain (Online). International Committee on Computational Linguistics."
      },
      {
        "tag": "Hessel and Schofield (2021)",
        "title": "How effective is BERT without word ordering? implications for language understanding and data privacy.",
        "authors": "Jack Hessel and Alexandra Schofield. 2021.",
        "journal": "InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 204\u2013211, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Devlin et\u00a0al. (2019)",
        "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
        "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
        "journal": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics."
      },
      {
        "tag": "Huang et\u00a0al. (2023)",
        "title": "Lexinvariant language models.",
        "authors": "Qian Huang, Eric Zelikman, Sarah\u00a0Li Chen, Yuhuai Wu, Gregory Valiant, and Percy Liang. 2023.",
        "journal": ""
      },
      {
        "tag": "Galke et\u00a0al. (2023)",
        "title": "What makes a language easy to deep-learn?",
        "authors": "Lukas Galke, Yoav Ram, and Limor Raviv. 2023.",
        "journal": ""
      }
    ]
  },
  "S2.SS3.p1": {
    "text": "A related line of research examines the abilities of neural language models to express formal languages, as defined by the Chomsky hierarchy Chomsky ( 1956 , 1959 ) . Human language is considered to be slightly more expressive than context-free languages due to certain syntactic phenomena that interleave constituents Shieber ( 1985 ); Joshi ( 1985 ) .\nPrevious work has shown that RNNs or related models can represent variants of counter and Dyck languages, which are context-free Weiss et\u00a0al. ( 2018 ); Merrill ( 2019 ); Merrill et\u00a0al. ( 2020 ); Hewitt et\u00a0al. ( 2020 ) . 2 2 2 Though counter and Dyck languages are context-free, some of the variants in the cited work are regular. Similar work on Transformer architectures has shown that, while they are theoretically Turing-complete provided arbitrary precision and decoder steps P\u00e9rez et\u00a0al. ( 2021 ) , they cannot empirically model many regular and non-regular languages Hahn ( 2020 ); Ebrahimi et\u00a0al. ( 2020 ); Deletang et\u00a0al. ( 2023 ) . Report issue for preceding element",
    "masked_text": "A related line of research examines the abilities of neural language models to express formal languages, as defined by the Chomsky hierarchy [CITATION]. Human language is considered to be slightly more expressive than context-free languages due to certain syntactic phenomena that interleave constituents [CITATION]. Previous work has shown that RNNs or related models can represent variants of counter and Dyck languages, which are context-free [CITATION].222Though counter and Dyck languages are context-free, some of the variants in the cited work are regular. Similar work on Transformer architectures has shown that, while they are theoretically Turing-complete provided arbitrary precision and decoder steps [CITATION], they cannot empirically model many regular and non-regular languages [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Deletang et\u00a0al. (2023)",
        "title": "Neural networks and the Chomsky hierarchy.",
        "authors": "Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li\u00a0Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro\u00a0A Ortega. 2023.",
        "journal": "InThe Eleventh International Conference on Learning Representations."
      },
      {
        "tag": "Merrill (2019)",
        "title": "Sequential neural networks as automata.",
        "authors": "William Merrill. 2019.",
        "journal": "InProceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 1\u201313, Florence. Association for Computational Linguistics."
      },
      {
        "tag": "Hahn (2020)",
        "title": "Theoretical limitations of self-attention in neural sequence models.",
        "authors": "Michael Hahn. 2020.",
        "journal": "Transactions of the Association for Computational Linguistics, 8:156\u2013171."
      },
      {
        "tag": "Weiss et\u00a0al. (2018)",
        "title": "On the practical computational power of finite precision RNNs for language recognition.",
        "authors": "Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018.",
        "journal": "InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740\u2013745, Melbourne, Australia. Association for Computational Linguistics."
      },
      {
        "tag": "Joshi (1985)",
        "title": "Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?, Studies in Natural Language Processing, page 206\u2013250. Cambridge University Press.",
        "authors": "Aravind\u00a0K. Joshi. 1985.",
        "journal": ""
      },
      {
        "tag": "Chomsky (1956)",
        "title": "Three models for the description of language.",
        "authors": "Noam Chomsky. 1956.",
        "journal": "IRE Transactions on Information Theory, 2(3):113\u2013124."
      },
      {
        "tag": "Merrill et\u00a0al. (2020)",
        "title": "A formal hierarchy of RNN architectures.",
        "authors": "William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah\u00a0A. Smith, and Eran Yahav. 2020.",
        "journal": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 443\u2013459, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Hewitt et\u00a0al. (2020)",
        "title": "RNNs can generate bounded hierarchical languages with optimal memory.",
        "authors": "John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher\u00a0D. Manning. 2020.",
        "journal": "InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1978\u20132010, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Shieber (1985)",
        "title": "Evidence against the context-freeness of natural language.",
        "authors": "Stuart\u00a0M. Shieber. 1985.",
        "journal": "Linguistics and Philosophy, 8(3):333\u2013343."
      },
      {
        "tag": "Chomsky (1959)",
        "title": "On certain formal properties of grammars.",
        "authors": "Noam Chomsky. 1959.",
        "journal": "Information and Control, 2(2):137\u2013167."
      },
      {
        "tag": "Ebrahimi et\u00a0al. (2020)",
        "title": "How can self-attention networks recognize Dyck-n languages?",
        "authors": "Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020.",
        "journal": "InFindings of the Association for Computational Linguistics: EMNLP 2020, pages 4301\u20134306, Online. Association for Computational Linguistics."
      },
      {
        "tag": "P\u00e9rez et\u00a0al. (2021)",
        "title": "Attention is Turing-complete.",
        "authors": "Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. 2021.",
        "journal": "Journal of Machine Learning Research, 22(75):1\u201335."
      }
    ]
  },
  "S2.SS3.p2": {
    "text": "The inability of Transformer-based language models to learn more complex languages in the Chomsky hierarchy seems surprising, given their impressive performance on natural language. This could be interpreted as evidence that theoretically weak computational models are sufficient for expressing human language. Alternatively, Transformer-based models can be augmented to have inductive biases for nested, hierarchical structures through architecture changes, like the addition of a stack component Hao et\u00a0al. ( 2018 ); Murty et\u00a0al. ( 2023 ) , or data-centered approaches, like structural pretraining Papadimitriou and Jurafsky ( 2023 ) . Report issue for preceding element",
    "masked_text": "The inability of Transformer-based language models to learn more complex languages in the Chomsky hierarchy seems surprising, given their impressive performance on natural language. This could be interpreted as evidence that theoretically weak computational models are sufficient for expressing human language. Alternatively, Transformer-based models can be augmented to have inductive biases for nested, hierarchical structures through architecture changes, like the addition of a stack component [CITATION], or data-centered approaches, like structural pretraining [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Hao et\u00a0al. (2018)",
        "title": "Context-free transductions with neural stacks.",
        "authors": "Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, and Simon Mendelsohn. 2018.",
        "journal": "InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 306\u2013315, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Papadimitriou and Jurafsky (2023)",
        "title": "Injecting structural hints: Using language models to study inductive biases in language learning.",
        "authors": "Isabel Papadimitriou and Dan Jurafsky. 2023.",
        "journal": ""
      },
      {
        "tag": "Murty et\u00a0al. (2023)",
        "title": "Pushdown layers: Encoding recursive structure in transformer language models.",
        "authors": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher\u00a0D. Manning. 2023.",
        "journal": ""
      }
    ]
  },
  "S3.p1": {
    "text": "Core to our experiments are the set of impossible languages we synthesize. In constructing these artificial counterfactual languages, we consider their information-theoretic attributes relevant to machine learning, such as entropy rate, as well as their formal linguistic characteristics, such as adherence to hierarchical grammatical structures. We believe that our choice of languages broadly spans the impossibility continuum hypothesized in Figure 1 . Report issue for preceding element",
    "masked_text": "Core to our experiments are the set of impossible languages we synthesize. In constructing these artificial counterfactual languages, we consider their information-theoretic attributes relevant to machine learning, such as entropy rate, as well as their formal linguistic characteristics, such as adherence to hierarchical grammatical structures. We believe that our choice of languages broadly spans the impossibility continuum hypothesized in Figure 1.Report issue for preceding element",
    "citations": []
  },
  "S3.p2": {
    "text": "Concretely, we specify impossible languages by defining perturbation functions of English sentences. These perturbation functions map English input sentences to sequences of tokens. We categorize our languages into three classes: *Shuffle , *Reverse , and *Hop , defined in the next subsections. Each class has one control language that represents unaltered English, or a pattern that is very similar to English. Table 1 provides examples of perturbed sentences in each language. Report issue for preceding element",
    "masked_text": "Concretely, we specify impossible languages by defining perturbation functions of English sentences. These perturbation functions map English input sentences to sequences of tokens. We categorize our languages into three classes: *Shuffle, *Reverse, and *Hop, defined in the next subsections. Each class has one control language that represents unaltered English, or a pattern that is very similar to English. Table 1 provides examples of perturbed sentences in each language.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.p1": {
    "text": "The first set of impossible languages, which we call the *Shuffle languages, involve different shuffles of tokenized English sentences. Report issue for preceding element",
    "masked_text": "The first set of impossible languages, which we call the *Shuffle languages, involve different shuffles of tokenized English sentences.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.p2": {
    "text": "1. NoShuffle : The input sentence is tokenized, and the token sequence is unaltered. This language is simply English, used for comparison with other *Shuffle languages. Report issue for preceding element 2. NondeterministicShuffle : The tokenized input sentence is randomly shuffled. A different random shuffle is used for each input sentence, with no consistency across inputs. Report issue for preceding element 3. DeterministicShuffle ( s \ud835\udc60 s italic_s ) : The tokenized input sentence is deterministically shuffled based on the length of the token sequence. For example, all token sequences of length\u00a05 are shuffled in the same order. We create several languages by varying the random seed s \ud835\udc60 s italic_s that produces the shuffle. Report issue for preceding element 4. LocalShuffle ( w \ud835\udc64 w italic_w ) : The tokenized input sentence is deterministically shuffled in local windows of a fixed size w \ud835\udc64 w italic_w . We create several languages by varying w \ud835\udc64 w italic_w . Report issue for preceding element 5. EvenOddShuffle : The tokenized input sentence is reordered such that all even-indexed tokens appear first, followed by all odd-indexed tokens. Report issue for preceding element",
    "masked_text": "1. NoShuffle: The input sentence is tokenized, and the token sequence is unaltered. This language is simply English, used for comparison with other *Shuffle languages.Report issue for preceding element 2. NondeterministicShuffle: The tokenized input sentence is randomly shuffled. A different random shuffle is used for each input sentence, with no consistency across inputs.Report issue for preceding element 3. DeterministicShuffle(s\ud835\udc60sitalic_s): The tokenized input sentence is deterministically shuffled based on the length of the token sequence. For example, all token sequences of length 5 are shuffled in the same order. We create several languages by varying the random seed s\ud835\udc60sitalic_s that produces the shuffle.Report issue for preceding element 4. LocalShuffle(w\ud835\udc64witalic_w): The tokenized input sentence is deterministically shuffled in local windows of a fixed size w\ud835\udc64witalic_w. We create several languages by varying w\ud835\udc64witalic_w.Report issue for preceding element 5. EvenOddShuffle: The tokenized input sentence is reordered such that all even-indexed tokens appear first, followed by all odd-indexed tokens.Report issue for preceding element",
    "citations": []
  },
  "S3.I1.i1.p1": {
    "text": "NoShuffle : The input sentence is tokenized, and the token sequence is unaltered. This language is simply English, used for comparison with other *Shuffle languages. Report issue for preceding element",
    "masked_text": "NoShuffle: The input sentence is tokenized, and the token sequence is unaltered. This language is simply English, used for comparison with other *Shuffle languages.Report issue for preceding element",
    "citations": []
  },
  "S3.I1.i2.p1": {
    "text": "NondeterministicShuffle : The tokenized input sentence is randomly shuffled. A different random shuffle is used for each input sentence, with no consistency across inputs. Report issue for preceding element",
    "masked_text": "NondeterministicShuffle: The tokenized input sentence is randomly shuffled. A different random shuffle is used for each input sentence, with no consistency across inputs.Report issue for preceding element",
    "citations": []
  },
  "S3.I1.i3.p1": {
    "text": "DeterministicShuffle ( s \ud835\udc60 s italic_s ) : The tokenized input sentence is deterministically shuffled based on the length of the token sequence. For example, all token sequences of length\u00a05 are shuffled in the same order. We create several languages by varying the random seed s \ud835\udc60 s italic_s that produces the shuffle. Report issue for preceding element",
    "masked_text": "DeterministicShuffle(s\ud835\udc60sitalic_s): The tokenized input sentence is deterministically shuffled based on the length of the token sequence. For example, all token sequences of length 5 are shuffled in the same order. We create several languages by varying the random seed s\ud835\udc60sitalic_s that produces the shuffle.Report issue for preceding element",
    "citations": []
  },
  "S3.I1.i4.p1": {
    "text": "LocalShuffle ( w \ud835\udc64 w italic_w ) : The tokenized input sentence is deterministically shuffled in local windows of a fixed size w \ud835\udc64 w italic_w . We create several languages by varying w \ud835\udc64 w italic_w . Report issue for preceding element",
    "masked_text": "LocalShuffle(w\ud835\udc64witalic_w): The tokenized input sentence is deterministically shuffled in local windows of a fixed size w\ud835\udc64witalic_w. We create several languages by varying w\ud835\udc64witalic_w.Report issue for preceding element",
    "citations": []
  },
  "S3.I1.i5.p1": {
    "text": "EvenOddShuffle : The tokenized input sentence is reordered such that all even-indexed tokens appear first, followed by all odd-indexed tokens. Report issue for preceding element",
    "masked_text": "EvenOddShuffle: The tokenized input sentence is reordered such that all even-indexed tokens appear first, followed by all odd-indexed tokens.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.p3": {
    "text": "The random shuffling function that generates the NondeterministicShuffle language is irreversible, resulting in sentences that are purely bags of words\u2014any structural information in the original linguistic signal is irretrievable.\nWhile the DeterministicShuffle languages are created using a reversible perturbation function, this function operates in an entirely non-linguistic manner; words are ordered based solely on the random seed and sentence length, without considerations for linguistic features or information locality \u2014the property that, when parts of text predict each other, they are often close together (Futrell, 2019 ; Mansfield and Kemp, 2023 ) . This method is arguably even less humanly feasible than NondeterministicShuffle , as it relies on an arbitrarily complex yet consistent rule to determine word order. 3 3 3 Even in the imaginable case of a language with completely free word order, it seems extremely unlikely that this freedom would be totally insensitive to any clause boundaries while the language otherwise looks morphologically like English does. It thus seems very safe to assume that our NondeterministicShuffle language counts as impossible. The question of ranking these two families of languages in the impossibility continuum probes at the definition of impossibility and whether reversibility to an attested language like English is a relevant quantity. Report issue for preceding element",
    "masked_text": "The random shuffling function that generates the NondeterministicShuffle language is irreversible, resulting in sentences that are purely bags of words\u2014any structural information in the original linguistic signal is irretrievable. While the DeterministicShuffle languages are created using a reversible perturbation function, this function operates in an entirely non-linguistic manner; words are ordered based solely on the random seed and sentence length, without considerations for linguistic features or information locality\u2014the property that, when parts of text predict each other, they are often close together [CITATION]. This method is arguably even less humanly feasible than NondeterministicShuffle, as it relies on an arbitrarily complex yet consistent rule to determine word order.333Even in the imaginable case of a language with completely free word order, it seems extremely unlikely that this freedom would be totally insensitive to any clause boundaries while the language otherwise looks morphologically like English does. It thus seems very safe to assume that our NondeterministicShuffle language counts as impossible. The question of ranking these two families of languages in the impossibility continuum probes at the definition of impossibility and whether reversibility to an attested language like English is a relevant quantity.Report issue for preceding element",
    "citations": [
      {
        "tag": "Mansfield and Kemp (2023)",
        "title": "The emergence of grammatical structure from inter-predictability.",
        "authors": "John Mansfield and Charles Kemp. 2023.",
        "journal": ""
      },
      {
        "tag": "Futrell (2019)",
        "title": "Information-theoretic locality properties of natural language.",
        "authors": "Richard Futrell. 2019.",
        "journal": "InProceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019), pages 2\u201315, Paris, France. Association for Computational Linguistics."
      }
    ]
  },
  "S3.SS1.p4": {
    "text": "The LocalShuffle languages offer a finer-grained testbed for the importance of information locality, since we can observe the effects of different window sizes. Finally, EvenOddShuffle also manipulates locality, but interestingly preserves part of the linear word order of English while introducing new long-distance dependencies. Report issue for preceding element",
    "masked_text": "The LocalShuffle languages offer a finer-grained testbed for the importance of information locality, since we can observe the effects of different window sizes. Finally, EvenOddShuffle also manipulates locality, but interestingly preserves part of the linear word order of English while introducing new long-distance dependencies.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p1": {
    "text": "The *Reverse impossible languages involve reversals of all or part of input sentences. Report issue for preceding element",
    "masked_text": "The *Reverse impossible languages involve reversals of all or part of input sentences.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p2": {
    "text": "1. NoReverse : The input sentence is tokenized, and a special marker token R is inserted at a random position in the token list. Like NoShuffle , this language is most similar to English. We use it for comparison with other *Reverse languages. Report issue for preceding element 2. PartialReverse : The input sentence is tokenized, a special marker token R is inserted at a random position in the list of tokens, and the following tokens are reversed. Report issue for preceding element 3. FullReverse : The input sentence is tokenized, a special marker token R is inserted at a random position in the token list, and all tokens are reversed. Report issue for preceding element",
    "masked_text": "1. NoReverse: The input sentence is tokenized, and a special marker token R is inserted at a random position in the token list. Like NoShuffle, this language is most similar to English. We use it for comparison with other *Reverse languages.Report issue for preceding element 2. PartialReverse: The input sentence is tokenized, a special marker token R is inserted at a random position in the list of tokens, and the following tokens are reversed.Report issue for preceding element 3. FullReverse: The input sentence is tokenized, a special marker token R is inserted at a random position in the token list, and all tokens are reversed.Report issue for preceding element",
    "citations": []
  },
  "S3.I2.i1.p1": {
    "text": "NoReverse : The input sentence is tokenized, and a special marker token R is inserted at a random position in the token list. Like NoShuffle , this language is most similar to English. We use it for comparison with other *Reverse languages. Report issue for preceding element",
    "masked_text": "NoReverse: The input sentence is tokenized, and a special marker token R is inserted at a random position in the token list. Like NoShuffle, this language is most similar to English. We use it for comparison with other *Reverse languages.Report issue for preceding element",
    "citations": []
  },
  "S3.I2.i2.p1": {
    "text": "PartialReverse : The input sentence is tokenized, a special marker token R is inserted at a random position in the list of tokens, and the following tokens are reversed. Report issue for preceding element",
    "masked_text": "PartialReverse: The input sentence is tokenized, a special marker token R is inserted at a random position in the list of tokens, and the following tokens are reversed.Report issue for preceding element",
    "citations": []
  },
  "S3.I2.i3.p1": {
    "text": "FullReverse : The input sentence is tokenized, a special marker token R is inserted at a random position in the token list, and all tokens are reversed. Report issue for preceding element",
    "masked_text": "FullReverse: The input sentence is tokenized, a special marker token R is inserted at a random position in the token list, and all tokens are reversed.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p3": {
    "text": "The PartialReverse language is inspired by the experiments of Mitchell and Bowers ( 2020 ) on partially reversed English data, though our experiments are not a direct replication, since we use a different model architecture and dataset. FullReverse may seem like a plausible language syntactically, but higher-level linguistic concepts like anaphora would be highly disrupted.\nThe R tokens are placed at the same positions across the data in all *Reverse languages to control for the entropy introduced by their random placement. Report issue for preceding element",
    "masked_text": "The PartialReverse language is inspired by the experiments of [CITATION] on partially reversed English data, though our experiments are not a direct replication, since we use a different model architecture and dataset. FullReverse may seem like a plausible language syntactically, but higher-level linguistic concepts like anaphora would be highly disrupted. The R tokens are placed at the same positions across the data in all *Reverse languages to control for the entropy introduced by their random placement.Report issue for preceding element",
    "citations": [
      {
        "tag": "Mitchell and Bowers (2020)",
        "title": "Priorless recurrent networks learn curiously.",
        "authors": "Jeff Mitchell and Jeffrey Bowers. 2020.",
        "journal": "InProceedings of the 28th International Conference on Computational Linguistics, pages 5147\u20135158, Barcelona, Spain (Online). International Committee on Computational Linguistics."
      }
    ]
  },
  "S3.SS3.p1": {
    "text": "The *Hop languages perturb verb inflection with counting rules. Report issue for preceding element",
    "masked_text": "The *Hop languages perturb verb inflection with counting rules.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.p2": {
    "text": "1. NoHop : All 3rd-person present tense verbs in the input sentence are lemmatized, and the sentence is tokenized. For each 3rd-person present tense verb, a special marker representing the verb\u2019s number and tense is placed right after the lemmatized verb. Singular verbs are marked with a special token S , and plural verbs are marked with P . Like the other control languages, NoHop has a pattern that is most similar to English. Report issue for preceding element 2. TokenHop : Identical transformation to NoHop , but the special number/tense markers are placed 4 tokens after the verb. Report issue for preceding element 3. WordHop : Identical transformation to NoHop and TokenHop , but the special number/tense markers are placed 4 words after the verb, skipping punctuation. Report issue for preceding element",
    "masked_text": "1. NoHop: All 3rd-person present tense verbs in the input sentence are lemmatized, and the sentence is tokenized. For each 3rd-person present tense verb, a special marker representing the verb\u2019s number and tense is placed right after the lemmatized verb. Singular verbs are marked with a special token S, and plural verbs are marked with P. Like the other control languages, NoHop has a pattern that is most similar to English.Report issue for preceding element 2. TokenHop: Identical transformation to NoHop, but the special number/tense markers are placed 4 tokens after the verb.Report issue for preceding element 3. WordHop: Identical transformation to NoHop and TokenHop, but the special number/tense markers are placed 4 words after the verb, skipping punctuation.Report issue for preceding element",
    "citations": []
  },
  "S3.I3.i1.p1": {
    "text": "NoHop : All 3rd-person present tense verbs in the input sentence are lemmatized, and the sentence is tokenized. For each 3rd-person present tense verb, a special marker representing the verb\u2019s number and tense is placed right after the lemmatized verb. Singular verbs are marked with a special token S , and plural verbs are marked with P . Like the other control languages, NoHop has a pattern that is most similar to English. Report issue for preceding element",
    "masked_text": "NoHop: All 3rd-person present tense verbs in the input sentence are lemmatized, and the sentence is tokenized. For each 3rd-person present tense verb, a special marker representing the verb\u2019s number and tense is placed right after the lemmatized verb. Singular verbs are marked with a special token S, and plural verbs are marked with P. Like the other control languages, NoHop has a pattern that is most similar to English.Report issue for preceding element",
    "citations": []
  },
  "S3.I3.i2.p1": {
    "text": "TokenHop : Identical transformation to NoHop , but the special number/tense markers are placed 4 tokens after the verb. Report issue for preceding element",
    "masked_text": "TokenHop: Identical transformation to NoHop, but the special number/tense markers are placed 4 tokens after the verb.Report issue for preceding element",
    "citations": []
  },
  "S3.I3.i3.p1": {
    "text": "WordHop : Identical transformation to NoHop and TokenHop , but the special number/tense markers are placed 4 words after the verb, skipping punctuation. Report issue for preceding element",
    "masked_text": "WordHop: Identical transformation to NoHop and TokenHop, but the special number/tense markers are placed 4 words after the verb, skipping punctuation.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.p3": {
    "text": "These languages specifically investigate GPT-2\u2019s ability to learn grammar rules that involve counting the positions of words or tokens. Report issue for preceding element",
    "masked_text": "These languages specifically investigate GPT-2\u2019s ability to learn grammar rules that involve counting the positions of words or tokens.Report issue for preceding element",
    "citations": []
  },
  "S4.p1": {
    "text": "We run several experiments to assess GPT-2\u2019s learning of our impossible languages. Our first experiment ( Section 4.2 ) uses perplexities as a general evaluation to compare how well each impossible language model has learned its own perturbed language and see whether this reflects the hypothesized impossibility continuum. In our second and third experiments, we conduct a closer examination of the *Hop languages. Given that their count-based verb marking rules appear to be the least clearly implausible among our proposed languages, we focus on examining these rules specifically through targeted assessments using surprisal theory ( Section 4.3 ). Finally, we dive deeper into the mechanisms each *Hop model uses to predict their respective verb marking rules using causal abstraction analysis ( Section 4.4 ). For all evaluations, we run tests on several model checkpoints to observe the learning process over intervals of training steps. 4 4 4 We also conduct a constituency probing experiment to test effects on GPT-2\u2019s implicit understanding of syntax, with minimal observed differences among models (see Appendix D ). Report issue for preceding element",
    "masked_text": "We run several experiments to assess GPT-2\u2019s learning of our impossible languages. Our first experiment (Section 4.2) uses perplexities as a general evaluation to compare how well each impossible language model has learned its own perturbed language and see whether this reflects the hypothesized impossibility continuum. In our second and third experiments, we conduct a closer examination of the *Hop languages. Given that their count-based verb marking rules appear to be the least clearly implausible among our proposed languages, we focus on examining these rules specifically through targeted assessments using surprisal theory (Section 4.3). Finally, we dive deeper into the mechanisms each *Hop model uses to predict their respective verb marking rules using causal abstraction analysis (Section 4.4). For all evaluations, we run tests on several model checkpoints to observe the learning process over intervals of training steps.444 We also conduct a constituency probing experiment to test effects on GPT-2\u2019s implicit understanding of syntax, with minimal observed differences among models (see Appendix D).Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.p1": {
    "text": "For each impossible language, we apply its perturbation function to each sentence of the BabyLM dataset Warstadt et\u00a0al. ( 2023 ) to create a transformed dataset. Appendix A provides details on preprocessing and formatting, and describes the language-specific filtering needed to achieve the criteria that define each language. Report issue for preceding element",
    "masked_text": "For each impossible language, we apply its perturbation function to each sentence of the BabyLM dataset [CITATION] to create a transformed dataset. Appendix A provides details on preprocessing and formatting, and describes the language-specific filtering needed to achieve the criteria that define each language.Report issue for preceding element",
    "citations": [
      {
        "tag": "Warstadt et\u00a0al. (2023)",
        "title": "Call for papers \u2013 the BabyLM challenge: Sample-efficient pretraining on a developmentally plausible corpus.",
        "authors": "Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang. 2023.",
        "journal": ""
      }
    ]
  },
  "S4.SS1.p2": {
    "text": "We train standard GPT-2 small models Radford et\u00a0al. ( 2018 , 2019 ) on each impossible language. To produce confidence intervals for our experiments, we train 5 sets of models for each language using different random seeds, which affect the model parameter initialization and dataset shuffling during training. Training and model hyperparameter choices are detailed in Appendix B .\nThe primary set of GPT-2 models we train have absolute positional encodings. We also train a set of GPT-2 small models with an architecture in which the positional encodings are removed, so that the models\u2019 only notion of word order is derived from GPT-2\u2019s causal language modeling learning objective Kazemnejad et\u00a0al. ( 2023 ) . Results for these additional experiments supported our main findings on the unaltered GPT-2 architecture. These results are provided in Appendix C . Report issue for preceding element",
    "masked_text": "We train standard GPT-2 small models [CITATION] on each impossible language. To produce confidence intervals for our experiments, we train 5 sets of models for each language using different random seeds, which affect the model parameter initialization and dataset shuffling during training. Training and model hyperparameter choices are detailed in Appendix B. The primary set of GPT-2 models we train have absolute positional encodings. We also train a set of GPT-2 small models with an architecture in which the positional encodings are removed, so that the models\u2019 only notion of word order is derived from GPT-2\u2019s causal language modeling learning objective [CITATION]. Results for these additional experiments supported our main findings on the unaltered GPT-2 architecture. These results are provided in Appendix C.Report issue for preceding element",
    "citations": [
      {
        "tag": "Radford et\u00a0al. (2019)",
        "title": "Language models are unsupervised multitask learners.",
        "authors": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
        "journal": "Ms, OpenAI."
      },
      {
        "tag": "Radford et\u00a0al. (2018)",
        "title": "Improving language understanding by generative pre-training.",
        "authors": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.",
        "journal": "Ms, OpenAI."
      },
      {
        "tag": "Kazemnejad et\u00a0al. (2023)",
        "title": "The impact of positional encoding on length generalization in transformers.",
        "authors": "Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan\u00a0Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023.",
        "journal": "arXiv preprint arXiv:2305.19466."
      }
    ]
  },
  "S4.SS2.p1": {
    "text": "We train GPT-2 models on all of the languages described in Table 1 , and evaluate each model\u2019s perplexities on a test set over the course of training. Test perplexities provide a general metric for the extent to which a model has learned a language. Report issue for preceding element",
    "masked_text": "We train GPT-2 models on all of the languages described in Table 1, and evaluate each model\u2019s perplexities on a test set over the course of training. Test perplexities provide a general metric for the extent to which a model has learned a language.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.SSS0.Px1.p1": {
    "text": "We sample 10K sentences from the BabyLM test set and perturb this sample for each impossible language. For a given impossible language model, we report the geometric mean of the individual sentence perplexities in the corresponding test sample. Report issue for preceding element",
    "masked_text": "We sample 10K sentences from the BabyLM test set and perturb this sample for each impossible language. For a given impossible language model, we report the geometric mean of the individual sentence perplexities in the corresponding test sample.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.SSS0.Px2.p1": {
    "text": "Models trained on possible languages will achieve lower average perplexities more quickly (as measured in training steps) than those trained on impossible languages. Report issue for preceding element",
    "masked_text": "Models trained on possible languages will achieve lower average perplexities more quickly (as measured in training steps) than those trained on impossible languages.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.SSS0.Px3.p1": {
    "text": "Our results are in Figure 2 .\nThere are clear distinctions between model perplexities after about 500 training steps. First considering the *Shuffle models, the NondeterministicShuffle model has the highest perplexities, followed by the three DeterministicShuffle models, indicating that GPT-2 is better at learning shuffling patterns when they are deterministic, invertible functions. 5 5 5 This result is also supported by separate evaluations of each DeterministicShuffle model on test data from other shuffles (see Appendix E ). Each model has lower perplexities on its own deterministic shuffle. The prevalence of certain sentence lengths in the corpus could also limit the variety of sentence shuffles in the DeterministicShuffle languages, potentially resulting in similarly functioning words frequently occupying the same token positions, thus increasing their predictability. Report issue for preceding element",
    "masked_text": "Our results are in Figure 2. There are clear distinctions between model perplexities after about 500 training steps. First considering the *Shuffle models, the NondeterministicShuffle model has the highest perplexities, followed by the three DeterministicShuffle models, indicating that GPT-2 is better at learning shuffling patterns when they are deterministic, invertible functions.555This result is also supported by separate evaluations of each DeterministicShuffle model on test data from other shuffles (see Appendix E). Each model has lower perplexities on its own deterministic shuffle. The prevalence of certain sentence lengths in the corpus could also limit the variety of sentence shuffles in the DeterministicShuffle languages, potentially resulting in similarly functioning words frequently occupying the same token positions, thus increasing their predictability.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.SSS0.Px3.p2": {
    "text": "Following the sentence-level shuffles, the next models in the order of decreasing perplexity are the three LocalShuffle models, with smaller window sizes having lower perplexities. LocalShuffle ( w = 3 ) \ud835\udc64 3 (w=3) ( italic_w = 3 ) and EvenOddShuffle have perplexities closest to the NoShuffle model (which represents unaltered English), but NoShuffle consistently has the lowest perplexities throughout the training process. Report issue for preceding element",
    "masked_text": "Following the sentence-level shuffles, the next models in the order of decreasing perplexity are the three LocalShuffle models, with smaller window sizes having lower perplexities. LocalShuffle(w=3)\ud835\udc643(w=3)( italic_w = 3 ) and EvenOddShuffle have perplexities closest to the NoShuffle model (which represents unaltered English), but NoShuffle consistently has the lowest perplexities throughout the training process.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.SSS0.Px3.p3": {
    "text": "Compared to the *Shuffle models, the experimental *Reverse models have perplexities that are much closer to the NoReverse model, and PartialReverse is slightly better than FullReverse . For the *Hop languages, their respective control model again has the lowest perplexities, although differences among the models are quite minimal.\nThis warrants our deep-dive into the particular verb marking patterns for this set of models. Report issue for preceding element",
    "masked_text": "Compared to the *Shuffle models, the experimental *Reverse models have perplexities that are much closer to the NoReverse model, and PartialReverse is slightly better than FullReverse. For the *Hop languages, their respective control model again has the lowest perplexities, although differences among the models are quite minimal. This warrants our deep-dive into the particular verb marking patterns for this set of models.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.p1": {
    "text": "In Experiment 1, we show that impossible languages are harder for GPT-2 to learn. However, perplexity is a coarse-grained metric of language learning, and the question remains: do language models learn natural grammatical structures better than impossible grammars? Report issue for preceding element",
    "masked_text": "In Experiment 1, we show that impossible languages are harder for GPT-2 to learn. However, perplexity is a coarse-grained metric of language learning, and the question remains: do language models learn natural grammatical structures better than impossible grammars?Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.p2": {
    "text": "The structure of the *Hop languages invites a\nfiner-grained evaluation of their verb marking rules. We use surprisals to measure how well each *Hop model can predict the placement of its\nverb marker tokens, S and P . The surprisal S \u2062 ( w i ) \ud835\udc46 subscript \ud835\udc64 \ud835\udc56 S(w_{i}) italic_S ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of a word w i subscript \ud835\udc64 \ud835\udc56 w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the negative log probability of w i subscript \ud835\udc64 \ud835\udc56 w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given\nthe context words w 1 , \u2026 , w i \u2212 1 subscript \ud835\udc64 1 \u2026 subscript \ud835\udc64 \ud835\udc56 1 w_{1},\\ldots,w_{i-1} italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_w start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT that precede it: S \u2062 ( w i ) = \u2212 log 2 \u2061 p \u2062 ( w i | w 1 , \u2026 , w i \u2212 1 ) \ud835\udc46 subscript \ud835\udc64 \ud835\udc56 subscript 2 \ud835\udc5d conditional subscript \ud835\udc64 \ud835\udc56 subscript \ud835\udc64 1 \u2026 subscript \ud835\udc64 \ud835\udc56 1 S(w_{i})=-\\log_{2}p(w_{i}|w_{1},\\ldots,w_{i-1}) italic_S ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_w start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) .\nSurprisals have been used as acceptability judgments from neural language models to probe for their processing of syntactic information Wilcox et\u00a0al. ( 2018 ); Futrell et\u00a0al. ( 2019 ); Hu et\u00a0al. ( 2020 ); Wilcox et\u00a0al. ( 2023 ) and have been shown to correlate with human sentence processing difficulty Hale ( 2001 ); Levy ( 2008 ) . Report issue for preceding element",
    "masked_text": "The structure of the *Hop languages invites a finer-grained evaluation of their verb marking rules. We use surprisals to measure how well each *Hop model can predict the placement of its verb marker tokens, S and P. The surprisal S\u2062(wi)\ud835\udc46subscript\ud835\udc64\ud835\udc56S(w_{i})italic_S ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of a word wisubscript\ud835\udc64\ud835\udc56w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the negative log probability of wisubscript\ud835\udc64\ud835\udc56w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT given the context words w1,\u2026,wi\u22121subscript\ud835\udc641\u2026subscript\ud835\udc64\ud835\udc561w_{1},\\ldots,w_{i-1}italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_w start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT that precede it: S\u2062(wi)=\u2212log2\u2061p\u2062(wi|w1,\u2026,wi\u22121)\ud835\udc46subscript\ud835\udc64\ud835\udc56subscript2\ud835\udc5dconditionalsubscript\ud835\udc64\ud835\udc56subscript\ud835\udc641\u2026subscript\ud835\udc64\ud835\udc561S(w_{i})=-\\log_{2}p(w_{i}|w_{1},\\ldots,w_{i-1})italic_S ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_w start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). Surprisals have been used as acceptability judgments from neural language models to probe for their processing of syntactic information [CITATION] and have been shown to correlate with human sentence processing difficulty [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Wilcox et\u00a0al. (2023)",
        "title": "Using computational models to test syntactic learnability.",
        "authors": "Ethan\u00a0Gotlieb Wilcox, Richard Futrell, and Roger Levy. 2023.",
        "journal": "Linguistic Inquiry, pages 1\u201344."
      },
      {
        "tag": "Wilcox et\u00a0al. (2018)",
        "title": "What do RNN language models learn about filler\u2013gap dependencies?",
        "authors": "Ethan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018.",
        "journal": "InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 211\u2013221, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Levy (2008)",
        "title": "Expectation-based syntactic comprehension.",
        "authors": "Roger Levy. 2008.",
        "journal": "Cognition, 106(3):1126\u20131177."
      },
      {
        "tag": "Hale (2001)",
        "title": "A probabilistic Earley parser as a psycholinguistic model.",
        "authors": "John Hale. 2001.",
        "journal": "InSecond Meeting of the North American Chapter of the Association for Computational Linguistics."
      },
      {
        "tag": "Hu et\u00a0al. (2020)",
        "title": "A systematic assessment of syntactic generalization in neural language models.",
        "authors": "Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020.",
        "journal": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1725\u20131744, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Futrell et\u00a0al. (2019)",
        "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state.",
        "authors": "Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019.",
        "journal": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 32\u201342, Minneapolis, Minnesota. Association for Computational Linguistics."
      }
    ]
  },
  "S4.SS3.SSS0.Px1.p1": {
    "text": "To test the *Hop models\u2019 sensitivity to marker placement, we conduct two tests on a sample of 10K sentences extracted from the BabyLM dataset containing the verb marker tokens ( S or P ).\nAs an example, consider the following pair of sentences for the NoHop language shown in 4.3 . Report issue for preceding element",
    "masked_text": "To test the *Hop models\u2019 sensitivity to marker placement, we conduct two tests on a sample of 10K sentences extracted from the BabyLM dataset containing the verb marker tokens (S or P). As an example, consider the following pair of sentences for the NoHop language shown in 4.3.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p2": {
    "text": "\\ex . \\a . He clean S his very messy books he lf . .\u0331 * He clean __ his very messy books he lf . Report issue for preceding element",
    "masked_text": "\\ex . \\a. He clean S his very messy books he lf . .\u0331 *He clean__ his very messy books he lf .Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p3": {
    "text": "Sentence 4.3 is an example in the NoHop language, and 4.3 is an ungrammatical counterfactual in which the marker token does not appear. Report issue for preceding element",
    "masked_text": "Sentence 4.3 is an example in the NoHop language, and 4.3 is an ungrammatical counterfactual in which the marker token does not appear.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p4": {
    "text": "In the first test, we compare the average surprisals of the marker tokens across the three *Hop languages, using grammatical examples like 4.3 .\nIn the case of 4.3 , the marker is singular, and its surprisal S \u2062 ( S ) \ud835\udc46 S S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}%\n}\\,}) italic_S ( typewriter_S ) is defined as: Report issue for preceding element S \u2062 ( S ) = \u2212 log 2 \u2061 p \u2062 ( S | He clean ) \ud835\udc46 S subscript 2 \ud835\udc5d S | He clean S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}%\n}\\,})=-\\log_{2}p(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color%\n{\\framebox{{S}}}\\,$|$\\,\\definecolor{hlcolor}{HTML}{FDFD95}\\lx@texthl@color{He}%\n\\,\\definecolor{hlcolor}{HTML}{FDE4CF}\\lx@texthl@color{ clean}\\,}) italic_S ( typewriter_S ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( typewriter_S | typewriter_He typewriter_clean ) We average this surprisal value for instances of S or P in the test sample. Report issue for preceding element",
    "masked_text": "In the first test, we compare the average surprisals of the marker tokens across the three *Hop languages, using grammatical examples like 4.3. In the case of 4.3, the marker is singular, and its surprisal S\u2062( S )\ud835\udc46 S S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}% }\\,})italic_S ( typewriter_S ) is defined as:Report issue for preceding element S\u2062( S )=\u2212log2\u2061p\u2062( S | He clean )\ud835\udc46 S subscript2\ud835\udc5d S | He clean S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}% }\\,})=-\\log_{2}p(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color% {\\framebox{{S}}}\\,$|$\\,\\definecolor{hlcolor}{HTML}{FDFD95}\\lx@texthl@color{He}% \\,\\definecolor{hlcolor}{HTML}{FDE4CF}\\lx@texthl@color{ clean}\\,})italic_S ( typewriter_S ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( typewriter_S | typewriter_He typewriter_clean ) We average this surprisal value for instances of S or P in the test sample.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p5": {
    "text": "In the second test, we construct minimal pairs from the example sentences in which the marker token appears and does not appear, and then compare the surprisal of the marker token to the surprisal of the token that follows it, both conditioned on the same context.\nIn example 4.3 , the surprisal of the following token S \u2062 ( his ) \ud835\udc46 his S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,}) italic_S ( typewriter_his ) is defined as: Report issue for preceding element S \u2062 ( his ) = \u2212 log 2 \u2061 p \u2062 ( his | He clean ) \ud835\udc46 his subscript 2 \ud835\udc5d his | He clean S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,})=-%\n\\log_{2}p(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}%\n\\,$|$\\,\\definecolor{hlcolor}{HTML}{FDFD95}\\lx@texthl@color{He}\\,\\definecolor{%\nhlcolor}{HTML}{FDE4CF}\\lx@texthl@color{ clean}\\,}) italic_S ( typewriter_his ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( typewriter_his | typewriter_He typewriter_clean ) We expect S \u2062 ( his ) \u2212 S \u2062 ( S ) \ud835\udc46 his \ud835\udc46 S S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,})-S(%\n\\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}}%\n\\,}) italic_S ( typewriter_his ) - italic_S ( typewriter_S ) to be a large positive value.\nWe average such surprisal differences over instances of the marker tokens in the test sample and similarly define marker surprisals and minimal pair configurations for the other *Hop languages. Report issue for preceding element",
    "masked_text": "In the second test, we construct minimal pairs from the example sentences in which the marker token appears and does not appear, and then compare the surprisal of the marker token to the surprisal of the token that follows it, both conditioned on the same context. In example 4.3, the surprisal of the following token S\u2062( his )\ud835\udc46 his S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,})italic_S ( typewriter_his ) is defined as:Report issue for preceding element S\u2062( his )=\u2212log2\u2061p\u2062( his | He clean )\ud835\udc46 his subscript2\ud835\udc5d his | He clean S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,})=-% \\log_{2}p(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}% \\,$|$\\,\\definecolor{hlcolor}{HTML}{FDFD95}\\lx@texthl@color{He}\\,\\definecolor{% hlcolor}{HTML}{FDE4CF}\\lx@texthl@color{ clean}\\,})italic_S ( typewriter_his ) = - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( typewriter_his | typewriter_He typewriter_clean ) We expect S\u2062( his )\u2212S\u2062( S )\ud835\udc46 his \ud835\udc46 S S(\\texttt{\\,\\definecolor{hlcolor}{HTML}{FFCFD2}\\lx@texthl@color{ his}\\,})-S(% \\texttt{\\,\\definecolor{hlcolor}{HTML}{FF77E6}\\lx@texthl@color{\\framebox{{S}}}% \\,})italic_S ( typewriter_his ) - italic_S ( typewriter_S ) to be a large positive value. We average such surprisal differences over instances of the marker tokens in the test sample and similarly define marker surprisals and minimal pair configurations for the other *Hop languages.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px2.p1": {
    "text": "For the first surprisal test, our hypothesis is that the mean surprisal of the marker tokens across test examples will be smaller for the control language than for the impossible languages.\nFor the second test, our hypothesis is that the mean surprisal difference across all test pairs will be larger for possible languages than for impossible ones. Report issue for preceding element",
    "masked_text": "For the first surprisal test, our hypothesis is that the mean surprisal of the marker tokens across test examples will be smaller for the control language than for the impossible languages. For the second test, our hypothesis is that the mean surprisal difference across all test pairs will be larger for possible languages than for impossible ones.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px3.p1": {
    "text": "Our results are presented in Figure 3 . The NoHop model, which has the verb marking pattern most similar to English, consistently has the lowest mean marker surprisal across training steps in test 1 ( Figure 3(a) ). The NoHop model also has the highest mean surprisal difference across training steps in test 2 ( Figure 3(b) ). Both of these results indicate that GPT-2 has learned to expect the marker tokens when they follow a more natural grammatical pattern and was very surprised when they did not appear at the correct positions. Report issue for preceding element",
    "masked_text": "Our results are presented in Figure 3. The NoHop model, which has the verb marking pattern most similar to English, consistently has the lowest mean marker surprisal across training steps in test 1 (Figure 3(a)). The NoHop model also has the highest mean surprisal difference across training steps in test 2 (Figure 3(b)). Both of these results indicate that GPT-2 has learned to expect the marker tokens when they follow a more natural grammatical pattern and was very surprised when they did not appear at the correct positions.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px3.p2": {
    "text": "GPT-2 learns to expect marker tokens at the right locations in the other *Hop models, just not as well as the control. TokenHop tends to have a lower marker surprisal and a higher mean surprisal difference compared to WordHop across training steps, indicating that GPT-2 is better at learning the verb marking rule when the units being counted are tokens instead of words. Report issue for preceding element",
    "masked_text": "GPT-2 learns to expect marker tokens at the right locations in the other *Hop models, just not as well as the control. TokenHop tends to have a lower marker surprisal and a higher mean surprisal difference compared to WordHop across training steps, indicating that GPT-2 is better at learning the verb marking rule when the units being counted are tokens instead of words.Report issue for preceding element",
    "citations": []
  },
  "S4.SS4.p1": {
    "text": "Experiment 2 demonstrates that, while GPT-2 favors natural grammar rules, it is also capable of acquiring count-based grammar rules like those seen in the verb marking patterns of our *Hop languages. But what sorts of internal mechanisms does it implement to learn such grammar rules, and how do these mechanisms compare to the more natural control? To address this, we conduct a final experiment using causal abstraction analysis , which offers an interpretability framework for identifying and examining causal mechanisms within neural models Geiger et\u00a0al. ( 2020 , 2021 ); Wu et\u00a0al. ( 2022 , 2023a , 2023b ); Geiger et\u00a0al. ( 2023 ) . We employ the interchange intervention technique on our *Hop models. To perform a basic interchange intervention on a neural model M \ud835\udc40 M italic_M , we create two instances of M \ud835\udc40 M italic_M that are provided two different inputs, the base input b \ud835\udc4f b italic_b and the source input s \ud835\udc60 s italic_s . Then, we interchange representations created while processing b \ud835\udc4f b italic_b with representations created while processing s \ud835\udc60 s italic_s and observe the effect on the output of M \ud835\udc40 M italic_M . Such interventions allow us to piece together a causal understanding of how the model processes inputs. Report issue for preceding element",
    "masked_text": "Experiment 2 demonstrates that, while GPT-2 favors natural grammar rules, it is also capable of acquiring count-based grammar rules like those seen in the verb marking patterns of our *Hop languages. But what sorts of internal mechanisms does it implement to learn such grammar rules, and how do these mechanisms compare to the more natural control? To address this, we conduct a final experiment using causal abstraction analysis, which offers an interpretability framework for identifying and examining causal mechanisms within neural models [CITATION]. We employ the interchange intervention technique on our *Hop models. To perform a basic interchange intervention on a neural model M\ud835\udc40Mitalic_M, we create two instances of M\ud835\udc40Mitalic_M that are provided two different inputs, the base input b\ud835\udc4fbitalic_b and the source input s\ud835\udc60sitalic_s. Then, we interchange representations created while processing b\ud835\udc4fbitalic_b with representations created while processing s\ud835\udc60sitalic_s and observe the effect on the output of M\ud835\udc40Mitalic_M. Such interventions allow us to piece together a causal understanding of how the model processes inputs.Report issue for preceding element",
    "citations": [
      {
        "tag": "Wu et\u00a0al. (2023a)",
        "title": "Causal proxy models for concept-based model explanations.",
        "authors": "Zhengxuan Wu, Karel D\u2019Oosterlinck, Atticus Geiger, Amir Zur, and Christopher Potts. 2023a.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages 37313\u201337334. PMLR."
      },
      {
        "tag": "Geiger et\u00a0al. (2021)",
        "title": "Causal abstractions of neural networks.",
        "authors": "Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. 2021.",
        "journal": "InAdvances in Neural Information Processing Systems, volume\u00a034, pages 9574\u20139586. Curran Associates, Inc."
      },
      {
        "tag": "Geiger et\u00a0al. (2020)",
        "title": "Neural natural language inference models partially embed theories of lexical entailment and negation.",
        "authors": "Atticus Geiger, Kyle Richardson, and Christopher Potts. 2020.",
        "journal": "InProceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 163\u2013173, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Wu et\u00a0al. (2022)",
        "title": "Causal distillation for language models.",
        "authors": "Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, and Noah Goodman. 2022.",
        "journal": "InProceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134295, Seattle, United States. Association for Computational Linguistics."
      },
      {
        "tag": "Wu et\u00a0al. (2023b)",
        "title": "Interpretability at scale: Identifying causal mechanisms in Alpaca.",
        "authors": "Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023b.",
        "journal": "InAdvances in Neural Information Processing Systems, volume\u00a036, pages 78205\u201378226. Curran Associates, Inc."
      },
      {
        "tag": "Geiger et\u00a0al. (2023)",
        "title": "Finding alignments between interpretable causal variables and distributed neural representations.",
        "authors": "Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah\u00a0D. Goodman. 2023.",
        "journal": "InProceedings of Causal Learning and Reasoning 2024."
      }
    ]
  },
  "S4.SS4.SSS0.Px1.p1": {
    "text": "We use interchange interventions to identify representations in our *Hop models\nthat have causal effects on their output behaviors on a subject\u2013verb agreement task. In\nour experimental setup, b \ud835\udc4f b italic_b is a sentence prefix with a singular subject and s \ud835\udc60 s italic_s is\nan identical prefix with the plural form of the subject. These prefixes include all\ntokens up to but not including the markers ( S and P ).\nWe interchange the GPT-2 block outputs from processing b \ud835\udc4f b italic_b with GPT-2 block outputs\nfrom processing s \ud835\udc60 s italic_s and observe whether the probability of plural marker P is higher than the probability\nof singular marker S after the intervention. This is\nshown more concretely in Figure 4 . Report issue for preceding element",
    "masked_text": "We use interchange interventions to identify representations in our *Hop models that have causal effects on their output behaviors on a subject\u2013verb agreement task. In our experimental setup, b\ud835\udc4fbitalic_b is a sentence prefix with a singular subject and s\ud835\udc60sitalic_s is an identical prefix with the plural form of the subject. These prefixes include all tokens up to but not including the markers (S and P). We interchange the GPT-2 block outputs from processing b\ud835\udc4fbitalic_b with GPT-2 block outputs from processing s\ud835\udc60sitalic_s and observe whether the probability of plural marker P is higher than the probability of singular marker S after the intervention. This is shown more concretely in Figure 4.Report issue for preceding element",
    "citations": []
  },
  "S4.SS4.SSS0.Px1.p2": {
    "text": "We run such interventions at each GPT-2 layer and token position to see which\nparts of the model cause a change in the marker prediction. We run all of these\ninterventions over several test examples and report the interchange intervention\naccuracy (IIA), a metric that represents the subject\u2013verb agreement accuracy if the\ncounterfactual (i.e.\u00a0plural) were the ground truth. The test examples for each *Hop model are extracted from their respective versions of the BabyLM test set, and minimally-different counterfactual examples are created by changing the singular subjects to plural subjects.\nTo ensure that interventions on different examples are analogous, we use regular expressions to locate examples that follow the same structure (i.e.\u00a0subjects and verbs at the same positions). Report issue for preceding element",
    "masked_text": "We run such interventions at each GPT-2 layer and token position to see which parts of the model cause a change in the marker prediction. We run all of these interventions over several test examples and report the interchange intervention accuracy (IIA), a metric that represents the subject\u2013verb agreement accuracy if the counterfactual (i.e. plural) were the ground truth. The test examples for each *Hop model are extracted from their respective versions of the BabyLM test set, and minimally-different counterfactual examples are created by changing the singular subjects to plural subjects. To ensure that interventions on different examples are analogous, we use regular expressions to locate examples that follow the same structure (i.e. subjects and verbs at the same positions).Report issue for preceding element",
    "citations": []
  },
  "S4.SS4.SSS0.Px2.p1": {
    "text": "Our results are presented in Figure 5 . The IIA graphs\ndemonstrate how information about the marker tokens flows through the models.\nWe can see that, in all three *Hop models, IIA is high at the token position\nof the subject up until about layer 3; then there is a transition to the position of\nthe last token in the prefix, preceding the location where the marker should be predicted.\nAll models develop the same modular solution to the task by tracking\nagreement through the representations at the relevant positions, but the NoHop model obtains nearly 100% IIA earlier during training, at about\n1,500 training steps, supporting the previous surprisal results. Report issue for preceding element",
    "masked_text": "Our results are presented in Figure 5. The IIA graphs demonstrate how information about the marker tokens flows through the models. We can see that, in all three *Hop models, IIA is high at the token position of the subject up until about layer 3; then there is a transition to the position of the last token in the prefix, preceding the location where the marker should be predicted. All models develop the same modular solution to the task by tracking agreement through the representations at the relevant positions, but the NoHop model obtains nearly 100% IIA earlier during training, at about 1,500 training steps, supporting the previous surprisal results.Report issue for preceding element",
    "citations": []
  },
  "S5.p1": {
    "text": "Contra claims by Chomsky and others that LLMs cannot possibly inform our understanding of human language, we argue there is great value in treating LLMs as a comparative system for human language and in understanding what systems like LLMs can and cannot learn.\nPrior explorations of neural language models have already been fruitful for understanding the generalization of syntactic principles from data Wilcox et\u00a0al. ( 2018 ); Marvin and Linzen ( 2018 ); Futrell et\u00a0al. ( 2019 ); Prasad et\u00a0al. ( 2019 ); Hu et\u00a0al. ( 2020 ) . Our paper complements this line of work.\nWe have shown that GPT-2 models do not master our set of synthetic impossible languages as well as natural ones, challenging the unfounded assertions stated previously. Report issue for preceding element",
    "masked_text": "Contra claims by Chomsky and others that LLMs cannot possibly inform our understanding of human language, we argue there is great value in treating LLMs as a comparative system for human language and in understanding what systems like LLMs can and cannot learn. Prior explorations of neural language models have already been fruitful for understanding the generalization of syntactic principles from data [CITATION]. Our paper complements this line of work. We have shown that GPT-2 models do not master our set of synthetic impossible languages as well as natural ones, challenging the unfounded assertions stated previously.Report issue for preceding element",
    "citations": [
      {
        "tag": "Wilcox et\u00a0al. (2018)",
        "title": "What do RNN language models learn about filler\u2013gap dependencies?",
        "authors": "Ethan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018.",
        "journal": "InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 211\u2013221, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Prasad et\u00a0al. (2019)",
        "title": "Using priming to uncover the organization of syntactic representations in neural language models.",
        "authors": "Grusha Prasad, Marten van Schijndel, and Tal Linzen. 2019.",
        "journal": "InProceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 66\u201376, Hong Kong, China. Association for Computational Linguistics."
      },
      {
        "tag": "Marvin and Linzen (2018)",
        "title": "Targeted syntactic evaluation of language models.",
        "authors": "Rebecca Marvin and Tal Linzen. 2018.",
        "journal": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202, Brussels, Belgium. Association for Computational Linguistics."
      },
      {
        "tag": "Hu et\u00a0al. (2020)",
        "title": "A systematic assessment of syntactic generalization in neural language models.",
        "authors": "Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020.",
        "journal": "InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1725\u20131744, Online. Association for Computational Linguistics."
      },
      {
        "tag": "Futrell et\u00a0al. (2019)",
        "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state.",
        "authors": "Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019.",
        "journal": "InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 32\u201342, Minneapolis, Minnesota. Association for Computational Linguistics."
      }
    ]
  },
  "S5.p2": {
    "text": "Even in the absence of a clear definition of what constitutes a possible or impossible language, we believe that our investigations advance this debate regarding LLMs. The lack of a definition does not hinder inquiry into this topic; in fact, it beckons further explorations of the boundary between the possible and impossible languages, as shown in our hypothesized continuum in Figure 1 . We believe that the *Hop languages we propose closely approach this boundary. Report issue for preceding element",
    "masked_text": "Even in the absence of a clear definition of what constitutes a possible or impossible language, we believe that our investigations advance this debate regarding LLMs. The lack of a definition does not hinder inquiry into this topic; in fact, it beckons further explorations of the boundary between the possible and impossible languages, as shown in our hypothesized continuum in Figure 1. We believe that the *Hop languages we propose closely approach this boundary.Report issue for preceding element",
    "citations": []
  },
  "S5.p3": {
    "text": "At the same time, conclusions about LLMs\u2019 linguistic competence and preferences for natural languages should be informed by an understanding of the ways that models fundamentally differ from humans. For instance, we saw that models can perform operations that involve counting tokens because LLMs rely on tokens as basic units. While humans are sensitive to morpheme boundaries and word boundaries, it is unlikely humans rely on atomic tokens in the way that LLMs do.\nThis does not mean that LLMs can fundamentally tell us nothing about human language.\nRather, as we did here, it is valuable to consider and control for this difference before making generalizations. Report issue for preceding element",
    "masked_text": "At the same time, conclusions about LLMs\u2019 linguistic competence and preferences for natural languages should be informed by an understanding of the ways that models fundamentally differ from humans. For instance, we saw that models can perform operations that involve counting tokens because LLMs rely on tokens as basic units. While humans are sensitive to morpheme boundaries and word boundaries, it is unlikely humans rely on atomic tokens in the way that LLMs do. This does not mean that LLMs can fundamentally tell us nothing about human language. Rather, as we did here, it is valuable to consider and control for this difference before making generalizations.Report issue for preceding element",
    "citations": []
  },
  "S5.p4": {
    "text": "Since at least the 1950s, a major line of linguistic inquiry has focused on what aspects of syntactic structure can be learned just from data, without domain-specific innate priors (e.g.\u00a0a Universal Grammar ).\nLLMs lack strong in-built linguistic priors, yet they can learn complex syntactic structures.\nWhile many LLMs are trained with vastly more data than children see, there is increasing evidence that even systems trained on smaller amounts of data can learn interesting linguistic information (Warstadt et\u00a0al., 2023 ) .\nThe current paper raises further questions along similar lines. Since we do find that real languages are more learnable by GPT-2, this leads us to wonder what inductive bias of GPT language models matches natural language.\nWe believe that this inductive bias is related to information locality, the tendency for statistical correlations in text to be short range.\nInformation locality arises in GPTs due to their autoregressive training objective and has been argued to arise in humans due to the incremental nature of real-time language processing (Futrell, 2019 ; Hahn et\u00a0al., 2021 ) . Report issue for preceding element",
    "masked_text": "Since at least the 1950s, a major line of linguistic inquiry has focused on what aspects of syntactic structure can be learned just from data, without domain-specific innate priors (e.g. a Universal Grammar). LLMs lack strong in-built linguistic priors, yet they can learn complex syntactic structures. While many LLMs are trained with vastly more data than children see, there is increasing evidence that even systems trained on smaller amounts of data can learn interesting linguistic information [CITATION]. The current paper raises further questions along similar lines. Since we do find that real languages are more learnable by GPT-2, this leads us to wonder what inductive bias of GPT language models matches natural language. We believe that this inductive bias is related to information locality, the tendency for statistical correlations in text to be short range. Information locality arises in GPTs due to their autoregressive training objective and has been argued to arise in humans due to the incremental nature of real-time language processing [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Warstadt et\u00a0al. (2023)",
        "title": "Call for papers \u2013 the BabyLM challenge: Sample-efficient pretraining on a developmentally plausible corpus.",
        "authors": "Alex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang. 2023.",
        "journal": ""
      },
      {
        "tag": "Futrell (2019)",
        "title": "Information-theoretic locality properties of natural language.",
        "authors": "Richard Futrell. 2019.",
        "journal": "InProceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019), pages 2\u201315, Paris, France. Association for Computational Linguistics."
      },
      {
        "tag": "Hahn et\u00a0al. (2021)",
        "title": "Modeling word and morpheme order in natural language as an efficient trade-off of memory and surprisal.",
        "authors": "Michael Hahn, Judith Degen, and Richard Futrell. 2021.",
        "journal": "Psychological Review, 128(4):726\u2013756."
      }
    ]
  },
  "S5.p5": {
    "text": "Since LLMs have been shown to learn the complex structures of human language and have a preference for learning such structures over unnatural counterfactuals, it follows that they are clearly relevant to investigations and claims about the necessary innate priors for language learning.\nArguments that they are \u201cby design, unlimited in what they can \u2018learn\u201d\u2019 and \u201cincapable of distinguishing the possible from the impossible\u201d (Chomsky et\u00a0al., 2023 ) do not offer convincing evidence otherwise. Report issue for preceding element",
    "masked_text": "Since LLMs have been shown to learn the complex structures of human language and have a preference for learning such structures over unnatural counterfactuals, it follows that they are clearly relevant to investigations and claims about the necessary innate priors for language learning. Arguments that they are \u201cby design, unlimited in what they can \u2018learn\u201d\u2019 and \u201cincapable of distinguishing the possible from the impossible\u201d [CITATION] do not offer convincing evidence otherwise.Report issue for preceding element",
    "citations": [
      {
        "tag": "Chomsky et\u00a0al. (2023)",
        "title": "Noam Chomsky: The false promise of ChatGPT.",
        "authors": "Noam Chomsky, Ian Roberts, and Jeffrey Watumull. 2023.",
        "journal": "The New York Times."
      }
    ]
  },
  "S6.p1": {
    "text": "The authors would like to thank Aryaman Arora, Christiane Fellbaum, Roger Levy, Tristan Thrush, and Diyi Yang for helpful comments on the project. We would also like to thank the members of the Stanford NLP Group, the MIT Computational Psycholinguistics Lab, and the anonymous reviewers for useful discussions. Julie Kallini is supported by a National Science Foundation Graduate Research Fellowship under grant number DGE-2146755. Report issue for preceding element",
    "masked_text": "The authors would like to thank Aryaman Arora, Christiane Fellbaum, Roger Levy, Tristan Thrush, and Diyi Yang for helpful comments on the project. We would also like to thank the members of the Stanford NLP Group, the MIT Computational Psycholinguistics Lab, and the anonymous reviewers for useful discussions. Julie Kallini is supported by a National Science Foundation Graduate Research Fellowship under grant number DGE-2146755.Report issue for preceding element",
    "citations": []
  },
  "S7.p1": {
    "text": "Due to resource constraints, we exclusively use the GPT-2 architecture to train models on our various synthetic impossible languages. Each of our experiments involves training a GPT-2 model from scratch on a different language dataset, and for every such language, we train multiple GPT-2 models to establish confidence intervals for our evaluation metrics. Applying this approach to several different model architectures would be quite resource-intensive, so we opted to choose a single architecture in this paper. Future work could apply our methodology to models trained with different architectures or training objectives. Report issue for preceding element",
    "masked_text": "Due to resource constraints, we exclusively use the GPT-2 architecture to train models on our various synthetic impossible languages. Each of our experiments involves training a GPT-2 model from scratch on a different language dataset, and for every such language, we train multiple GPT-2 models to establish confidence intervals for our evaluation metrics. Applying this approach to several different model architectures would be quite resource-intensive, so we opted to choose a single architecture in this paper. Future work could apply our methodology to models trained with different architectures or training objectives.Report issue for preceding element",
    "citations": []
  },
  "S7.p2": {
    "text": "Our impossible languages are derived by manipulating an English dataset. While we do not conduct experiments that use other natural languages as a starting point, our experimental choices (i.e.\u00a0the synthetic languages we design) are informed by linguistic diversity and typology, distinguishing our impossible languages from those that are rare but attested. However, future work might involve deriving impossible languages from base languages other than English and include more morphological manipulations. Report issue for preceding element",
    "masked_text": "Our impossible languages are derived by manipulating an English dataset. While we do not conduct experiments that use other natural languages as a starting point, our experimental choices (i.e. the synthetic languages we design) are informed by linguistic diversity and typology, distinguishing our impossible languages from those that are rare but attested. However, future work might involve deriving impossible languages from base languages other than English and include more morphological manipulations.Report issue for preceding element",
    "citations": []
  },
  "S8.p1": {
    "text": "While this work makes the case for language models as useful tools for cognitive science and linguistics research, these models learn and generate language through processes that are fundamentally different from those employed by humans. Making direct claims about human language learning based on the results of this paper could pose potential risks and harms. This research merely aims to explore the learnability of different languages (specifically, those languages that cannot be acquired by humans and are not representative of any known human language) through the lens of neural models. Report issue for preceding element",
    "masked_text": "While this work makes the case for language models as useful tools for cognitive science and linguistics research, these models learn and generate language through processes that are fundamentally different from those employed by humans. Making direct claims about human language learning based on the results of this paper could pose potential risks and harms. This research merely aims to explore the learnability of different languages (specifically, those languages that cannot be acquired by humans and are not representative of any known human language) through the lens of neural models.Report issue for preceding element",
    "citations": []
  }
}