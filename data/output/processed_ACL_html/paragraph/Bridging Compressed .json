{
  "S1.p1": {
    "text": "Large Language Models (LLMs) (Touvron et\u00a0al., 2023b ; a ) have demonstrated impressive abilities in various Natural Language Processing (NLP) tasks. Building upon their success, the recent surge of research on Multimodal Large Language Models (MLLMs) extends LLM\u2019s abilities to modalities beyond languages, particularly images, opening up promising opportunities in various applications (Achiam et\u00a0al., 2023 ; Cha et\u00a0al., 2024 ; Lin et\u00a0al., 2024 ; Zhang et\u00a0al., 2024a ) . MLLMs have shown surprising capability for many vision tasks such as classification (Zhu et\u00a0al., 2024b ) , image captioning (Zhang et\u00a0al., 2024a ) , Visual Question Answering (VQA) (Cha et\u00a0al., 2024 ; Lin et\u00a0al., 2024 ) , and meme interpretation (Achiam et\u00a0al., 2023 ) . These models excel in unseen tasks through instruction following or in-context learning, which is impossible for traditional vision networks. Report issue for preceding element",
    "masked_text": "Large Language Models (LLMs) [CITATION] have demonstrated impressive abilities in various Natural Language Processing (NLP) tasks. Building upon their success, the recent surge of research on Multimodal Large Language Models (MLLMs) extends LLM\u2019s abilities to modalities beyond languages, particularly images, opening up promising opportunities in various applications [CITATION]. MLLMs have shown surprising capability for many vision tasks such as classification [CITATION], image captioning [CITATION], Visual Question Answering (VQA) [CITATION], and meme interpretation [CITATION]. These models excel in unseen tasks through instruction following or in-context learning, which is impossible for traditional vision networks.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2024a)",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention.",
        "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu\u00a0Qiao.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024a."
      },
      {
        "tag": "Touvron et\u00a0al. (2023a)",
        "title": "Llama: Open and efficient foundation language models.",
        "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2302.13971, 2023a."
      },
      {
        "tag": "Achiam et\u00a0al. (2023)",
        "title": "Gpt-4 technical report.",
        "authors": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2303.08774, 2023."
      },
      {
        "tag": "Cha et\u00a0al. (2024)",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm.",
        "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024."
      },
      {
        "tag": "Touvron et\u00a0al. (2023b)",
        "title": "Llama 2: Open foundation and fine-tuned chat models.",
        "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2307.09288, 2023b."
      },
      {
        "tag": "Zhu et\u00a0al. (2024b)",
        "title": "Beyond text: Frozen large language models in visual signal comprehension.",
        "authors": "Lei Zhu, Fangyun Wei, and Yanye Lu.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b."
      },
      {
        "tag": "Lin et\u00a0al. (2024)",
        "title": "Vila: On pre-training for visual language models.",
        "authors": "Ji\u00a0Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024."
      }
    ]
  },
  "S1.p2": {
    "text": "However, the massive scale of MLLMs, often comprising billions of parameters, poses significant challenges for deployment on resource-constrained end devices. While computation can be offloaded to the cloud, transmitting images to cloud-hosted MLLMs becomes necessary. In this case, efficient image compression techniques are crucial to reducing the required transmission bit-rate. Without compression, transmitting raw images incurs significant costs, particularly at scale with numerous users. Our study shows that simply feeding the decoded image, generated by a fixed image codec trained for human perception, into an MLLM (Figure 1 (a)) substantially degrades task performance, particularly when the image is coded at low rates. This highlights the critical need for efficient image compression that considers the requirements of downstream MLLM-based vision tasks. Report issue for preceding element",
    "masked_text": "However, the massive scale of MLLMs, often comprising billions of parameters, poses significant challenges for deployment on resource-constrained end devices. While computation can be offloaded to the cloud, transmitting images to cloud-hosted MLLMs becomes necessary. In this case, efficient image compression techniques are crucial to reducing the required transmission bit-rate. Without compression, transmitting raw images incurs significant costs, particularly at scale with numerous users. Our study shows that simply feeding the decoded image, generated by a fixed image codec trained for human perception, into an MLLM (Figure 1 (a)) substantially degrades task performance, particularly when the image is coded at low rates. This highlights the critical need for efficient image compression that considers the requirements of downstream MLLM-based vision tasks.Report issue for preceding element",
    "citations": []
  },
  "S1.p3": {
    "text": "To the best of our knowledge, there have been no attempts to tackle image compression specifically for MLLMs. While many prior works address image compression for machine vision, commonly referred to as coding for machines (Le et\u00a0al., 2021b ; Chamain et\u00a0al., 2021 ; Matsubara et\u00a0al., 2022 ; Liu et\u00a0al., 2022b ) , these approaches cannot be directly applied to MLLMs. Two common approaches to coding for machines are image coding and feature coding. The image coding approaches (Le et\u00a0al., 2021b ; a ) optimize the image codec for specific downstream tasks and/or networks (Figure 1 (b)), while the feature coding approaches (Ding et\u00a0al., 2024 ) divide the task network into two parts and focus on compressing the intermediate features (Figure 1 (c)). However, both approaches face the same issue: the training process becomes challenging when one needs to back-propagate a training objective through a massive MLLM to train the neural image codec. In practice, the billion-scale parameters of MLLMs make the existing coding for machine methods inapplicable. Report issue for preceding element",
    "masked_text": "To the best of our knowledge, there have been no attempts to tackle image compression specifically for MLLMs. While many prior works address image compression for machine vision, commonly referred to as coding for machines [CITATION], these approaches cannot be directly applied to MLLMs. Two common approaches to coding for machines are image coding and feature coding. The image coding approaches [CITATION] optimize the image codec for specific downstream tasks and/or networks (Figure 1 (b)), while the feature coding approaches [CITATION] divide the task network into two parts and focus on compressing the intermediate features (Figure 1 (c)). However, both approaches face the same issue: the training process becomes challenging when one needs to back-propagate a training objective through a massive MLLM to train the neural image codec. In practice, the billion-scale parameters of MLLMs make the existing coding for machine methods inapplicable.Report issue for preceding element",
    "citations": [
      {
        "tag": "Le et\u00a0al. (2021b)",
        "title": "Learned image coding for machines: A content-adaptive approach.",
        "authors": "Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Hamed\u00a0Rezazadegan Tavakoli, and Esa Rahtu.",
        "journal": "In2021 IEEE International Conference on Multimedia and Expo (ICME), pp.\u00a0 1\u20136. IEEE, 2021b."
      },
      {
        "tag": "Liu et\u00a0al. (2022b)",
        "title": "Improving multiple machine vision tasks in the compressed domain.",
        "authors": "Jinming Liu, Heming Sun, and Jiro Katto.",
        "journal": "In2022 26th International Conference on Pattern Recognition (ICPR), pp.\u00a0 331\u2013337. IEEE, 2022b."
      },
      {
        "tag": "Ding et\u00a0al. (2024)",
        "title": "Hierarchical image feature compression for machines via feature sparsity learning.",
        "authors": "Ding Ding, Zhenzhong Chen, Zizheng Liu, Xiaozhong Xu, and Shan Liu.",
        "journal": "IEEE Signal Processing Letters, 2024."
      },
      {
        "tag": "Matsubara et\u00a0al. (2022)",
        "title": "Supervised compression for resource-constrained edge computing systems.",
        "authors": "Yoshitomo Matsubara, Ruihan Yang, Marco Levorato, and Stephan Mandt.",
        "journal": "InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.\u00a0 2685\u20132695, 2022."
      },
      {
        "tag": "Chamain et\u00a0al. (2021)",
        "title": "End-to-end optimized image compression for machines, a study.",
        "authors": "Lahiru\u00a0D Chamain, Fabien Racap\u00e9, Jean B\u00e9gaint, Akshay Pushparaja, and Simon Feltman.",
        "journal": "In2021 Data Compression Conference (DCC), pp.\u00a0 163\u2013172. IEEE, 2021."
      },
      {
        "tag": "Le et\u00a0al. (2021a)",
        "title": "Image coding for machines: an end-to-end learned approach.",
        "authors": "Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, and Esa Rahtu.",
        "journal": "InICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.\u00a0 1590\u20131594. IEEE, 2021a."
      }
    ]
  },
  "S1.p4": {
    "text": "In this paper, we propose the first neural image compression system for MLLM-based vision tasks that enables compressed latents to suit the needs of downstream MLLMs. Notably, it is not our objective to develop a new image codec specifically for MLLM-based tasks. Instead, our method involves a lightweight transform-neck and a novel surrogate loss. The transform-neck adapts the compressed image latents of an off-the-shelf neural image codec to match the intermediate features of the visual encoder\u2013i.e. a component of MLLMs\u2013bypassing the needs for full image reconstruction and reducing computational complexity. Our proposed surrogate loss function, which combines the cross-entropy and distillation terms, enables our system to be trained by back-propagating solely through the visual encoder, thus eliminating the need for back-propagation through the entire MLLM. Report issue for preceding element",
    "masked_text": "In this paper, we propose the first neural image compression system for MLLM-based vision tasks that enables compressed latents to suit the needs of downstream MLLMs. Notably, it is not our objective to develop a new image codec specifically for MLLM-based tasks. Instead, our method involves a lightweight transform-neck and a novel surrogate loss. The transform-neck adapts the compressed image latents of an off-the-shelf neural image codec to match the intermediate features of the visual encoder\u2013i.e. a component of MLLMs\u2013bypassing the needs for full image reconstruction and reducing computational complexity. Our proposed surrogate loss function, which combines the cross-entropy and distillation terms, enables our system to be trained by back-propagating solely through the visual encoder, thus eliminating the need for back-propagation through the entire MLLM.Report issue for preceding element",
    "citations": []
  },
  "S1.p5": {
    "text": "The proposed method is general in that it is applicable to different neural image codecs under various application scenarios. First, if the downstream applications prioritize the image reconstruction quality for human interaction, our method can work with an off-the-shelf image codec trained for human perception (Figure 1 (d1)). Without any modification or re-training of the codec, our method adapts the compressed image latents while maintaining the same image reconstruction quality. Second, when allowing the image codec to be updated, we propose a multi-task training strategy that optimizes the codec for both human and machine perception (Figure 1 (d2)). This significantly improves MLLM performance at the cost of a marginal drop in the image\u2019s reconstruction quality. Finally, we consider an extreme setting in which the applications prioritize machine perception over image reconstruction. In this case, the encoder and the transform-neck are jointly optimized for the MLLM systems exclusively (Figure 1 (d3)). The main contributions of this work are summarized as follows: Report issue for preceding element \u2022 It marks the first exploration into the field of neural image coding for MLLMs. Report issue for preceding element \u2022 The proposed transform-neck adapts the compressed image latents to downstream MLLMs, avoiding the need for image reconstruction and thus saving computational complexity. Report issue for preceding element \u2022 The proposed surrogate loss leverages the visual encoder to update the system, avoiding back-propagating the task loss through the heavy MLLM. Report issue for preceding element \u2022 The proposed framework is broadly applicable to a wide range of neural image codecs and MLLMs, regardless of their architectures. Report issue for preceding element \u2022 It is able to accommodate various application scenarios that involve human perception, machine perception, or both. Report issue for preceding element",
    "masked_text": "The proposed method is general in that it is applicable to different neural image codecs under various application scenarios. First, if the downstream applications prioritize the image reconstruction quality for human interaction, our method can work with an off-the-shelf image codec trained for human perception (Figure 1 (d1)). Without any modification or re-training of the codec, our method adapts the compressed image latents while maintaining the same image reconstruction quality. Second, when allowing the image codec to be updated, we propose a multi-task training strategy that optimizes the codec for both human and machine perception (Figure 1 (d2)). This significantly improves MLLM performance at the cost of a marginal drop in the image\u2019s reconstruction quality. Finally, we consider an extreme setting in which the applications prioritize machine perception over image reconstruction. In this case, the encoder and the transform-neck are jointly optimized for the MLLM systems exclusively (Figure 1 (d3)). The main contributions of this work are summarized as follows:Report issue for preceding element \u2022 It marks the first exploration into the field of neural image coding for MLLMs.Report issue for preceding element \u2022 The proposed transform-neck adapts the compressed image latents to downstream MLLMs, avoiding the need for image reconstruction and thus saving computational complexity.Report issue for preceding element \u2022 The proposed surrogate loss leverages the visual encoder to update the system, avoiding back-propagating the task loss through the heavy MLLM.Report issue for preceding element \u2022 The proposed framework is broadly applicable to a wide range of neural image codecs and MLLMs, regardless of their architectures. Report issue for preceding element \u2022 It is able to accommodate various application scenarios that involve human perception, machine perception, or both.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i1.p1": {
    "text": "It marks the first exploration into the field of neural image coding for MLLMs. Report issue for preceding element",
    "masked_text": "It marks the first exploration into the field of neural image coding for MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i2.p1": {
    "text": "The proposed transform-neck adapts the compressed image latents to downstream MLLMs, avoiding the need for image reconstruction and thus saving computational complexity. Report issue for preceding element",
    "masked_text": "The proposed transform-neck adapts the compressed image latents to downstream MLLMs, avoiding the need for image reconstruction and thus saving computational complexity.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i3.p1": {
    "text": "The proposed surrogate loss leverages the visual encoder to update the system, avoiding back-propagating the task loss through the heavy MLLM. Report issue for preceding element",
    "masked_text": "The proposed surrogate loss leverages the visual encoder to update the system, avoiding back-propagating the task loss through the heavy MLLM.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i4.p1": {
    "text": "The proposed framework is broadly applicable to a wide range of neural image codecs and MLLMs, regardless of their architectures. Report issue for preceding element",
    "masked_text": "The proposed framework is broadly applicable to a wide range of neural image codecs and MLLMs, regardless of their architectures. Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i5.p1": {
    "text": "It is able to accommodate various application scenarios that involve human perception, machine perception, or both. Report issue for preceding element",
    "masked_text": "It is able to accommodate various application scenarios that involve human perception, machine perception, or both.Report issue for preceding element",
    "citations": []
  },
  "S1.p6": {
    "text": "Last but not least, the transform-neck trained with our surrogate loss exhibits a degree of universality, since it is readily applicable to multiple MLLMs that share the same visual encoder, without the need for retraining. Our method achieves (1) up to 60-80% bit-rate reductions under the same recognition accuracy over existing image codecs (e.g. ELIC (He et\u00a0al., 2022 ) and VVC intra coding (Bross et\u00a0al., 2021 ) ) (Sections 4.2 and A.2 ) and (2) a nearly 95% reduction in decoding kMAC/pixel as compared to performing full image reconstruction followed by enhancing the reconstructed image for MLLM-based tasks (Section 4.4 ). Our system can be successfully trained under various application scenarios on one RTX 4090 with 24GB of memory. This is not possible when the entire MLLM is involved in the training process. Report issue for preceding element",
    "masked_text": "Last but not least, the transform-neck trained with our surrogate loss exhibits a degree of universality, since it is readily applicable to multiple MLLMs that share the same visual encoder, without the need for retraining. Our method achieves (1) up to 60-80% bit-rate reductions under the same recognition accuracy over existing image codecs (e.g. ELIC [CITATION] and VVC intra coding [CITATION]) (Sections 4.2 and A.2) and (2) a nearly 95% reduction in decoding kMAC/pixel as compared to performing full image reconstruction followed by enhancing the reconstructed image for MLLM-based tasks (Section 4.4). Our system can be successfully trained under various application scenarios on one RTX 4090 with 24GB of memory. This is not possible when the entire MLLM is involved in the training process.Report issue for preceding element",
    "citations": [
      {
        "tag": "Bross et\u00a0al. (2021)",
        "title": "Overview of the versatile video coding (vvc) standard and its applications.",
        "authors": "Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary\u00a0J Sullivan, and Jens-Rainer Ohm.",
        "journal": "IEEE Transactions on Circuits and Systems for Video Technology, 31(10):3736\u20133764, 2021."
      },
      {
        "tag": "He et\u00a0al. (2022)",
        "title": "Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding.",
        "authors": "Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 5718\u20135727, 2022."
      }
    ]
  },
  "S2.SS1.p1": {
    "text": "In recent years, there has been a surge of interest in MLLMs following the impressive demonstration of LLM\u2019s ability in the NLP field (Touvron et\u00a0al., 2023b ; a ; Jiang et\u00a0al., 2023 ) . Many have sought to extend the success of these models from text to other modalities, particularly images, and several works have shown their effectiveness on various tasks, such as image captioning (Li et\u00a0al., 2023b ; Lin et\u00a0al., 2024 ; Liu et\u00a0al., 2023a ) , VQA (Cha et\u00a0al., 2024 ; Zhang et\u00a0al., 2024a ) , Referring Expression Comprehension (REC) (Chen et\u00a0al., 2023a ; Peng et\u00a0al., 2024 ; Zhang et\u00a0al., 2024b ) , few-shot classification (Yu et\u00a0al., 2024 ; Zhu et\u00a0al., 2024b ) , action anticipation (Mittal et\u00a0al., 2024 ) . Report issue for preceding element",
    "masked_text": "In recent years, there has been a surge of interest in MLLMs following the impressive demonstration of LLM\u2019s ability in the NLP field [CITATION]. Many have sought to extend the success of these models from text to other modalities, particularly images, and several works have shown their effectiveness on various tasks, such as image captioning [CITATION], VQA [CITATION], Referring Expression Comprehension (REC) [CITATION], few-shot classification [CITATION], action anticipation [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2024a)",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention.",
        "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu\u00a0Qiao.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024a."
      },
      {
        "tag": "Zhang et\u00a0al. (2024b)",
        "title": "Groundhog: Grounding large language models to holistic segmentation.",
        "authors": "Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b."
      },
      {
        "tag": "Touvron et\u00a0al. (2023a)",
        "title": "Llama: Open and efficient foundation language models.",
        "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2302.13971, 2023a."
      },
      {
        "tag": "Jiang et\u00a0al. (2023)",
        "title": "Mistral 7b.",
        "authors": "Albert\u00a0Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2310.06825, 2023."
      },
      {
        "tag": "Cha et\u00a0al. (2024)",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm.",
        "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
        "journal": "InConference on Neural Information Processing Systems (NeurIPS), 2023a."
      },
      {
        "tag": "Touvron et\u00a0al. (2023b)",
        "title": "Llama 2: Open foundation and fine-tuned chat models.",
        "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2307.09288, 2023b."
      },
      {
        "tag": "Zhu et\u00a0al. (2024b)",
        "title": "Beyond text: Frozen large language models in visual signal comprehension.",
        "authors": "Lei Zhu, Fangyun Wei, and Yanye Lu.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b."
      },
      {
        "tag": "Yu et\u00a0al. (2024)",
        "title": "Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms.",
        "authors": "Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et\u00a0al.",
        "journal": "Advances in Neural Information Processing Systems, 36, 2024."
      },
      {
        "tag": "Chen et\u00a0al. (2023a)",
        "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic.",
        "authors": "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.",
        "journal": "arXiv preprint arXiv:2306.15195, 2023a."
      },
      {
        "tag": "Peng et\u00a0al. (2024)",
        "title": "Kosmos-2: Grounding multimodal large language models to the world.",
        "authors": "Zhiliang Peng, Wenhui Wang, Li\u00a0Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024."
      },
      {
        "tag": "Li et\u00a0al. (2023b)",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
        "journal": "InInternational conference on machine learning, pp.\u00a0 19730\u201319742. PMLR, 2023b."
      },
      {
        "tag": "Lin et\u00a0al. (2024)",
        "title": "Vila: On pre-training for visual language models.",
        "authors": "Ji\u00a0Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024."
      },
      {
        "tag": "Mittal et\u00a0al. (2024)",
        "title": "Can\u2019t make an omelette without breaking some eggs: Plausible action.",
        "authors": "Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, and Kwonjoon Lee.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024."
      }
    ]
  },
  "S2.SS1.p2": {
    "text": "Most existing MLLM approaches use a visual encoder to process the input image data, and then introduce a connector to bridge the image features to the tokens understandable by the LLM. Earlier works adopt simpler connector designs, such as linear projectors (Chen et\u00a0al., 2023a ; Liu et\u00a0al., 2023a ) , while subsequent works (Li et\u00a0al., 2023b ; Cha et\u00a0al., 2024 ; Zhang et\u00a0al., 2024a ) have refined upon the design for both performance and complexity. Furthermore, the entire MLLM can be further fine-tuned to enhance its capabilities through instruction tuning (Liu et\u00a0al., 2023a ; Zhu et\u00a0al., 2024a ) . Report issue for preceding element",
    "masked_text": "Most existing MLLM approaches use a visual encoder to process the input image data, and then introduce a connector to bridge the image features to the tokens understandable by the LLM. Earlier works adopt simpler connector designs, such as linear projectors [CITATION], while subsequent works [CITATION] have refined upon the design for both performance and complexity. Furthermore, the entire MLLM can be further fine-tuned to enhance its capabilities through instruction tuning [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2024a)",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention.",
        "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu\u00a0Qiao.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024a."
      },
      {
        "tag": "Zhu et\u00a0al. (2024a)",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
        "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024a."
      },
      {
        "tag": "Cha et\u00a0al. (2024)",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm.",
        "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024."
      },
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
        "journal": "InConference on Neural Information Processing Systems (NeurIPS), 2023a."
      },
      {
        "tag": "Chen et\u00a0al. (2023a)",
        "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic.",
        "authors": "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.",
        "journal": "arXiv preprint arXiv:2306.15195, 2023a."
      },
      {
        "tag": "Li et\u00a0al. (2023b)",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
        "journal": "InInternational conference on machine learning, pp.\u00a0 19730\u201319742. PMLR, 2023b."
      }
    ]
  },
  "S2.SS1.p3": {
    "text": "A notable aspect of the MLLMs is their reliance on existing pre-trained visual encoders in their systems, with CLIP (Radford et\u00a0al., 2021 ) visual encoder being a very common choice for a large number of methods (Cha et\u00a0al., 2024 ; Chen et\u00a0al., 2023a ; Zhang et\u00a0al., 2024a ; Zhu et\u00a0al., 2024b ; Lin et\u00a0al., 2024 ; Li et\u00a0al., 2023b ) . Trained on large image-text pair data, the CLIP visual encoder offers the feature space that combines language and image modalities in a sense, making it a desirable feature for MLLMs. Notably, all the existing works on MLLMs do not consider the scenarios where image compression is present, which is a significant departure from our work. We note that some approaches (Shi et\u00a0al., 2024 ; Li et\u00a0al., 2024 ) perform token reduction to minimize the inference cost of the downstream MLLMs. These techniques are orthogonal to and can be combined with our method (see Section A.6 for more discussions). Report issue for preceding element",
    "masked_text": "A notable aspect of the MLLMs is their reliance on existing pre-trained visual encoders in their systems, with CLIP [CITATION] visual encoder being a very common choice for a large number of methods [CITATION]. Trained on large image-text pair data, the CLIP visual encoder offers the feature space that combines language and image modalities in a sense, making it a desirable feature for MLLMs. Notably, all the existing works on MLLMs do not consider the scenarios where image compression is present, which is a significant departure from our work. We note that some approaches [CITATION] perform token reduction to minimize the inference cost of the downstream MLLMs. These techniques are orthogonal to and can be combined with our method (see Section A.6 for more discussions).Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2024a)",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention.",
        "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu\u00a0Qiao.",
        "journal": "InInternational Conference on Learning Representations (ICLR), 2024a."
      },
      {
        "tag": "Shi et\u00a0al. (2024)",
        "title": "Crossget: Cross-guided ensemble of tokens for accelerating vision-language transformers.",
        "authors": "Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang.",
        "journal": "InInternational Conference on Machine Learning (ICML), 2024."
      },
      {
        "tag": "Cha et\u00a0al. (2024)",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm.",
        "authors": "Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024."
      },
      {
        "tag": "Li et\u00a0al. (2024)",
        "title": "Tokenpacker: Efficient visual projector for multimodal llm.",
        "authors": "Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang.",
        "journal": "arXiv preprint arXiv:2407.02392, 2024."
      },
      {
        "tag": "Zhu et\u00a0al. (2024b)",
        "title": "Beyond text: Frozen large language models in visual signal comprehension.",
        "authors": "Lei Zhu, Fangyun Wei, and Yanye Lu.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b."
      },
      {
        "tag": "Radford et\u00a0al. (2021)",
        "title": "Learning transferable visual models from natural language supervision.",
        "authors": "Alec Radford, Jong\u00a0Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et\u00a0al.",
        "journal": "InInternational conference on machine learning, pp.\u00a0 8748\u20138763. PMLR, 2021."
      },
      {
        "tag": "Chen et\u00a0al. (2023a)",
        "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic.",
        "authors": "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.",
        "journal": "arXiv preprint arXiv:2306.15195, 2023a."
      },
      {
        "tag": "Li et\u00a0al. (2023b)",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
        "journal": "InInternational conference on machine learning, pp.\u00a0 19730\u201319742. PMLR, 2023b."
      },
      {
        "tag": "Lin et\u00a0al. (2024)",
        "title": "Vila: On pre-training for visual language models.",
        "authors": "Ji\u00a0Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024."
      }
    ]
  },
  "S2.SS2.p1": {
    "text": "Neural image compression systems have made significant progress in the past few years. As a matter of fact, several works (He et\u00a0al., 2022 ; Liu et\u00a0al., 2023b ) have even outperformed the traditional codecs such as intra coding in VVC (Bross et\u00a0al., 2021 ) . However, these methods primarily focus on the quality of reconstructed images for human perception. Coding for machines, in contrast, targets downstream machine vision over human perception, and it has attracted increasing attention recently. Report issue for preceding element",
    "masked_text": "Neural image compression systems have made significant progress in the past few years. As a matter of fact, several works [CITATION] have even outperformed the traditional codecs such as intra coding in VVC [CITATION]. However, these methods primarily focus on the quality of reconstructed images for human perception. Coding for machines, in contrast, targets downstream machine vision over human perception, and it has attracted increasing attention recently.Report issue for preceding element",
    "citations": [
      {
        "tag": "Bross et\u00a0al. (2021)",
        "title": "Overview of the versatile video coding (vvc) standard and its applications.",
        "authors": "Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary\u00a0J Sullivan, and Jens-Rainer Ohm.",
        "journal": "IEEE Transactions on Circuits and Systems for Video Technology, 31(10):3736\u20133764, 2021."
      },
      {
        "tag": "He et\u00a0al. (2022)",
        "title": "Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding.",
        "authors": "Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 5718\u20135727, 2022."
      },
      {
        "tag": "Liu et\u00a0al. (2023b)",
        "title": "Learned image compression with mixed transformer-cnn architectures.",
        "authors": "Jinming Liu, Heming Sun, and Jiro Katto.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 14388\u201314397, 2023b."
      }
    ]
  },
  "S2.SS2.p2": {
    "text": "A common approach simply involves training the compression system for a predefined target downstream computer vision task (Le et\u00a0al., 2021b ; a ; Wang et\u00a0al., 2022 ) , enabling the reconstructed image to be suitable for machine vision, albeit potentially sacrificing perceptual quality. Conversely, Chamain et\u00a0al. ( 2021 ) tune the task network to better process the compressed images, while Chen et\u00a0al. ( 2023b ) leverage prompt-tuning method on Transformer-based codecs to boost performance on multiple tasks. Also, with the trend of the new JPEG AI learning-based image coding standard (Ascenso et\u00a0al., 2023 ) , some methods (Liu et\u00a0al., 2022a ; 2021 ; Mei et\u00a0al., 2021 ; Singh et\u00a0al., 2020 ) utilize the compressed image latents instead of the reconstructed image for recognition through bridging the latents to task network. On the other hand, (Ding et\u00a0al., 2024 ) directly compress the intermediate features of recognition networks, while (Feng et\u00a0al., 2022 ) learn the omnipotent features suitable for various tasks in a self-supervised manner and fine-tune each task network tail on such features. Report issue for preceding element",
    "masked_text": "A common approach simply involves training the compression system for a predefined target downstream computer vision task [CITATION], enabling the reconstructed image to be suitable for machine vision, albeit potentially sacrificing perceptual quality. Conversely, [CITATION] tune the task network to better process the compressed images, while [CITATION] leverage prompt-tuning method on Transformer-based codecs to boost performance on multiple tasks. Also, with the trend of the new JPEG AI learning-based image coding standard [CITATION], some methods [CITATION] utilize the compressed image latents instead of the reconstructed image for recognition through bridging the latents to task network. On the other hand, [CITATION] directly compress the intermediate features of recognition networks, while [CITATION] learn the omnipotent features suitable for various tasks in a self-supervised manner and fine-tune each task network tail on such features.Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2022a)",
        "title": "Improving multiple machine vision tasks in the compressed domain.",
        "authors": "Jinming Liu, Heming Sun, and Jiro Katto.",
        "journal": "In2022 26th International Conference on Pattern Recognition (ICPR), pp.\u00a0 331\u2013337. IEEE, 2022a."
      },
      {
        "tag": "Le et\u00a0al. (2021b)",
        "title": "Learned image coding for machines: A content-adaptive approach.",
        "authors": "Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Hamed\u00a0Rezazadegan Tavakoli, and Esa Rahtu.",
        "journal": "In2021 IEEE International Conference on Multimedia and Expo (ICME), pp.\u00a0 1\u20136. IEEE, 2021b."
      },
      {
        "tag": "Feng et\u00a0al. (2022)",
        "title": "Image coding for machines with omnipotent feature learning.",
        "authors": "Ruoyu Feng, Xin Jin, Zongyu Guo, Runsen Feng, Yixin Gao, Tianyu He, Zhizheng Zhang, Simeng Sun, and Zhibo Chen.",
        "journal": "InComputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVII, pp.\u00a0 510\u2013528. Springer, 2022."
      },
      {
        "tag": "Wang et\u00a0al. (2022)",
        "title": "Deep image compression towards machine vision: A unified optimization framework.",
        "authors": "Shurun Wang, Zhao Wang, Shiqi Wang, and Yan Ye.",
        "journal": "IEEE Transactions on Circuits and Systems for Video Technology, 2022."
      },
      {
        "tag": "Ascenso et\u00a0al. (2023)",
        "title": "The jpeg ai standard: Providing efficient human and machine visual data consumption.",
        "authors": "Joao Ascenso, Elena Alshina, and Touradj Ebrahimi.",
        "journal": "Ieee Multimedia, 30(1):100\u2013111, 2023."
      },
      {
        "tag": "Singh et\u00a0al. (2020)",
        "title": "End-to-end learning of compressible features.",
        "authors": "Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Johannes Ball\u00e9, Abhinav Shrivastava, and George Toderici.",
        "journal": "In2020 IEEE International Conference on Image Processing (ICIP), pp.\u00a0 3349\u20133353. IEEE, 2020."
      },
      {
        "tag": "Chen et\u00a0al. (2023b)",
        "title": "Transtic: Transferring transformer-based image compression from human perception to machine perception.",
        "authors": "Yi-Hsin Chen, Ying-Chieh Weng, Chia-Hao Kao, Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng.",
        "journal": "InProceedings of the IEEE/CVF International Conference on Computer Vision, pp.\u00a0 23297\u201323307, 2023b."
      },
      {
        "tag": "Ding et\u00a0al. (2024)",
        "title": "Hierarchical image feature compression for machines via feature sparsity learning.",
        "authors": "Ding Ding, Zhenzhong Chen, Zizheng Liu, Xiaozhong Xu, and Shan Liu.",
        "journal": "IEEE Signal Processing Letters, 2024."
      },
      {
        "tag": "Mei et\u00a0al. (2021)",
        "title": "Learn a compression for objection detection - vae with a bridge.",
        "authors": "Yixin Mei, Fan Li, Li\u00a0Li, and Zhu Li.",
        "journal": "In2021 International Conference on Visual Communications and Image Processing (VCIP), pp.\u00a0 1\u20135, 2021."
      },
      {
        "tag": "Chamain et\u00a0al. (2021)",
        "title": "End-to-end optimized image compression for machines, a study.",
        "authors": "Lahiru\u00a0D Chamain, Fabien Racap\u00e9, Jean B\u00e9gaint, Akshay Pushparaja, and Simon Feltman.",
        "journal": "In2021 Data Compression Conference (DCC), pp.\u00a0 163\u2013172. IEEE, 2021."
      },
      {
        "tag": "Le et\u00a0al. (2021a)",
        "title": "Image coding for machines: an end-to-end learned approach.",
        "authors": "Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, and Esa Rahtu.",
        "journal": "InICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.\u00a0 1590\u20131594. IEEE, 2021a."
      },
      {
        "tag": "Liu et\u00a0al. (2021)",
        "title": "Learning in compressed domain for faster machine vision tasks.",
        "authors": "Jinming Liu, Heming Sun, and Jiro Katto.",
        "journal": "In2021 International Conference on Visual Communications and Image Processing (VCIP), pp.\u00a0 01\u201305, 2021."
      }
    ]
  },
  "S2.SS2.p3": {
    "text": "It is crucial to note that none of the coding for machine methods considers MLLMs at the receiver side. All the above-mentioned methods leverage back-propagation through recognition models to update the system or even re-train the recognition network itself, both of which are prohibitively expensive for MLLMs due to their huge scale. Therefore, the direct application of the same methods on MLLMs is almost infeasible. In addition, mainstream image coding for machines methods (e.g. Chen et\u00a0al. ( 2023b ); Ascenso et\u00a0al. ( 2023 ) ) remain mostly task-specific. They typically adopt a task-based loss, which restricts the resulting models to be optimized for a single task and recognition model, thus requiring re-training for each new task and incurring additional costs. We aim to be the first to propose a neural image compression system designed for MLLMs, achieved through a lightweight transform-neck and a surrogate loss, which bypasses the necessity of involving the entire billion-scale MLLM in the training process. Moreover, our surrogate loss incorporates a cross-entropy loss to bridge visual features with the text domain for MLLMs, complementing the feature-constraining distillation loss. This combination further differentiates our approach from existing methods. Report issue for preceding element",
    "masked_text": "It is crucial to note that none of the coding for machine methods considers MLLMs at the receiver side. All the above-mentioned methods leverage back-propagation through recognition models to update the system or even re-train the recognition network itself, both of which are prohibitively expensive for MLLMs due to their huge scale. Therefore, the direct application of the same methods on MLLMs is almost infeasible. In addition, mainstream image coding for machines methods (e.g. [CITATION]) remain mostly task-specific. They typically adopt a task-based loss, which restricts the resulting models to be optimized for a single task and recognition model, thus requiring re-training for each new task and incurring additional costs. We aim to be the first to propose a neural image compression system designed for MLLMs, achieved through a lightweight transform-neck and a surrogate loss, which bypasses the necessity of involving the entire billion-scale MLLM in the training process. Moreover, our surrogate loss incorporates a cross-entropy loss to bridge visual features with the text domain for MLLMs, complementing the feature-constraining distillation loss. This combination further differentiates our approach from existing methods.Report issue for preceding element",
    "citations": [
      {
        "tag": "Ascenso et\u00a0al. (2023)",
        "title": "The jpeg ai standard: Providing efficient human and machine visual data consumption.",
        "authors": "Joao Ascenso, Elena Alshina, and Touradj Ebrahimi.",
        "journal": "Ieee Multimedia, 30(1):100\u2013111, 2023."
      },
      {
        "tag": "Chen et\u00a0al. (2023b)",
        "title": "Transtic: Transferring transformer-based image compression from human perception to machine perception.",
        "authors": "Yi-Hsin Chen, Ying-Chieh Weng, Chia-Hao Kao, Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng.",
        "journal": "InProceedings of the IEEE/CVF International Conference on Computer Vision, pp.\u00a0 23297\u201323307, 2023b."
      }
    ]
  },
  "S3.SS1.p1": {
    "text": "The high-level architecture of a neural image codec is depicted in the top central green box in Figure 2 . In a typical hyperprior-based neural image codec (Ball\u00e9 et\u00a0al., 2018 ) , the key components include the main encoder g a subscript \ud835\udc54 \ud835\udc4e g_{a} italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , the main decoder g s subscript \ud835\udc54 \ud835\udc60 g_{s} italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , as well as the hyperprior encoder h a subscript \u210e \ud835\udc4e h_{a} italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and decoder h s subscript \u210e \ud835\udc60 h_{s} italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT . Given an RGB image x \u2208 \u211d 3 \u00d7 H \u00d7 W \ud835\udc65 superscript \u211d 3 \ud835\udc3b \ud835\udc4a x\\in\\mathbb{R}^{3\\times H\\times W} italic_x \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_H \u00d7 italic_W end_POSTSUPERSCRIPT , where H \ud835\udc3b H italic_H and W \ud835\udc4a W italic_W represent the height and width of the image, respectively, g a subscript \ud835\udc54 \ud835\udc4e g_{a} italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT performs the analysis transform of x \ud835\udc65 x italic_x and generates the image latent representation y \u2208 \u211d N \u00d7 H 16 \u00d7 W 16 \ud835\udc66 superscript \u211d \ud835\udc41 \ud835\udc3b 16 \ud835\udc4a 16 y\\in\\mathbb{R}^{N\\times\\frac{H}{16}\\times\\frac{W}{16}} italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG end_POSTSUPERSCRIPT , with N \ud835\udc41 N italic_N indicating the channel size. To transmit y \ud835\udc66 y italic_y more efficiently, it is first uniformly quantized into y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG and then entropy coded considering a learned prior distribution p \u2062 ( y ^ ) \ud835\udc5d ^ \ud835\udc66 p(\\hat{y}) italic_p ( over^ start_ARG italic_y end_ARG ) . This learned distribution is content dependent, thanks to the hyperprior encoder h a subscript \u210e \ud835\udc4e h_{a} italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and decoder h s subscript \u210e \ud835\udc60 h_{s} italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT . In particular, h a subscript \u210e \ud835\udc4e h_{a} italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT takes y \ud835\udc66 y italic_y as input and produces the side information z \u2208 \u211d N h \u00d7 H 64 \u00d7 W 64 \ud835\udc67 superscript \u211d subscript \ud835\udc41 \u210e \ud835\udc3b 64 \ud835\udc4a 64 z\\in\\mathbb{R}^{N_{h}\\times\\frac{H}{64}\\times\\frac{W}{64}} italic_z \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u00d7 divide start_ARG italic_H end_ARG start_ARG 64 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 64 end_ARG end_POSTSUPERSCRIPT , that is used to generate the learned distribution for entropy coding, where N h subscript \ud835\udc41 \u210e N_{h} italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is the channel size of the side information. The quantized version of z \ud835\udc67 z italic_z , denoted as z ^ ^ \ud835\udc67 \\hat{z} over^ start_ARG italic_z end_ARG , is transmitted into the bitstream, in order to recover y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG . Lastly, y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG undergoes the synthesis transform with g s subscript \ud835\udc54 \ud835\udc60 g_{s} italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , which reconstructs the image x ^ \u2208 \u211d 3 \u00d7 H \u00d7 W ^ \ud835\udc65 superscript \u211d 3 \ud835\udc3b \ud835\udc4a \\hat{x}\\in\\mathbb{R}^{3\\times H\\times W} over^ start_ARG italic_x end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_H \u00d7 italic_W end_POSTSUPERSCRIPT . Report issue for preceding element",
    "masked_text": "The high-level architecture of a neural image codec is depicted in the top central green box in Figure 2. In a typical hyperprior-based neural image codec [CITATION], the key components include the main encoder gasubscript\ud835\udc54\ud835\udc4eg_{a}italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, the main decoder gssubscript\ud835\udc54\ud835\udc60g_{s}italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, as well as the hyperprior encoder hasubscript\u210e\ud835\udc4eh_{a}italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and decoder hssubscript\u210e\ud835\udc60h_{s}italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. Given an RGB image x\u2208\u211d3\u00d7H\u00d7W\ud835\udc65superscript\u211d3\ud835\udc3b\ud835\udc4ax\\in\\mathbb{R}^{3\\times H\\times W}italic_x \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_H \u00d7 italic_W end_POSTSUPERSCRIPT, where H\ud835\udc3bHitalic_H and W\ud835\udc4aWitalic_W represent the height and width of the image, respectively, gasubscript\ud835\udc54\ud835\udc4eg_{a}italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT performs the analysis transform of x\ud835\udc65xitalic_x and generates the image latent representation y\u2208\u211dN\u00d7H16\u00d7W16\ud835\udc66superscript\u211d\ud835\udc41\ud835\udc3b16\ud835\udc4a16y\\in\\mathbb{R}^{N\\times\\frac{H}{16}\\times\\frac{W}{16}}italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG end_POSTSUPERSCRIPT, with N\ud835\udc41Nitalic_N indicating the channel size. To transmit y\ud835\udc66yitalic_y more efficiently, it is first uniformly quantized into y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG and then entropy coded considering a learned prior distribution p\u2062(y^)\ud835\udc5d^\ud835\udc66p(\\hat{y})italic_p ( over^ start_ARG italic_y end_ARG ). This learned distribution is content dependent, thanks to the hyperprior encoder hasubscript\u210e\ud835\udc4eh_{a}italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and decoder hssubscript\u210e\ud835\udc60h_{s}italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. In particular, hasubscript\u210e\ud835\udc4eh_{a}italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT takes y\ud835\udc66yitalic_y as input and produces the side information z\u2208\u211dNh\u00d7H64\u00d7W64\ud835\udc67superscript\u211dsubscript\ud835\udc41\u210e\ud835\udc3b64\ud835\udc4a64z\\in\\mathbb{R}^{N_{h}\\times\\frac{H}{64}\\times\\frac{W}{64}}italic_z \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u00d7 divide start_ARG italic_H end_ARG start_ARG 64 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 64 end_ARG end_POSTSUPERSCRIPT, that is used to generate the learned distribution for entropy coding, where Nhsubscript\ud835\udc41\u210eN_{h}italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is the channel size of the side information. The quantized version of z\ud835\udc67zitalic_z, denoted as z^^\ud835\udc67\\hat{z}over^ start_ARG italic_z end_ARG, is transmitted into the bitstream, in order to recover y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG. Lastly, y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG undergoes the synthesis transform with gssubscript\ud835\udc54\ud835\udc60g_{s}italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, which reconstructs the image x^\u2208\u211d3\u00d7H\u00d7W^\ud835\udc65superscript\u211d3\ud835\udc3b\ud835\udc4a\\hat{x}\\in\\mathbb{R}^{3\\times H\\times W}over^ start_ARG italic_x end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_H \u00d7 italic_W end_POSTSUPERSCRIPT.Report issue for preceding element",
    "citations": [
      {
        "tag": "Ball\u00e9 et\u00a0al. (2018)",
        "title": "Variational image compression with a scale hyperprior.",
        "authors": "Johannes Ball\u00e9, David Minnen, Saurabh Singh, Sung\u00a0Jin Hwang, and Nick Johnston.",
        "journal": "arXiv preprint arXiv:1802.01436, 2018."
      }
    ]
  },
  "S3.SS2.p1": {
    "text": "In this work, we focus on the scenario where MLLMs are hosted on the server side, while users on end devices need to perform inference on the remote model using both text and images as inputs. Given the necessity of incorporating image compression to ensure efficient transmission, we propose the first compression framework with the consideration of MLLMs as downstream application networks, aiming to mitigate the potential task performance drop caused by image compression. Figure 2 illustrates our overall framework, which includes three major components: the neural image codec, our proposed transform-neck, and the MLLM. The depicted MLLM system adheres to a typical structure, consisting of a visual encoder, an LLM, and a connector component facilitating the transformation of features from the visual encoder to the LLM. Note that all the MLLMs are adopted off-the-shelf and without any update. Report issue for preceding element",
    "masked_text": "In this work, we focus on the scenario where MLLMs are hosted on the server side, while users on end devices need to perform inference on the remote model using both text and images as inputs. Given the necessity of incorporating image compression to ensure efficient transmission, we propose the first compression framework with the consideration of MLLMs as downstream application networks, aiming to mitigate the potential task performance drop caused by image compression. Figure 2 illustrates our overall framework, which includes three major components: the neural image codec, our proposed transform-neck, and the MLLM. The depicted MLLM system adheres to a typical structure, consisting of a visual encoder, an LLM, and a connector component facilitating the transformation of features from the visual encoder to the LLM. Note that all the MLLMs are adopted off-the-shelf and without any update.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p2": {
    "text": "During inference, the input image at the end device is passed through an encoder g a subscript \ud835\udc54 \ud835\udc4e g_{a} italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT to generate the quantized latents y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG for transmission. Next, y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG is directly passed through a lightweight transform-neck T \ud835\udc47 T italic_T for transformation into a middle layer of the visual encoder of an MLLM. We opt to adapt the image latents rather than the reconstructed images because the image latents inherently contain the information needed for reconstructing the image, and potentially the semantic information for the downstream tasks (when the image encoder is guided properly). By skipping the image decoding process, our method offers reduced computational complexity while maintaining the task performance. The rest of the MLLM system operates without any changes to generate the desired output response. Report issue for preceding element",
    "masked_text": "During inference, the input image at the end device is passed through an encoder gasubscript\ud835\udc54\ud835\udc4eg_{a}italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT to generate the quantized latents y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG for transmission. Next, y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG is directly passed through a lightweight transform-neck T\ud835\udc47Titalic_T for transformation into a middle layer of the visual encoder of an MLLM. We opt to adapt the image latents rather than the reconstructed images because the image latents inherently contain the information needed for reconstructing the image, and potentially the semantic information for the downstream tasks (when the image encoder is guided properly). By skipping the image decoding process, our method offers reduced computational complexity while maintaining the task performance. The rest of the MLLM system operates without any changes to generate the desired output response.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.p3": {
    "text": "In training, to address the challenge of back-propagating the task loss through the entire MLLM, we propose a novel surrogate loss that updates the system by back-propagating solely from the visual encoder (which is not re-trained), bypassing the billion-parameter LLM.\nIn our work, we examine three distinct settings denoted as (d1), (d2) and (d3), as illustrated in Figure 1 . Firstly, in (d1), we consider the practical scenario where a fixed off-the-shelf image codec pre-trained for human perception is directly used alongside our transform-neck. In this setting, our framework offers the option for users to decode the image latents y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG for reconstruction by using the decoder g s subscript \ud835\udc54 \ud835\udc60 g_{s} italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT instead of the transform-neck. In this way, the quality of the decoded image is not affected, as the image codec is not updated in the present case. Then, we extend the analysis to scenarios (d2) and (d3) to examine the impact of jointly training the image codec and transform-neck. In (d2), the entire image codec undergoes re-training to accommodate both human and machine perception, while in (d3), the encoder is re-trained specifically for machine perception. Report issue for preceding element",
    "masked_text": "In training, to address the challenge of back-propagating the task loss through the entire MLLM, we propose a novel surrogate loss that updates the system by back-propagating solely from the visual encoder (which is not re-trained), bypassing the billion-parameter LLM. In our work, we examine three distinct settings denoted as (d1), (d2) and (d3), as illustrated in Figure 1. Firstly, in (d1), we consider the practical scenario where a fixed off-the-shelf image codec pre-trained for human perception is directly used alongside our transform-neck. In this setting, our framework offers the option for users to decode the image latents y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG for reconstruction by using the decoder gssubscript\ud835\udc54\ud835\udc60g_{s}italic_g start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT instead of the transform-neck. In this way, the quality of the decoded image is not affected, as the image codec is not updated in the present case. Then, we extend the analysis to scenarios (d2) and (d3) to examine the impact of jointly training the image codec and transform-neck. In (d2), the entire image codec undergoes re-training to accommodate both human and machine perception, while in (d3), the encoder is re-trained specifically for machine perception.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.p1": {
    "text": "Our transform-neck is designed to be a lightweight module, consisting only of a linear projection, a self-attention mechanism, a feed-forward layer, and two layer norms, as shown in the central red box in Figure 2 . Its purpose is to adapt the compressed image latents y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG into an efficient representation for consumption by the downstream MLLMs. In fact, rather than reconstructing the image and using it as input to the MLLM, we propose leveraging the latent representation directly. Report issue for preceding element",
    "masked_text": "Our transform-neck is designed to be a lightweight module, consisting only of a linear projection, a self-attention mechanism, a feed-forward layer, and two layer norms, as shown in the central red box in Figure 2. Its purpose is to adapt the compressed image latents y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG into an efficient representation for consumption by the downstream MLLMs. In fact, rather than reconstructing the image and using it as input to the MLLM, we propose leveraging the latent representation directly.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.p2": {
    "text": "Since the image encoder g a subscript \ud835\udc54 \ud835\udc4e g_{a} italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT already functions as a feature extractor, similar to the early layers of the visual encoder C \ud835\udc36 C italic_C , we bridge the output of our transform-neck directly into the intermediate features of C \ud835\udc36 C italic_C , effectively integrating the image codec with the MLLM system. The decision on which initial layers to bypass depends on the specific type of visual encoder used in the MLLM. For instance, when using the CLIP visual encoder as C \ud835\udc36 C italic_C , we found that connecting the transform-neck to the third Transformer layer, bypassing the first two, yields optimal results (see Section 4.5 for ablation experiments justifying this design). Note that skipping the initial layers of C \ud835\udc36 C italic_C further reduces computational complexity of our framework. We denote the partial visual encoder as C \u2032 superscript \ud835\udc36 \u2032 C^{\\prime} italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT . Report issue for preceding element",
    "masked_text": "Since the image encoder gasubscript\ud835\udc54\ud835\udc4eg_{a}italic_g start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT already functions as a feature extractor, similar to the early layers of the visual encoder C\ud835\udc36Citalic_C, we bridge the output of our transform-neck directly into the intermediate features of C\ud835\udc36Citalic_C, effectively integrating the image codec with the MLLM system. The decision on which initial layers to bypass depends on the specific type of visual encoder used in the MLLM. For instance, when using the CLIP visual encoder as C\ud835\udc36Citalic_C, we found that connecting the transform-neck to the third Transformer layer, bypassing the first two, yields optimal results (see Section 4.5 for ablation experiments justifying this design). Note that skipping the initial layers of C\ud835\udc36Citalic_C further reduces computational complexity of our framework. We denote the partial visual encoder as C\u2032superscript\ud835\udc36\u2032C^{\\prime}italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.Report issue for preceding element",
    "citations": []
  },
  "S3.SS4.p1": {
    "text": "To avoid involving huge MLLMs in the training process, and thus bypassing back-propagation through their entire structure, we propose a surrogate loss \u2112 S subscript \u2112 \ud835\udc46 \\mathcal{L}_{S} caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT which is back-propagated through only the partial visual encoder C \u2032 superscript \ud835\udc36 \u2032 C^{\\prime} italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT . Specifically, we design our surrogate loss to consist of two terms: distillation loss \u2112 d \u2062 i \u2062 s \u2062 t subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 \\mathcal{L}_{dist} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT and cross-entropy loss \u2112 C \u2062 E subscript \u2112 \ud835\udc36 \ud835\udc38 \\mathcal{L}_{CE} caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT . Report issue for preceding element",
    "masked_text": "To avoid involving huge MLLMs in the training process, and thus bypassing back-propagation through their entire structure, we propose a surrogate loss \u2112Ssubscript\u2112\ud835\udc46\\mathcal{L}_{S}caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT which is back-propagated through only the partial visual encoder C\u2032superscript\ud835\udc36\u2032C^{\\prime}italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. Specifically, we design our surrogate loss to consist of two terms: distillation loss \u2112d\u2062i\u2062s\u2062tsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\mathcal{L}_{dist}caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT and cross-entropy loss \u2112C\u2062Esubscript\u2112\ud835\udc36\ud835\udc38\\mathcal{L}_{CE}caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT.Report issue for preceding element",
    "citations": []
  },
  "S3.SS4.p2": {
    "text": "To retain the downstream MLLM performance, the resulting features C \u2032 \u2062 ( T \u2062 ( y ^ ) ) superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 C^{\\prime}(T(\\hat{y})) italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) when using our transformed latents should resemble closely those obtained when inputting the uncompressed image into C \ud835\udc36 C italic_C , that is C \u2062 ( x ) \ud835\udc36 \ud835\udc65 C(x) italic_C ( italic_x ) . To this end, we introduce the following distillation loss \u2112 d \u2062 i \u2062 s \u2062 t subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 \\mathcal{L}_{dist} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , aiming to minimize the Mean Squared Error (MSE) between the two output features: Report issue for preceding element \u2112 d \u2062 i \u2062 s \u2062 t = MSE \u2062 ( C \u2032 \u2062 ( T \u2062 ( y ^ ) ) , C \u2062 ( x ) ) . subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 MSE superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 \ud835\udc36 \ud835\udc65 \\displaystyle\\mathcal{L}_{dist}=\\text{MSE}(C^{\\prime}(T(\\hat{y})),C(x)).%\n\\vspace{-2mm} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT = MSE ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C ( italic_x ) ) . (1)",
    "masked_text": "To retain the downstream MLLM performance, the resulting features C\u2032\u2062(T\u2062(y^))superscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66C^{\\prime}(T(\\hat{y}))italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) when using our transformed latents should resemble closely those obtained when inputting the uncompressed image into C\ud835\udc36Citalic_C, that is C\u2062(x)\ud835\udc36\ud835\udc65C(x)italic_C ( italic_x ). To this end, we introduce the following distillation loss \u2112d\u2062i\u2062s\u2062tsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\mathcal{L}_{dist}caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT, aiming to minimize the Mean Squared Error (MSE) between the two output features:Report issue for preceding element \u2112d\u2062i\u2062s\u2062t=MSE\u2062(C\u2032\u2062(T\u2062(y^)),C\u2062(x)).subscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61MSEsuperscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66\ud835\udc36\ud835\udc65\\displaystyle\\mathcal{L}_{dist}=\\text{MSE}(C^{\\prime}(T(\\hat{y})),C(x)).% \\vspace{-2mm}caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT = MSE ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C ( italic_x ) ) . (1)",
    "citations": []
  },
  "S3.SS4.p3": {
    "text": "In addition to the distillation loss, we propose incorporating a second term, which is the cross-entropy loss \u2112 C \u2062 E subscript \u2112 \ud835\udc36 \ud835\udc38 \\mathcal{L}_{CE} caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT calculated using the following procedure.\nWe take a classification dataset consisting of n \ud835\udc5b n italic_n images and m \ud835\udc5a m italic_m text labels { l 1 , l 2 , \u2026 , l m } subscript \ud835\udc59 1 subscript \ud835\udc59 2 \u2026 subscript \ud835\udc59 \ud835\udc5a \\{l_{1},l_{2},\\dots,l_{m}\\} { italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } , which correspond to m \ud835\udc5a m italic_m distinct classes. Each image belongs to one of these m \ud835\udc5a m italic_m classes. Next, we compute: (1) the n \ud835\udc5b n italic_n image embedding C \u2032 \u2062 ( T \u2062 ( y ^ ) ) superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 C^{\\prime}(T(\\hat{y})) italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) and (2) the m \ud835\udc5a m italic_m text embeddings C t \u2062 ( l j ) subscript \ud835\udc36 \ud835\udc61 subscript \ud835\udc59 \ud835\udc57 C_{t}(l_{j}) italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , where C t subscript \ud835\udc36 \ud835\udc61 C_{t} italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a text encoder applied to each text label l j subscript \ud835\udc59 \ud835\udc57 l_{j} italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT .\nIn particular, we use the CLIP text encoder, independently of the visual encoder integrated into the MLLM under consideration.\nFor each image, we then calculate the cosine similarity CS \u2062 ( \u22c5 , \u22c5 ) CS \u22c5 \u22c5 \\text{CS}(\\cdot,\\cdot) CS ( \u22c5 , \u22c5 ) between its image embedding C \u2032 \u2062 ( T \u2062 ( y ^ ) ) superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 C^{\\prime}(T(\\hat{y})) italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) and each of the m \ud835\udc5a m italic_m text embeddings C t \u2062 ( l j ) subscript \ud835\udc36 \ud835\udc61 subscript \ud835\udc59 \ud835\udc57 C_{t}(l_{j}) italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) for j = 1 , 2 , \u2026 , m \ud835\udc57 1 2 \u2026 \ud835\udc5a j=1,2,\\dots,m italic_j = 1 , 2 , \u2026 , italic_m . This produces a vector v \ud835\udc63 v italic_v of similarity scores between the given image and all the m \ud835\udc5a m italic_m classes, that is: Report issue for preceding element v = [ CS \u2062 ( C \u2032 \u2062 ( T \u2062 ( y ^ ) ) , C t \u2062 ( l 1 ) ) , \u2026 , CS \u2062 ( C \u2032 \u2062 ( T \u2062 ( y ^ ) ) , C t \u2062 ( l m ) ) ] . \ud835\udc63 CS superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 subscript \ud835\udc36 \ud835\udc61 subscript \ud835\udc59 1 \u2026 CS superscript \ud835\udc36 \u2032 \ud835\udc47 ^ \ud835\udc66 subscript \ud835\udc36 \ud835\udc61 subscript \ud835\udc59 \ud835\udc5a v=[\\text{CS}(C^{\\prime}(T(\\hat{y})),C_{t}(l_{1})),\\ldots,\\text{CS}(C^{\\prime}(%\nT(\\hat{y})),C_{t}(l_{m}))]. italic_v = [ CS ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) , \u2026 , CS ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) ) ] . (2) The resulting similarity vector is transformed into a probability distribution over the m \ud835\udc5a m italic_m classes using a softmax function. Finally, the cross-entropy loss is computed with respect to the corresponding one-hot encoded ground truth label vector \ud835\udc2d \ud835\udc2d \\mathbf{t} bold_t , thus: Report issue for preceding element \u2112 C \u2062 E = CE \u2062 ( Softmax \u2062 ( v ) , \ud835\udc2d ) . subscript \u2112 \ud835\udc36 \ud835\udc38 CE Softmax \ud835\udc63 \ud835\udc2d \\displaystyle\\mathcal{L}_{CE}=\\text{CE}(\\text{Softmax}(v),\\mathbf{t}).\\vspace{%\n-2mm} caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT = CE ( Softmax ( italic_v ) , bold_t ) . (3) This approach aims to bridge the visual features to the text domain specifically for MLLM-based vision tasks, distinguishing our method from existing works in the field of coding for machines. We validate the effectiveness of our loss function design in the ablation study (Section 4.5 ). Report issue for preceding element",
    "masked_text": "In addition to the distillation loss, we propose incorporating a second term, which is the cross-entropy loss \u2112C\u2062Esubscript\u2112\ud835\udc36\ud835\udc38\\mathcal{L}_{CE}caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT calculated using the following procedure. We take a classification dataset consisting of n\ud835\udc5bnitalic_n images and m\ud835\udc5amitalic_m text labels {l1,l2,\u2026,lm}subscript\ud835\udc591subscript\ud835\udc592\u2026subscript\ud835\udc59\ud835\udc5a\\{l_{1},l_{2},\\dots,l_{m}\\}{ italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, which correspond to m\ud835\udc5amitalic_m distinct classes. Each image belongs to one of these m\ud835\udc5amitalic_m classes. Next, we compute: (1) the n\ud835\udc5bnitalic_n image embedding C\u2032\u2062(T\u2062(y^))superscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66C^{\\prime}(T(\\hat{y}))italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) and (2) the m\ud835\udc5amitalic_m text embeddings Ct\u2062(lj)subscript\ud835\udc36\ud835\udc61subscript\ud835\udc59\ud835\udc57C_{t}(l_{j})italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), where Ctsubscript\ud835\udc36\ud835\udc61C_{t}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a text encoder applied to each text label ljsubscript\ud835\udc59\ud835\udc57l_{j}italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. In particular, we use the CLIP text encoder, independently of the visual encoder integrated into the MLLM under consideration. For each image, we then calculate the cosine similarity CS\u2062(\u22c5,\u22c5)CS\u22c5\u22c5\\text{CS}(\\cdot,\\cdot)CS ( \u22c5 , \u22c5 ) between its image embedding C\u2032\u2062(T\u2062(y^))superscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66C^{\\prime}(T(\\hat{y}))italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) and each of the m\ud835\udc5amitalic_m text embeddings Ct\u2062(lj)subscript\ud835\udc36\ud835\udc61subscript\ud835\udc59\ud835\udc57C_{t}(l_{j})italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) for j=1,2,\u2026,m\ud835\udc5712\u2026\ud835\udc5aj=1,2,\\dots,mitalic_j = 1 , 2 , \u2026 , italic_m. This produces a vector v\ud835\udc63vitalic_v of similarity scores between the given image and all the m\ud835\udc5amitalic_m classes, that is:Report issue for preceding element v=[CS\u2062(C\u2032\u2062(T\u2062(y^)),Ct\u2062(l1)),\u2026,CS\u2062(C\u2032\u2062(T\u2062(y^)),Ct\u2062(lm))].\ud835\udc63CSsuperscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66subscript\ud835\udc36\ud835\udc61subscript\ud835\udc591\u2026CSsuperscript\ud835\udc36\u2032\ud835\udc47^\ud835\udc66subscript\ud835\udc36\ud835\udc61subscript\ud835\udc59\ud835\udc5av=[\\text{CS}(C^{\\prime}(T(\\hat{y})),C_{t}(l_{1})),\\ldots,\\text{CS}(C^{\\prime}(% T(\\hat{y})),C_{t}(l_{m}))].italic_v = [ CS ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) , \u2026 , CS ( italic_C start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_T ( over^ start_ARG italic_y end_ARG ) ) , italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) ) ] . (2) The resulting similarity vector is transformed into a probability distribution over the m\ud835\udc5amitalic_m classes using a softmax function. Finally, the cross-entropy loss is computed with respect to the corresponding one-hot encoded ground truth label vector \ud835\udc2d\ud835\udc2d\\mathbf{t}bold_t, thus: Report issue for preceding element \u2112C\u2062E=CE\u2062(Softmax\u2062(v),\ud835\udc2d).subscript\u2112\ud835\udc36\ud835\udc38CESoftmax\ud835\udc63\ud835\udc2d\\displaystyle\\mathcal{L}_{CE}=\\text{CE}(\\text{Softmax}(v),\\mathbf{t}).\\vspace{% -2mm}caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT = CE ( Softmax ( italic_v ) , bold_t ) . (3) This approach aims to bridge the visual features to the text domain specifically for MLLM-based vision tasks, distinguishing our method from existing works in the field of coding for machines. We validate the effectiveness of our loss function design in the ablation study (Section 4.5).Report issue for preceding element",
    "citations": []
  },
  "S3.SS5.p1": {
    "text": "To explore the capabilities of our method under the application scenarios introduced in Section 3.2 , we employ a two-phase training procedure that adapts to different scenarios. The first phase is shared for all scenarios and focuses on training the transform-neck exclusively. The second phase, applicable only to scenarios where the codec can be updated, involves joint optimization of both the transform-neck and the image codec. Table 1 provides a summary of the training procedure. Report issue for preceding element",
    "masked_text": "To explore the capabilities of our method under the application scenarios introduced in Section 3.2, we employ a two-phase training procedure that adapts to different scenarios. The first phase is shared for all scenarios and focuses on training the transform-neck exclusively. The second phase, applicable only to scenarios where the codec can be updated, involves joint optimization of both the transform-neck and the image codec. Table 1 provides a summary of the training procedure.Report issue for preceding element",
    "citations": []
  },
  "S3.SS5.SSS0.Px1.p1": {
    "text": "In the first phase, we train only the transform-neck using a progressive training strategy with the surrogate loss \u2112 S subscript \u2112 \ud835\udc46 \\mathcal{L}_{S} caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and an off-the-shelf image codec. This phase is divided into three stages: Report issue for preceding element \u2112 S = { \u2112 C \u2062 E , epoch < E 1 , \u03b1 \u2062 \u2112 C \u2062 E + \u03b2 \u2062 \u2112 d \u2062 i \u2062 s \u2062 t , E 1 \u2264 epoch < E 2 , \u2112 d \u2062 i \u2062 s \u2062 t , epoch \u2265 E 2 , subscript \u2112 \ud835\udc46 cases subscript \u2112 \ud835\udc36 \ud835\udc38 epoch subscript \ud835\udc38 1 \ud835\udefc subscript \u2112 \ud835\udc36 \ud835\udc38 \ud835\udefd subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 subscript \ud835\udc38 1 epoch subscript \ud835\udc38 2 subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 epoch subscript \ud835\udc38 2 \\displaystyle\\mathcal{L}_{S}=\\begin{cases}\\mathcal{L}_{CE},&\\text{epoch}<E_{1}%\n,\\\\\n\\alpha\\mathcal{L}_{CE}+\\beta\\mathcal{L}_{dist},&E_{1}\\leq\\text{epoch}<E_{2},\\\\\n\\mathcal{L}_{dist},&\\text{epoch}\\geq E_{2},\\end{cases} caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = { start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT , end_CELL start_CELL epoch < italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_\u03b1 caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT + italic_\u03b2 caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2264 epoch < italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL epoch \u2265 italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW (4) where the weighting factors \u03b1 \ud835\udefc \\alpha italic_\u03b1 and \u03b2 \ud835\udefd \\beta italic_\u03b2 are set with a ratio of 1:100 for the two loss terms, and E 1 , E 2 subscript \ud835\udc38 1 subscript \ud835\udc38 2 E_{1},E_{2} italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are empirically set to 20 and 40, respectively, in our experiments. This initial phase ensures that the transform-neck learns the transformation to align with the target latent space. Report issue for preceding element",
    "masked_text": "In the first phase, we train only the transform-neck using a progressive training strategy with the surrogate loss \u2112Ssubscript\u2112\ud835\udc46\\mathcal{L}_{S}caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and an off-the-shelf image codec. This phase is divided into three stages:Report issue for preceding element \u2112S={\u2112C\u2062E,epoch<E1,\u03b1\u2062\u2112C\u2062E+\u03b2\u2062\u2112d\u2062i\u2062s\u2062t,E1\u2264epoch<E2,\u2112d\u2062i\u2062s\u2062t,epoch\u2265E2,subscript\u2112\ud835\udc46casessubscript\u2112\ud835\udc36\ud835\udc38epochsubscript\ud835\udc381\ud835\udefcsubscript\u2112\ud835\udc36\ud835\udc38\ud835\udefdsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61subscript\ud835\udc381epochsubscript\ud835\udc382subscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61epochsubscript\ud835\udc382\\displaystyle\\mathcal{L}_{S}=\\begin{cases}\\mathcal{L}_{CE},&\\text{epoch}<E_{1}% ,\\\\ \\alpha\\mathcal{L}_{CE}+\\beta\\mathcal{L}_{dist},&E_{1}\\leq\\text{epoch}<E_{2},\\\\ \\mathcal{L}_{dist},&\\text{epoch}\\geq E_{2},\\end{cases}caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = { start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT , end_CELL start_CELL epoch < italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_\u03b1 caligraphic_L start_POSTSUBSCRIPT italic_C italic_E end_POSTSUBSCRIPT + italic_\u03b2 caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2264 epoch < italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , end_CELL start_CELL epoch \u2265 italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , end_CELL end_ROW (4) where the weighting factors \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b2\ud835\udefd\\betaitalic_\u03b2 are set with a ratio of 1:100 for the two loss terms, and E1,E2subscript\ud835\udc381subscript\ud835\udc382E_{1},E_{2}italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are empirically set to 20 and 40, respectively, in our experiments. This initial phase ensures that the transform-neck learns the transformation to align with the target latent space.Report issue for preceding element",
    "citations": []
  },
  "S3.SS5.SSS0.Px2.p1": {
    "text": "For (d2) and (d3), where the image codec is allowed to be re-trained to produce latent representation more suitable for machine perception, the second phase is introduced with transform-neck and image codec jointly updated after phase 1 converges. Report issue for preceding element",
    "masked_text": "For (d2) and (d3), where the image codec is allowed to be re-trained to produce latent representation more suitable for machine perception, the second phase is introduced with transform-neck and image codec jointly updated after phase 1 converges.Report issue for preceding element",
    "citations": []
  },
  "S3.SS5.SSS0.Px2.p2": {
    "text": "The scenario (d2), referred to as multi-task, aims to accommodate both human and machine perception. As a result, it is trained jointly with the transform-neck on both the distillation loss and traditional rate-distortion loss, i.e. \u2112 d \u2062 2 = R + \u03bb \u2062 ( \u03b3 \u2062 d r \u2062 e \u2062 c \u2062 o \u2062 n \u2062 ( x , x ^ ) + \u03b4 \u2062 \u2112 d \u2062 i \u2062 s \u2062 t ) subscript \u2112 \ud835\udc51 2 \ud835\udc45 \ud835\udf06 \ud835\udefe subscript \ud835\udc51 \ud835\udc5f \ud835\udc52 \ud835\udc50 \ud835\udc5c \ud835\udc5b \ud835\udc65 ^ \ud835\udc65 \ud835\udeff subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 \\mathcal{L}_{d2}=R+\\lambda(\\gamma d_{recon}(x,\\hat{x})+\\delta\\mathcal{L}_{dist}) caligraphic_L start_POSTSUBSCRIPT italic_d 2 end_POSTSUBSCRIPT = italic_R + italic_\u03bb ( italic_\u03b3 italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT ( italic_x , over^ start_ARG italic_x end_ARG ) + italic_\u03b4 caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT ) , where R = \u2212 log \u2061 p \u2062 ( z ^ ) \u2212 log \u2061 p \u2062 ( y ^ | z ^ ) \ud835\udc45 \ud835\udc5d ^ \ud835\udc67 \ud835\udc5d conditional ^ \ud835\udc66 ^ \ud835\udc67 R=-\\log p(\\hat{z})-\\log p(\\hat{y}|\\hat{z}) italic_R = - roman_log italic_p ( over^ start_ARG italic_z end_ARG ) - roman_log italic_p ( over^ start_ARG italic_y end_ARG | over^ start_ARG italic_z end_ARG ) is the estimated rate of y ^ ^ \ud835\udc66 \\hat{y} over^ start_ARG italic_y end_ARG and z ^ ^ \ud835\udc67 \\hat{z} over^ start_ARG italic_z end_ARG , and d r \u2062 e \u2062 c \u2062 o \u2062 n subscript \ud835\udc51 \ud835\udc5f \ud835\udc52 \ud835\udc50 \ud835\udc5c \ud835\udc5b d_{recon} italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT is the reconstruction loss calculated as the MSE between the uncompressed image x \ud835\udc65 x italic_x and the reconstructed image x ^ ^ \ud835\udc65 \\hat{x} over^ start_ARG italic_x end_ARG . The hyper-parameter \u03bb \ud835\udf06 \\lambda italic_\u03bb controls the rate-distortion trade-off, while \u03b3 \ud835\udefe \\gamma italic_\u03b3 and \u03b4 \ud835\udeff \\delta italic_\u03b4 weight the two losses. Report issue for preceding element",
    "masked_text": "The scenario (d2), referred to as multi-task, aims to accommodate both human and machine perception. As a result, it is trained jointly with the transform-neck on both the distillation loss and traditional rate-distortion loss, i.e. \u2112d\u20622=R+\u03bb\u2062(\u03b3\u2062dr\u2062e\u2062c\u2062o\u2062n\u2062(x,x^)+\u03b4\u2062\u2112d\u2062i\u2062s\u2062t)subscript\u2112\ud835\udc512\ud835\udc45\ud835\udf06\ud835\udefesubscript\ud835\udc51\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc65^\ud835\udc65\ud835\udeffsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\mathcal{L}_{d2}=R+\\lambda(\\gamma d_{recon}(x,\\hat{x})+\\delta\\mathcal{L}_{dist})caligraphic_L start_POSTSUBSCRIPT italic_d 2 end_POSTSUBSCRIPT = italic_R + italic_\u03bb ( italic_\u03b3 italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT ( italic_x , over^ start_ARG italic_x end_ARG ) + italic_\u03b4 caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT ), where R=\u2212log\u2061p\u2062(z^)\u2212log\u2061p\u2062(y^|z^)\ud835\udc45\ud835\udc5d^\ud835\udc67\ud835\udc5dconditional^\ud835\udc66^\ud835\udc67R=-\\log p(\\hat{z})-\\log p(\\hat{y}|\\hat{z})italic_R = - roman_log italic_p ( over^ start_ARG italic_z end_ARG ) - roman_log italic_p ( over^ start_ARG italic_y end_ARG | over^ start_ARG italic_z end_ARG ) is the estimated rate of y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG and z^^\ud835\udc67\\hat{z}over^ start_ARG italic_z end_ARG, and dr\u2062e\u2062c\u2062o\u2062nsubscript\ud835\udc51\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc5c\ud835\udc5bd_{recon}italic_d start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPT is the reconstruction loss calculated as the MSE between the uncompressed image x\ud835\udc65xitalic_x and the reconstructed image x^^\ud835\udc65\\hat{x}over^ start_ARG italic_x end_ARG. The hyper-parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb controls the rate-distortion trade-off, while \u03b3\ud835\udefe\\gammaitalic_\u03b3 and \u03b4\ud835\udeff\\deltaitalic_\u03b4 weight the two losses.Report issue for preceding element",
    "citations": []
  },
  "S3.SS5.SSS0.Px2.p3": {
    "text": "In (d3), where the downstream applications do not require image reconstruction, the encoder and transform-neck are jointly optimized to minimize the trade-off cost between the rate R \ud835\udc45 R italic_R and the distillation loss \u2112 d \u2062 i \u2062 s \u2062 t subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 \\mathcal{L}_{dist} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT , thus disregarding the reconstruction quality. The training objective is thus \u2112 d \u2062 3 = R + \u03bb \u2062 \u2112 d \u2062 i \u2062 s \u2062 t subscript \u2112 \ud835\udc51 3 \ud835\udc45 \ud835\udf06 subscript \u2112 \ud835\udc51 \ud835\udc56 \ud835\udc60 \ud835\udc61 \\mathcal{L}_{d3}=R+\\lambda\\mathcal{L}_{dist} caligraphic_L start_POSTSUBSCRIPT italic_d 3 end_POSTSUBSCRIPT = italic_R + italic_\u03bb caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT . Report issue for preceding element",
    "masked_text": "In (d3), where the downstream applications do not require image reconstruction, the encoder and transform-neck are jointly optimized to minimize the trade-off cost between the rate R\ud835\udc45Ritalic_R and the distillation loss \u2112d\u2062i\u2062s\u2062tsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\mathcal{L}_{dist}caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT, thus disregarding the reconstruction quality. The training objective is thus \u2112d\u20623=R+\u03bb\u2062\u2112d\u2062i\u2062s\u2062tsubscript\u2112\ud835\udc513\ud835\udc45\ud835\udf06subscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\mathcal{L}_{d3}=R+\\lambda\\mathcal{L}_{dist}caligraphic_L start_POSTSUBSCRIPT italic_d 3 end_POSTSUBSCRIPT = italic_R + italic_\u03bb caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT.Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.SSS0.Px1.p1": {
    "text": "We utilize ELIC (He et\u00a0al., 2022 ) as our image codec, which outputs image and hyperprior latents with N = 320 \ud835\udc41 320 N=320 italic_N = 320 and N h = 192 subscript \ud835\udc41 \u210e 192 N_{h}=192 italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 192 , respectively. ELIC is trained for human perception and adheres to the training strategy outlined in He et\u00a0al. ( 2022 ) , using 8,000 images of the highest spatial resolution selected from ImageNet dataset. Four models are trained for four different rate points, corresponding to \u03bb = [ 0.004 , 0.008 , 0.016 , 0.032 ] \ud835\udf06 0.004 0.008 0.016 0.032 \\lambda=[0.004,0.008,0.016,0.032] italic_\u03bb = [ 0.004 , 0.008 , 0.016 , 0.032 ] in (He et\u00a0al., 2022 ) .\nFor each of our scenarios (d1), (d2) and (d3), separate transform-necks are trained on ImageNet dataset (Deng et\u00a0al., 2009 ) for individual \u03bb \ud835\udf06 \\lambda italic_\u03bb values. For the scenario (d2) specifically, we find empirically that fixing the ratio \u03b3 : \u03b4 = 60 : 1 : \ud835\udefe \ud835\udeff 60 : 1 \\gamma:\\delta=60:1 italic_\u03b3 : italic_\u03b4 = 60 : 1 leads to a good trade-off between human and machine perception.\nGiven that most MLLMs adopt the pre-trained visual encoder of CLIP ViT-L/14 (Radford et\u00a0al., 2021 ) for image modality, as discussed in Section 2.1 , we use the CLIP visual encoder as C \ud835\udc36 C italic_C for training and conduct our primary experiments on MLLMs that incorporate it.\nIt is worth noting that, since we consider MLLMs sharing the same visual encoder, we do not need to train separate systems for the different MLLMs or tasks.\nFurthermore, we provide additional experiments in Section 4.6 with MLLMs that use visual encoders other than CLIP ViT to demonstrate the generalizability of our approach. Report issue for preceding element",
    "masked_text": "We utilize ELIC [CITATION] as our image codec, which outputs image and hyperprior latents with N=320\ud835\udc41320N=320italic_N = 320 and Nh=192subscript\ud835\udc41\u210e192N_{h}=192italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 192, respectively. ELIC is trained for human perception and adheres to the training strategy outlined in [CITATION], using 8,000 images of the highest spatial resolution selected from ImageNet dataset. Four models are trained for four different rate points, corresponding to \u03bb=[0.004,0.008,0.016,0.032]\ud835\udf060.0040.0080.0160.032\\lambda=[0.004,0.008,0.016,0.032]italic_\u03bb = [ 0.004 , 0.008 , 0.016 , 0.032 ] in [CITATION]. For each of our scenarios (d1), (d2) and (d3), separate transform-necks are trained on ImageNet dataset [CITATION] for individual \u03bb\ud835\udf06\\lambdaitalic_\u03bb values. For the scenario (d2) specifically, we find empirically that fixing the ratio \u03b3:\u03b4=60:1:\ud835\udefe\ud835\udeff60:1\\gamma:\\delta=60:1italic_\u03b3 : italic_\u03b4 = 60 : 1 leads to a good trade-off between human and machine perception. Given that most MLLMs adopt the pre-trained visual encoder of CLIP ViT-L/14 [CITATION] for image modality, as discussed in Section 2.1, we use the CLIP visual encoder as C\ud835\udc36Citalic_C for training and conduct our primary experiments on MLLMs that incorporate it. It is worth noting that, since we consider MLLMs sharing the same visual encoder, we do not need to train separate systems for the different MLLMs or tasks. Furthermore, we provide additional experiments in Section 4.6 with MLLMs that use visual encoders other than CLIP ViT to demonstrate the generalizability of our approach.Report issue for preceding element",
    "citations": [
      {
        "tag": "He et\u00a0al. (2022)",
        "title": "Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding.",
        "authors": "Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 5718\u20135727, 2022."
      },
      {
        "tag": "Deng et\u00a0al. (2009)",
        "title": "Imagenet: A large-scale hierarchical image database.",
        "authors": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.",
        "journal": "In2009 IEEE conference on computer vision and pattern recognition, pp.\u00a0 248\u2013255. Ieee, 2009."
      },
      {
        "tag": "Radford et\u00a0al. (2021)",
        "title": "Learning transferable visual models from natural language supervision.",
        "authors": "Alec Radford, Jong\u00a0Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et\u00a0al.",
        "journal": "InInternational conference on machine learning, pp.\u00a0 8748\u20138763. PMLR, 2021."
      }
    ]
  },
  "S4.SS1.SSS0.Px2.p1": {
    "text": "To validate the generalization ability of our proposed method, we evaluate its performance on four different MLLM systems for four different tasks. The tasks, datasets, corresponding MLLMs, and metrics are listed in Table 4.1 .\nThese configurations follow the settings outlined in their original papers and the accompanying code, except for the few-shot classification task due to the inaccessibility of the code.\nWe thus design a 5-way 1-shot classification scenario to evaluate the performance with in-context learning; the detailed setting is described in supplementary material. All MLLMs are used off-the-shelf without any fine-tuning. Report issue for preceding element",
    "masked_text": "To validate the generalization ability of our proposed method, we evaluate its performance on four different MLLM systems for four different tasks. The tasks, datasets, corresponding MLLMs, and metrics are listed in Table 4.1. These configurations follow the settings outlined in their original papers and the accompanying code, except for the few-shot classification task due to the inaccessibility of the code. We thus design a 5-way 1-shot classification scenario to evaluate the performance with in-context learning; the detailed setting is described in supplementary material. All MLLMs are used off-the-shelf without any fine-tuning.Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.SSS0.Px3.p1": {
    "text": "We introduce two baseline methods for comparison. The first one, denoted as Reconstruction , involves inputting the reconstructed image generated by ELIC to the MLLM system. The second one, denoted as Post-processing , adapts the reconstructed image to MLLMs through a U-Net (Ronneberger et\u00a0al., 2015 ) post-processing network, which is trained using the same surrogate loss as that adopted by our method. We remark that these image-domain baselines incur higher complexity than our lightweight transform-neck, as they involve decoding the image and potentially processing it further with the post-processing network. Report issue for preceding element",
    "masked_text": "We introduce two baseline methods for comparison. The first one, denoted as Reconstruction, involves inputting the reconstructed image generated by ELIC to the MLLM system. The second one, denoted as Post-processing, adapts the reconstructed image to MLLMs through a U-Net [CITATION] post-processing network, which is trained using the same surrogate loss as that adopted by our method. We remark that these image-domain baselines incur higher complexity than our lightweight transform-neck, as they involve decoding the image and potentially processing it further with the post-processing network.Report issue for preceding element",
    "citations": [
      {
        "tag": "Ronneberger et\u00a0al. (2015)",
        "title": "U-net: Convolutional networks for biomedical image segmentation.",
        "authors": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox.",
        "journal": "InMedical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp.\u00a0 234\u2013241. Springer, 2015."
      }
    ]
  },
  "S4.SS2.p1": {
    "text": "Figure 3 illustrates the performance of the baseline methods and our proposed scheme for the three examined scenarios with regards to two aspects: coding bit-rate, calculated as bits per pixel (bpp), and task performance. When comparing the baselines and our method in scenario (d1), where the original ELIC is trained solely for human perception, we make the following observations. (1) Straightforwardly using the reconstructed images generated by a codec trained for human perception leads to a significant performance drop across all the tasks ( Reconstruction ). Such performance decline is expected because the MLLMs are not trained with compressed images, thus hindering their recognition performance. This highlights the necessity of adapting image compression and/or image latents to MLLMs. (2) In contrast, our transform-neck method successfully boosts the performance using the same latent representations for reconstructing the image in Reconstruction , confirming the effectiveness of the proposed latent transformation without the decoding process. (3) Post-processing is able to reach comparable performance to our (d1) , offering another viable solution to the problem. However, it is worth noting that Post-processing requires relatively higher computational complexity with respect to our transform-neck method, rendering our approach preferable (see Section 4.4 ) . Report issue for preceding element",
    "masked_text": "Figure 3 illustrates the performance of the baseline methods and our proposed scheme for the three examined scenarios with regards to two aspects: coding bit-rate, calculated as bits per pixel (bpp), and task performance. When comparing the baselines and our method in scenario (d1), where the original ELIC is trained solely for human perception, we make the following observations. (1) Straightforwardly using the reconstructed images generated by a codec trained for human perception leads to a significant performance drop across all the tasks (Reconstruction). Such performance decline is expected because the MLLMs are not trained with compressed images, thus hindering their recognition performance. This highlights the necessity of adapting image compression and/or image latents to MLLMs. (2) In contrast, our transform-neck method successfully boosts the performance using the same latent representations for reconstructing the image in Reconstruction, confirming the effectiveness of the proposed latent transformation without the decoding process. (3) Post-processing is able to reach comparable performance to our (d1), offering another viable solution to the problem. However, it is worth noting that Post-processing requires relatively higher computational complexity with respect to our transform-neck method, rendering our approach preferable (see Section 4.4).Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.p2": {
    "text": "Next, we evaluate the effects of allowing the image codec to be re-trained. First, we observe that (d2) outperforms both (d1) and Post-processing . This indicates that fine-tuning the encoder indeed results in a more suitable latent representation that can be better adapted to MLLMs.\nWhen examining the extreme setting (d3), we see significant further improvement in the task performance, approaching the performance upper bound with uncompressed images.\nThis improvement comes at the cost of the image reconstruction quality, which, however, is not relevant in (d3).\nFigure 4 illustrates the rate-visual quality curves associated with the three scenarios. Interestingly, (d2) exhibits only a marginal PSNR drop compared to (d1), while (d3) significantly compromises the quality of the decoded image.\nWe stress that our framework (i.e. the surrogate loss and transform-neck) is able to accommodate different application scenarios, allowing for a variable trade-off between the task performance and the image reconstruction quality. Report issue for preceding element",
    "masked_text": "Next, we evaluate the effects of allowing the image codec to be re-trained. First, we observe that (d2) outperforms both (d1) and Post-processing. This indicates that fine-tuning the encoder indeed results in a more suitable latent representation that can be better adapted to MLLMs. When examining the extreme setting (d3), we see significant further improvement in the task performance, approaching the performance upper bound with uncompressed images. This improvement comes at the cost of the image reconstruction quality, which, however, is not relevant in (d3). Figure 4 illustrates the rate-visual quality curves associated with the three scenarios. Interestingly, (d2) exhibits only a marginal PSNR drop compared to (d1), while (d3) significantly compromises the quality of the decoded image. We stress that our framework (i.e. the surrogate loss and transform-neck) is able to accommodate different application scenarios, allowing for a variable trade-off between the task performance and the image reconstruction quality.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.p1": {
    "text": "We present the visualization of outcomes with downstream MLLM-based vision tasks in Figure 5 . Our method (d1) is compared with the two baseline methods, Reconstruction and Post-processing , with particular focus on how these models work at low bitrates to reflect a bandwidth-limited scenario. In the second and third columns, we visualize the reconstructed and post-processed images from the two baselines, respectively, which exhibit drastically different characteristics. The former ( Reconstruction ) produces blurry and smooth images, while the latter ( Post-processing ) introduces some artificial patterns into the post-processed images.\nCompared with the baselines, our method yields better MLLM results. More visualization are presented in Section A.3 of the supplementary material. Report issue for preceding element",
    "masked_text": "We present the visualization of outcomes with downstream MLLM-based vision tasks in Figure 5. Our method (d1) is compared with the two baseline methods, Reconstruction and Post-processing, with particular focus on how these models work at low bitrates to reflect a bandwidth-limited scenario. In the second and third columns, we visualize the reconstructed and post-processed images from the two baselines, respectively, which exhibit drastically different characteristics. The former (Reconstruction) produces blurry and smooth images, while the latter (Post-processing) introduces some artificial patterns into the post-processed images. Compared with the baselines, our method yields better MLLM results. More visualization are presented in Section A.3 of the supplementary material.Report issue for preceding element",
    "citations": []
  },
  "S4.SS4.p1": {
    "text": "Table 3 compares the computational complexity between our proposed method and Post-processing baseline in terms of model size and the kilo-multiply-accumulate-operations per pixel (kMACs/pixel). Note that our method in Table 3 refers to any of (d1), (d2), and (d3), since they share the same computational complexity characteristics at inference time. Our method offers a lightweight solution with only 13 million parameters, as opposed to 64 million parameters with the post-processing approach. Moreover, in terms of kMAC/pixel, the difference stands out even more, considering that the post-processing network operates at the full image resolution while our method operates in the latent domain, where the image latents have a much smaller spatial resolution. Report issue for preceding element",
    "masked_text": "Table 3 compares the computational complexity between our proposed method and Post-processing baseline in terms of model size and the kilo-multiply-accumulate-operations per pixel (kMACs/pixel). Note that our method in Table 3 refers to any of (d1), (d2), and (d3), since they share the same computational complexity characteristics at inference time. Our method offers a lightweight solution with only 13 million parameters, as opposed to 64 million parameters with the post-processing approach. Moreover, in terms of kMAC/pixel, the difference stands out even more, considering that the post-processing network operates at the full image resolution while our method operates in the latent domain, where the image latents have a much smaller spatial resolution.Report issue for preceding element",
    "citations": []
  },
  "S4.SS5.p1": {
    "text": "The following ablation experiments are performed based on (d1) to justify our design choices. Report issue for preceding element",
    "masked_text": "The following ablation experiments are performed based on (d1) to justify our design choices.Report issue for preceding element",
    "citations": []
  },
  "S4.SS5.SSS0.Px1.p1": {
    "text": "Figure 6 (b) presents the performance of our method when trained exclusively with the cross-entropy loss or distillation loss. It is observed that training with only the cross-entropy loss results in a significant performance drop. Although providing a good initial update direction, this strategy is unable to learn an effective transformation for MLLMs. Instead, training solely with the distillation loss fails to update the transform-neck properly and leads to far inferior performance. This is potentially due to the stringent requirement of fitting the exact feature representations. Our proposed method, not a simple application of distillation, achieves the highest performance. Report issue for preceding element",
    "masked_text": "Figure 6 (b) presents the performance of our method when trained exclusively with the cross-entropy loss or distillation loss. It is observed that training with only the cross-entropy loss results in a significant performance drop. Although providing a good initial update direction, this strategy is unable to learn an effective transformation for MLLMs. Instead, training solely with the distillation loss fails to update the transform-neck properly and leads to far inferior performance. This is potentially due to the stringent requirement of fitting the exact feature representations. Our proposed method, not a simple application of distillation, achieves the highest performance.Report issue for preceding element",
    "citations": []
  },
  "S4.SS5.SSS0.Px1.p2": {
    "text": "To further support this finding, we present the following analysis in Figure 7 : we first calculate the mean squared error (MSE) between CLIP visual encoder output tokens derived from uncompressed images and from our transform-neck, both before and after the transform-neck has been trained. Then, we compute the difference between these MSEs to measure the improvement achieved by the specific training objectives all with equal training steps.\nFigure 7 shows that the cross-entropy loss reduces feature matching errors primarily in foreground object regions, while the distillation loss reduces global matching errors. Our proposed progressive training strategy integrates these two losses, leading to a greater reduction in MSE and thus improved rate-accuracy performance. Report issue for preceding element",
    "masked_text": "To further support this finding, we present the following analysis in Figure 7: we first calculate the mean squared error (MSE) between CLIP visual encoder output tokens derived from uncompressed images and from our transform-neck, both before and after the transform-neck has been trained. Then, we compute the difference between these MSEs to measure the improvement achieved by the specific training objectives all with equal training steps. Figure 7 shows that the cross-entropy loss reduces feature matching errors primarily in foreground object regions, while the distillation loss reduces global matching errors. Our proposed progressive training strategy integrates these two losses, leading to a greater reduction in MSE and thus improved rate-accuracy performance.Report issue for preceding element",
    "citations": []
  },
  "S4.SS5.SSS0.Px2.p1": {
    "text": "This experiment investigates the proper number of Transformer layers to remove from the CLIP visual encoder in order to strike a good balance between complexity and performance. As shown in Figure 6 (a), removing the first one or two layers achieves similar performance, whereas removing four or eight layers results in a noticeable performance drop. We thus remove the first two layers. Report issue for preceding element",
    "masked_text": "This experiment investigates the proper number of Transformer layers to remove from the CLIP visual encoder in order to strike a good balance between complexity and performance. As shown in Figure 6 (a), removing the first one or two layers achieves similar performance, whereas removing four or eight layers results in a noticeable performance drop. We thus remove the first two layers.Report issue for preceding element",
    "citations": []
  },
  "S4.SS5.SSS0.Px3.p1": {
    "text": "Figure 6 (c) presents the performance comparison between our method and Reconstruction when they are tested with ELIC and TIC (Lu et\u00a0al., 2022a ; b ) .\nTIC is a Transformer-based codec, whereas ELIC is a convolutional neural network-based codec.\nWe see that our transform-neck still outperforms Reconstruction by a significant margin when the image codec is changed from ELIC to TIC. This indicates that our method is still effective on a different type of image codec. Report issue for preceding element",
    "masked_text": "Figure 6 (c) presents the performance comparison between our method and Reconstruction when they are tested with ELIC and TIC [CITATION]. TIC is a Transformer-based codec, whereas ELIC is a convolutional neural network-based codec. We see that our transform-neck still outperforms Reconstruction by a significant margin when the image codec is changed from ELIC to TIC. This indicates that our method is still effective on a different type of image codec.Report issue for preceding element",
    "citations": [
      {
        "tag": "Lu et\u00a0al. (2022a)",
        "title": "High-efficiency lossy image coding through adaptive neighborhood information aggregation.",
        "authors": "Ming Lu, Fangdong Chen, Shiliang Pu, and Zhan Ma.",
        "journal": "arXiv preprint arXiv:2204.11448, 2022a."
      },
      {
        "tag": "Lu et\u00a0al. (2022b)",
        "title": "Transformer-based image compression.",
        "authors": "Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma.",
        "journal": "InData Compression Conference, 2022b."
      }
    ]
  },
  "S4.SS6.p1": {
    "text": "While we utilize the CLIP ViT visual encoder in our main experiments due to its wide popularity, our proposed method is applicable to various downstream MLLMs regardless of the visual encoder they adopt. As illustrative examples, Figure 8 presents the rate-accuracy performance of our re-trained scheme applied to two MLLMs that do not use the pre-trained CLIP ViT visual encoder: (1) mPlug-Owl2 (Ye et\u00a0al., 2024 ) with a custom-trained ViT visual encoder, and (2) Osprey (Yuan et\u00a0al., 2024 ) with a CLIP ConvNeXt-based visual encoder. Our method under all three settings clearly outperforms the Reconstruction baseline, confirming the generalizability of the proposed framework. Report issue for preceding element",
    "masked_text": "While we utilize the CLIP ViT visual encoder in our main experiments due to its wide popularity, our proposed method is applicable to various downstream MLLMs regardless of the visual encoder they adopt. As illustrative examples, Figure 8 presents the rate-accuracy performance of our re-trained scheme applied to two MLLMs that do not use the pre-trained CLIP ViT visual encoder: (1) mPlug-Owl2 [CITATION] with a custom-trained ViT visual encoder, and (2) Osprey [CITATION] with a CLIP ConvNeXt-based visual encoder. Our method under all three settings clearly outperforms the Reconstruction baseline, confirming the generalizability of the proposed framework.Report issue for preceding element",
    "citations": [
      {
        "tag": "Ye et\u00a0al. (2024)",
        "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.",
        "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi\u00a0Qian, Ji\u00a0Zhang, and Fei Huang.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 13040\u201313051, 2024."
      },
      {
        "tag": "Yuan et\u00a0al. (2024)",
        "title": "Osprey: Pixel understanding with visual instruction tuning.",
        "authors": "Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 28202\u201328211, 2024."
      }
    ]
  },
  "S5.p1": {
    "text": "This paper proposes the first image compression system tailored to Multimodal Large Language Models (MLLMs). It introduces a transform-neck that bridges the compressed image latents and the intermediate layer of the visual encoder. By using our proposed surrogate loss, we avoid involving the entire MLLM in the training process and ensure downstream task performance. With lower computational complexity, our method has demonstrated effectiveness across a wide variety of tasks, MLLMs, and neural image codecs, outperforming other baselines in extensive experiments.\nOne consideration is that this paper focuses solely on the image compression aspect, leaving the exploration of MLLM-based video or audio coding for future work. Report issue for preceding element",
    "masked_text": "This paper proposes the first image compression system tailored to Multimodal Large Language Models (MLLMs). It introduces a transform-neck that bridges the compressed image latents and the intermediate layer of the visual encoder. By using our proposed surrogate loss, we avoid involving the entire MLLM in the training process and ensure downstream task performance. With lower computational complexity, our method has demonstrated effectiveness across a wide variety of tasks, MLLMs, and neural image codecs, outperforming other baselines in extensive experiments. One consideration is that this paper focuses solely on the image compression aspect, leaving the exploration of MLLM-based video or audio coding for future work.Report issue for preceding element",
    "citations": []
  },
  "S5.SS0.SSSx1.p1": {
    "text": "This work is supported by National Science and Technology Council, Taiwan under the Grant NSTC 113-2634-F-A49-007-, MediaTek,\nand National Center for High-performance Computing, Taiwan. Report issue for preceding element",
    "masked_text": "This work is supported by National Science and Technology Council, Taiwan under the Grant NSTC 113-2634-F-A49-007-, MediaTek, and National Center for High-performance Computing, Taiwan.Report issue for preceding element",
    "citations": []
  }
}