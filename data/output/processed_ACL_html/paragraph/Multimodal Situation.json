{
  "S1.p1": {
    "text": "Multimodal Large Language Models (MLLMs) (Zhu et\u00a0al., 2023 ; Li et\u00a0al., 2023 ; Liu et\u00a0al., 2023a ; OpenAI, 2023c ; Reid et\u00a0al., 2024 ) can understand visual contexts, follow instructions, and generate language responses, enabling them to serve as multimodal assistants capable of interacting with humans and real-world environments (Zheng et\u00a0al., 2022 ; Driess et\u00a0al., 2023 ) . With the enhanced capabilities and diverse application scenarios, the safety of MLLMs has become more critical, and there have been various works assessing and improving the safety of MLLMs (Liu et\u00a0al., 2023c ; Gong et\u00a0al., 2023 ; Shayegani et\u00a0al., 2023 ; Qi et\u00a0al., 2024 ; Luo et\u00a0al., 2024 ) . Report issue for preceding element",
    "masked_text": "Multimodal Large Language Models (MLLMs) [CITATION] can understand visual contexts, follow instructions, and generate language responses, enabling them to serve as multimodal assistants capable of interacting with humans and real-world environments [CITATION]. With the enhanced capabilities and diverse application scenarios, the safety of MLLMs has become more critical, and there have been various works assessing and improving the safety of MLLMs [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
        "journal": "InNeurIPS, 2023a."
      },
      {
        "tag": "Luo et\u00a0al. (2024)",
        "title": "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
        "authors": "Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao.",
        "journal": "arXiv preprint arXiv:2404.03027, 2024."
      },
      {
        "tag": "Li et\u00a0al. (2023)",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
        "authors": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.",
        "journal": "arXiv preprint arXiv:2301.12597, 2023."
      },
      {
        "tag": "Zheng et\u00a0al. (2022)",
        "title": "Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents.",
        "authors": "Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Di, Xuehai He, and Xin\u00a0Eric Wang.",
        "journal": "arXiv preprint arXiv:2208.13266, 2022."
      },
      {
        "tag": "Gong et\u00a0al. (2023)",
        "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
        "authors": "Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang.",
        "journal": "arXiv preprint arXiv:2311.05608, 2023."
      },
      {
        "tag": "Zhu et\u00a0al. (2023)",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
        "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
        "journal": "arXiv preprint arXiv:2304.10592, 2023."
      },
      {
        "tag": "OpenAI (2023c)",
        "title": "Gpt-4 technical report, 2023c.",
        "authors": "OpenAI.",
        "journal": ""
      },
      {
        "tag": "Liu et\u00a0al. (2023c)",
        "title": "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
        "authors": "X\u00a0Liu, Y\u00a0Zhu, J\u00a0Gu, Y\u00a0Lan, C\u00a0Yang, and Y\u00a0Qiao.",
        "journal": "arXiv preprint arXiv:2311.17600, 2023c."
      },
      {
        "tag": "Reid et\u00a0al. (2024)",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
        "authors": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2403.05530, 2024."
      },
      {
        "tag": "Driess et\u00a0al. (2023)",
        "title": "Palm-e: An embodied multimodal language model.",
        "authors": "Danny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2303.03378, 2023."
      },
      {
        "tag": "Qi et\u00a0al. (2024)",
        "title": "Visual adversarial examples jailbreak aligned large language models.",
        "authors": "Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal.",
        "journal": "InProceedings of the AAAI Conference on Artificial Intelligence, volume\u00a038, pp.\u00a0 21527\u201321536, 2024."
      },
      {
        "tag": "Shayegani et\u00a0al. (2023)",
        "title": "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
        "authors": "Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh.",
        "journal": "InThe Twelfth International Conference on Learning Representations, 2023."
      }
    ]
  },
  "S1.p2": {
    "text": "In the current MLLM safety assessment, the intent of the language query is clearly unsafe, and the visual input serves for attack purposes.\nHowever, the application of multimodal assistants introduces a new safety problem, where the visual context holds crucial information affecting the safety of user queries. For instance, as depicted in Fig. 1 (left), asking a model how to practice running is a benign query when the visual context is a clean walkway. However, if the model perceives the user is near the edge of a cliff, it should recognize it is very dangerous to practice running here and highlight the potential safety risks in such an environment.\nTo better evaluate the safety of current MLLMs in multimodal assistant scenarios, we define a new safety problem \u2013 Multimodal Situational Safety : given a language query and a real-time visual context, the model must judge the safety of the query based on the visual context. Report issue for preceding element",
    "masked_text": "In the current MLLM safety assessment, the intent of the language query is clearly unsafe, and the visual input serves for attack purposes. However, the application of multimodal assistants introduces a new safety problem, where the visual context holds crucial information affecting the safety of user queries. For instance, as depicted in Fig. 1 (left), asking a model how to practice running is a benign query when the visual context is a clean walkway. However, if the model perceives the user is near the edge of a cliff, it should recognize it is very dangerous to practice running here and highlight the potential safety risks in such an environment. To better evaluate the safety of current MLLMs in multimodal assistant scenarios, we define a new safety problem \u2013 Multimodal Situational Safety: given a language query and a real-time visual context, the model must judge the safety of the query based on the visual context.Report issue for preceding element",
    "citations": []
  },
  "S1.p3": {
    "text": "To comprehensively evaluate the current MLLM\u2019s situational safety performance, we introduce a Multimodal Situational Safety benchmark (MSSBench) with 1960 language-image pairs. To assess unbalanced model behaviors, in half of the data, the image is a safe situation for answering the query, and in the other half, the image context is unsafe.\nOur benchmark considers two multimodal assistant scenarios: multimodal chat agents that answer users\u2019 questions and multimodal embodied agents that plan and take actions following instructions of daily tasks.\nFor the chat scenario, we leverage LLMs to generate candidate activities as user intents and envision an unsafe situation for these activities. Then, the examples will go through two filtering processes: LLM automatic verification and human verification performed by domain experts to ensure data quality.\nFinally, we prompt the LLMs to generate user queries with the intent to perform these activities.\nFor embodied scenarios, we first manually create potentially unsafe household tasks, and define safe and unsafe situations. Then, we collect safe and unsafe visual contexts from the embodied AI simulators. Report issue for preceding element",
    "masked_text": "To comprehensively evaluate the current MLLM\u2019s situational safety performance, we introduce a Multimodal Situational Safety benchmark (MSSBench) with 1960 language-image pairs. To assess unbalanced model behaviors, in half of the data, the image is a safe situation for answering the query, and in the other half, the image context is unsafe. Our benchmark considers two multimodal assistant scenarios: multimodal chat agents that answer users\u2019 questions and multimodal embodied agents that plan and take actions following instructions of daily tasks. For the chat scenario, we leverage LLMs to generate candidate activities as user intents and envision an unsafe situation for these activities. Then, the examples will go through two filtering processes: LLM automatic verification and human verification performed by domain experts to ensure data quality. Finally, we prompt the LLMs to generate user queries with the intent to perform these activities. For embodied scenarios, we first manually create potentially unsafe household tasks, and define safe and unsafe situations. Then, we collect safe and unsafe visual contexts from the embodied AI simulators.Report issue for preceding element",
    "citations": []
  },
  "S1.p4": {
    "text": "We evaluate popular open-sourced and proprietary MLLMs on the MSSBench. The results show that current MLLMs struggle with recognizing unsafe situations when answering user queries.\nThen, we create different evaluation variants to analyze key safety aspects of MLLMs, including explicit safety reasoning, visual understanding, and situational safety reasoning.\nOur main findings include: (1) Explicit safety reasoning can improve the average situational safety performance of MLLMs, but will also introduce over-sensitivity in safe situations. (2) MLLMs perform poorly in embodied scenarios due to the lack of precise visual understanding and situation safety judgment abilities. (3) Open-source MLLMs sometimes ignore crucial safety clues in the image. (4) Under settings with more subtasks, the safety performance of MLLMs decreases due to task complexity. Report issue for preceding element",
    "masked_text": "We evaluate popular open-sourced and proprietary MLLMs on the MSSBench. The results show that current MLLMs struggle with recognizing unsafe situations when answering user queries. Then, we create different evaluation variants to analyze key safety aspects of MLLMs, including explicit safety reasoning, visual understanding, and situational safety reasoning. Our main findings include: (1) Explicit safety reasoning can improve the average situational safety performance of MLLMs, but will also introduce over-sensitivity in safe situations. (2) MLLMs perform poorly in embodied scenarios due to the lack of precise visual understanding and situation safety judgment abilities. (3) Open-source MLLMs sometimes ignore crucial safety clues in the image. (4) Under settings with more subtasks, the safety performance of MLLMs decreases due to task complexity. Report issue for preceding element",
    "citations": []
  },
  "S1.p5": {
    "text": "Based on our findings, to improve multimodal situational safety awareness when responding to language queries, we introduce multi-agent situational reasoning pipelines, which break down subtasks in safety and query-responding to different agents so that each subtask can be executed with higher accuracy. Our pipeline can improve the average safety accuracy for almost all the MLLMs, but the models\u2019 performance is still imperfect, especially in the embodied task scenarios.\nTo sum up, our contributions are listed as follows: Report issue for preceding element \u2022 We propose the Multimodal Situational Safety benchmark that focuses on evaluating the model\u2019s ability to judge the safety of queries based on the situation indicated in the visual context in both chat and embodied scenarios. Report issue for preceding element \u2022 We evaluate state-of-the-art open-sourced and proprietary MLLMs with our created benchmark and find that all models tested face a significant challenge in recognizing unsafe situations with visual context. Report issue for preceding element \u2022 We diagnose MLLMs\u2019 performance in-depth by designing different evaluation settings to see which capabilities are the bottleneck for the model\u2019s safety performance, including explicit safety reasoning, visual understanding, and situational safety reasoning abilities. Report issue for preceding element \u2022 Finally, we investigate the potential of breaking down subtasks and designing multi-agent reasoning pipelines for answering language queries with safety awareness. Report issue for preceding element",
    "masked_text": "Based on our findings, to improve multimodal situational safety awareness when responding to language queries, we introduce multi-agent situational reasoning pipelines, which break down subtasks in safety and query-responding to different agents so that each subtask can be executed with higher accuracy. Our pipeline can improve the average safety accuracy for almost all the MLLMs, but the models\u2019 performance is still imperfect, especially in the embodied task scenarios. To sum up, our contributions are listed as follows:Report issue for preceding element \u2022 We propose the Multimodal Situational Safety benchmark that focuses on evaluating the model\u2019s ability to judge the safety of queries based on the situation indicated in the visual context in both chat and embodied scenarios.Report issue for preceding element \u2022 We evaluate state-of-the-art open-sourced and proprietary MLLMs with our created benchmark and find that all models tested face a significant challenge in recognizing unsafe situations with visual context.Report issue for preceding element \u2022 We diagnose MLLMs\u2019 performance in-depth by designing different evaluation settings to see which capabilities are the bottleneck for the model\u2019s safety performance, including explicit safety reasoning, visual understanding, and situational safety reasoning abilities.Report issue for preceding element \u2022 Finally, we investigate the potential of breaking down subtasks and designing multi-agent reasoning pipelines for answering language queries with safety awareness.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i1.p1": {
    "text": "We propose the Multimodal Situational Safety benchmark that focuses on evaluating the model\u2019s ability to judge the safety of queries based on the situation indicated in the visual context in both chat and embodied scenarios. Report issue for preceding element",
    "masked_text": "We propose the Multimodal Situational Safety benchmark that focuses on evaluating the model\u2019s ability to judge the safety of queries based on the situation indicated in the visual context in both chat and embodied scenarios.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i2.p1": {
    "text": "We evaluate state-of-the-art open-sourced and proprietary MLLMs with our created benchmark and find that all models tested face a significant challenge in recognizing unsafe situations with visual context. Report issue for preceding element",
    "masked_text": "We evaluate state-of-the-art open-sourced and proprietary MLLMs with our created benchmark and find that all models tested face a significant challenge in recognizing unsafe situations with visual context.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i3.p1": {
    "text": "We diagnose MLLMs\u2019 performance in-depth by designing different evaluation settings to see which capabilities are the bottleneck for the model\u2019s safety performance, including explicit safety reasoning, visual understanding, and situational safety reasoning abilities. Report issue for preceding element",
    "masked_text": "We diagnose MLLMs\u2019 performance in-depth by designing different evaluation settings to see which capabilities are the bottleneck for the model\u2019s safety performance, including explicit safety reasoning, visual understanding, and situational safety reasoning abilities.Report issue for preceding element",
    "citations": []
  },
  "S1.I1.i4.p1": {
    "text": "Finally, we investigate the potential of breaking down subtasks and designing multi-agent reasoning pipelines for answering language queries with safety awareness. Report issue for preceding element",
    "masked_text": "Finally, we investigate the potential of breaking down subtasks and designing multi-agent reasoning pipelines for answering language queries with safety awareness.Report issue for preceding element",
    "citations": []
  },
  "S2.SS0.SSS0.Px1.p1": {
    "text": "Recently, the development of multimodal large language models (MLLMs) has been driven by enabling LLMs with visual perception abilities (Alayrac et\u00a0al., 2022 ; Dai et\u00a0al., 2023 ; Liu et\u00a0al., 2023a ; Reid et\u00a0al., 2024 ) .\nThese models are applied widely in various vision and language tasks. The success of the two tasks makes them very helpful chat and embodied multimodal assistants in real life.\nThe first one is Visual Question Answering (Antol et\u00a0al., 2015 ; Marino et\u00a0al., 2019 ; Schwenk et\u00a0al., 2022 ; Fan et\u00a0al., 2024 ) , which requires them to respond with their knowledge and opinion based on the user\u2019s question and the visual input (Dai et\u00a0al., 2023 ; Zhou et\u00a0al., 2023 ; OpenAI, 2023c ) . This enables the users to ask the MLLMs for questions about real-life visual input. Report issue for preceding element",
    "masked_text": "Recently, the development of multimodal large language models (MLLMs) has been driven by enabling LLMs with visual perception abilities [CITATION]. These models are applied widely in various vision and language tasks. The success of the two tasks makes them very helpful chat and embodied multimodal assistants in real life. The first one is Visual Question Answering [CITATION], which requires them to respond with their knowledge and opinion based on the user\u2019s question and the visual input [CITATION]. This enables the users to ask the MLLMs for questions about real-life visual input.Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2023a)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
        "journal": "InNeurIPS, 2023a."
      },
      {
        "tag": "Alayrac et\u00a0al. (2022)",
        "title": "Flamingo: a visual language model for few-shot learning.",
        "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et\u00a0al.",
        "journal": "Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022."
      },
      {
        "tag": "Antol et\u00a0al. (2015)",
        "title": "Vqa: Visual question answering.",
        "authors": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C\u00a0Lawrence Zitnick, and Devi Parikh.",
        "journal": "InProceedings of the IEEE international conference on computer vision, pp.\u00a0 2425\u20132433, 2015."
      },
      {
        "tag": "Zhou et\u00a0al. (2023)",
        "title": "Vicor: Bridging visual understanding and commonsense reasoning with large language models.",
        "authors": "Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, and Xin\u00a0Eric Wang.",
        "journal": "ACL, 2023."
      },
      {
        "tag": "Schwenk et\u00a0al. (2022)",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge.",
        "authors": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.",
        "journal": "InEuropean conference on computer vision, pp.\u00a0 146\u2013162. Springer, 2022."
      },
      {
        "tag": "OpenAI (2023c)",
        "title": "Gpt-4 technical report, 2023c.",
        "authors": "OpenAI.",
        "journal": ""
      },
      {
        "tag": "Fan et\u00a0al. (2024)",
        "title": "Muffin or chihuahua? challenging large vision-language models with multipanel vqa.",
        "authors": "Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, and Xin\u00a0Eric Wang.",
        "journal": "ACL, 2024."
      },
      {
        "tag": "Reid et\u00a0al. (2024)",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
        "authors": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2403.05530, 2024."
      },
      {
        "tag": "Dai et\u00a0al. (2023)",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.",
        "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng\u00a0Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.",
        "journal": ""
      },
      {
        "tag": "Marino et\u00a0al. (2019)",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge.",
        "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.",
        "journal": "InProceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp.\u00a0 3195\u20133204, 2019."
      }
    ]
  },
  "S2.SS0.SSS0.Px1.p2": {
    "text": "The second one is embodied decision-making and task planning (Shridhar et\u00a0al., 2020 ; Szot et\u00a0al., 2024 ) , which requires the MLLMs to serve as the \u2019brain\u2019 of the embodied agent that plan actions for a robot to execute to complete a given household task (Driess et\u00a0al., 2023 ; Yang et\u00a0al., 2024 ; Li et\u00a0al., 2024b ; Wang et\u00a0al., 2024a ) . This enables the MLLMs to control a robot and make it an embodied assistant.\nHowever, the improved abilities of current MLLMs on these tasks and new applications introduce new safety problems, and the safety of MLLMs under multimodal assistant scenarios has not been thoroughly studied. Report issue for preceding element",
    "masked_text": "The second one is embodied decision-making and task planning [CITATION], which requires the MLLMs to serve as the \u2019brain\u2019 of the embodied agent that plan actions for a robot to execute to complete a given household task [CITATION]. This enables the MLLMs to control a robot and make it an embodied assistant. However, the improved abilities of current MLLMs on these tasks and new applications introduce new safety problems, and the safety of MLLMs under multimodal assistant scenarios has not been thoroughly studied.Report issue for preceding element",
    "citations": [
      {
        "tag": "Li et\u00a0al. (2024b)",
        "title": "Manipllm: Embodied multimodal large language model for object-centric robotic manipulation.",
        "authors": "Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 18061\u201318070, 2024b."
      },
      {
        "tag": "Shridhar et\u00a0al. (2020)",
        "title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks.",
        "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox.",
        "journal": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\u00a0 10740\u201310749, 2020."
      },
      {
        "tag": "Wang et\u00a0al. (2024a)",
        "title": "Large language models for robotics: Opportunities, challenges, and perspectives.",
        "authors": "Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2401.04334, 2024a."
      },
      {
        "tag": "Szot et\u00a0al. (2024)",
        "title": "Large language models as generalizable policies for embodied tasks.",
        "authors": "Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, R\u00a0Devon Hjelm, and Alexander\u00a0T Toshev.",
        "journal": "InThe Twelfth International Conference on Learning Representations, 2024."
      },
      {
        "tag": "Driess et\u00a0al. (2023)",
        "title": "Palm-e: An embodied multimodal language model.",
        "authors": "Danny Driess, Fei Xia, Mehdi\u00a0SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2303.03378, 2023."
      },
      {
        "tag": "Yang et\u00a0al. (2024)",
        "title": "Embodied multi-modal agent trained by an llm from a parallel textworld.",
        "authors": "Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li\u00a0Shen, Xiaodong He, Jing Jiang, and Yuhui Shi.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 26275\u201326285, 2024."
      }
    ]
  },
  "S2.SS0.SSS0.Px2.p1": {
    "text": "The generative abilities of LLMs and MLLMs carry the risk of being misused to generate harmful content. Recently, lots of efforts have been put into red-teaming MLLMs (Liu et\u00a0al., 2023c ; Gong et\u00a0al., 2023 ; Shayegani et\u00a0al., 2023 ; Qi et\u00a0al., 2024 ; Luo et\u00a0al., 2024 ; Shi et\u00a0al., 2024 ) .\nHowever, most of the current benchmarks study the scenarios where the language itself is clearly unsafe and leverage image modality as an attack or safety-unrelated background to trick the MLLMs into answering unsafe queries. These include using query-relevant images (Liu et\u00a0al., 2023c ) , direct text embedding (Gong et\u00a0al., 2023 ) , and optimized adversarial images (Shayegani et\u00a0al., 2023 ) to induce them to generate harmful responses. Moreover, Shi et\u00a0al. ( 2024 ) access the safety in multimodal chat scenarios with images as additional context.\nBesides these, there were also concurrent efforts studying the over-sensitivity of MLLMs (Li et\u00a0al., 2024c ) , and find that the combination of safe image and safe text inputs could be unsafe (Wang et\u00a0al., 2024b ) .\nDifferent from existing works, we first propose a new safety problem for MLLMs in multimodal assistant applications \u2013 multimodal situational safety, where the safety of language queries varies with different visual situations. Based on this, we collect a benchmark containing chat and embodied scenarios to evaluate the MLLMs\u2019 safety awareness. We also investigate in-depth how far we can leverage MLLMs\u2019 capabilities to improve safety performance. Report issue for preceding element",
    "masked_text": "The generative abilities of LLMs and MLLMs carry the risk of being misused to generate harmful content. Recently, lots of efforts have been put into red-teaming MLLMs [CITATION]. However, most of the current benchmarks study the scenarios where the language itself is clearly unsafe and leverage image modality as an attack or safety-unrelated background to trick the MLLMs into answering unsafe queries. These include using query-relevant images [CITATION], direct text embedding [CITATION], and optimized adversarial images [CITATION] to induce them to generate harmful responses. Moreover, [CITATION] access the safety in multimodal chat scenarios with images as additional context. Besides these, there were also concurrent efforts studying the over-sensitivity of MLLMs [CITATION], and find that the combination of safe image and safe text inputs could be unsafe [CITATION]. Different from existing works, we first propose a new safety problem for MLLMs in multimodal assistant applications \u2013 multimodal situational safety, where the safety of language queries varies with different visual situations. Based on this, we collect a benchmark containing chat and embodied scenarios to evaluate the MLLMs\u2019 safety awareness. We also investigate in-depth how far we can leverage MLLMs\u2019 capabilities to improve safety performance.Report issue for preceding element",
    "citations": [
      {
        "tag": "Luo et\u00a0al. (2024)",
        "title": "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
        "authors": "Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao.",
        "journal": "arXiv preprint arXiv:2404.03027, 2024."
      },
      {
        "tag": "Shi et\u00a0al. (2024)",
        "title": "Assessment of multimodal large language models in alignment with human values.",
        "authors": "Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu\u00a0Sheng, Yu\u00a0Qiao, and Jing Shao.",
        "journal": "arXiv preprint arXiv:2403.17830, 2024."
      },
      {
        "tag": "Liu et\u00a0al. (2023c)",
        "title": "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
        "authors": "X\u00a0Liu, Y\u00a0Zhu, J\u00a0Gu, Y\u00a0Lan, C\u00a0Yang, and Y\u00a0Qiao.",
        "journal": "arXiv preprint arXiv:2311.17600, 2023c."
      },
      {
        "tag": "Gong et\u00a0al. (2023)",
        "title": "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
        "authors": "Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang.",
        "journal": "arXiv preprint arXiv:2311.05608, 2023."
      },
      {
        "tag": "Li et\u00a0al. (2024c)",
        "title": "Mossbench: Is your multimodal language model oversensitive to safe queries?",
        "authors": "Xirui Li, Hengguang Zhou, Ruochen Wang, Tianyi Zhou, Minhao Cheng, and Cho-Jui Hsieh.",
        "journal": "arXiv preprint arXiv:2406.17806, 2024c."
      },
      {
        "tag": "Wang et\u00a0al. (2024b)",
        "title": "Cross-modality safety alignment.",
        "authors": "Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, and Xuanjing Huang.",
        "journal": "arXiv preprint arXiv:2406.15279, 2024b."
      },
      {
        "tag": "Qi et\u00a0al. (2024)",
        "title": "Visual adversarial examples jailbreak aligned large language models.",
        "authors": "Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal.",
        "journal": "InProceedings of the AAAI Conference on Artificial Intelligence, volume\u00a038, pp.\u00a0 21527\u201321536, 2024."
      },
      {
        "tag": "Shayegani et\u00a0al. (2023)",
        "title": "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
        "authors": "Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh.",
        "journal": "InThe Twelfth International Conference on Learning Representations, 2023."
      }
    ]
  },
  "S3.SS1.SSS0.Px1.p1": {
    "text": "We define the problem of multimodal situational safety as follows: Given a language query Q \ud835\udc44 Q italic_Q and a real-time visual context V \ud835\udc49 V italic_V , the model needs to determine a safety score, denoted as S \u2062 ( Q , V ) \ud835\udc46 \ud835\udc44 \ud835\udc49 S(Q,V) italic_S ( italic_Q , italic_V ) , which represents the safety of the intent of this query Q \ud835\udc44 Q italic_Q in the context of the visual information V \ud835\udc49 V italic_V . Specifically, the safety score S \u2062 ( Q ) \ud835\udc46 \ud835\udc44 S(Q) italic_S ( italic_Q ) depends on the visual context, meaning that it should be difficult to determine S \u2062 ( Q ) \ud835\udc46 \ud835\udc44 S(Q) italic_S ( italic_Q ) without the visual input. Report issue for preceding element",
    "masked_text": "We define the problem of multimodal situational safety as follows: Given a language query Q\ud835\udc44Qitalic_Q and a real-time visual context V\ud835\udc49Vitalic_V, the model needs to determine a safety score, denoted as S\u2062(Q,V)\ud835\udc46\ud835\udc44\ud835\udc49S(Q,V)italic_S ( italic_Q , italic_V ), which represents the safety of the intent of this query Q\ud835\udc44Qitalic_Q in the context of the visual information V\ud835\udc49Vitalic_V. Specifically, the safety score S\u2062(Q)\ud835\udc46\ud835\udc44S(Q)italic_S ( italic_Q ) depends on the visual context, meaning that it should be difficult to determine S\u2062(Q)\ud835\udc46\ud835\udc44S(Q)italic_S ( italic_Q ) without the visual input.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS0.Px2.p1": {
    "text": "We introduce the Multimodal Situational Safety benchmark (MSSBench) to evaluate the model\u2019s ability to judge the safety of answering a language query based on a situation given by a visual context. As shown in Fig. 3 , each data instance contains a language query and a safe or unsafe visual context as the real-time observation of the MLLM.\nOur benchmark contains two different multimodal assistant scenarios: chat assistant and embodied assistant. For chat assistant, the language query indicates the intent to perform a certain activity. For embodied assistant, each language query is a household task instruction, and the images depict safe and unsafe situations in which to perform the task. Report issue for preceding element",
    "masked_text": "We introduce the Multimodal Situational Safety benchmark (MSSBench) to evaluate the model\u2019s ability to judge the safety of answering a language query based on a situation given by a visual context. As shown in Fig. 3, each data instance contains a language query and a safe or unsafe visual context as the real-time observation of the MLLM. Our benchmark contains two different multimodal assistant scenarios: chat assistant and embodied assistant. For chat assistant, the language query indicates the intent to perform a certain activity. For embodied assistant, each language query is a household task instruction, and the images depict safe and unsafe situations in which to perform the task.Report issue for preceding element",
    "citations": []
  },
  "S3.SS1.SSS0.Px3.p1": {
    "text": "As shown in Fig. 2 , we develop a multimodal situational safety categorization system based on the potential unsafe outcomes by answering the query. We find that many safety categories used in former LLM safety assessments (Shen et\u00a0al., 2023 ; Li et\u00a0al., 2024a ) do not often apply to Multimodal Situational Safety, such as fraud, political lobbying, etc. Therefore, our categorization covers four core domains where the safety of the intent of the query is frequently conditioned on the visual context: (1) Physical Harm, including activities that in certain situations may cause bodily harm, subdivided into self-harm (such as eating disorders and danger activities) and other-harm (activities that could potentially harm others). (2) Property damage, defined as activities that cause harm to personal or public property, is categorized into personal property damage and public property damage. (3) Illegal Activities, encompassing behaviors that violate the law but do not directly cause physical harm or property damage, divided into human-restricting activities (e.g., child abuse, making noise at night, and privacy invasion), property-restricting activities(e.g., illegal trespassing, taking restricted photographs, and hit-and-run incidents), and organism-restricting activities (e.g., animal abuse). (4) Offensive Activities, including activities that may breach cultural or religious beliefs or cause discomfort, are categorized into cultural belief violations, religious belief infringements, and disruptive behaviors. Report issue for preceding element",
    "masked_text": "As shown in Fig. 2, we develop a multimodal situational safety categorization system based on the potential unsafe outcomes by answering the query. We find that many safety categories used in former LLM safety assessments [CITATION] do not often apply to Multimodal Situational Safety, such as fraud, political lobbying, etc. Therefore, our categorization covers four core domains where the safety of the intent of the query is frequently conditioned on the visual context: (1) Physical Harm, including activities that in certain situations may cause bodily harm, subdivided into self-harm (such as eating disorders and danger activities) and other-harm (activities that could potentially harm others). (2) Property damage, defined as activities that cause harm to personal or public property, is categorized into personal property damage and public property damage. (3) Illegal Activities, encompassing behaviors that violate the law but do not directly cause physical harm or property damage, divided into human-restricting activities (e.g., child abuse, making noise at night, and privacy invasion), property-restricting activities(e.g., illegal trespassing, taking restricted photographs, and hit-and-run incidents), and organism-restricting activities (e.g., animal abuse). (4) Offensive Activities, including activities that may breach cultural or religious beliefs or cause discomfort, are categorized into cultural belief violations, religious belief infringements, and disruptive behaviors.Report issue for preceding element",
    "citations": [
      {
        "tag": "Shen et\u00a0al. (2023)",
        "title": "\u201d do anything now\u201d: Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
        "authors": "Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.",
        "journal": "arXiv preprint arXiv:2308.03825, 2023."
      },
      {
        "tag": "Li et\u00a0al. (2024a)",
        "title": "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.",
        "authors": "Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu\u00a0Qiao, and Jing Shao.",
        "journal": "arXiv preprint arXiv:2402.05044, 2024a."
      }
    ]
  },
  "S3.SS2.p1": {
    "text": "We design a data collection pipeline to collect queries that are safe to answer in certain situations but are unsafe to answer in others.\nThis pipeline involves four steps: (1) generating user intented activities and textual unsafe situations corresponding to situational safety categories; (2) filtering out situations that do not meet the criteria; (3) retrieving images that depict the unsafe context to construct multimodal situations; and (4) generating user queries with the aforementioned intents after human verification. We use GPT-4o as the large language model (LLM) in the data generation pipeline to ensure the efficient generation and processing of these situation pairs. Report issue for preceding element",
    "masked_text": "We design a data collection pipeline to collect queries that are safe to answer in certain situations but are unsafe to answer in others. This pipeline involves four steps: (1) generating user intented activities and textual unsafe situations corresponding to situational safety categories; (2) filtering out situations that do not meet the criteria; (3) retrieving images that depict the unsafe context to construct multimodal situations; and (4) generating user queries with the aforementioned intents after human verification. We use GPT-4o as the large language model (LLM) in the data generation pipeline to ensure the efficient generation and processing of these situation pairs.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.SSS0.Px1.p1": {
    "text": "Initially, we randomly select 5,000 images I = { i 1 , \u2026 , i N } \ud835\udc3c subscript \ud835\udc56 1 \u2026 subscript \ud835\udc56 \ud835\udc41 I=\\{i_{1},...,i_{N}\\} italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_i start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } from the COCO dataset (Lin et\u00a0al., 2014 ) for each situational safety category, considering them as safe images. We prompt the LLM to generate intented activities A s \u2062 a \u2062 f \u2062 e subscript \ud835\udc34 \ud835\udc60 \ud835\udc4e \ud835\udc53 \ud835\udc52 A_{s\\mkern-1.0mua\\mkern-2.0muf\\mkern-1.0mue} italic_A start_POSTSUBSCRIPT italic_s italic_a italic_f italic_e end_POSTSUBSCRIPT that are safe to perform in the context of the images.\nThese activities, along with the corresponding images and safety category descriptions, are input into the LLM to generate unsafe situations T u \u2062 n \u2062 s \u2062 a \u2062 f \u2062 e subscript \ud835\udc47 \ud835\udc62 \ud835\udc5b \ud835\udc60 \ud835\udc4e \ud835\udc53 \ud835\udc52 T_{u\\mkern-1.0mun\\mkern-1.0mus\\mkern-1.0mua\\mkern-1.0muf\\mkern-1.0mue} italic_T start_POSTSUBSCRIPT italic_u italic_n italic_s italic_a italic_f italic_e end_POSTSUBSCRIPT where performing the activity can lead to unsafe outcomes. For example, in the domain of property damage, if the image I i subscript \ud835\udc3c \ud835\udc56 I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT depicts \u201cPeople playing baseball on the field,\u201d a possible safe activity a i subscript \ud835\udc4e \ud835\udc56 a_{i} italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is \u201cSwinging a baseball bat to hit the ball\u201d while a possible unsafe situation t i subscript \ud835\udc61 \ud835\udc56 t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is \u201cInside a store.\u201d Report issue for preceding element",
    "masked_text": "Initially, we randomly select 5,000 images I={i1,\u2026,iN}\ud835\udc3csubscript\ud835\udc561\u2026subscript\ud835\udc56\ud835\udc41I=\\{i_{1},...,i_{N}\\}italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_i start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } from the COCO dataset [CITATION] for each situational safety category, considering them as safe images. We prompt the LLM to generate intented activities As\u2062a\u2062f\u2062esubscript\ud835\udc34\ud835\udc60\ud835\udc4e\ud835\udc53\ud835\udc52A_{s\\mkern-1.0mua\\mkern-2.0muf\\mkern-1.0mue}italic_A start_POSTSUBSCRIPT italic_s italic_a italic_f italic_e end_POSTSUBSCRIPT that are safe to perform in the context of the images. These activities, along with the corresponding images and safety category descriptions, are input into the LLM to generate unsafe situations Tu\u2062n\u2062s\u2062a\u2062f\u2062esubscript\ud835\udc47\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc4e\ud835\udc53\ud835\udc52T_{u\\mkern-1.0mun\\mkern-1.0mus\\mkern-1.0mua\\mkern-1.0muf\\mkern-1.0mue}italic_T start_POSTSUBSCRIPT italic_u italic_n italic_s italic_a italic_f italic_e end_POSTSUBSCRIPT where performing the activity can lead to unsafe outcomes. For example, in the domain of property damage, if the image Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT depicts \u201cPeople playing baseball on the field,\u201d a possible safe activity aisubscript\ud835\udc4e\ud835\udc56a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is \u201cSwinging a baseball bat to hit the ball\u201d while a possible unsafe situation tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is \u201cInside a store.\u201dReport issue for preceding element",
    "citations": [
      {
        "tag": "Lin et\u00a0al. (2014)",
        "title": "Microsoft coco: Common objects in context.",
        "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C\u00a0Lawrence Zitnick.",
        "journal": "InEuropean conference on computer vision, 2014."
      }
    ]
  },
  "S3.SS2.SSS0.Px2.p1": {
    "text": "We implement two automated filters using GPT-4o to address the issue of the LLM generating unsafe situations that deviate from the intended safety category or involve impossible activities.\nThe first filter eliminates situations that do not meet the safe and unsafe criteria of the designated safety category. For instance, if the category is offensive behavior, scenarios such as \u201cpracticing skateboarding in the middle of a road\u201d are filtered out as they do not fit the category.\nThe second filter eliminates impossible intented activities, which means that the activity contradicts the situation, such as \u201cobeying traffic lights\u201d in an image of \u201cdriving on a highway\u201d because highways typically do not have traffic lights.\nAfter filtering, we obtain a set of textual intented activities and unsafe situations: ( A f \u2062 i \u2062 l \u2062 t \u2062 e \u2062 r , T f \u2062 i \u2062 l \u2062 t \u2062 e \u2062 r ) = ( { a 1 , \u2026 , a L } , { t 1 , \u2026 , t L } ) subscript \ud835\udc34 \ud835\udc53 \ud835\udc56 \ud835\udc59 \ud835\udc61 \ud835\udc52 \ud835\udc5f subscript \ud835\udc47 \ud835\udc53 \ud835\udc56 \ud835\udc59 \ud835\udc61 \ud835\udc52 \ud835\udc5f subscript \ud835\udc4e 1 \u2026 subscript \ud835\udc4e \ud835\udc3f subscript \ud835\udc61 1 \u2026 subscript \ud835\udc61 \ud835\udc3f (A_{f\\mkern-1.0mui\\mkern-1.0mul\\mkern-1.0mut\\mkern-1.0mue\\mkern-1.0mur},T_{f%\n\\mkern-1.0mui\\mkern-1.0mul\\mkern-1.0mut\\mkern-1.0mue\\mkern-1.0mur})=\\left(\\{a_%\n{1},\\dots,a_{L}\\},\\{t_{1},\\dots,t_{L}\\}\\right) ( italic_A start_POSTSUBSCRIPT italic_f italic_i italic_l italic_t italic_e italic_r end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_f italic_i italic_l italic_t italic_e italic_r end_POSTSUBSCRIPT ) = ( { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_a start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } , { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } ) , where L \ud835\udc3f L italic_L is the number of instances after filtration. Report issue for preceding element",
    "masked_text": "We implement two automated filters using GPT-4o to address the issue of the LLM generating unsafe situations that deviate from the intended safety category or involve impossible activities. The first filter eliminates situations that do not meet the safe and unsafe criteria of the designated safety category. For instance, if the category is offensive behavior, scenarios such as \u201cpracticing skateboarding in the middle of a road\u201d are filtered out as they do not fit the category. The second filter eliminates impossible intented activities, which means that the activity contradicts the situation, such as \u201cobeying traffic lights\u201d in an image of \u201cdriving on a highway\u201d because highways typically do not have traffic lights. After filtering, we obtain a set of textual intented activities and unsafe situations: (Af\u2062i\u2062l\u2062t\u2062e\u2062r,Tf\u2062i\u2062l\u2062t\u2062e\u2062r)=({a1,\u2026,aL},{t1,\u2026,tL})subscript\ud835\udc34\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5fsubscript\ud835\udc47\ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5fsubscript\ud835\udc4e1\u2026subscript\ud835\udc4e\ud835\udc3fsubscript\ud835\udc611\u2026subscript\ud835\udc61\ud835\udc3f(A_{f\\mkern-1.0mui\\mkern-1.0mul\\mkern-1.0mut\\mkern-1.0mue\\mkern-1.0mur},T_{f% \\mkern-1.0mui\\mkern-1.0mul\\mkern-1.0mut\\mkern-1.0mue\\mkern-1.0mur})=\\left(\\{a_% {1},\\dots,a_{L}\\},\\{t_{1},\\dots,t_{L}\\}\\right)( italic_A start_POSTSUBSCRIPT italic_f italic_i italic_l italic_t italic_e italic_r end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_f italic_i italic_l italic_t italic_e italic_r end_POSTSUBSCRIPT ) = ( { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_a start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } , { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } ), where L\ud835\udc3fLitalic_L is the number of instances after filtration.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.SSS0.Px3.p1": {
    "text": "We construct a Multimodal Situation Safety Dataset \ud835\udc9f = { \ud835\udcae , \ud835\udcb0 } \ud835\udc9f \ud835\udcae \ud835\udcb0 \\mathcal{D}=\\{\\mathcal{S},\\mathcal{U}\\} caligraphic_D = { caligraphic_S , caligraphic_U } , where \ud835\udcae \ud835\udcae \\mathcal{S} caligraphic_S contains pairs of activities a \ud835\udc4e a italic_a and their corresponding safe images i \ud835\udc56 i italic_i . Conversely, \ud835\udcb0 = { ( t 1 , i ~ 1 ) , \u2026 , ( t L , i ~ L ) } \ud835\udcb0 subscript \ud835\udc61 1 subscript ~ \ud835\udc56 1 \u2026 subscript \ud835\udc61 \ud835\udc3f subscript ~ \ud835\udc56 \ud835\udc3f \\mathcal{U}=\\{(t_{1},\\tilde{i}_{1}),\\dots,(t_{L},\\tilde{i}_{L})\\} caligraphic_U = { ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over~ start_ARG italic_i end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , \u2026 , ( italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , over~ start_ARG italic_i end_ARG start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) } includes pairs where t \ud835\udc61 t italic_t represents the unsafe textual situations and i ~ ~ \ud835\udc56 \\tilde{i} over~ start_ARG italic_i end_ARG are unsafe images retrieved based on t \ud835\udc61 t italic_t via Bing search. To ensure the diversity and precision of image retrieval, three images are initially retrieved for each t \ud835\udc61 t italic_t , followed by a rigorous manual selection process to identify the most suitable unsafe image. The specific verification process will be elaborated in the following subsection. Report issue for preceding element",
    "masked_text": "We construct a Multimodal Situation Safety Dataset \ud835\udc9f={\ud835\udcae,\ud835\udcb0}\ud835\udc9f\ud835\udcae\ud835\udcb0\\mathcal{D}=\\{\\mathcal{S},\\mathcal{U}\\}caligraphic_D = { caligraphic_S , caligraphic_U }, where \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S contains pairs of activities a\ud835\udc4eaitalic_a and their corresponding safe images i\ud835\udc56iitalic_i. Conversely, \ud835\udcb0={(t1,i~1),\u2026,(tL,i~L)}\ud835\udcb0subscript\ud835\udc611subscript~\ud835\udc561\u2026subscript\ud835\udc61\ud835\udc3fsubscript~\ud835\udc56\ud835\udc3f\\mathcal{U}=\\{(t_{1},\\tilde{i}_{1}),\\dots,(t_{L},\\tilde{i}_{L})\\}caligraphic_U = { ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over~ start_ARG italic_i end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , \u2026 , ( italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , over~ start_ARG italic_i end_ARG start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) } includes pairs where t\ud835\udc61titalic_t represents the unsafe textual situations and i~~\ud835\udc56\\tilde{i}over~ start_ARG italic_i end_ARG are unsafe images retrieved based on t\ud835\udc61titalic_t via Bing search. To ensure the diversity and precision of image retrieval, three images are initially retrieved for each t\ud835\udc61titalic_t, followed by a rigorous manual selection process to identify the most suitable unsafe image. The specific verification process will be elaborated in the following subsection.Report issue for preceding element",
    "citations": []
  },
  "S3.SS2.SSS0.Px4.p1": {
    "text": "While automated filters assist in the initial screening, they remain insufficient for fully eliminating non-compliant instances. To ensure data accuracy, three researchers manually validated the dataset \ud835\udc9f \ud835\udc9f \\mathcal{D} caligraphic_D based on the following criteria: (1) the activity must be safe in the context of a safe image; (2) the activity must align with unsafe conditions in an unsafe image; (3) the activity must neither contradict nor be irrelevant to the image.\nQualified multimodal data \ud835\udc9f q = { \ud835\udcae q , \ud835\udcb0 q } subscript \ud835\udc9f \ud835\udc5e subscript \ud835\udcae \ud835\udc5e subscript \ud835\udcb0 \ud835\udc5e \\mathcal{D}_{q}=\\{\\mathcal{S}_{q},\\mathcal{U}_{q}\\} caligraphic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = { caligraphic_S start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , caligraphic_U start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } are selected following the human validation process. To construct real-life chat scenarios, we leverage LLM to generate typical user queries with the intent to perform the activities A i \u2062 n \u2062 t subscript \ud835\udc34 \ud835\udc56 \ud835\udc5b \ud835\udc61 A_{i\\mkern-1.0mun\\mkern-1.0mut} italic_A start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT in \ud835\udcae q subscript \ud835\udcae \ud835\udc5e \\mathcal{S}_{q} caligraphic_S start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . For example, given a skiing scenario, possible queries might include \u201cHow can I improve my skiing skills here?\u201d.\nSpecifically, the generated queries are used to evaluate the situational safety performance of MLLMs in handling both safe and unsafe images. Report issue for preceding element",
    "masked_text": "While automated filters assist in the initial screening, they remain insufficient for fully eliminating non-compliant instances. To ensure data accuracy, three researchers manually validated the dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D based on the following criteria: (1) the activity must be safe in the context of a safe image; (2) the activity must align with unsafe conditions in an unsafe image; (3) the activity must neither contradict nor be irrelevant to the image. Qualified multimodal data \ud835\udc9fq={\ud835\udcaeq,\ud835\udcb0q}subscript\ud835\udc9f\ud835\udc5esubscript\ud835\udcae\ud835\udc5esubscript\ud835\udcb0\ud835\udc5e\\mathcal{D}_{q}=\\{\\mathcal{S}_{q},\\mathcal{U}_{q}\\}caligraphic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = { caligraphic_S start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , caligraphic_U start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } are selected following the human validation process. To construct real-life chat scenarios, we leverage LLM to generate typical user queries with the intent to perform the activities Ai\u2062n\u2062tsubscript\ud835\udc34\ud835\udc56\ud835\udc5b\ud835\udc61A_{i\\mkern-1.0mun\\mkern-1.0mut}italic_A start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT in \ud835\udcaeqsubscript\ud835\udcae\ud835\udc5e\\mathcal{S}_{q}caligraphic_S start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT. For example, given a skiing scenario, possible queries might include \u201cHow can I improve my skiing skills here?\u201d. Specifically, the generated queries are used to evaluate the situational safety performance of MLLMs in handling both safe and unsafe images.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.p1": {
    "text": "The collection of the embodied data consists of two steps: Report issue for preceding element",
    "masked_text": "The collection of the embodied data consists of two steps:Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.SSS0.Px1.p1": {
    "text": "We mainly consider five task categories: place an {object in hand} on a {receptacle} ( Place ), toggle a {receptacle} ( Toggle ), drop an {object in hand} ( Drop ), heat an {object} with microwave ( Heat ), pick and place {objects} ( Pick&place ).\nFor each category, we define different safe and unsafe tasks by changing the objects or receptacles in the placeholder and environment states. Unsafe tasks contain uncommon environment states that could lead to unsafe outcomes. For instance, the knife in the microwave in Fig. 3 .\nIn total, we define 38 safe tasks and 38 unsafe tasks.\nThen, we create 5 instruction templates for each task. In total, we have 5 \u00d7 ( 38 + 38 ) = 380 5 38 38 380 5\\times(38+38)=380 5 \u00d7 ( 38 + 38 ) = 380 embodied instructions. Report issue for preceding element",
    "masked_text": "We mainly consider five task categories: place an {object in hand} on a {receptacle} (Place), toggle a {receptacle} (Toggle), drop an {object in hand} (Drop), heat an {object} with microwave (Heat), pick and place {objects} (Pick&place). For each category, we define different safe and unsafe tasks by changing the objects or receptacles in the placeholder and environment states. Unsafe tasks contain uncommon environment states that could lead to unsafe outcomes. For instance, the knife in the microwave in Fig. 3. In total, we define 38 safe tasks and 38 unsafe tasks. Then, we create 5 instruction templates for each task. In total, we have 5\u00d7(38+38)=380538383805\\times(38+38)=3805 \u00d7 ( 38 + 38 ) = 380 embodied instructions.Report issue for preceding element",
    "citations": []
  },
  "S3.SS3.SSS0.Px2.p1": {
    "text": "After we determine the {object}, {receptacle} in the task, we run a ` \u2062 ` \u2062 P \u2062 i \u2062 c \u2062 k \u2062 _ \u2062 { o \u2062 b \u2062 j \u2062 e \u2062 c \u2062 t } \u2062 a \u2062 n \u2062 d \u2062 _ \u2062 P \u2062 l \u2062 a \u2062 c \u2062 e \u2062 { r \u2062 e \u2062 c \u2062 e \u2062 p \u2062 t \u2062 a \u2062 c \u2062 l \u2062 e } \u2062 \" ` ` \ud835\udc43 \ud835\udc56 \ud835\udc50 \ud835\udc58 _ \ud835\udc5c \ud835\udc4f \ud835\udc57 \ud835\udc52 \ud835\udc50 \ud835\udc61 \ud835\udc4e \ud835\udc5b \ud835\udc51 _ \ud835\udc43 \ud835\udc59 \ud835\udc4e \ud835\udc50 \ud835\udc52 \ud835\udc5f \ud835\udc52 \ud835\udc50 \ud835\udc52 \ud835\udc5d \ud835\udc61 \ud835\udc4e \ud835\udc50 \ud835\udc59 \ud835\udc52 \" ``Pick\\_\\{object\\}and\\_Place\\{receptacle\\}\" ` ` italic_P italic_i italic_c italic_k _ { italic_o italic_b italic_j italic_e italic_c italic_t } italic_a italic_n italic_d _ italic_P italic_l italic_a italic_c italic_e { italic_r italic_e italic_c italic_e italic_p italic_t italic_a italic_c italic_l italic_e } \" task defined in Shridhar et\u00a0al. ( 2020 ) with the determined {object} and {receptacle}.\nFor the Place task and the Drop task, we randomly collect two egocentric images after the agent picks up the object and before the agent places the object.\nFor the Toggle task, we collect an egocentric image right after the agent places the object on the receptacle from two different episodes. For Heat and Pick&place task, we collect two observations using DALL-E 3 ( Betker et\u00a0al., ) , due to better flexibility. Therefore, we have 760 samples in total. One data example is shown in Fig. 3 (right). Report issue for preceding element",
    "masked_text": "After we determine the {object}, {receptacle} in the task, we run a `\u2062`\u2062P\u2062i\u2062c\u2062k\u2062_\u2062{o\u2062b\u2062j\u2062e\u2062c\u2062t}\u2062a\u2062n\u2062d\u2062_\u2062P\u2062l\u2062a\u2062c\u2062e\u2062{r\u2062e\u2062c\u2062e\u2062p\u2062t\u2062a\u2062c\u2062l\u2062e}\u2062\"``\ud835\udc43\ud835\udc56\ud835\udc50\ud835\udc58_\ud835\udc5c\ud835\udc4f\ud835\udc57\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc51_\ud835\udc43\ud835\udc59\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc5d\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc59\ud835\udc52\"``Pick\\_\\{object\\}and\\_Place\\{receptacle\\}\"` ` italic_P italic_i italic_c italic_k _ { italic_o italic_b italic_j italic_e italic_c italic_t } italic_a italic_n italic_d _ italic_P italic_l italic_a italic_c italic_e { italic_r italic_e italic_c italic_e italic_p italic_t italic_a italic_c italic_l italic_e } \" task defined in [CITATION] with the determined {object} and {receptacle}. For the Place task and the Drop task, we randomly collect two egocentric images after the agent picks up the object and before the agent places the object. For the Toggle task, we collect an egocentric image right after the agent places the object on the receptacle from two different episodes. For Heat and Pick&place task, we collect two observations using DALL-E 3 [CITATION], due to better flexibility. Therefore, we have 760 samples in total. One data example is shown in Fig. 3 (right).Report issue for preceding element",
    "citations": [
      {
        "tag": "(4)",
        "title": "Improving image generation with better captions.",
        "authors": "James Betker, Gabriel Goh, Li\u00a0Jing, \u2020 TimBrooks, Jianfeng Wang, Linjie Li, \u2020 LongOuyang, \u2020 JuntangZhuang, \u2020 JoyceLee, \u2020 YufeiGuo, \u2020 WesamManassra, \u2020 PrafullaDhariwal, \u2020 CaseyChu, \u2020 YunxinJiao, and Aditya Ramesh.",
        "journal": "URLhttps://api.semanticscholar.org/CorpusID:264403242."
      },
      {
        "tag": "Shridhar et\u00a0al. (2020)",
        "title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks.",
        "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox.",
        "journal": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\u00a0 10740\u201310749, 2020."
      }
    ]
  },
  "S3.SS4.p1": {
    "text": "The Multimodal Situational Safety benchmark consists of a substantial collection of 1960 Image-Query pairs, encompassing two subsets: the embodied assistant subset, which contains 760 pairs sourced from embodied scenarios, and the chat assistant subset, comprising a larger set of 1200 pairs designed for broader situational QA scenarios. Our dataset is a balance dataset, with half of the data containing safe situations and half containing unsafe situations.\nThe statistical details of the data in the MSSBench are presented in Table. 1 . Report issue for preceding element",
    "masked_text": "The Multimodal Situational Safety benchmark consists of a substantial collection of 1960 Image-Query pairs, encompassing two subsets: the embodied assistant subset, which contains 760 pairs sourced from embodied scenarios, and the chat assistant subset, comprising a larger set of 1200 pairs designed for broader situational QA scenarios. Our dataset is a balance dataset, with half of the data containing safe situations and half containing unsafe situations. The statistical details of the data in the MSSBench are presented in Table. 1.Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.p1": {
    "text": "MLLMs. The MLLMs we benchmark include both open-source models and proprietary models accessible only via API. The open-source MLLMs are: (i) LLaVA-1.6 (Liu et\u00a0al., 2023b ) , (ii) MiniGPT4-v2 (Chen et\u00a0al., 2023 ) , (iii) Qwen-VL (Bai et\u00a0al., 2023 ) , (iv) DeepSeek (Lu et\u00a0al., 2024 ) and (v) mPLUG-Owl2 (Ye et\u00a0al., 2024 ) . We implemented these models with their 7B version and using their default settings. For the proprietary models, we evaluated Claude 3.5 Sonnet, GPT-4o (OpenAI, 2023b ) , and Gemini Pro-1.5 (Reid et\u00a0al., 2024 ) . Report issue for preceding element",
    "masked_text": "MLLMs. The MLLMs we benchmark include both open-source models and proprietary models accessible only via API. The open-source MLLMs are: (i) LLaVA-1.6 [CITATION], (ii) MiniGPT4-v2 [CITATION], (iii) Qwen-VL [CITATION], (iv) DeepSeek [CITATION] and (v) mPLUG-Owl2 [CITATION]. We implemented these models with their 7B version and using their default settings. For the proprietary models, we evaluated Claude 3.5 Sonnet, GPT-4o [CITATION], and Gemini Pro-1.5 [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Liu et\u00a0al. (2023b)",
        "title": "Visual instruction tuning.",
        "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
        "journal": "arXiv preprint arXiv:2304.08485, 2023b."
      },
      {
        "tag": "Chen et\u00a0al. (2023)",
        "title": "Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning.",
        "authors": "Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.",
        "journal": "arXiv:2310.09478, 2023."
      },
      {
        "tag": "OpenAI (2023b)",
        "title": "Gpt-4v(ision) technical work and authors.",
        "authors": "OpenAI.",
        "journal": "Technical report., 2023b."
      },
      {
        "tag": "Bai et\u00a0al. (2023)",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
        "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.",
        "journal": "2023."
      },
      {
        "tag": "Ye et\u00a0al. (2024)",
        "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.",
        "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi\u00a0Qian, Ji\u00a0Zhang, and Fei Huang.",
        "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 13040\u201313051, 2024."
      },
      {
        "tag": "Reid et\u00a0al. (2024)",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
        "authors": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2403.05530, 2024."
      },
      {
        "tag": "Lu et\u00a0al. (2024)",
        "title": "Deepseek-vl: towards real-world vision-language understanding.",
        "authors": "Haoyu Lu, Wen Liu, Bo\u00a0Zhang, Bingxuan Wang, Kai Dong, Bo\u00a0Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et\u00a0al.",
        "journal": "arXiv preprint arXiv:2403.05525, 2024."
      }
    ]
  },
  "S4.SS1.p2": {
    "text": "Evaluation. For the instruction following setting, we use GPT-4o (OpenAI, 2023a ) to categorize the response generated by MLLMs into safe and unsafe categories. The categories description is introduced in Tables. 4 and 5 in Sec. A.4 .\nAfter categorization, we use accuracy to evaluate MLLM\u2019s safety performance. Report issue for preceding element",
    "masked_text": "Evaluation. For the instruction following setting, we use GPT-4o [CITATION] to categorize the response generated by MLLMs into safe and unsafe categories. The categories description is introduced in Tables. 4 and 5 in Sec. A.4. After categorization, we use accuracy to evaluate MLLM\u2019s safety performance.Report issue for preceding element",
    "citations": [
      {
        "tag": "OpenAI (2023a)",
        "title": "Gpt-4 technical report.",
        "authors": "OpenAI.",
        "journal": "Technical report., 2023a."
      }
    ]
  },
  "S4.SS2.p1": {
    "text": "To begin with, we assess the performance of 8 leading multimodal large language models (MLLMs) on our MSS benchmark, the results are shown in Table. 2 . To mimic the chat assistant scenario, we inform the MLLM that the image is its first-person view and the query is from a user staying with it, see the \u2018Common Prompt\u2019 in Fig. 4 . The full prompt can be found in Sec. A.7 .\nFirst, a common trend among all the MLLMs is that they tend to comply with and answer users\u2019 queries in both safe and unsafe scenarios. This leads to a high safety accuracy when the situation is safe for the user\u2019s intent and a low accuracy when the situation is unsafe.\nSecond, comparing open-source models and proprietary models, we find that proprietary models perform better in unsafe scenarios, with a higher frequency of detecting the unsafe intent from the user\u2019s query under the current situation, and pointing out the unsafe outcomes or rejecting to answer. Meanwhile, proprietary MLLMs are not over-sensitive in safe situations; therefore, they obtain higher average safety accuracy than open-source MLLMs.\nThird, by comparing the performance on Chat and Embodied scenarios, we find that MLLMs all perform worse on Embodied scenarios, especially in recognizing unsafe situations.\nLastly, the best-performed model, Claude 3.5 Sonnet, only scores an average accuracy of 64.0%, indicating the situation safety awareness of current MLLMs needs to be improved. Report issue for preceding element",
    "masked_text": "To begin with, we assess the performance of 8 leading multimodal large language models (MLLMs) on our MSS benchmark, the results are shown in Table. 2. To mimic the chat assistant scenario, we inform the MLLM that the image is its first-person view and the query is from a user staying with it, see the \u2018Common Prompt\u2019 in Fig. 4. The full prompt can be found in Sec. A.7. First, a common trend among all the MLLMs is that they tend to comply with and answer users\u2019 queries in both safe and unsafe scenarios. This leads to a high safety accuracy when the situation is safe for the user\u2019s intent and a low accuracy when the situation is unsafe. Second, comparing open-source models and proprietary models, we find that proprietary models perform better in unsafe scenarios, with a higher frequency of detecting the unsafe intent from the user\u2019s query under the current situation, and pointing out the unsafe outcomes or rejecting to answer. Meanwhile, proprietary MLLMs are not over-sensitive in safe situations; therefore, they obtain higher average safety accuracy than open-source MLLMs. Third, by comparing the performance on Chat and Embodied scenarios, we find that MLLMs all perform worse on Embodied scenarios, especially in recognizing unsafe situations. Lastly, the best-performed model, Claude 3.5 Sonnet, only scores an average accuracy of 64.0%, indicating the situation safety awareness of current MLLMs needs to be improved.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.p1": {
    "text": "We propose three hypothesis reasons that led to MLLM\u2019s poor performance on the MSS benchmark: (1) lack of explicit safety reasoning, (2) lack of visual understanding ability, and (3) lack of situational safety judgment ability.\nTo validate these hypotheses reasons, we design four variant evaluation settings: (1) letting MLLMs explicitly reason the safety of user query, (2) explicitly reason the safety of user\u2019s intent, (3) explicitly reason the safety of user\u2019s intent providing with self-caption, and (4) explicitly reason the safety of user\u2019s intent providing with ground-truth situation information. The difference between all 5 settings is shown in Fig. 4 . Report issue for preceding element",
    "masked_text": "We propose three hypothesis reasons that led to MLLM\u2019s poor performance on the MSS benchmark: (1) lack of explicit safety reasoning, (2) lack of visual understanding ability, and (3) lack of situational safety judgment ability. To validate these hypotheses reasons, we design four variant evaluation settings: (1) letting MLLMs explicitly reason the safety of user query, (2) explicitly reason the safety of user\u2019s intent, (3) explicitly reason the safety of user\u2019s intent providing with self-caption, and (4) explicitly reason the safety of user\u2019s intent providing with ground-truth situation information. The difference between all 5 settings is shown in Fig. 4.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p1": {
    "text": "To see whether lacking explicit safety reasoning causes poor performance, we design two settings that let MLLMs explicitly classify the user\u2019s query or intent into two classes: safe and unsafe. The performance in this setting is shown in Fig. 4 .\nFirst, we observe that all models benefit from explicit safety reasoning. What is more, the performance improvement of proprietary models is larger, which is due to their stronger visual understanding and safety reasoning abilities. GPT4o especially benefits the most from explicit reasoning, demonstrating strong reasoning abilities but weak safety awareness in the normal instruction following setting.\nThen, we look into the more detailed performance of MLLMs. We find that explicit safety reasoning significantly improves the MLLMs\u2019 safety performance in unsafe situations, enabling them to recognize more unsafe user intents.\nHowever, it decreases the performance in safe situations , as shown in Fig. 14(a) in the Sec. A.5 . This means that all models are over-sensitive and more inclined to think the user\u2019s intent is unsafe. Report issue for preceding element",
    "masked_text": "To see whether lacking explicit safety reasoning causes poor performance, we design two settings that let MLLMs explicitly classify the user\u2019s query or intent into two classes: safe and unsafe. The performance in this setting is shown in Fig. 4. First, we observe that all models benefit from explicit safety reasoning. What is more, the performance improvement of proprietary models is larger, which is due to their stronger visual understanding and safety reasoning abilities. GPT4o especially benefits the most from explicit reasoning, demonstrating strong reasoning abilities but weak safety awareness in the normal instruction following setting. Then, we look into the more detailed performance of MLLMs. We find that explicit safety reasoning significantly improves the MLLMs\u2019 safety performance in unsafe situations, enabling them to recognize more unsafe user intents. However, it decreases the performance in safe situations, as shown in Fig. 14(a) in the Sec. A.5. This means that all models are over-sensitive and more inclined to think the user\u2019s intent is unsafe.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p2": {
    "text": "Secondly, by comparing chat and embodied scenarios, we can find that the improvement of MLLMs on embodied tasks is very limited , even proprietary MLLMs only achieve around 59% accuracy. We identified three main reasons from the models\u2019 output. The first one is weaker safety knowledge in the embodied scenario, leading to more safety judgment errors. For instance, Gemini thinks dropping the phone on the floor will not cause any damage. Second, MLLMs make more visual understanding errors in the embodied scenarios and cannot identify key objects or identify them wrongly, due to MLLMs being trained less on the embodied data, and the key visual factor in the embodied scenario is more subtle. Third, MLLMs ignore key factors that may affect the safety of a task when making judgments. For example, GPT4o does not consider what object the robot is holding (a remote control) when judging whether placing it in the sink is safe. Report issue for preceding element",
    "masked_text": "Secondly, by comparing chat and embodied scenarios, we can find that the improvement of MLLMs on embodied tasks is very limited, even proprietary MLLMs only achieve around 59% accuracy. We identified three main reasons from the models\u2019 output. The first one is weaker safety knowledge in the embodied scenario, leading to more safety judgment errors. For instance, Gemini thinks dropping the phone on the floor will not cause any damage. Second, MLLMs make more visual understanding errors in the embodied scenarios and cannot identify key objects or identify them wrongly, due to MLLMs being trained less on the embodied data, and the key visual factor in the embodied scenario is more subtle. Third, MLLMs ignore key factors that may affect the safety of a task when making judgments. For example, GPT4o does not consider what object the robot is holding (a remote control) when judging whether placing it in the sink is safe.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px1.p3": {
    "text": "Thirdly, by comparing classifying intent and query, we find that classifying the safety of intent has a higher accuracy for both closed and open-source models. After looking into the model\u2019s output, we see three main error patterns caused by the task of classifying the safety of the query being more complex, with the extra task of recognizing the user\u2019s potential intent.\nThe first one is the model ignores the unsafe situation in the image. In the example shown in Fig. 5 (middle), Gemini did not recognize the scenario is in a lab where eating might be prohibited.\nThe second one is the model made hallucinates about safety, leading to incorrect safety judgment. For example, in Fig. 5 (left), Gemini thinks parking behind or in front of the car is dangerous without any support.\nThe third one is the model did not follow the instructions to judge the safety of the user\u2019s intent in the given situation. For instance, in Fig. 5 (right), llava did not judge the safety of the user\u2019s query. Instead, it comments the user\u2019s query in a general way. Report issue for preceding element",
    "masked_text": "Thirdly, by comparing classifying intent and query, we find that classifying the safety of intent has a higher accuracy for both closed and open-source models. After looking into the model\u2019s output, we see three main error patterns caused by the task of classifying the safety of the query being more complex, with the extra task of recognizing the user\u2019s potential intent. The first one is the model ignores the unsafe situation in the image. In the example shown in Fig. 5 (middle), Gemini did not recognize the scenario is in a lab where eating might be prohibited. The second one is the model made hallucinates about safety, leading to incorrect safety judgment. For example, in Fig. 5 (left), Gemini thinks parking behind or in front of the car is dangerous without any support. The third one is the model did not follow the instructions to judge the safety of the user\u2019s intent in the given situation. For instance, in Fig. 5 (right), llava did not judge the safety of the user\u2019s query. Instead, it comments the user\u2019s query in a general way.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px2.p1": {
    "text": "Then, to explore whether the lack of understanding of the image content affects the performance, we let MLLMs classify the user\u2019s intent with both image and self or ground-truth caption (Fig. 4 ) provided as the situation description.\nWe label the ground-truth caption manually to ensure that the caption is faithful to the image content and contains the necessary information for safety judgment (E.g., \u2018A knife is in the microwave.\u2019 for the task of \u2018Turn on the microwave.\u2019).\nFor self-caption, we prompt the MLLMs with the prompt \u201dDescribe the image in one long sentence\u201d. Report issue for preceding element",
    "masked_text": "Then, to explore whether the lack of understanding of the image content affects the performance, we let MLLMs classify the user\u2019s intent with both image and self or ground-truth caption (Fig. 4) provided as the situation description. We label the ground-truth caption manually to ensure that the caption is faithful to the image content and contains the necessary information for safety judgment (E.g., \u2018A knife is in the microwave.\u2019 for the task of \u2018Turn on the microwave.\u2019). For self-caption, we prompt the MLLMs with the prompt \u201dDescribe the image in one long sentence\u201d.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px2.p2": {
    "text": "First, from Fig. 4 , we can see that ground truth caption improves the performance of both open-source and proprietary models, and the improvement on open-source models is larger. This indicates that open-source models are not as capable of recognizing image contents that influence the safety of users\u2019 intent as proprietary models. For chat scenarios, visual understanding is not a significant bottleneck for the proprietary MLLMs. Report issue for preceding element",
    "masked_text": "First, from Fig. 4, we can see that ground truth caption improves the performance of both open-source and proprietary models, and the improvement on open-source models is larger. This indicates that open-source models are not as capable of recognizing image contents that influence the safety of users\u2019 intent as proprietary models. For chat scenarios, visual understanding is not a significant bottleneck for the proprietary MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S4.SS3.SSS0.Px2.p3": {
    "text": "To determine whether the lack of visual understanding is due to the weak visual understanding ability or open-source MLLMs not fully leveraging their visual understanding. We test the self-captioning setting and find that self-captions can improve the performance of open-source models in chat scenarios.\nThe model\u2019s outputs show that the open-source MLLMs can sometimes recognize important information in the image that affects safety during captioning but ignore it when asked to judge the safety without explicit captioning and hallucinate wrong judgment. This is potentially because the vision and language alignment of MLLMs are weaker; therefore, given a novel task, open-source MLLMs can not combine information from two modalities to make correct reasoning .\nFor embodied scenarios, we find that self-captioning decreases the performance of both open-source and close-source models. From the models\u2019 outputs, we find that the MLLMs\u2019s caption usually contains too much information unrelated to the task, which misleads the model\u2019s safety judgment. Report issue for preceding element",
    "masked_text": "To determine whether the lack of visual understanding is due to the weak visual understanding ability or open-source MLLMs not fully leveraging their visual understanding. We test the self-captioning setting and find that self-captions can improve the performance of open-source models in chat scenarios. The model\u2019s outputs show that the open-source MLLMs can sometimes recognize important information in the image that affects safety during captioning but ignore it when asked to judge the safety without explicit captioning and hallucinate wrong judgment. This is potentially because the vision and language alignment of MLLMs are weaker; therefore, given a novel task, open-source MLLMs can not combine information from two modalities to make correct reasoning. For embodied scenarios, we find that self-captioning decreases the performance of both open-source and close-source models. From the models\u2019 outputs, we find that the MLLMs\u2019s caption usually contains too much information unrelated to the task, which misleads the model\u2019s safety judgment.Report issue for preceding element",
    "citations": []
  },
  "S5.SS1.p1": {
    "text": "We aim to leverage our analysis results to improve the MLLM\u2019s safety awareness when answering user\u2019s queries. First, we introduce explicit safety reasoning, which has shown significant safety performance improvement for both chat and embodied scenarios.\nSecond, based on our findings that more complex task settings decrease the safety judgment performance of MLLMs, we explore leveraging the multi-agent systems. Specifically, we split the task of answering questions safely into several subtasks and assigned them to different MLLM agents. Report issue for preceding element",
    "masked_text": "We aim to leverage our analysis results to improve the MLLM\u2019s safety awareness when answering user\u2019s queries. First, we introduce explicit safety reasoning, which has shown significant safety performance improvement for both chat and embodied scenarios. Second, based on our findings that more complex task settings decrease the safety judgment performance of MLLMs, we explore leveraging the multi-agent systems. Specifically, we split the task of answering questions safely into several subtasks and assigned them to different MLLM agents.Report issue for preceding element",
    "citations": []
  },
  "S5.SS1.p2": {
    "text": "For chat scenarios, as shown in Fig. 6 , we design a four-agent framework for open-source MLLMs comprising an intent reasoning agent, a visual understanding agent, a safety judgment agent, and a question-answering agent. The intent reasoning agent is responsible for thinking about the user\u2019s intent based on their query. The visual understanding agent provides a caption for the given image. The safety judgment agent will then judge the safety of the user\u2019s intent based on the image and the caption. The safety judgment will determine whether the question-answering agent will answer the user\u2019s query or remind the user about the safety risk.\nFor proprietary MLLMs, due to their stronger ability to judge safety based on image content, we remove the visual understanding agent and form a three-agent framework.\nFor embodied scenarios, given the former analysis that MLLMs often can not locate the most important visual evidence, we design a two-agent framework with the first agent locating the most important environment state (which object is required to be identified to ensure safety), then the second agent will reason the safety of the task instruction and generate respond by focusing on the reasoned environment state. The visualization is shown in Fig. 15 in the Sec. A.5 . Report issue for preceding element",
    "masked_text": "For chat scenarios, as shown in Fig. 6, we design a four-agent framework for open-source MLLMs comprising an intent reasoning agent, a visual understanding agent, a safety judgment agent, and a question-answering agent. The intent reasoning agent is responsible for thinking about the user\u2019s intent based on their query. The visual understanding agent provides a caption for the given image. The safety judgment agent will then judge the safety of the user\u2019s intent based on the image and the caption. The safety judgment will determine whether the question-answering agent will answer the user\u2019s query or remind the user about the safety risk. For proprietary MLLMs, due to their stronger ability to judge safety based on image content, we remove the visual understanding agent and form a three-agent framework. For embodied scenarios, given the former analysis that MLLMs often can not locate the most important visual evidence, we design a two-agent framework with the first agent locating the most important environment state (which object is required to be identified to ensure safety), then the second agent will reason the safety of the task instruction and generate respond by focusing on the reasoned environment state. The visualization is shown in Fig. 15 in the Sec. A.5.Report issue for preceding element",
    "citations": []
  },
  "S5.SS2.p1": {
    "text": "We consider two baseline settings. The first one is the setting in Table. 2 , where the prompt instructs the MLLMs to answer the user\u2019s query. Second, we let MLLMs perform the intent reasoning, safety judgment, and query-responding in one step.\nThe results of our multi-agent framework are in Fig. 7 , showing that the multi-agent pipeline improves the performance for almost all the models in both embodied and chat subtasks.\nIn the chat scenario, the multi-agent framework significantly enhances open-source MLLMs, which struggle with solving all subtasks simultaneously due to weaker abilities. Most open-source MLLMs fail to improve in this setting, but with our multi-agent design, they can reach Gemini-level performance, demonstrating our method\u2019s effectiveness. Report issue for preceding element",
    "masked_text": "We consider two baseline settings. The first one is the setting in Table. 2, where the prompt instructs the MLLMs to answer the user\u2019s query. Second, we let MLLMs perform the intent reasoning, safety judgment, and query-responding in one step. The results of our multi-agent framework are in Fig. 7, showing that the multi-agent pipeline improves the performance for almost all the models in both embodied and chat subtasks. In the chat scenario, the multi-agent framework significantly enhances open-source MLLMs, which struggle with solving all subtasks simultaneously due to weaker abilities. Most open-source MLLMs fail to improve in this setting, but with our multi-agent design, they can reach Gemini-level performance, demonstrating our method\u2019s effectiveness.Report issue for preceding element",
    "citations": []
  },
  "S5.SS2.p2": {
    "text": "In the embodied scenario, the multi-agent design improves more on proprietary MLLMs. For most open-sourced MLLMs, the CoT reasoning and multi-agent do not significantly improve perfor- Report issue for preceding element",
    "masked_text": "In the embodied scenario, the multi-agent design improves more on proprietary MLLMs. For most open-sourced MLLMs, the CoT reasoning and multi-agent do not significantly improve perfor-Report issue for preceding element",
    "citations": []
  },
  "S5.SS2.p3": {
    "text": "mance. Also, even the performance of the best MLLM, GPT4o, is far from perfect. To investigate the reason, we perform two ablation studies on two best-performing models: GPT-4o and Claude.\nWe replace the reasoned important environment state by the first agent with the ground truth environment state that determines the safety of a task or the ground truth observation of this environment state.\nThe result is in Table. 3 , which shows both replacements improve performance. This means that MLLMs sometimes incorrectly locate the important environment state, make visual recognition errors, or have hallucinations regarding safety judgment. For example, GPT-4o falsely thinks that toggling a sink with a knife on it could cause injury and does not see that the object that needs to be dropped on the floor is a cell phone. This shows that safety training in the embodied scenarios needs to be improved. Report issue for preceding element",
    "masked_text": "mance. Also, even the performance of the best MLLM, GPT4o, is far from perfect. To investigate the reason, we perform two ablation studies on two best-performing models: GPT-4o and Claude. We replace the reasoned important environment state by the first agent with the ground truth environment state that determines the safety of a task or the ground truth observation of this environment state. The result is in Table. 3, which shows both replacements improve performance. This means that MLLMs sometimes incorrectly locate the important environment state, make visual recognition errors, or have hallucinations regarding safety judgment. For example, GPT-4o falsely thinks that toggling a sink with a knife on it could cause injury and does not see that the object that needs to be dropped on the floor is a cell phone. This shows that safety training in the embodied scenarios needs to be improved.Report issue for preceding element",
    "citations": []
  },
  "S6.p1": {
    "text": "In conclusion, this paper introduces the novel problem of Multimodal Situational Safety to evaluate the safety awareness of Multimodal Large Language Models (MLLMs) in scenarios where the safety of user queries depends on the visual context.\nBy creating a comprehensive benchmark containing both safe and unsafe scenarios in chat and embodied assistant settings, the study reveals significant challenges that current MLLMs face in recognizing unsafe situations when answering a query, especially in embodied scenarios.\nThrough further diagnosis, we find that enabling explicit safety reasoning and better safety-relevant visual understanding can improve the safety performance of MLLMs. Based on our findings, we propose multi-agent approaches in which we let different agents perform different subtasks to improve the safety performance of MLLMs. Report issue for preceding element",
    "masked_text": "In conclusion, this paper introduces the novel problem of Multimodal Situational Safety to evaluate the safety awareness of Multimodal Large Language Models (MLLMs) in scenarios where the safety of user queries depends on the visual context. By creating a comprehensive benchmark containing both safe and unsafe scenarios in chat and embodied assistant settings, the study reveals significant challenges that current MLLMs face in recognizing unsafe situations when answering a query, especially in embodied scenarios. Through further diagnosis, we find that enabling explicit safety reasoning and better safety-relevant visual understanding can improve the safety performance of MLLMs. Based on our findings, we propose multi-agent approaches in which we let different agents perform different subtasks to improve the safety performance of MLLMs.Report issue for preceding element",
    "citations": []
  },
  "S6.p2": {
    "text": "Our method shows promise for improving situational safety performance, but there is still considerable work left to enhance situational safety. First, the performance of multi-agent is still far from perfect due to MLLMs\u2019s imperfect visual understanding and safety judgment. Second, multi-agent pipelines take a longer time to answer a user\u2019s query since the model will explicitly reason multiple steps. Safety alignment training has enabled LLMs to refuse malicious language queries instantly without long reasoning (Wang et\u00a0al., 2024c ) . We believe this could be a promising step in addressing the multimodal situational safety problem. Moreover, enabling extra visual tools for visual prompting could also be a promising direction to mitigate incorrect visual understanding (Yang et\u00a0al., 2023 ) . Report issue for preceding element",
    "masked_text": "Our method shows promise for improving situational safety performance, but there is still considerable work left to enhance situational safety. First, the performance of multi-agent is still far from perfect due to MLLMs\u2019s imperfect visual understanding and safety judgment. Second, multi-agent pipelines take a longer time to answer a user\u2019s query since the model will explicitly reason multiple steps. Safety alignment training has enabled LLMs to refuse malicious language queries instantly without long reasoning [CITATION]. We believe this could be a promising step in addressing the multimodal situational safety problem. Moreover, enabling extra visual tools for visual prompting could also be a promising direction to mitigate incorrect visual understanding [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Wang et\u00a0al. (2024c)",
        "title": "Do-not-answer: Evaluating safeguards in LLMs.",
        "authors": "Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.",
        "journal": "In Yvette Graham and Matthew Purver (eds.),Findings of the Association for Computational Linguistics: EACL 2024, pp.\u00a0 896\u2013911, St. Julian\u2019s, Malta, March 2024c. Association for Computational Linguistics."
      },
      {
        "tag": "Yang et\u00a0al. (2023)",
        "title": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.",
        "authors": "Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao.",
        "journal": "arXiv preprint arXiv:2310.11441, 2023."
      }
    ]
  },
  "Sx1.p1": {
    "text": "This project was benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program. Report issue for preceding element",
    "masked_text": "This project was benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.Report issue for preceding element",
    "citations": []
  }
}