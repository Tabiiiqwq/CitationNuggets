{
  "S1.p1": {
    "text": "Large language models (LMs) are often pretrained with a single pass on web-scale datasets (Raffel et\u00a0al., 2020 ; Gao et\u00a0al., 2020 ; Penedo et\u00a0al., 2023 , inter alia ) .\nGiven the colossal size of these training sets, one may expect each individual instance to have little impact on the final model.\nYet, LMs can still reproduce entire sequences from their training set verbatim (Carlini et\u00a0al., 2021 ) , suggesting that models can store, or memorise , precise knowledge about individual training instances.\nIn the era of large LMs, measuring memorisation is crucial for NLP practitioners; it has implications for copyright and data protection (Hu et\u00a0al., 2022 ; Vyas et\u00a0al., 2023 ; Lee et\u00a0al., 2023 ) , for how models encode factual information (Cao et\u00a0al., 2022 ; Tirumala et\u00a0al., 2022 ) , and for understanding their training dynamics Arpit et\u00a0al. ( 2017 ); Chang and Bergen ( 2024 ) . Report issue for preceding element",
    "masked_text": "Large language models (LMs) are often pretrained with a single pass on web-scale datasets [CITATION]. Given the colossal size of these training sets, one may expect each individual instance to have little impact on the final model. Yet, LMs can still reproduce entire sequences from their training set verbatim [CITATION], suggesting that models can store, or memorise, precise knowledge about individual training instances. In the era of large LMs, measuring memorisation is crucial for NLP practitioners; it has implications for copyright and data protection [CITATION], for how models encode factual information [CITATION], and for understanding their training dynamics [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Carlini et\u00a0al. (2021)",
        "title": "Extracting training data from large language models.",
        "authors": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.",
        "journal": "In30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association."
      },
      {
        "tag": "Cao et\u00a0al. (2022)",
        "title": "Benign overfitting in two-layer convolutional neural networks.",
        "authors": "Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. 2022.",
        "journal": "Advances in Neural Information Processing Systems, 35:25237\u201325250."
      },
      {
        "tag": "Hu et\u00a0al. (2022)",
        "title": "Membership inference attacks on machine learning: A survey.",
        "authors": "Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip\u00a0S. Yu, and Xuyun Zhang. 2022.",
        "journal": "ACM Comput. Surv., 54(11s)."
      },
      {
        "tag": "Lee et\u00a0al. (2023)",
        "title": "Do language models plagiarize?",
        "authors": "Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. 2023.",
        "journal": "InProceedings of the ACM Web Conference 2023, WWW \u201923, page 3637\u20133647, New York, NY, USA. Association for Computing Machinery."
      },
      {
        "tag": "Chang and Bergen (2024)",
        "title": "Language model behavior: A comprehensive survey.",
        "authors": "Tyler\u00a0A. Chang and Benjamin\u00a0K. Bergen. 2024.",
        "journal": "Computational Linguistics, pages 1\u201358."
      },
      {
        "tag": "Raffel et\u00a0al. (2020)",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
        "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J. Liu. 2020.",
        "journal": "Journal of Machine Learning Research, 21(140):1\u201367."
      },
      {
        "tag": "Arpit et\u00a0al. (2017)",
        "title": "A closer look at memorization in deep networks.",
        "authors": "Devansh Arpit, Stanis\u0142aw Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder\u00a0S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017.",
        "journal": "InProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 233\u2013242. JMLR.org."
      },
      {
        "tag": "Penedo et\u00a0al. (2023)",
        "title": "The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only.",
        "authors": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.",
        "journal": "arXiv preprint 2306.01116."
      },
      {
        "tag": "Vyas et\u00a0al. (2023)",
        "title": "On provable copyright protection for generative models.",
        "authors": "Nikhil Vyas, Sham\u00a0M. Kakade, and Boaz Barak. 2023.",
        "journal": "InInternational Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pages 35277\u201335299. PMLR."
      },
      {
        "tag": "Tirumala et\u00a0al. (2022)",
        "title": "Memorization without overfitting: Analyzing the training dynamics of large language models.",
        "authors": "Kushal Tirumala, Aram\u00a0H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.",
        "journal": "InAdvances in Neural Information Processing Systems."
      },
      {
        "tag": "Gao et\u00a0al. (2020)",
        "title": "The Pile: An 800GB dataset of diverse text for language modeling.",
        "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.",
        "journal": "arXiv preprint 2101.00027."
      }
    ]
  },
  "S1.p2": {
    "text": "One line of prior work has adopted a causal definition of memorisation : it is the causal effect of observing an instance during training on a model\u2019s ability to correctly predict that instance (Feldman, 2020 ) .\nDespite being an intuitive concept, quantification of this definition is not straightforward as it requires knowledge of a counterfactual : 1 1 1 Counterfactuals are thought experiments about what would have happened if a present condition were changed while keeping everything else unchanged (McCloskey, 2016 ) . we must know how our model would have performed on a training instance had the model not been trained on it.\nTo overcome this challenge, prior work has proposed a variety of methods to estimate memorisation.\nSome estimate it by training a model multiple times on different subsets of the data (e.g., Feldman and Zhang, 2020 ; Zheng and Jiang, 2022 ) , while others implicitly assume this counterfactual\u2019s value to be negligible (Carlini et\u00a0al., 2021 ) .\nBoth these approaches, however, have drawbacks:\nthe first computes memorisation for an architecture rather than a specific model, while the second relies on a strong assumption (we discuss this in detail in \u00a7 5 ). Report issue for preceding element",
    "masked_text": "One line of prior work has adopted a causal definition of memorisation: it is the causal effect of observing an instance during training on a model\u2019s ability to correctly predict that instance [CITATION]. Despite being an intuitive concept, quantification of this definition is not straightforward as it requires knowledge of a counterfactual:111Counterfactuals are thought experiments about what would have happened if a present condition were changed while keeping everything else unchanged [CITATION]. we must know how our model would have performed on a training instance had the model not been trained on it. To overcome this challenge, prior work has proposed a variety of methods to estimate memorisation. Some estimate it by training a model multiple times on different subsets of the data [CITATION], while others implicitly assume this counterfactual\u2019s value to be negligible [CITATION]. Both these approaches, however, have drawbacks: the first computes memorisation for an architecture rather than a specific model, while the second relies on a strong assumption (we discuss this in detail in \u00a7 5).Report issue for preceding element",
    "citations": [
      {
        "tag": "Carlini et\u00a0al. (2021)",
        "title": "Extracting training data from large language models.",
        "authors": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.",
        "journal": "In30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association."
      },
      {
        "tag": "McCloskey (2016)",
        "title": "Counterfactuals, pages 1\u20135. Palgrave Macmillan UK, London.",
        "authors": "Donald\u00a0N. McCloskey. 2016.",
        "journal": ""
      },
      {
        "tag": "Feldman (2020)",
        "title": "Does learning require memorization? A short tale about a long tail.",
        "authors": "Vitaly Feldman. 2020.",
        "journal": "InProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, pages 954\u2013959, New York, NY, USA. Association for Computing Machinery."
      },
      {
        "tag": "Feldman and Zhang (2020)",
        "title": "What neural networks memorize and why: Discovering the long tail via influence estimation.",
        "authors": "Vitaly Feldman and Chiyuan Zhang. 2020.",
        "journal": "InAdvances in Neural Information Processing Systems, volume\u00a033, pages 2881\u20132891. Curran Associates, Inc."
      },
      {
        "tag": "Zheng and Jiang (2022)",
        "title": "An empirical study of memorization in NLP.",
        "authors": "Xiaosen Zheng and Jing Jiang. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6265\u20136278, Dublin, Ireland. Association for Computational Linguistics."
      }
    ]
  },
  "S1.p3": {
    "text": "In this paper, we first formalise counterfactual memorisation as the difference between two potential outcomes; notably, our formalisation generalises prior definitions of memorisation, allowing us to compare them within a unified framework.\nWe then draw from the econometrics literature (Callaway and Sant\u2019Anna, 2021 ) and propose a new method which estimates memorisation using only observational data;\nour method simply needs a model\u2019s performance measurements (e.g., log-likelihood) on a subset of the training data throughout training.\nThe output of our method is what we term a memorisation profile : a model\u2019s memorisation of training batches over the course of training. Report issue for preceding element",
    "masked_text": "In this paper, we first formalise counterfactual memorisation as the difference between two potential outcomes; notably, our formalisation generalises prior definitions of memorisation, allowing us to compare them within a unified framework. We then draw from the econometrics literature [CITATION] and propose a new method which estimates memorisation using only observational data; our method simply needs a model\u2019s performance measurements (e.g., log-likelihood) on a subset of the training data throughout training. The output of our method is what we term a memorisation profile: a model\u2019s memorisation of training batches over the course of training.Report issue for preceding element",
    "citations": [
      {
        "tag": "Callaway and Sant\u2019Anna (2021)",
        "title": "Difference-in-Differences with multiple time periods.",
        "authors": "Brantly Callaway and Pedro H.\u00a0C. Sant\u2019Anna. 2021.",
        "journal": "Journal of Econometrics, 225(2):200\u2013230."
      }
    ]
  },
  "S1.p4": {
    "text": "Empirically, we use this method to analyse memorisation for models in the Pythia suite (Biderman et\u00a0al., 2023b ) and characterise their memorisation profiles; e.g., Fig. 1 reports the memorisation profile of Pythia \\qty [mode=math]6.9.\nBy studying these memorisation profiles we find that memorisation is stronger and more persistent in larger models.\nFurthermore, both the learning rate and the position of an instance in the training set considerably impact how strongly that instance is memorised.\nFinally, memorisation profiles are stable across model sizes; thus, we can predict memorisation in larger models from the memorisation observed in smaller ones. Report issue for preceding element",
    "masked_text": "Empirically, we use this method to analyse memorisation for models in the Pythia suite [CITATION] and characterise their memorisation profiles; e.g., Fig. 1 reports the memorisation profile of Pythia \\qty[mode=math]6.9. By studying these memorisation profiles we find that memorisation is stronger and more persistent in larger models. Furthermore, both the learning rate and the position of an instance in the training set considerably impact how strongly that instance is memorised. Finally, memorisation profiles are stable across model sizes; thus, we can predict memorisation in larger models from the memorisation observed in smaller ones.Report issue for preceding element",
    "citations": [
      {
        "tag": "Biderman et\u00a0al. (2023b)",
        "title": "Pythia: A suite for analyzing large language models across training and scaling.",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der\u00a0Wal. 2023b.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, ICML\u201923."
      }
    ]
  },
  "S2.p1": {
    "text": "In this section, we introduce some background on language modelling and on causal analysis which will be required throughout our paper. Report issue for preceding element",
    "masked_text": "In this section, we introduce some background on language modelling and on causal analysis which will be required throughout our paper.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.p1": {
    "text": "We start by providing some background on language modelling. 2 2 2 We frame our exposition in terms of language models, but our framework can be trivially applied to any neural model and input modality (e.g., images). Let p \\scaleto \u2062 \ud835\udf3d \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) subscript \ud835\udc5d \\scaleto \ud835\udf3d 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 p_{\\scaleto{\\bm{\\theta}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) be a language model with parameters \ud835\udf3d \u2208 \u211d d \ud835\udf3d superscript \u211d \ud835\udc51 \\bm{\\theta}\\in\\mathbb{R}^{d} bold_italic_\u03b8 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\nThis model defines a probability distribution over \ud835\udc99 \u2208 \ud835\udcb1 \u2217 \ud835\udc99 superscript \ud835\udcb1 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\in\\mathcal{V}^{*} bold_italic_x \u2208 caligraphic_V start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , the set of all finite sequences that can be constructed from elements in the alphabet \ud835\udcb1 \ud835\udcb1 \\mathcal{V} caligraphic_V .\nTo train this model, we start with a set of randomly selected initial parameters, \ud835\udf3d 0 subscript \ud835\udf3d 0 \\bm{\\theta}_{0} bold_italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .\nWe then learn parameters \ud835\udf3d \ud835\udf3d \\bm{\\theta} bold_italic_\u03b8 using a dataset \ud835\udc9f \ud835\udc9f \\mathcal{D} caligraphic_D and an optimisation procedure defined with respect to a loss function \u2112 \u2112 \\mathcal{L} caligraphic_L . Report issue for preceding element",
    "masked_text": "We start by providing some background on language modelling.222We frame our exposition in terms of language models, but our framework can be trivially applied to any neural model and input modality (e.g., images). Let p\\scaleto\u2062\ud835\udf3d\u20624\u2062p\u2062t\u2062(\ud835\udc99)subscript\ud835\udc5d\\scaleto\ud835\udf3d4\ud835\udc5d\ud835\udc61\ud835\udc99p_{\\scaleto{\\bm{\\theta}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) be a language model with parameters \ud835\udf3d\u2208\u211dd\ud835\udf3dsuperscript\u211d\ud835\udc51\\bm{\\theta}\\in\\mathbb{R}^{d}bold_italic_\u03b8 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. This model defines a probability distribution over \ud835\udc99\u2208\ud835\udcb1\u2217\ud835\udc99superscript\ud835\udcb1{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\in\\mathcal{V}^{*}bold_italic_x \u2208 caligraphic_V start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT, the set of all finite sequences that can be constructed from elements in the alphabet \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V. To train this model, we start with a set of randomly selected initial parameters, \ud835\udf3d0subscript\ud835\udf3d0\\bm{\\theta}_{0}bold_italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. We then learn parameters \ud835\udf3d\ud835\udf3d\\bm{\\theta}bold_italic_\u03b8 using a dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D and an optimisation procedure defined with respect to a loss function \u2112\u2112\\mathcal{L}caligraphic_L.Report issue for preceding element",
    "citations": []
  },
  "S2.SS1.p2": {
    "text": "Specifically, let \ud835\udc9f = { \ud835\udc99 n } n = 1 N \ud835\udc9f superscript subscript subscript \ud835\udc99 \ud835\udc5b \ud835\udc5b 1 \ud835\udc41 \\mathcal{D}=\\{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill%\n{0.91}{0}{0.88}{0.12}\\bm{x}}_{n}\\}_{n=1}^{N} caligraphic_D = { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be a dataset whose instances \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x are sequences drawn from a target (unknown) distribution p \u2062 ( \ud835\udc99 ) \ud835\udc5d \ud835\udc99 p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_p ( bold_italic_x ) .\nThese instances are typically assumed to be sampled i.i.d., and are shuffled using a permutation function \u03c3 : { 1 , \u2026 , N } \u2192 { 1 , \u2026 , N } : \ud835\udf0e \u2192 1 \u2026 \ud835\udc41 1 \u2026 \ud835\udc41 \\sigma\\colon\\{1,...,N\\}\\!\\to\\!\\{1,...,N\\} italic_\u03c3 : { 1 , \u2026 , italic_N } \u2192 { 1 , \u2026 , italic_N } .\nFor a given batch size B \ud835\udc35 B italic_B , we then split this dataset into T \u2264 \u230a N / B \u230b \ud835\udc47 \ud835\udc41 \ud835\udc35 T\\!\\leq\\!\\lfloor\\nicefrac{{N}}{{B}}\\rfloor italic_T \u2264 \u230a / start_ARG italic_N end_ARG start_ARG italic_B end_ARG \u230b batches \u212c t subscript \u212c \ud835\udc61 \\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .\nWe iterate through these batches performing gradient updates on the model parameters: Report issue for preceding element \ud835\udf3d t = \ud835\udf3d t \u2212 1 \u2212 \u03b7 \u2062 \u2207 \ud835\udf3d \u2112 \u2062 ( \ud835\udf3d t \u2212 1 , \u212c t ) subscript \ud835\udf3d \ud835\udc61 subscript \ud835\udf3d \ud835\udc61 1 \ud835\udf02 subscript \u2207 \ud835\udf3d \u2112 subscript \ud835\udf3d \ud835\udc61 1 subscript \u212c \ud835\udc61 \\displaystyle\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}t}}=\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}t-1}}-\\eta\\,\\nabla_{\\bm{%\n\\theta}}\\mathcal{L}(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}t-1}},\\mathcal{B}_{{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}t}}) bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_\u03b7 \u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (1) where \u03b7 \u2208 \u211d \ud835\udf02 \u211d \\eta\\in\\mathbb{R} italic_\u03b7 \u2208 blackboard_R is a learning rate. 3 3 3 The learning rate can be a function of other quantities (e.g., Duchi et\u00a0al., 2011 ; Kingma and Ba, 2015 , inter alia ) . Notably, this procedure consists of a single pass on the training set and is standard for recent LMs (e.g., Touvron et\u00a0al., 2023 ; Jiang et\u00a0al., 2023 ; Dey et\u00a0al., 2023 ) . Report issue for preceding element",
    "masked_text": "Specifically, let \ud835\udc9f={\ud835\udc99n}n=1N\ud835\udc9fsuperscriptsubscriptsubscript\ud835\udc99\ud835\udc5b\ud835\udc5b1\ud835\udc41\\mathcal{D}=\\{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill% {0.91}{0}{0.88}{0.12}\\bm{x}}_{n}\\}_{n=1}^{N}caligraphic_D = { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be a dataset whose instances \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x are sequences drawn from a target (unknown) distribution p\u2062(\ud835\udc99)\ud835\udc5d\ud835\udc99p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_p ( bold_italic_x ). These instances are typically assumed to be sampled i.i.d., and are shuffled using a permutation function \u03c3:{1,\u2026,N}\u2192{1,\u2026,N}:\ud835\udf0e\u21921\u2026\ud835\udc411\u2026\ud835\udc41\\sigma\\colon\\{1,...,N\\}\\!\\to\\!\\{1,...,N\\}italic_\u03c3 : { 1 , \u2026 , italic_N } \u2192 { 1 , \u2026 , italic_N }. For a given batch size B\ud835\udc35Bitalic_B, we then split this dataset into T\u2264\u230aN/B\u230b\ud835\udc47\ud835\udc41\ud835\udc35T\\!\\leq\\!\\lfloor\\nicefrac{{N}}{{B}}\\rflooritalic_T \u2264 \u230a / start_ARG italic_N end_ARG start_ARG italic_B end_ARG \u230b batches \u212ctsubscript\u212c\ud835\udc61\\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}t}}caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We iterate through these batches performing gradient updates on the model parameters:Report issue for preceding element \ud835\udf3dt=\ud835\udf3dt\u22121\u2212\u03b7\u2062\u2207\ud835\udf3d\u2112\u2062(\ud835\udf3dt\u22121,\u212ct)subscript\ud835\udf3d\ud835\udc61subscript\ud835\udf3d\ud835\udc611\ud835\udf02subscript\u2207\ud835\udf3d\u2112subscript\ud835\udf3d\ud835\udc611subscript\u212c\ud835\udc61\\displaystyle\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}t}}=\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}t-1}}-\\eta\\,\\nabla_{\\bm{% \\theta}}\\mathcal{L}(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}t-1}},\\mathcal{B}_{{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}t}})bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_\u03b7 \u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (1) where \u03b7\u2208\u211d\ud835\udf02\u211d\\eta\\in\\mathbb{R}italic_\u03b7 \u2208 blackboard_R is a learning rate.333The learning rate can be a function of other quantities [CITATION]. Notably, this procedure consists of a single pass on the training set and is standard for recent LMs [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Touvron et\u00a0al. (2023)",
        "title": "Llama 2: Open foundation and fine-tuned chat models.",
        "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\u00a0Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit\u00a0Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric\u00a0Michael Smith, Ranjan Subramanian, Xiaoqing\u00a0Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian\u00a0Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023.",
        "journal": "arXiv preprint 2307.09288."
      },
      {
        "tag": "Kingma and Ba (2015)",
        "title": "Adam: A method for stochastic optimization.",
        "authors": "Diederik\u00a0P. Kingma and Jimmy Ba. 2015.",
        "journal": "In3rd International Conference on Learning Representations."
      },
      {
        "tag": "Jiang et\u00a0al. (2023)",
        "title": "Mistral 7B.",
        "authors": "Albert\u00a0Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra\u00a0Singh Chaplot, Diego de\u00a0las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio\u00a0Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven\u00a0Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William\u00a0El Sayed. 2023.",
        "journal": "arXiv preprint 2310.06825."
      },
      {
        "tag": "Duchi et\u00a0al. (2011)",
        "title": "Adaptive subgradient methods for online learning and stochastic optimization.",
        "authors": "John Duchi, Elad Hazan, and Yoram Singer. 2011.",
        "journal": "Journal of Machine Learning Research, 12(61):2121\u20132159."
      },
      {
        "tag": "Dey et\u00a0al. (2023)",
        "title": "Cerebras-GPT: Open compute-optimal language models trained on the cerebras wafer-scale cluster.",
        "authors": "Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023.",
        "journal": "arXiv preprint 2304.03208."
      }
    ]
  },
  "S2.SS1.p3": {
    "text": "At each iteration, we use a batch \u212c t subscript \u212c \ud835\udc61 \\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to obtain a new model checkpoint, \ud835\udf3d t subscript \ud835\udf3d \ud835\udc61 \\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}t}} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .\nWe introduce a notation which distinguishes the indexing of checkpoints and batches.\nWe use c \u2208 { 0 , 1 , \u2026 , T } \ud835\udc50 0 1 \u2026 \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n\\in\\{0,1,...,T\\} italic_c \u2208 { 0 , 1 , \u2026 , italic_T } to denote a checkpoint step (e.g., \ud835\udf3d c subscript \ud835\udf3d \ud835\udc50 \\bm{\\theta}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ). Further, we use g \u2208 { 1 , \u2026 , T } \u222a { \u221e } \ud835\udc54 1 \u2026 \ud835\udc47 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{1,.%\n..,T\\}\\cup\\{\\infty\\} italic_g \u2208 { 1 , \u2026 , italic_T } \u222a { \u221e } to denote the timestep at which a batch is used for training (e.g., \u212c g subscript \u212c \ud835\udc54 \\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ); we term this a treatment step , borrowing this terminology from the econometrics literature.\nWe denote as g = \u221e \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\!=\\!\\infty italic_g = \u221e a batch composed of instances that are not used for training and which form a validation set. Report issue for preceding element",
    "masked_text": "At each iteration, we use a batch \u212ctsubscript\u212c\ud835\udc61\\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}t}}caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to obtain a new model checkpoint, \ud835\udf3dtsubscript\ud835\udf3d\ud835\udc61\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}t}}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We introduce a notation which distinguishes the indexing of checkpoints and batches. We use c\u2208{0,1,\u2026,T}\ud835\udc5001\u2026\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% \\in\\{0,1,...,T\\}italic_c \u2208 { 0 , 1 , \u2026 , italic_T } to denote a checkpoint step (e.g., \ud835\udf3dcsubscript\ud835\udf3d\ud835\udc50\\bm{\\theta}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT). Further, we use g\u2208{1,\u2026,T}\u222a{\u221e}\ud835\udc541\u2026\ud835\udc47{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{1,.% ..,T\\}\\cup\\{\\infty\\}italic_g \u2208 { 1 , \u2026 , italic_T } \u222a { \u221e } to denote the timestep at which a batch is used for training (e.g., \u212cgsubscript\u212c\ud835\udc54\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT); we term this a treatment step, borrowing this terminology from the econometrics literature. We denote as g=\u221e\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\!=\\!\\inftyitalic_g = \u221e a batch composed of instances that are not used for training and which form a validation set.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.p1": {
    "text": "Causal estimation is typically split into three steps.\nFirst, we define a causal estimand , the target quantity we want to estimate.\nSecond, we state the assumptions needed to rewrite this causal estimand in terms of observable data, thus defining a statistical estimand ; this process is called identification.\nFinally, we define an estimator , a statistical procedure to approximate the statistical estimand. Report issue for preceding element",
    "masked_text": "Causal estimation is typically split into three steps. First, we define a causal estimand, the target quantity we want to estimate. Second, we state the assumptions needed to rewrite this causal estimand in terms of observable data, thus defining a statistical estimand; this process is called identification. Finally, we define an estimator, a statistical procedure to approximate the statistical estimand.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.p2": {
    "text": "To formally define memorisation as a causal estimand, we will use the potential outcomes framework of Rubin ( 1974 , 2005 ) . 4 4 4 For a comparison of causal frameworks, see Ibeling and Icard ( 2023 ) . For an introduction to causal inference, see Pearl ( 2009 ) and Imbens and Rubin ( 2015 ) . This framework allows us to formally describe the causal effect of an intervention, or treatment , on some target quantity, or outcome .\nIn \u00a7 1 , we defined memorisation as the causal effect of training on an instance on a model\u2019s ability to predict it.\nThus, the act of training on \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x defines the treatment, while the model\u2019s ability to predict an instance defines the outcome. Report issue for preceding element",
    "masked_text": "To formally define memorisation as a causal estimand, we will use the potential outcomes framework of [CITATION].444For a comparison of causal frameworks, see [CITATION]. For an introduction to causal inference, see [CITATION] and [CITATION]. This framework allows us to formally describe the causal effect of an intervention, or treatment, on some target quantity, or outcome. In \u00a7 1, we defined memorisation as the causal effect of training on an instance on a model\u2019s ability to predict it. Thus, the act of training on \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x defines the treatment, while the model\u2019s ability to predict an instance defines the outcome.Report issue for preceding element",
    "citations": [
      {
        "tag": "Rubin (1974)",
        "title": "Estimating causal effects of treatments in randomized and nonrandomized studies.",
        "authors": "Donald\u00a0B. Rubin. 1974.",
        "journal": "Journal of Educational Psychology, 66(5):688\u2013701."
      },
      {
        "tag": "Pearl (2009)",
        "title": "Causality: Models, Reasoning and Inference, 2nd edition.",
        "authors": "Judea Pearl. 2009.",
        "journal": "Cambridge University Press, USA."
      },
      {
        "tag": "Ibeling and Icard (2023)",
        "title": "Comparing causal frameworks: Potential outcomes, structural models, graphs, and abstractions.",
        "authors": "Duligur Ibeling and Thomas Icard. 2023.",
        "journal": "Advances in Neural Information Processing Systems, 36:80130\u201380141."
      },
      {
        "tag": "Imbens and Rubin (2015)",
        "title": "Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction.",
        "authors": "Guido\u00a0W. Imbens and Donald\u00a0B. Rubin. 2015.",
        "journal": "Cambridge University Press."
      },
      {
        "tag": "Rubin (2005)",
        "title": "Causal inference using potential outcomes.",
        "authors": "Donald\u00a0B. Rubin. 2005.",
        "journal": "Journal of the American Statistical Association, 100(469):322\u2013331."
      }
    ]
  },
  "S2.SS2.p3": {
    "text": "Since training is performed iteratively over batches, instances are treated at different timesteps.\nThus, we use a treatment assignment variable G \u2062 ( \ud835\udc99 ) \ud835\udc3a \ud835\udc99 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_G ( bold_italic_x ) to denote the step g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g an instance is trained on.\nFurther, to quantify the ability of a model with parameters \ud835\udf3d c subscript \ud835\udf3d \ud835\udc50 \\bm{\\theta}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT to predict \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x , we use a performance function \u03b3 \ud835\udefe \\gamma italic_\u03b3 .\nWe then define the outcome variable as Y c \u2062 ( \ud835\udc99 ) = def \u03b3 \u2062 ( \ud835\udf3d c , \ud835\udc99 ) superscript def subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udefe subscript \ud835\udf3d \ud835\udc50 \ud835\udc99 Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}%\n\\gamma(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{.75,0,.25}c}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) , and, unless noted otherwise, we set this performance function to be the log-likelihood of \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x under p \\scaleto \u2062 \ud835\udf3d c \u2062 4 \u2062 p \u2062 t subscript \ud835\udc5d \\scaleto subscript \ud835\udf3d \ud835\udc50 4 \ud835\udc5d \ud835\udc61 p_{\\scaleto{\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}} italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT : \u03b3 \u2062 ( \ud835\udf3d c , \ud835\udc99 ) = log \u2061 p \\scaleto \u2062 \ud835\udf3d c \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) \ud835\udefe subscript \ud835\udf3d \ud835\udc50 \ud835\udc99 subscript \ud835\udc5d \\scaleto subscript \ud835\udf3d \ud835\udc50 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \\gamma(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{.75,0,.25}c}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})=\\log p_{\\scaleto{\\bm{%\n\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 5 5 5 We experiment with other functions in the appendix. Report issue for preceding element",
    "masked_text": "Since training is performed iteratively over batches, instances are treated at different timesteps. Thus, we use a treatment assignment variable G\u2062(\ud835\udc99)\ud835\udc3a\ud835\udc99G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_G ( bold_italic_x ) to denote the step g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g an instance is trained on. Further, to quantify the ability of a model with parameters \ud835\udf3dcsubscript\ud835\udf3d\ud835\udc50\\bm{\\theta}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT to predict \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x, we use a performance function \u03b3\ud835\udefe\\gammaitalic_\u03b3. We then define the outcome variable as Yc\u2062(\ud835\udc99)=def\u03b3\u2062(\ud835\udf3dc,\ud835\udc99)superscriptdefsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udefesubscript\ud835\udf3d\ud835\udc50\ud835\udc99Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% ({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}% \\gamma(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}% {rgb}{.75,0,.25}c}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ), and, unless noted otherwise, we set this performance function to be the log-likelihood of \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x under p\\scaleto\u2062\ud835\udf3dc\u20624\u2062p\u2062tsubscript\ud835\udc5d\\scaletosubscript\ud835\udf3d\ud835\udc504\ud835\udc5d\ud835\udc61p_{\\scaleto{\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT: \u03b3\u2062(\ud835\udf3dc,\ud835\udc99)=log\u2061p\\scaleto\u2062\ud835\udf3dc\u20624\u2062p\u2062t\u2062(\ud835\udc99)\ud835\udefesubscript\ud835\udf3d\ud835\udc50\ud835\udc99subscript\ud835\udc5d\\scaletosubscript\ud835\udf3d\ud835\udc504\ud835\udc5d\ud835\udc61\ud835\udc99\\gamma(\\bm{\\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}% {rgb}{.75,0,.25}c}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})=\\log p_{\\scaleto{\\bm{% \\theta}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}% {rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ).555We experiment with other functions in the appendix.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.p4": {
    "text": "To define memorisation we need to represent both observed\u2014i.e., Y c \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) \u2014and counterfactual outcomes\u2014i.e., the performance of the model on \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x had we not trained on it.\nThe potential outcomes notation (Splawa-Neyman, 1923 ) enables us to represent both types of outcomes consistently. Report issue for preceding element",
    "masked_text": "To define memorisation we need to represent both observed\u2014i.e., Yc\u2062(\ud835\udc99)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% ({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x )\u2014and counterfactual outcomes\u2014i.e., the performance of the model on \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x had we not trained on it. The potential outcomes notation [CITATION] enables us to represent both types of outcomes consistently.Report issue for preceding element",
    "citations": [
      {
        "tag": "Splawa-Neyman (1923)",
        "title": "On the application of probability theory to agricultural experiments. Essay on principles.",
        "authors": "Jerzy Splawa-Neyman. 1923.",
        "journal": "Annals of Agricultural Sciences, pages 1\u201351."
      }
    ]
  },
  "Thmdefinition1.p1": {
    "text": "The potential outcome of an instance \ud835\udc31 \ud835\udc31 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x at timestep c \ud835\udc50 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c under treatment assignment g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , denoted as Y c \u2062 ( \ud835\udc31 ; g ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc31 \ud835\udc54 Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , is the value that the outcome would have taken if G \u2062 ( \ud835\udc31 ) \ud835\udc3a \ud835\udc31 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_G ( bold_italic_x ) was equal to g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element",
    "masked_text": "The potential outcome of an instance \ud835\udc31\ud835\udc31{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x at timestep c\ud835\udc50{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c under treatment assignment g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g, denoted as Yc\u2062(\ud835\udc31;g)subscript\ud835\udc4c\ud835\udc50\ud835\udc31\ud835\udc54Y_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% ({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ), is the value that the outcome would have taken if G\u2062(\ud835\udc31)\ud835\udc3a\ud835\udc31G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_G ( bold_italic_x ) was equal to g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g.Report issue for preceding element",
    "citations": []
  },
  "S2.SS2.p5": {
    "text": "Since we only observe a single permutation of the data, we only see one specific treatment step for each instance, i.e., G \u2062 ( \ud835\udc99 ) \ud835\udc3a \ud835\udc99 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_G ( bold_italic_x ) .\nThus, the potential outcome of an instance is observed only for the actual treatment assignment g = G \u2062 ( \ud835\udc99 ) \ud835\udc54 \ud835\udc3a \ud835\udc99 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\!=\\!G({%\n\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_g = italic_G ( bold_italic_x ) .\nIn this case, we can equate potential and observed outcomes, that is Y c \u2062 ( \ud835\udc99 ; g ) = Y c \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g})=Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) , a property called consistency (Cole and Frangakis, 2009 ) .\nFor any other treatment step g \u2062 \u2260 G \u2062 ( \ud835\udc99 ) \ud835\udc54 \ud835\udc3a \ud835\udc99 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{%\n\\neq}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%\n}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0%\n}{0.88}{0.12}\\bm{x}}) italic_g \u2260 italic_G ( bold_italic_x ) , the potential outcome is counterfactual and, thus, unobservable from the data. Report issue for preceding element",
    "masked_text": "Since we only observe a single permutation of the data, we only see one specific treatment step for each instance, i.e., G\u2062(\ud835\udc99)\ud835\udc3a\ud835\udc99G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_G ( bold_italic_x ). Thus, the potential outcome of an instance is observed only for the actual treatment assignment g=G\u2062(\ud835\udc99)\ud835\udc54\ud835\udc3a\ud835\udc99{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\!=\\!G({% \\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_g = italic_G ( bold_italic_x ). In this case, we can equate potential and observed outcomes, that is Yc\u2062(\ud835\udc99;g)=Yc\u2062(\ud835\udc99)subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})=Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{% rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ), a property called consistency [CITATION]. For any other treatment step g\u2062\u2260G\u2062(\ud835\udc99)\ud835\udc54\ud835\udc3a\ud835\udc99{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{% \\neq}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0% }\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0% }{0.88}{0.12}\\bm{x}})italic_g \u2260 italic_G ( bold_italic_x ), the potential outcome is counterfactual and, thus, unobservable from the data.Report issue for preceding element",
    "citations": [
      {
        "tag": "Cole and Frangakis (2009)",
        "title": "The consistency statement in causal inference: A definition or an assumption?",
        "authors": "Stephen\u00a0R. Cole and Constantine\u00a0E. Frangakis. 2009.",
        "journal": "Epidemiology, 20(1)."
      }
    ]
  },
  "S3.p1": {
    "text": "Intuitively, counterfactual memorisation can be understood as the answer to the question: how would the model\u2019s performance on instance \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x at timestep c \ud835\udc50 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c be different if we had not trained on it at timestep g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g ?\nUsing the potential outcomes notation, we formalise this definition as follows. Report issue for preceding element",
    "masked_text": "Intuitively, counterfactual memorisation can be understood as the answer to the question: how would the model\u2019s performance on instance \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x at timestep c\ud835\udc50{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c be different if we had not trained on it at timestep g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g? Using the potential outcomes notation, we formalise this definition as follows.Report issue for preceding element",
    "citations": []
  },
  "Thmdefinition2.p1": {
    "text": "Counterfactual memorisation is the causal effect of using instance \ud835\udc31 \ud835\udc31 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x for training at the observed timestep G \u2062 ( \ud835\udc31 ) \u2062 = g G \ud835\udc31 g G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g on the model\u2019s performance on this same instance at timestep c c {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : Report issue for preceding element \u03c4 \ud835\udc99 , c = def Y c \u2062 ( \ud835\udc99 ; g ) \u23df performance on \ud835\udc99 when trained with \ud835\udc99 \u2212 Y c \u2062 ( \ud835\udc99 ; \u221e ) \u23df performance on \ud835\udc99 when not trained with \ud835\udc99 superscript def subscript \ud835\udf0f \ud835\udc99 \ud835\udc50 subscript \u23df subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 performance on \ud835\udc99 when trained with \ud835\udc99 subscript \u23df subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 performance on \ud835\udc99 when not trained with \ud835\udc99 \\displaystyle\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}\\,\\,\\,\\mathrel{\\stackrel%\n{{\\scriptstyle\\textnormal{def}}}{{=}}}\\underbrace{Y_{{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.%\n91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[%\nrgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})}_{\\begin{%\nsubarray}{c}\\text{performance on ${\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\\\\n\\text{when trained with ${\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\end{subarray}}-%\n\\underbrace{Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}%\n{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill%\n{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}\\infty})}_{\\begin{subarray}{c}\\text{performance on%\n ${\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}$}\\\\\n\\text{when not trained with ${\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\end{subarray}}\\!\\!\\!\\! italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP under\u23df start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT - under\u23df start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when not trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT (2)",
    "masked_text": "Counterfactual memorisation is the causal effect of using instance \ud835\udc31\ud835\udc31{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x for training at the observed timestep G\u2062(\ud835\udc31)\u2062=gG\ud835\udc31gG({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}italic_G ( bold_italic_x ) = italic_g on the model\u2019s performance on this same instance at timestep cc{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c:Report issue for preceding element \u03c4\ud835\udc99,c=defYc\u2062(\ud835\udc99;g)\u23dfperformance on \ud835\udc99when trained with \ud835\udc99\u2212Yc\u2062(\ud835\udc99;\u221e)\u23dfperformance on \ud835\udc99when not trained with \ud835\udc99superscriptdefsubscript\ud835\udf0f\ud835\udc99\ud835\udc50subscript\u23dfsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54performance on \ud835\udc99when trained with \ud835\udc99subscript\u23dfsubscript\ud835\udc4c\ud835\udc50\ud835\udc99performance on \ud835\udc99when not trained with \ud835\udc99\\displaystyle\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}\\,\\,\\,\\mathrel{\\stackrel% {{\\scriptstyle\\textnormal{def}}}{{=}}}\\underbrace{Y_{{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.% 91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[% rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})}_{\\begin{% subarray}{c}\\text{performance on ${\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\\\ \\text{when trained with ${\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\end{subarray}}-% \\underbrace{Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}% {.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill% {0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}\\infty})}_{\\begin{subarray}{c}\\text{performance on% ${\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}$}\\\\ \\text{when not trained with ${\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}$}\\end{subarray}}\\!\\!\\!\\!italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP under\u23df start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT - under\u23df start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when not trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT (2)",
    "citations": []
  },
  "S3.p2": {
    "text": "In econometrics, eq. 2 is called an individual treatment effect (ITE).\nNotably, the first potential outcome in this equation, Y c \u2062 ( \ud835\udc99 ; g ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , can be observed from the data since, by definition, we trained on \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x at timestep G \u2062 ( \ud835\udc99 ) \u2062 = g \ud835\udc3a \ud835\udc99 \ud835\udc54 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g .\nHowever, the second term, Y c \u2062 ( \ud835\udc99 ; \u221e ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) , is counterfactual.\nTo compute the ITE, we would need to estimate this counterfactual outcome for a specific instance, which is challenging due to unobserved factors and heterogeneity 6 6 6 I.e.,\u00a0non-random variability across instances. (Lu et\u00a0al., 2018 ) .\nWhile we would ideally estimate memorisation at the instance level, we focus on average effects instead, as is common in the econometrics literature (Angrist and Pischke, 2015 ) . Report issue for preceding element",
    "masked_text": "In econometrics, eq. 2 is called an individual treatment effect (ITE). Notably, the first potential outcome in this equation, Yc\u2062(\ud835\udc99;g)subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ), can be observed from the data since, by definition, we trained on \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x at timestep G\u2062(\ud835\udc99)\u2062=g\ud835\udc3a\ud835\udc99\ud835\udc54G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}italic_G ( bold_italic_x ) = italic_g. However, the second term, Yc\u2062(\ud835\udc99;\u221e)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ), is counterfactual. To compute the ITE, we would need to estimate this counterfactual outcome for a specific instance, which is challenging due to unobserved factors and heterogeneity666I.e., non-random variability across instances. [CITATION]. While we would ideally estimate memorisation at the instance level, we focus on average effects instead, as is common in the econometrics literature [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Lu et\u00a0al. (2018)",
        "title": "Estimating individual treatment effect in observational data using random forest methods.",
        "authors": "Min Lu, Saad Sadiq, Daniel\u00a0J Feaster, and Hemant Ishwaran. 2018.",
        "journal": "Journal of Computational and Graphical Statistics, 27(1):209\u2013219."
      },
      {
        "tag": "Angrist and Pischke (2015)",
        "title": "Mastering \u2019Metrics: The Path from Cause to Effect.",
        "authors": "Joshua Angrist and J\u00f6rn-Steffen Pischke. 2015.",
        "journal": "Princeton University Press."
      }
    ]
  },
  "Thmdefinition3.p1": {
    "text": "Expected counterfactual memorisation is the average causal effect of using instances for training at timestep g g {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g on the model\u2019s performance on these same instances at timestep c c {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : 7 7 7 In econometrics, eq. 3 is called an average treatment effect on the treated (ATT), as it is defined in terms of an expectation over \ud835\udc31 \u223c p \u2062 ( \ud835\udc31 \u2062 \u2223 G \u2062 ( \ud835\udc31 ) \u2062 = g ) similar-to \ud835\udc31 p \ud835\udc31 \u2223 G \ud835\udc31 g {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\sim p({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]%\n{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}) bold_italic_x \u223c italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) .\nIn other words, this expectation is taken with respect to the instance distribution conditioned on it being selected for training at step g g {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g .\nAssuming the training set is sampled i.i.d.\u00a0and that its permutation is random (as discussed in \u00a7 2.1 ), then p \u2062 ( \ud835\udc31 \u2062 \u2223 G \u2062 ( \ud835\udc31 ) \u2062 = g ) = p \u2062 ( \ud835\udc31 ) p \ud835\udc31 \u2223 G \ud835\udc31 g p \ud835\udc31 p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ) .\nGiven these assumptions, eq. 3 would also be an average treatment effect (ATE), which would allow us to make causal claims about the entire population. Report issue for preceding element \u03c4 g , c = def \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; g ) \u2212 Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] superscript def subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 conditional subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc3a \ud835\udc99 \ud835\udc54 \\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}%\n\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\Big{[}Y_{{\\color[rgb]{%\n.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}\\Big{]} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (3)",
    "masked_text": "Expected counterfactual memorisation is the average causal effect of using instances for training at timestep gg{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g on the model\u2019s performance on these same instances at timestep cc{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c:777In econometrics, eq. 3 is called an average treatment effect on the treated (ATT), as it is defined in terms of an expectation over \ud835\udc31\u223cp\u2062(\ud835\udc31\u2062\u2223G\u2062(\ud835\udc31)\u2062=g)similar-to\ud835\udc31p\ud835\udc31\u2223G\ud835\udc31g{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\sim p({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]% {0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g})bold_italic_x \u223c italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ). In other words, this expectation is taken with respect to the instance distribution conditioned on it being selected for training at step gg{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g. Assuming the training set is sampled i.i.d. and that its permutation is random (as discussed in \u00a7 2.1), then p\u2062(\ud835\udc31\u2062\u2223G\u2062(\ud835\udc31)\u2062=g)=p\u2062(\ud835\udc31)p\ud835\udc31\u2223G\ud835\udc31gp\ud835\udc31p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ). Given these assumptions, eq. 3 would also be an average treatment effect (ATE), which would allow us to make causal claims about the entire population.Report issue for preceding element \u03c4g,c=def\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;g)\u2212Yc\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=g]superscriptdefsubscript\ud835\udf0f\ud835\udc54\ud835\udc50subscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54conditionalsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc3a\ud835\udc99\ud835\udc54\\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb% }{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}% \\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\Big{[}Y_{{\\color[rgb]{% .75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{% rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}\\Big{]}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (3)",
    "citations": []
  },
  "S3.p3": {
    "text": "Together, the \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT form a matrix which we term the model\u2019s memorisation profile ;\neach row therein is the memorisation path of a batch.\nMemorisation profiles and paths allow us to analyse a model\u2019s memorisation patterns across different treatment and checkpoint steps.\nNotably, there cannot be memorisation whenever c < g \ud835\udc50 \ud835\udc54 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%\n\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g , as the instances have not been seen by the model yet, so \u03c4 g , c = 0 subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 0 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}=0 italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = 0 in those cases.\nWe term \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT as instantaneous memorisation when c = g \ud835\udc50 \ud835\udc54 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%\n\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c = italic_g , as persistent memorisation when c > g \ud835\udc50 \ud835\udc54 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}>{%\n\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c > italic_g , and as residual memorisation when c = T \ud835\udc50 \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T . Report issue for preceding element",
    "masked_text": "Together, the \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT form a matrix which we term the model\u2019s memorisation profile; each row therein is the memorisation path of a batch. Memorisation profiles and paths allow us to analyse a model\u2019s memorisation patterns across different treatment and checkpoint steps. Notably, there cannot be memorisation whenever c<g\ud835\udc50\ud835\udc54{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{% \\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_c < italic_g, as the instances have not been seen by the model yet, so \u03c4g,c=0subscript\ud835\udf0f\ud835\udc54\ud835\udc500\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}=0italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = 0 in those cases. We term \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT as instantaneous memorisation when c=g\ud835\udc50\ud835\udc54{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={% \\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_c = italic_g, as persistent memorisation when c>g\ud835\udc50\ud835\udc54{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}>{% \\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_c > italic_g, and as residual memorisation when c=T\ud835\udc50\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}italic_c = italic_T.Report issue for preceding element",
    "citations": []
  },
  "S4.p1": {
    "text": "The practical implication of defining memorisation at a treatment level g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g is that we can only make causal claims for groups of instances treated at the same timestep (i.e., a batch \u212c g subscript \u212c \ud835\udc54 \\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ), rather than for individual instances.\nHowever, even though this approach simplifies the problem, estimating eq. 3 still poses major challenges as it contains a counterfactual.\nA simple decomposition makes this counterfactual explicit: Report issue for preceding element \u03c4 g , c = subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 absent \\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}= italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = (4) \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; g ) \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] \u23df 1 Report issue for preceding element \u2062 - \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] \u23df 2 Report issue for preceding element subscript \u23df subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 \u2223 \ud835\udc3a \ud835\udc99 \ud835\udc54 1 Report issue for preceding element subscript \u23df subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 \ud835\udc54 2 Report issue for preceding element \\displaystyle\\underbrace{\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.%\n91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\!\\big{[%\n}Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}%\nc}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g})\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}\\!}_{%\n\\leavevmode\\hbox to8.9pt{\\vbox to8.9pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 4.%\n45195pt\\lower-4.45195pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }%\n\\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}%\n\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }%\n\\pgfsys@setlinewidth{0.4pt}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{%\n\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%\n}{}{}{{}\\pgfsys@moveto{4.25195pt}{0.0pt}\\pgfsys@curveto{4.25195pt}{2.34831pt}{%\n2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\\pgfsys@curveto{-2.34831pt}{4.25195pt}{%\n-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\\pgfsys@curveto{-4.25195pt}{-2.34831%\npt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\\pgfsys@curveto{2.34831pt}{-4.25%\n195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto%\n{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1%\n.0}{-1.75pt}{-2.25555pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{%\nrgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }%\n\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{{1}}}\n}}\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}}\n\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope{{{}}}{}{}\\hss}%\n\\pgfsys@discardpath\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope\\hss}}%\n\\lxSVG@closescope\\endpgfpicture}}}\\mathop{-}\\underbrace{\\operatorname*{\\mathbb%\n{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}}\\!\\big{[}Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mathop{\\mid}G({\\color%\n[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}\\!}_{\\leavevmode\\hbox to8.9pt{\\vbox to8.9%\npt{\\pgfpicture\\makeatletter\\hbox{\\hskip 4.45195pt\\lower-4.45195pt\\hbox to0.0pt%\n{\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}%\n{0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{0.4pt}\\pgfsys@invoke{ }\\nullfont\\hbox\nto%\n0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{\n{{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%\n}{}{}{{}\\pgfsys@moveto{4.25195pt}{0.0pt}\\pgfsys@curveto{4.25195pt}{2.34831pt}{%\n2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\\pgfsys@curveto{-2.34831pt}{4.25195pt}{%\n-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\\pgfsys@curveto{-4.25195pt}{-2.34831%\npt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\\pgfsys@curveto{2.34831pt}{-4.25%\n195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto%\n{0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ }\n}{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1%\n.0}{-1.75pt}{-2.25555pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{%\nrgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }%\n\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{{2}}}\n}}\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}}\n\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}}\n}\n\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope{{{}}}{}{}\\hss}%\n\\pgfsys@discardpath\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope\\hss}}%\n\\lxSVG@closescope\\endpgfpicture}}} under\u23df start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - under\u23df start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Expectation 1 Report issue for preceding element can be directly estimated from the data because batch \u212c g subscript \u212c \ud835\udc54 \\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT contains a set of examples \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x for which G \u2062 ( \ud835\udc99 ) \u2062 = g \ud835\udc3a \ud835\udc99 \ud835\udc54 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g and, thus, we can invoke the consistency assumption to equate Y c \u2062 ( \ud835\udc99 ; g ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) with the observed outcome Y c \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) .\nLet us define the mean across instances in a batch as: Report issue for preceding element Y \u00af c \u2062 ( g ) = def 1 | \u212c g | \u2062 \u2211 \ud835\udc99 \u2208 \u212c g Y c \u2062 ( \ud835\udc99 ) superscript def subscript \u00af \ud835\udc4c \ud835\udc50 \ud835\udc54 1 subscript \u212c \ud835\udc54 subscript \ud835\udc99 subscript \u212c \ud835\udc54 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \\displaystyle\\overline{Y}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g})\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def%\n}}}{{=}}}\\frac{1}{|\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}|}\\sum_{{\\color[rgb]{0,0.88,0}\\definecolor[named%\n]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}\\in\\mathcal{B}_{\\color[rgb]%\n{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\\color[rgb]{%\n.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x \u2208 caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (5) We can thus use eq. 5 as an estimator for expectation 1 Report issue for preceding element . Expectation 2 Report issue for preceding element , however, is counterfactual: we cannot observe the potential outcome Y c \u2062 ( \ud835\udc99 ; \u221e ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) for instances treated at timestep g \u2062 \u2260 \u221e \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{%\n\\neq}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty} italic_g \u2260 \u221e .\nThe presence of counterfactual potential outcomes in causal estimands creates challenges for their estimation, being known as the fundamental problem of causal inference (Holland, 1986 ) .\nThe goal of causal methods is then to estimate these counterfactual outcomes from observed ones, using comparable groups of instances.\nThus far, we have defined our causal estimand.\nIn this section, we perform steps two and three of causal estimation ( \u00a7 2.2 ): we derive two statistical estimands for our causal estimand (identifying it under specific assumptions), and provide concrete estimators for them. Report issue for preceding element",
    "masked_text": "The practical implication of defining memorisation at a treatment level g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g is that we can only make causal claims for groups of instances treated at the same timestep (i.e., a batch \u212cgsubscript\u212c\ud835\udc54\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT), rather than for individual instances. However, even though this approach simplifies the problem, estimating eq. 3 still poses major challenges as it contains a counterfactual. A simple decomposition makes this counterfactual explicit:Report issue for preceding element \u03c4g,c=subscript\ud835\udf0f\ud835\udc54\ud835\udc50absent\\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb% }{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}=italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = (4) \ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;g)\u2062\u2223G\u2062(\ud835\udc99)\u2062=g]\u23df1Report issue for preceding element\u2062-\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2062\u2223G\u2062(\ud835\udc99)\u2062=g]\u23df2Report issue for preceding elementsubscript\u23dfsubscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54\u2223\ud835\udc3a\ud835\udc99\ud835\udc541Report issue for preceding elementsubscript\u23dfsubscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99\u2223\ud835\udc3a\ud835\udc99\ud835\udc542Report issue for preceding element\\displaystyle\\underbrace{\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.% 91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\!\\big{[% }Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}% c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}\\!}_{% \\leavevmode\\hbox to8.9pt{\\vbox to8.9pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 4.% 45195pt\\lower-4.45195pt\\hbox to0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }% \\definecolor{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}% \\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }% \\pgfsys@setlinewidth{0.4pt}\\pgfsys@invoke{ }\\nullfont\\hbox to0.0pt{% \\pgfsys@beginscope\\pgfsys@invoke{ }{ {{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{% }{}{}{{}\\pgfsys@moveto{4.25195pt}{0.0pt}\\pgfsys@curveto{4.25195pt}{2.34831pt}{% 2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\\pgfsys@curveto{-2.34831pt}{4.25195pt}{% -4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\\pgfsys@curveto{-4.25195pt}{-2.34831% pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\\pgfsys@curveto{2.34831pt}{-4.25% 195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto% {0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ } }{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1% .0}{-1.75pt}{-2.25555pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{% rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }% \\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{{1}}} }}\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}} \\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}} } \\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope{{{}}}{}{}\\hss}% \\pgfsys@discardpath\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope\\hss}}% \\lxSVG@closescope\\endpgfpicture}}}\\mathop{-}\\underbrace{\\operatorname*{\\mathbb% {E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}}\\!\\big{[}Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mathop{\\mid}G({\\color% [rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}\\!}_{\\leavevmode\\hbox to8.9pt{\\vbox to8.9% pt{\\pgfpicture\\makeatletter\\hbox{\\hskip 4.45195pt\\lower-4.45195pt\\hbox to0.0pt% {\\pgfsys@beginscope\\pgfsys@invoke{ }\\definecolor{pgfstrokecolor}{rgb}{0,0,0}% \\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }\\pgfsys@color@rgb@fill{0}{0}% {0}\\pgfsys@invoke{ }\\pgfsys@setlinewidth{0.4pt}\\pgfsys@invoke{ }\\nullfont\\hbox to% 0.0pt{\\pgfsys@beginscope\\pgfsys@invoke{ }{ {{}}\\hbox{\\hbox{{\\pgfsys@beginscope\\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{% }{}{}{{}\\pgfsys@moveto{4.25195pt}{0.0pt}\\pgfsys@curveto{4.25195pt}{2.34831pt}{% 2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\\pgfsys@curveto{-2.34831pt}{4.25195pt}{% -4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\\pgfsys@curveto{-4.25195pt}{-2.34831% pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\\pgfsys@curveto{2.34831pt}{-4.25% 195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\\pgfsys@closepath\\pgfsys@moveto% {0.0pt}{0.0pt}\\pgfsys@stroke\\pgfsys@invoke{ } }{{{{}}\\pgfsys@beginscope\\pgfsys@invoke{ }\\pgfsys@transformcm{1.0}{0.0}{0.0}{1% .0}{-1.75pt}{-2.25555pt}\\pgfsys@invoke{ }\\hbox{{\\definecolor{pgfstrokecolor}{% rgb}{0,0,0}\\pgfsys@color@rgb@stroke{0}{0}{0}\\pgfsys@invoke{ }% \\pgfsys@color@rgb@fill{0}{0}{0}\\pgfsys@invoke{ }\\hbox{{{2}}} }}\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}} \\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope}}} } \\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope{{{}}}{}{}\\hss}% \\pgfsys@discardpath\\pgfsys@invoke{\\lxSVG@closescope }\\pgfsys@endscope\\hss}}% \\lxSVG@closescope\\endpgfpicture}}}under\u23df start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - under\u23df start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Expectation 1Report issue for preceding element can be directly estimated from the data because batch \u212cgsubscript\u212c\ud835\udc54\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT contains a set of examples \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x for which G\u2062(\ud835\udc99)\u2062=g\ud835\udc3a\ud835\udc99\ud835\udc54G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}italic_G ( bold_italic_x ) = italic_g and, thus, we can invoke the consistency assumption to equate Yc\u2062(\ud835\udc99;g)subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) with the observed outcome Yc\u2062(\ud835\udc99)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ). Let us define the mean across instances in a batch as:Report issue for preceding element Y\u00afc\u2062(g)=def1|\u212cg|\u2062\u2211\ud835\udc99\u2208\u212cgYc\u2062(\ud835\udc99)superscriptdefsubscript\u00af\ud835\udc4c\ud835\udc50\ud835\udc541subscript\u212c\ud835\udc54subscript\ud835\udc99subscript\u212c\ud835\udc54subscript\ud835\udc4c\ud835\udc50\ud835\udc99\\displaystyle\\overline{Y}_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g})\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def% }}}{{=}}}\\frac{1}{|\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}|}\\sum_{{\\color[rgb]{0,0.88,0}\\definecolor[named% ]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}\\in\\mathcal{B}_{\\color[rgb]% {1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\\color[rgb]{% .75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x \u2208 caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (5) We can thus use eq. 5 as an estimator for expectation 1Report issue for preceding element. Expectation 2Report issue for preceding element, however, is counterfactual: we cannot observe the potential outcome Yc\u2062(\ud835\udc99;\u221e)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) for instances treated at timestep g\u2062\u2260\u221e\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{% \\neq}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}italic_g \u2260 \u221e. The presence of counterfactual potential outcomes in causal estimands creates challenges for their estimation, being known as the fundamental problem of causal inference [CITATION]. The goal of causal methods is then to estimate these counterfactual outcomes from observed ones, using comparable groups of instances. Thus far, we have defined our causal estimand. In this section, we perform steps two and three of causal estimation (\u00a7 2.2): we derive two statistical estimands for our causal estimand (identifying it under specific assumptions), and provide concrete estimators for them.Report issue for preceding element",
    "citations": [
      {
        "tag": "Holland (1986)",
        "title": "Statistics and causal inference.",
        "authors": "Paul\u00a0W. Holland. 1986.",
        "journal": "Journal of the American Statistical Association, 81(396):945\u2013960."
      }
    ]
  },
  "S4.SS1.p1": {
    "text": "Our first approach to estimate memorisation is straightforward and only requires the observed outcomes of a held-out validation set.\nHowever, it relies on a strong identification assumption. Report issue for preceding element",
    "masked_text": "Our first approach to estimate memorisation is straightforward and only requires the observed outcomes of a held-out validation set. However, it relies on a strong identification assumption.Report issue for preceding element",
    "citations": []
  },
  "Thmassumption1.p1": {
    "text": "Instances \ud835\udc31 \ud835\udc31 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x are independently and identically distributed, following p \u2062 ( \ud835\udc31 ) \ud835\udc5d \ud835\udc31 p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_p ( bold_italic_x ) , and are randomly assigned to treatment groups g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element",
    "masked_text": "Instances \ud835\udc31\ud835\udc31{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x are independently and identically distributed, following p\u2062(\ud835\udc31)\ud835\udc5d\ud835\udc31p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_p ( bold_italic_x ), and are randomly assigned to treatment groups g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g.Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.p2": {
    "text": "Under this assumption, we have that p \u2062 ( \ud835\udc99 \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ) = p \u2062 ( \ud835\udc99 \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = \u221e ) = p \u2062 ( \ud835\udc99 ) \ud835\udc5d \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 \ud835\udc54 \ud835\udc5d \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 \ud835\udc5d \ud835\udc99 p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\!=\\!p({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\!=\\!p({\\color[%\nrgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = \u221e ) = italic_p ( bold_italic_x ) .\nThus, the following statistical estimand identifies \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element \u03c4 g , c \ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8f = \ud835\udd3c \ud835\udc99 [ \\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}^{\\mathtt{diff}}=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}}\\big{[} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ Y c ( \ud835\udc99 ; g ) \u2223 G ( \ud835\udc99 ) = g ] \\displaystyle Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}\\big{]} italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (6) \u2212 \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = \u221e ] subscript \ud835\udd3c \ud835\udc99 conditional subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc3a \ud835\udc99 \\displaystyle-\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb%\n]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb%\n]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\mid\\mathop{G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}}\\big{]} - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 start_BIGOP italic_G ( bold_italic_x ) = \u221e end_BIGOP ] (See Lemma 1 in \u00a7 A.1 for a proof.)\nNote that, unlike eq. 3 , the second term in this estimand is not counterfactual: it is the expected observed outcome of validation instances, \u212c \u221e subscript \u212c \\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}\\infty}} caligraphic_B start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT .\nThis statistical estimand can then be estimated as follows. Report issue for preceding element",
    "masked_text": "Under this assumption, we have that p\u2062(\ud835\udc99\u2062\u2223G\u2062(\ud835\udc99)\u2062=g)=p\u2062(\ud835\udc99\u2062\u2223G\u2062(\ud835\udc99)\u2062=\u221e)=p\u2062(\ud835\udc99)\ud835\udc5d\ud835\udc99\u2223\ud835\udc3a\ud835\udc99\ud835\udc54\ud835\udc5d\ud835\udc99\u2223\ud835\udc3a\ud835\udc99\ud835\udc5d\ud835\udc99p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\!=\\!p({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\!=\\!p({\\color[% rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = \u221e ) = italic_p ( bold_italic_x ). Thus, the following statistical estimand identifies \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT:Report issue for preceding element \u03c4g,c\ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8f=\ud835\udd3c\ud835\udc99[\\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb% }{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}^{\\mathtt{diff}}=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}}\\big{[}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ Yc(\ud835\udc99;g)\u2223G(\ud835\udc99)=g]\\displaystyle Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{% rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (6) \u2212\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=\u221e]subscript\ud835\udd3c\ud835\udc99conditionalsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc3a\ud835\udc99\\displaystyle-\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb% ]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb% ]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\mid\\mathop{G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}}\\big{]}- blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 start_BIGOP italic_G ( bold_italic_x ) = \u221e end_BIGOP ] (See Lemma 1 in \u00a7 A.1 for a proof.) Note that, unlike eq. 3, the second term in this estimand is not counterfactual: it is the expected observed outcome of validation instances, \u212c\u221esubscript\u212c\\mathcal{B}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}\\infty}}caligraphic_B start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT. This statistical estimand can then be estimated as follows.Report issue for preceding element",
    "citations": []
  },
  "Thmestimator1.p1": {
    "text": "The difference estimator , defined as: Report issue for preceding element \u03c4 ^ g , c \ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8f = Y \u00af c \u2062 ( g ) \u2212 Y \u00af c \u2062 ( \u221e ) superscript subscript ^ \ud835\udf0f \ud835\udc54 \ud835\udc50 \ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8f subscript \u00af \ud835\udc4c \ud835\udc50 \ud835\udc54 subscript \u00af \ud835\udc4c \ud835\udc50 \\widehat{\\tau}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}^{\\mathtt{diff}}=\\overline{Y}_{{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\\overline{Y}_{{\\color[rgb]%\n{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]%\n{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}) over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ) (7) is an unbiased estimator of \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1 . Report issue for preceding element",
    "masked_text": "The difference estimator, defined as:Report issue for preceding element \u03c4^g,c\ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8f=Y\u00afc\u2062(g)\u2212Y\u00afc\u2062(\u221e)superscriptsubscript^\ud835\udf0f\ud835\udc54\ud835\udc50\ud835\ude8d\ud835\ude92\ud835\ude8f\ud835\ude8fsubscript\u00af\ud835\udc4c\ud835\udc50\ud835\udc54subscript\u00af\ud835\udc4c\ud835\udc50\\widehat{\\tau}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}^{\\mathtt{diff}}=\\overline{Y}_{{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\\overline{Y}_{{\\color[rgb]% {.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]% {1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ) (7) is an unbiased estimator of \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1.Report issue for preceding element",
    "citations": []
  },
  "S4.SS1.1.p1": {
    "text": "See Lemma 2 in \u00a7 A.1 for a proof.\n\u220e Report issue for preceding element",
    "masked_text": "See Lemma 2 in \u00a7 A.1 for a proof. \u220eReport issue for preceding element",
    "citations": []
  },
  "S4.SS1.p3": {
    "text": "Notably, Assump. 1 is satisfied by the training procedure we described in \u00a7 2.1 , and is commonly true in machine learning.\nHowever, it might not hold in general, as the train and validation distributions may not match exactly.\nFor example, NLP practitioners might deduplicate their training set but not validation (Biderman et\u00a0al., 2023b ) or might use challenge sets for validation (Kiela et\u00a0al., 2021 ) .\nMoreover, even when Assump. 1 holds, eq. 7 is low-variance only if we have large enough samples to compute Y \u00af c \u2062 ( g ) subscript \u00af \ud835\udc4c \ud835\udc50 \ud835\udc54 \\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g}) over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y \u00af c \u2062 ( \u221e ) subscript \u00af \ud835\udc4c \ud835\udc50 \\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}\\infty}) over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ) .\nUnfortunately, given the size of state-of-the-art LMs and their datasets, it can be expensive\u2014both in terms of computation and memory usage\u2014to extract the performance measures Y c \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) for all instances \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x and checkpoints c \ud835\udc50 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .\nEven with unlimited compute, Y \u00af c \u2062 ( g ) subscript \u00af \ud835\udc4c \ud835\udc50 \ud835\udc54 \\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g}) over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) can only be estimated using instances in \u212c g subscript \u212c \ud835\udc54 \\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , which lower bounds the variance of this estimator. Report issue for preceding element",
    "masked_text": "Notably, Assump. 1 is satisfied by the training procedure we described in \u00a7 2.1, and is commonly true in machine learning. However, it might not hold in general, as the train and validation distributions may not match exactly. For example, NLP practitioners might deduplicate their training set but not validation [CITATION] or might use challenge sets for validation [CITATION]. Moreover, even when Assump. 1 holds, eq. 7 is low-variance only if we have large enough samples to compute Y\u00afc\u2062(g)subscript\u00af\ud835\udc4c\ud835\udc50\ud835\udc54\\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g})over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y\u00afc\u2062(\u221e)subscript\u00af\ud835\udc4c\ud835\udc50\\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}\\infty})over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ). Unfortunately, given the size of state-of-the-art LMs and their datasets, it can be expensive\u2014both in terms of computation and memory usage\u2014to extract the performance measures Yc\u2062(\ud835\udc99)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) for all instances \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x and checkpoints c\ud835\udc50{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c. Even with unlimited compute, Y\u00afc\u2062(g)subscript\u00af\ud835\udc4c\ud835\udc50\ud835\udc54\\overline{Y}_{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g})over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) can only be estimated using instances in \u212cgsubscript\u212c\ud835\udc54\\mathcal{B}_{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, which lower bounds the variance of this estimator.Report issue for preceding element",
    "citations": [
      {
        "tag": "Biderman et\u00a0al. (2023b)",
        "title": "Pythia: A suite for analyzing large language models across training and scaling.",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der\u00a0Wal. 2023b.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, ICML\u201923."
      },
      {
        "tag": "Kiela et\u00a0al. (2021)",
        "title": "Dynabench: Rethinking benchmarking in NLP.",
        "authors": "Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021.",
        "journal": "InProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110\u20134124, Online. Association for Computational Linguistics."
      }
    ]
  },
  "S4.SS1.p4": {
    "text": "While the difference estimator is a first step towards a principled estimator of counterfactual memorisation, we can do better.\nIn the next section, we describe an estimator that has lower variance and requires weaker assumptions. Report issue for preceding element",
    "masked_text": "While the difference estimator is a first step towards a principled estimator of counterfactual memorisation, we can do better. In the next section, we describe an estimator that has lower variance and requires weaker assumptions.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.p1": {
    "text": "We now introduce another causal estimator based on the difference-in-differences (DiD) design.\nThe intuition behind DiD is to use the time dimension to help with identification; DiD identifies a causal estimand using the difference in the trends over time of the outcome on treated vs.\u00a0untreated instances.\nIn our specific setting, DiD relies on the assumption that changes in model performance over time would follow similar trends in different batches if they had not been used for training.\nWe formalise this assumption as follows. Report issue for preceding element",
    "masked_text": "We now introduce another causal estimator based on the difference-in-differences (DiD) design. The intuition behind DiD is to use the time dimension to help with identification; DiD identifies a causal estimand using the difference in the trends over time of the outcome on treated vs. untreated instances. In our specific setting, DiD relies on the assumption that changes in model performance over time would follow similar trends in different batches if they had not been used for training. We formalise this assumption as follows.Report issue for preceding element",
    "citations": []
  },
  "Thmassumption2.p1": {
    "text": "In the absence of training, the expected change in model performance across checkpoints would be the same regardless of treatment.\nThat is, for all c , c \u2032 \u2265 g \u2062 - 1 \ud835\udc50 superscript \ud835\udc50 \u2032 \ud835\udc54 1 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{%\n\\prime}}\\geq{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g}\\mathop{-}1 italic_c , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2265 italic_g - 1 : Report issue for preceding element \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2212 Y c \u2032 \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 conditional subscript \ud835\udc4c superscript \ud835\udc50 \u2032 \ud835\udc99 \ud835\udc3a \ud835\udc99 \ud835\udc54 \\displaystyle\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb%\n]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb%\n]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c^{\\prime}}}({\\color[rgb]{0,0.88,0}\\definecolor%\n[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%\n{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}\\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (8) = \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2212 Y c \u2032 \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = \u221e ] absent subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 conditional subscript \ud835\udc4c superscript \ud835\udc50 \u2032 \ud835\udc99 \ud835\udc3a \ud835\udc99 \\displaystyle\\qquad=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.%\n91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y%\n_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c^{\\prime}}}({\\color[rgb]{0,0.88,0}\\definecolor%\n[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%\n{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid\\mathop{G({\\color[%\nrgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}\\infty}}\\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 start_BIGOP italic_G ( bold_italic_x ) = \u221e end_BIGOP ]",
    "masked_text": "In the absence of training, the expected change in model performance across checkpoints would be the same regardless of treatment. That is, for all c,c\u2032\u2265g\u2062-1\ud835\udc50superscript\ud835\udc50\u2032\ud835\udc541{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{% \\prime}}\\geq{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}\\mathop{-}1italic_c , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2265 italic_g - 1:Report issue for preceding element \ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2212Yc\u2032\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=g]subscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99conditionalsubscript\ud835\udc4csuperscript\ud835\udc50\u2032\ud835\udc99\ud835\udc3a\ud835\udc99\ud835\udc54\\displaystyle\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb% ]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb% ]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c^{\\prime}}}({\\color[rgb]{0,0.88,0}\\definecolor% [named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}% {0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (8) =\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2212Yc\u2032\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=\u221e]absentsubscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99conditionalsubscript\ud835\udc4csuperscript\ud835\udc50\u2032\ud835\udc99\ud835\udc3a\ud835\udc99\\displaystyle\\qquad=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.% 91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y% _{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% }({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})-Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c^{\\prime}}}({\\color[rgb]{0,0.88,0}\\definecolor% [named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}% {0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid\\mathop{G({\\color[% rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}\\infty}}\\big{]}= blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 start_BIGOP italic_G ( bold_italic_x ) = \u221e end_BIGOP ]",
    "citations": []
  },
  "S4.SS2.p2": {
    "text": "We need a second assumption before we can apply the DiD design to our setting. Report issue for preceding element",
    "masked_text": "We need a second assumption before we can apply the DiD design to our setting.Report issue for preceding element",
    "citations": []
  },
  "Thmassumption3.p1": {
    "text": "Training has no effect before it happens.\nThat is, for all c < g \ud835\udc50 \ud835\udc54 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%\n\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g : Report issue for preceding element \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; g ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] subscript \ud835\udd3c \ud835\udc99 conditional subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 \ud835\udc3a \ud835\udc99 \ud835\udc54 \\displaystyle\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb%\n]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb%\n]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g})\\mid G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor%\n}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (9) = \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] absent subscript \ud835\udd3c \ud835\udc99 conditional subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc3a \ud835\udc99 \ud835\udc54 \\displaystyle\\qquad\\qquad\\quad=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}\\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ]",
    "masked_text": "Training has no effect before it happens. That is, for all c<g\ud835\udc50\ud835\udc54{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{% \\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_c < italic_g:Report issue for preceding element \ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;g)\u2223G\u2062(\ud835\udc99)\u2062=g]subscript\ud835\udd3c\ud835\udc99conditionalsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54\ud835\udc3a\ud835\udc99\ud835\udc54\\displaystyle\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb% ]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb% ]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})\\mid G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor% }{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (9) =\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=g]absentsubscript\ud835\udd3c\ud835\udc99conditionalsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc3a\ud835\udc99\ud835\udc54\\displaystyle\\qquad\\qquad\\quad=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}}\\big{[}Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}\\big{]}= blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = italic_g ]",
    "citations": []
  },
  "S4.SS2.p3": {
    "text": "Given these two assumptions, we can now follow Callaway and Sant\u2019Anna ( 2021 ) in identifying our target statistical estimand. 8 8 8 The DiD design was originally proposed for the case with only two treatment and checkpoint steps (i.e., g \u2208 { 1 , \u221e } \ud835\udc54 1 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{1,{%\n\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\} italic_g \u2208 { 1 , \u221e } and c \u2208 { 0 , 1 } \ud835\udc50 0 1 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n\\in\\{0,1\\} italic_c \u2208 { 0 , 1 } ). Previous work has shown the challenges of extending DiD to multiple timesteps, especially when allowing for heterogeneous treatment effects (Roth et\u00a0al., 2023 ) . Callaway and Sant\u2019Anna ( 2021 ) propose an extension which identifies eq. 3 while allowing for treatment effect heterogeneity across checkpoint and treatment steps. Combined, these assumptions allow us to rewrite expectation 2 Report issue for preceding element in eq. 4 as a function of potential outcomes that are observable: Y c \u2062 ( \ud835\udc99 ; \u221e ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) , Y g \u2212 1 \u2062 ( \ud835\udc99 ; \u221e ) subscript \ud835\udc4c \ud835\udc54 1 \ud835\udc99 Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%\ng}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill%\n{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}\\infty}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) given G \u2062 ( \ud835\udc99 ) \u2062 = \u221e \ud835\udc3a \ud835\udc99 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}\\infty} italic_G ( bold_italic_x ) = \u221e are observable on a held-out validation set, while Y g \u2212 1 \u2062 ( \ud835\udc99 ; g ) subscript \ud835\udc4c \ud835\udc54 1 \ud835\udc99 \ud835\udc54 Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%\ng}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill%\n{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) given G \u2062 ( \ud835\udc99 ) \u2062 = g \ud835\udc3a \ud835\udc99 \ud835\udc54 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g is observable on the training set.\nThe following statistical estimand thus identifies \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element \u03c4 g , c \ud835\ude8d\ud835\ude92\ud835\ude8d = \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; g ) \u2212 Y g \u2212 1 \u2062 ( \ud835\udc99 ; g ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ] superscript subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \ud835\ude8d\ud835\ude92\ud835\ude8d subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \ud835\udc54 conditional subscript \ud835\udc4c \ud835\udc54 1 \ud835\udc99 \ud835\udc54 \ud835\udc3a \ud835\udc99 \ud835\udc54 \\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}c}}^{\\mathtt{did}}=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0%\n}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0%\n.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}[Y_{{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%\n\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g})\\!-\\!Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\!\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}]\\! italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (10) \u2212 \ud835\udd3c \ud835\udc99 [ Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2212 Y g \u2212 1 \u2062 ( \ud835\udc99 ; \u221e ) \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = \u221e ] subscript \ud835\udd3c \ud835\udc99 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 conditional subscript \ud835\udc4c \ud835\udc54 1 \ud835\udc99 \ud835\udc3a \ud835\udc99 \\displaystyle\\qquad\\,\\,\\,-\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.%\n91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}[Y_{{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%\n\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\!-\\!Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]%\n{pgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\!\\mid G({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}\\infty}] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = \u221e ] (See Lemma 3 in \u00a7 A.2 for a proof.)\nThis leads to the following DiD estimator. Report issue for preceding element",
    "masked_text": "Given these two assumptions, we can now follow [CITATION] in identifying our target statistical estimand.888The DiD design was originally proposed for the case with only two treatment and checkpoint steps (i.e., g\u2208{1,\u221e}\ud835\udc541{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{1,{% \\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\}italic_g \u2208 { 1 , \u221e } and c\u2208{0,1}\ud835\udc5001{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% \\in\\{0,1\\}italic_c \u2208 { 0 , 1 }). Previous work has shown the challenges of extending DiD to multiple timesteps, especially when allowing for heterogeneous treatment effects [CITATION]. [CITATION] propose an extension which identifies eq. 3 while allowing for treatment effect heterogeneity across checkpoint and treatment steps. Combined, these assumptions allow us to rewrite expectation 2Report issue for preceding element in eq. 4 as a function of potential outcomes that are observable: Yc\u2062(\ud835\udc99;\u221e)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ), Yg\u22121\u2062(\ud835\udc99;\u221e)subscript\ud835\udc4c\ud835\udc541\ud835\udc99Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}% g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill% {0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}\\infty})italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) given G\u2062(\ud835\udc99)\u2062=\u221e\ud835\udc3a\ud835\udc99G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}\\infty}italic_G ( bold_italic_x ) = \u221e are observable on a held-out validation set, while Yg\u22121\u2062(\ud835\udc99;g)subscript\ud835\udc4c\ud835\udc541\ud835\udc99\ud835\udc54Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}% g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill% {0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g})italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) given G\u2062(\ud835\udc99)\u2062=g\ud835\udc3a\ud835\udc99\ud835\udc54G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}italic_G ( bold_italic_x ) = italic_g is observable on the training set. The following statistical estimand thus identifies \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT:Report issue for preceding element \u03c4g,c\ud835\ude8d\ud835\ude92\ud835\ude8d=\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;g)\u2212Yg\u22121\u2062(\ud835\udc99;g)\u2223G\u2062(\ud835\udc99)\u2062=g]superscriptsubscript\ud835\udf0f\ud835\udc54\ud835\udc50\ud835\ude8d\ud835\ude92\ud835\ude8dsubscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99\ud835\udc54conditionalsubscript\ud835\udc4c\ud835\udc541\ud835\udc99\ud835\udc54\ud835\udc3a\ud835\udc99\ud835\udc54\\displaystyle\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb% }{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}c}}^{\\mathtt{did}}=\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0% }\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0% .91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}[Y_{{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({% \\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g})\\!-\\!Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\!\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}]\\!italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) \u2223 italic_G ( bold_italic_x ) = italic_g ] (10) \u2212\ud835\udd3c\ud835\udc99[Yc\u2062(\ud835\udc99;\u221e)\u2212Yg\u22121\u2062(\ud835\udc99;\u221e)\u2223G\u2062(\ud835\udc99)\u2062=\u221e]subscript\ud835\udd3c\ud835\udc99subscript\ud835\udc4c\ud835\udc50\ud835\udc99conditionalsubscript\ud835\udc4c\ud835\udc541\ud835\udc99\ud835\udc3a\ud835\udc99\\displaystyle\\qquad\\,\\,\\,-\\operatorname*{\\mathbb{E}}_{{\\color[rgb]{0,0.88,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.% 91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}}[Y_{{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({% \\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\!-\\!Y_{\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]% {pgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\!\\mid G({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}\\infty}]- blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) = \u221e ] (See Lemma 3 in \u00a7 A.2 for a proof.) This leads to the following DiD estimator.Report issue for preceding element",
    "citations": [
      {
        "tag": "Roth et\u00a0al. (2023)",
        "title": "What\u2019s trending in difference-in-differences? A synthesis of the recent econometrics literature.",
        "authors": "Jonathan Roth, Pedro H.\u00a0C. Sant\u2019Anna, Alyssa Bilinski, and John Poe. 2023.",
        "journal": "Journal of Econometrics, 235(2):2218\u20132244."
      },
      {
        "tag": "Callaway and Sant\u2019Anna (2021)",
        "title": "Difference-in-Differences with multiple time periods.",
        "authors": "Brantly Callaway and Pedro H.\u00a0C. Sant\u2019Anna. 2021.",
        "journal": "Journal of Econometrics, 225(2):200\u2013230."
      }
    ]
  },
  "Thmestimator2.p1": {
    "text": "The difference-in-differences estimator (DiD), defined as: Report issue for preceding element \u03c4 ^ g , c \ud835\ude8d\ud835\ude92\ud835\ude8d = ( Y \u00af c \u2062 ( g ) \u2062 - Y \u00af g \u2212 1 \u2062 ( g ) ) \u23df diff in trained \u2212 ( Y \u00af c \u2062 ( \u221e ) \u2062 - Y \u00af g \u2212 1 \u2062 ( \u221e ) ) \u23df diff in untrained superscript subscript ^ \ud835\udf0f \ud835\udc54 \ud835\udc50 \ud835\ude8d\ud835\ude92\ud835\ude8d subscript \u23df subscript \u00af \ud835\udc4c \ud835\udc50 \ud835\udc54 subscript \u00af \ud835\udc4c \ud835\udc54 1 \ud835\udc54 diff in trained subscript \u23df subscript \u00af \ud835\udc4c \ud835\udc50 subscript \u00af \ud835\udc4c \ud835\udc54 1 diff in untrained \\displaystyle\\widehat{\\tau}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}c}}^{\\mathtt{did}}=\\underbrace{\\!\\Big{(}%\n\\overline{Y}_{\\!\\!{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb%\n}{1,.5,0}g})\\mathop{-}\\overline{Y}_{\\!\\!\\mathop{{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\Big{)}\\!}_{\\emph{diff in %\ntrained}}-\\underbrace{\\!\\Big{(}\\overline{Y}_{\\!\\!{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mathop{-}\\overline{Y}%\n_{\\!\\!\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%\ng}-1}}}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%\n\\infty})\\Big{)}\\!}_{\\emph{diff in untrained}} over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = under\u23df start_ARG ( over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( italic_g ) ) end_ARG start_POSTSUBSCRIPT diff in trained end_POSTSUBSCRIPT - under\u23df start_ARG ( over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( \u221e ) ) end_ARG start_POSTSUBSCRIPT diff in untrained end_POSTSUBSCRIPT (11) is an unbiased estimator of \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 . Report issue for preceding element",
    "masked_text": "The difference-in-differences estimator (DiD), defined as:Report issue for preceding element \u03c4^g,c\ud835\ude8d\ud835\ude92\ud835\ude8d=(Y\u00afc\u2062(g)\u2062-Y\u00afg\u22121\u2062(g))\u23dfdiff in trained\u2212(Y\u00afc\u2062(\u221e)\u2062-Y\u00afg\u22121\u2062(\u221e))\u23dfdiff in untrainedsuperscriptsubscript^\ud835\udf0f\ud835\udc54\ud835\udc50\ud835\ude8d\ud835\ude92\ud835\ude8dsubscript\u23dfsubscript\u00af\ud835\udc4c\ud835\udc50\ud835\udc54subscript\u00af\ud835\udc4c\ud835\udc541\ud835\udc54diff in trainedsubscript\u23dfsubscript\u00af\ud835\udc4c\ud835\udc50subscript\u00af\ud835\udc4c\ud835\udc541diff in untrained\\displaystyle\\widehat{\\tau}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{1,.5,0}g},{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\\mathtt{did}}=\\underbrace{\\!\\Big{(}% \\overline{Y}_{\\!\\!{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{% rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb% }{1,.5,0}g})\\mathop{-}\\overline{Y}_{\\!\\!\\mathop{{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\\Big{)}\\!}_{\\emph{diff in % trained}}-\\underbrace{\\!\\Big{(}\\overline{Y}_{\\!\\!{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})\\mathop{-}\\overline{Y}% _{\\!\\!\\mathop{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}% g}-1}}}({\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}% \\infty})\\Big{)}\\!}_{\\emph{diff in untrained}}over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = under\u23df start_ARG ( over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( italic_g ) ) end_ARG start_POSTSUBSCRIPT diff in trained end_POSTSUBSCRIPT - under\u23df start_ARG ( over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( \u221e ) - over\u00af start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( \u221e ) ) end_ARG start_POSTSUBSCRIPT diff in untrained end_POSTSUBSCRIPT (11) is an unbiased estimator of \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.1.p1": {
    "text": "See Lemma 4 in \u00a7 A.2 for a proof.\n\u220e Report issue for preceding element",
    "masked_text": "See Lemma 4 in \u00a7 A.2 for a proof. \u220eReport issue for preceding element",
    "citations": []
  },
  "S4.SS2.p4": {
    "text": "The DiD estimator depends on weaker assumptions and has a lower variance (under mild assumptions, see \u00a7 A.3 ) than the difference estimator in eq. 7 .\nSpecifically, the parallel trends assumption ( Assump. 2 ) is strictly weaker than the i.i.d.\u00a0one ( Assump. 1 ): if p \u2062 ( \ud835\udc99 \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = g ) = p \u2062 ( \ud835\udc99 \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 = \u221e ) \ud835\udc5d \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 \ud835\udc54 \ud835\udc5d \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}) italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = \u221e ) , then it is trivial that performances should present parallel trends.\nMoreover, Assump. 2 only requires that the training and validation sets follow similar trends on average, which might be true even in the case of, e.g., challenge validation sets or deduplicated training data.\nTherefore, the assumptions underpinning DiD are more likely to hold in practice and we will use it to estimate memorisation here. Report issue for preceding element",
    "masked_text": "The DiD estimator depends on weaker assumptions and has a lower variance (under mild assumptions, see \u00a7 A.3) than the difference estimator in eq. 7. Specifically, the parallel trends assumption (Assump. 2) is strictly weaker than the i.i.d. one (Assump. 1): if p\u2062(\ud835\udc99\u2062\u2223G\u2062(\ud835\udc99)\u2062=g)=p\u2062(\ud835\udc99\u2062\u2223G\u2062(\ud835\udc99)\u2062=\u221e)\ud835\udc5d\ud835\udc99\u2223\ud835\udc3a\ud835\udc99\ud835\udc54\ud835\udc5d\ud835\udc99\u2223\ud835\udc3a\ud835\udc99p({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{=}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x \u2223 italic_G ( bold_italic_x ) = \u221e ), then it is trivial that performances should present parallel trends. Moreover, Assump. 2 only requires that the training and validation sets follow similar trends on average, which might be true even in the case of, e.g., challenge validation sets or deduplicated training data. Therefore, the assumptions underpinning DiD are more likely to hold in practice and we will use it to estimate memorisation here.Report issue for preceding element",
    "citations": []
  },
  "S4.SS2.p5": {
    "text": "In practice, the difference-in-differences estimation procedure includes two steps.\nFirst, we compute the model\u2019s performance on a subset of analysed instances\u2014i.e., samples from the training and validation sets\u2014using the available checkpoints; thus forming a panel of observed outcomes, as it is usually termed in econometrics.\nThen, we use this panel to compute the estimates in eq. 11 . Report issue for preceding element",
    "masked_text": "In practice, the difference-in-differences estimation procedure includes two steps. First, we compute the model\u2019s performance on a subset of analysed instances\u2014i.e., samples from the training and validation sets\u2014using the available checkpoints; thus forming a panel of observed outcomes, as it is usually termed in econometrics. Then, we use this panel to compute the estimates in eq. 11.Report issue for preceding element",
    "citations": []
  },
  "S5.p1": {
    "text": "Memorisation has recently received much attention (Arpit et\u00a0al., 2017 ; Carlini et\u00a0al., 2019 , 2021 ; Anagnostidis et\u00a0al., 2023 , inter alia ) . 9 9 9 See Hartmann et\u00a0al. ( 2023 ) or Ishihara ( 2023 ) for surveys. Prior work has studied how model architecture and training choices influence memorisation (Tirumala et\u00a0al., 2022 ; Kandpal et\u00a0al., 2022 ; Lee et\u00a0al., 2022 ; Biderman et\u00a0al., 2023a ) , and where memorised instances are stored within a model (Maini et\u00a0al., 2023 ; Stoehr et\u00a0al., 2024 ) .\nIn this section, we use our framework to discuss three prior notions of memorisation which we consider most relevant to our paper: previous operationalisations of counterfactual memorisation (e.g., Feldman, 2020 ) , influence functions (Zheng and Jiang, 2022 ) , and extractable memorisation (Carlini et\u00a0al., 2023 ) . Report issue for preceding element",
    "masked_text": "Memorisation has recently received much attention [CITATION].999See [CITATION] or [CITATION] for surveys. Prior work has studied how model architecture and training choices influence memorisation [CITATION], and where memorised instances are stored within a model [CITATION]. In this section, we use our framework to discuss three prior notions of memorisation which we consider most relevant to our paper: previous operationalisations of counterfactual memorisation [CITATION], influence functions [CITATION], and extractable memorisation [CITATION].Report issue for preceding element",
    "citations": [
      {
        "tag": "Carlini et\u00a0al. (2021)",
        "title": "Extracting training data from large language models.",
        "authors": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.",
        "journal": "In30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association."
      },
      {
        "tag": "Lee et\u00a0al. (2022)",
        "title": "Deduplicating training data makes language models better.",
        "authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Anagnostidis et\u00a0al. (2023)",
        "title": "The curious case of benign memorization.",
        "authors": "Sotiris Anagnostidis, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. 2023.",
        "journal": "InThe Eleventh International Conference on Learning Representations."
      },
      {
        "tag": "Feldman (2020)",
        "title": "Does learning require memorization? A short tale about a long tail.",
        "authors": "Vitaly Feldman. 2020.",
        "journal": "InProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, pages 954\u2013959, New York, NY, USA. Association for Computing Machinery."
      },
      {
        "tag": "Arpit et\u00a0al. (2017)",
        "title": "A closer look at memorization in deep networks.",
        "authors": "Devansh Arpit, Stanis\u0142aw Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder\u00a0S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017.",
        "journal": "InProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 233\u2013242. JMLR.org."
      },
      {
        "tag": "Zheng and Jiang (2022)",
        "title": "An empirical study of memorization in NLP.",
        "authors": "Xiaosen Zheng and Jing Jiang. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6265\u20136278, Dublin, Ireland. Association for Computational Linguistics."
      },
      {
        "tag": "Biderman et\u00a0al. (2023a)",
        "title": "Emergent and predictable memorization in large language models.",
        "authors": "Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023a.",
        "journal": "Advances in Neural Information Processing Systems, 36:28072\u201328090."
      },
      {
        "tag": "Carlini et\u00a0al. (2023)",
        "title": "Quantifying memorization across neural language models.",
        "authors": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023.",
        "journal": "InThe Eleventh International Conference on Learning Representations."
      },
      {
        "tag": "Stoehr et\u00a0al. (2024)",
        "title": "Localizing paragraph memorization in language models.",
        "authors": "Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. 2024.",
        "journal": "arxiv preprint 2403.19851."
      },
      {
        "tag": "Maini et\u00a0al. (2023)",
        "title": "Can neural network memorization be localized?",
        "authors": "Pratyush Maini, Michael\u00a0C. Mozer, Hanie Sedghi, Zachary\u00a0C. Lipton, J.\u00a0Zico Kolter, and Chiyuan Zhang. 2023.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org."
      },
      {
        "tag": "Ishihara (2023)",
        "title": "Training data extraction from pre-trained language models: A survey.",
        "authors": "Shotaro Ishihara. 2023.",
        "journal": "InProceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 260\u2013275, Toronto, Canada. Association for Computational Linguistics."
      },
      {
        "tag": "Tirumala et\u00a0al. (2022)",
        "title": "Memorization without overfitting: Analyzing the training dynamics of large language models.",
        "authors": "Kushal Tirumala, Aram\u00a0H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.",
        "journal": "InAdvances in Neural Information Processing Systems."
      },
      {
        "tag": "Carlini et\u00a0al. (2019)",
        "title": "The Secret Sharer: Evaluating and testing unintended memorization in neural networks.",
        "authors": "Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song. 2019.",
        "journal": "InProceedings of the 28th USENIX Conference on Security Symposium, SEC\u201919, pages 267\u2013\u2013284, USA. USENIX Association."
      },
      {
        "tag": "Kandpal et\u00a0al. (2022)",
        "title": "Deduplicating training data mitigates privacy risks in language models.",
        "authors": "Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.",
        "journal": "InInternational Conference on Machine Learning, pages 10697\u201310707. PMLR."
      },
      {
        "tag": "Hartmann et\u00a0al. (2023)",
        "title": "SoK: Memorization in general-purpose large language models.",
        "authors": "Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, and Robert West. 2023.",
        "journal": "arXiv preprint 2310.18362."
      }
    ]
  },
  "S5.SS1.p1": {
    "text": "As mentioned before, estimating an instance\u2019s memorisation \u03c4 \ud835\udc99 , c subscript \ud835\udf0f \ud835\udc99 \ud835\udc50 \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT is non-trivial due to the counterfactual component in its definition.\nWe avoid this issue by estimating expected memorisation \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT instead.\nPrior work (Feldman, 2020 ; Feldman and Zhang, 2020 ; Zhang et\u00a0al., 2023 ; Lukasik et\u00a0al., 2024 ) takes a different approach, comparing the performance of models trained with and without that instance.\nIn doing so, they average performance across training runs, measuring what we term architectural counterfactual memorisation . Report issue for preceding element",
    "masked_text": "As mentioned before, estimating an instance\u2019s memorisation \u03c4\ud835\udc99,csubscript\ud835\udf0f\ud835\udc99\ud835\udc50\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}% {rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT is non-trivial due to the counterfactual component in its definition. We avoid this issue by estimating expected memorisation \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT instead. Prior work [CITATION] takes a different approach, comparing the performance of models trained with and without that instance. In doing so, they average performance across training runs, measuring what we term architectural counterfactual memorisation.Report issue for preceding element",
    "citations": [
      {
        "tag": "Zhang et\u00a0al. (2023)",
        "title": "Counterfactual memorization in neural language models.",
        "authors": "Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas Carlini. 2023.",
        "journal": "InThirty-seventh Conference on Neural Information Processing Systems."
      },
      {
        "tag": "Feldman (2020)",
        "title": "Does learning require memorization? A short tale about a long tail.",
        "authors": "Vitaly Feldman. 2020.",
        "journal": "InProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, pages 954\u2013959, New York, NY, USA. Association for Computing Machinery."
      },
      {
        "tag": "Feldman and Zhang (2020)",
        "title": "What neural networks memorize and why: Discovering the long tail via influence estimation.",
        "authors": "Vitaly Feldman and Chiyuan Zhang. 2020.",
        "journal": "InAdvances in Neural Information Processing Systems, volume\u00a033, pages 2881\u20132891. Curran Associates, Inc."
      },
      {
        "tag": "Lukasik et\u00a0al. (2024)",
        "title": "What do larger image classifiers memorise?",
        "authors": "Michal Lukasik, Vaishnavh Nagarajan, Ankit\u00a0Singh Rawat, Aditya\u00a0Krishna Menon, and Sanjiv Kumar. 2024.",
        "journal": "Transactions on Machine Learning Research."
      }
    ]
  },
  "S5.SS1.p2": {
    "text": "Formally, let \ud835\udf4d \ud835\udf4d \\bm{\\psi} bold_italic_\u03c8 be a vector of variables responsible for training variance.\nThis includes, e.g., the data permutation induced by \u03c3 \ud835\udf0e \\sigma italic_\u03c3 and the initial model parameters \ud835\udf3d 0 subscript \ud835\udf3d 0 \\bm{\\theta}_{0} bold_italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .\nBy defining a distribution p \u2062 ( \ud835\udf4d ) \ud835\udc5d \ud835\udf4d p(\\bm{\\psi}) italic_p ( bold_italic_\u03c8 ) over these variables, architectural memorisation can be defined as follows. Report issue for preceding element",
    "masked_text": "Formally, let \ud835\udf4d\ud835\udf4d\\bm{\\psi}bold_italic_\u03c8 be a vector of variables responsible for training variance. This includes, e.g., the data permutation induced by \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 and the initial model parameters \ud835\udf3d0subscript\ud835\udf3d0\\bm{\\theta}_{0}bold_italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. By defining a distribution p\u2062(\ud835\udf4d)\ud835\udc5d\ud835\udf4dp(\\bm{\\psi})italic_p ( bold_italic_\u03c8 ) over these variables, architectural memorisation can be defined as follows.Report issue for preceding element",
    "citations": []
  },
  "Thmdefinition4.p1": {
    "text": "Architectural counterfactual memorisation is the counterfactual memorisation \u03c4 \ud835\udc31 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t subscript \u03c4 \ud835\udc31 \\scaleto T 4 p t \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT when marginalising over training variables \ud835\uded9 \ud835\uded9 \\bm{\\psi} bold_italic_\u03c8 : Report issue for preceding element \u03c4 \ud835\udc99 , \\scaleto \u2062 p \u2062 ( \ud835\udf4d ) \u2062 6 \u2062 p \u2062 t subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc5d \ud835\udf4d 6 \ud835\udc5d \ud835\udc61 \\displaystyle\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT = def \ud835\udd3c \ud835\udf4d [ \u03c4 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 \u2260 \u221e ] superscript def absent subscript \ud835\udd3c \ud835\udf4d subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \u2223 \ud835\udc3a \ud835\udc99 \\displaystyle\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}%\n\\operatorname*{\\mathbb{E}}_{\\bm{\\psi}}\\left[\\tau_{{\\color[rgb]{0,0.88,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.%\n91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto%\n{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}%\n{4pt}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}%\n{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{\\neq}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\right] start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_\u03c8 end_POSTSUBSCRIPT [ italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT \u2223 italic_G ( bold_italic_x ) \u2260 \u221e ] (12) = \ud835\udd3c \ud835\udf4d [ Y \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ; G \u2062 ( \ud835\udc99 ) ) \u2062 - Y \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ; \u221e ) \u2062 \u2223 G \u2062 ( \ud835\udc99 ) \u2062 \u2260 \u221e ] absent subscript \ud835\udd3c \ud835\udf4d subscript \ud835\udc4c \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \ud835\udc3a \ud835\udc99 subscript \ud835\udc4c \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \u2223 \ud835\udc3a \ud835\udc99 \\displaystyle=\\operatorname*{\\mathbb{E}}_{\\bm{\\psi}}\\left[Y_{\\scaleto{{\\color[%\nrgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%\n\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor%\n}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}))\\mathop{-}Y_{\\scaleto{{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4%\npt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{\\neq}{\\color[rgb]{%\n1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_\u03c8 end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) \u2260 \u221e ] where G \u2062 ( \ud835\udc31 ) \ud835\udc3a \ud835\udc31 G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_G ( bold_italic_x ) in the first potential outcome depends on which batch the shuffling function \u03c3 \ud835\udf0e \\sigma italic_\u03c3 puts \ud835\udc31 \ud835\udc31 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x in. Report issue for preceding element",
    "masked_text": "Architectural counterfactual memorisation is the counterfactual memorisation \u03c4\ud835\udc31,\\scaleto\u2062T\u20624\u2062p\u2062tsubscript\u03c4\ud835\udc31\\scaletoT4pt\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT when marginalising over training variables \ud835\uded9\ud835\uded9\\bm{\\psi}bold_italic_\u03c8:Report issue for preceding element \u03c4\ud835\udc99,\\scaleto\u2062p\u2062(\ud835\udf4d)\u20626\u2062p\u2062tsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc5d\ud835\udf4d6\ud835\udc5d\ud835\udc61\\displaystyle\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT =def\ud835\udd3c\ud835\udf4d[\u03c4\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\u2062\u2223G\u2062(\ud835\udc99)\u2062\u2260\u221e]superscriptdefabsentsubscript\ud835\udd3c\ud835\udf4dsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\u2223\ud835\udc3a\ud835\udc99\\displaystyle\\mathrel{\\stackrel{{\\scriptstyle\\textnormal{def}}}{{=}}}% \\operatorname*{\\mathbb{E}}_{\\bm{\\psi}}\\left[\\tau_{{\\color[rgb]{0,0.88,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.% 91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto% {{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}% {4pt}}\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}% {rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{\\neq}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\right]start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_\u03c8 end_POSTSUBSCRIPT [ italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT \u2223 italic_G ( bold_italic_x ) \u2260 \u221e ] (12) =\ud835\udd3c\ud835\udf4d[Y\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99;G\u2062(\ud835\udc99))\u2062-Y\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99;\u221e)\u2062\u2223G\u2062(\ud835\udc99)\u2062\u2260\u221e]absentsubscript\ud835\udd3c\ud835\udf4dsubscript\ud835\udc4c\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99\ud835\udc3a\ud835\udc99subscript\ud835\udc4c\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99\u2223\ud835\udc3a\ud835\udc99\\displaystyle=\\operatorname*{\\mathbb{E}}_{\\bm{\\psi}}\\left[Y_{\\scaleto{{\\color[% rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({% \\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor% }{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}))\\mathop{-}Y_{\\scaleto{{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4% pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\mathop{\\mid}G({\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathop{\\neq}{\\color[rgb]{% 1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}\\right]= blackboard_E start_POSTSUBSCRIPT bold_italic_\u03c8 end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) \u2223 italic_G ( bold_italic_x ) \u2260 \u221e ] where G\u2062(\ud835\udc31)\ud835\udc3a\ud835\udc31G({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_G ( bold_italic_x ) in the first potential outcome depends on which batch the shuffling function \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 puts \ud835\udc31\ud835\udc31{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x in.Report issue for preceding element",
    "citations": []
  },
  "S5.SS1.p3": {
    "text": "Prior work has proposed a number of methods to estimate this value (Bachmann et\u00a0al., 2022 ; Lin et\u00a0al., 2022 ; Ilyas et\u00a0al., 2022 ; Park et\u00a0al., 2023 ) .\nThe simplest of these is to train several models while including or not \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x in the training set; these models are then used to approximate the expectation above.\nWe describe a statistical estimand and estimator for \u03c4 \ud835\udc99 , \\scaleto \u2062 p \u2062 ( \ud835\udf4d ) \u2062 6 \u2062 p \u2062 t subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc5d \ud835\udf4d 6 \ud835\udc5d \ud835\udc61 \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT in \u00a7 B.1 , discussing the assumptions needed by this approach. Report issue for preceding element",
    "masked_text": "Prior work has proposed a number of methods to estimate this value [CITATION]. The simplest of these is to train several models while including or not \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x in the training set; these models are then used to approximate the expectation above. We describe a statistical estimand and estimator for \u03c4\ud835\udc99,\\scaleto\u2062p\u2062(\ud835\udf4d)\u20626\u2062p\u2062tsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc5d\ud835\udf4d6\ud835\udc5d\ud835\udc61\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT in \u00a7 B.1, discussing the assumptions needed by this approach.Report issue for preceding element",
    "citations": [
      {
        "tag": "Bachmann et\u00a0al. (2022)",
        "title": "Generalization through the lens of leave-one-out error.",
        "authors": "Gregor Bachmann, Thomas Hofmann, and Aurelien Lucchi. 2022.",
        "journal": "InInternational Conference on Learning Representations."
      },
      {
        "tag": "Park et\u00a0al. (2023)",
        "title": "TRAK: Attributing model behavior at scale.",
        "authors": "Sung\u00a0Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, pages 27074\u201327113. PMLR."
      },
      {
        "tag": "Ilyas et\u00a0al. (2022)",
        "title": "Datamodels: Understanding predictions with data and data with predictions.",
        "authors": "Andrew Ilyas, Sung\u00a0Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022.",
        "journal": "InProceedings of the 39th International Conference on Machine Learning, pages 9525\u20139587. PMLR."
      },
      {
        "tag": "Lin et\u00a0al. (2022)",
        "title": "Measuring the effect of training data on deep learning predictions via randomized experiments.",
        "authors": "Jinkun Lin, Anqi Zhang, Mathias L\u00e9cuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. 2022.",
        "journal": "InProceedings of the 39th International Conference on Machine Learning, pages 13468\u201313504. PMLR."
      }
    ]
  },
  "S5.SS1.p4": {
    "text": "Notably, this operationalisation has the advantage of estimating memorisation at the instance level.\nHowever, it also has drawbacks\u2014beyond just being computationally expensive to estimate.\nThese become apparent upon closer inspection of the definition of \u03c4 \ud835\udc99 , \\scaleto \u2062 p \u2062 ( \ud835\udf4d ) \u2062 6 \u2062 p \u2062 t subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc5d \ud835\udf4d 6 \ud835\udc5d \ud835\udc61 \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT .\nFirst, it does not provide insights into the effect of the checkpoint step or treatment step on memorisation;\nthis is because T \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T is hard-coded into \u03c4 \ud835\udc99 , \\scaleto \u2062 p \u2062 ( \ud835\udf4d ) \u2062 6 \u2062 p \u2062 t subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc5d \ud835\udf4d 6 \ud835\udc5d \ud835\udc61 \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT \u2019s definition and because it marginalises over permutations of the data.\nWhile it is trivial to generalise this definition to other checkpoint steps c \ud835\udc50 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c or to specific g \ud835\udc54 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , prior work has mainly focused on these choices, overlooking the impact of training dynamics on memorisation.\nFurther, and perhaps more importantly, marginalising over p \u2062 ( \ud835\udf4d ) \ud835\udc5d \ud835\udf4d p(\\bm{\\psi}) italic_p ( bold_italic_\u03c8 ) means that this metric quantifies memorisation for a model architecture, rather than for a specific model. Report issue for preceding element",
    "masked_text": "Notably, this operationalisation has the advantage of estimating memorisation at the instance level. However, it also has drawbacks\u2014beyond just being computationally expensive to estimate. These become apparent upon closer inspection of the definition of \u03c4\ud835\udc99,\\scaleto\u2062p\u2062(\ud835\udf4d)\u20626\u2062p\u2062tsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc5d\ud835\udf4d6\ud835\udc5d\ud835\udc61\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT. First, it does not provide insights into the effect of the checkpoint step or treatment step on memorisation; this is because T\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}italic_T is hard-coded into \u03c4\ud835\udc99,\\scaleto\u2062p\u2062(\ud835\udf4d)\u20626\u2062p\u2062tsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc5d\ud835\udf4d6\ud835\udc5d\ud835\udc61\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{p(\\bm{\\psi})}{6pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_\u03c8 ) 6 italic_p italic_t end_POSTSUBSCRIPT\u2019s definition and because it marginalises over permutations of the data. While it is trivial to generalise this definition to other checkpoint steps c\ud835\udc50{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c or to specific g\ud835\udc54{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_g, prior work has mainly focused on these choices, overlooking the impact of training dynamics on memorisation. Further, and perhaps more importantly, marginalising over p\u2062(\ud835\udf4d)\ud835\udc5d\ud835\udf4dp(\\bm{\\psi})italic_p ( bold_italic_\u03c8 ) means that this metric quantifies memorisation for a model architecture, rather than for a specific model.Report issue for preceding element",
    "citations": []
  },
  "S5.SS2.p1": {
    "text": "Influence functions (Hampel, 1974 ; Cook and Weisberg, 1980 ) estimate\u2014without re-training a model\u2014how its parameters would change if an instance \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x was removed from the training set.\nSpecifically, the new set of parameters can be approximated as follows (Koh and Liang, 2017 ) : Report issue for preceding element \ud835\udf3d \u2212 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2248 \ud835\udf3d \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t + 1 / N \u2062 H \ud835\udf3d \u2212 1 \u2062 \u2207 \ud835\udf3d \u2112 \u2062 ( \ud835\udf3d \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t , \ud835\udc99 ) subscript \ud835\udf3d \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 subscript \ud835\udf3d \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 1 \ud835\udc41 subscript superscript H 1 \ud835\udf3d subscript \u2207 \ud835\udf3d \u2112 subscript \ud835\udf3d \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \\displaystyle\\bm{\\theta}_{-{\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{%\n.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}\\approx%\n\\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}+\\nicefrac{{1}}{{N}}\\,\\mathrm{H}^{%\n\\mathop{-1}}_{\\bm{\\theta}}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta}_{%\n\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}T}}{4pt}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) bold_italic_\u03b8 start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT \u2248 bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT + / start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT \u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , bold_italic_x ) (13) where H \ud835\udf3d subscript H \ud835\udf3d \\mathrm{H}_{\\bm{\\theta}} roman_H start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT is the hessian of \u2112 \u2112 \\mathcal{L} caligraphic_L evaluated at \ud835\udf3d \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t subscript \ud835\udf3d \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .\nThis approximation is based on a first-order Taylor expansion of the training objective around \ud835\udf3d \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t subscript \ud835\udf3d \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , and should lead to small errors under the following assumptions:\n(i) the loss function is strictly convex in \ud835\udf3d \ud835\udf3d \\bm{\\theta} bold_italic_\u03b8 , (ii) H \ud835\udf3d subscript H \ud835\udf3d \\mathrm{H}_{\\bm{\\theta}} roman_H start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (i.e., the gradient is zero).\nGiven these assumptions, influence functions can be used to efficiently estimate the counterfactual term Y \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ; \u221e ) subscript \ud835\udc4c \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) in the definition of \u03c4 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t subscript \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .\nSpecifically, we can measure the model\u2019s performance using the updated parameters, Y \u2212 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) = def \u03b3 \u2062 ( \ud835\udf3d \u2212 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t ; \ud835\udc99 ) superscript def subscript \ud835\udc4c \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \ud835\udefe subscript \ud835\udf3d \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 Y_{-{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathrel{\\stackrel{{%\n\\scriptstyle\\textnormal{def}}}{{=}}}\\gamma(\\bm{\\theta}_{-{\\color[rgb]{0,0.88,0%\n}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0%\n.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},%\n\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}T}}{4pt}};{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ) , and equate Y \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ; \u221e ) = Y \u2212 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 subscript \ud835\udc4c \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})=Y_{-{\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) .\nThe influence function estimator of memorisation can then be written as: Report issue for preceding element \u03c4 ^ \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \ud835\ude92\ud835\ude97\ud835\ude8f\ud835\ude95 = Y \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) \u2212 Y \u2212 \ud835\udc99 , \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) superscript subscript ^ \ud835\udf0f \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\ude92\ud835\ude97\ud835\ude8f\ud835\ude95 subscript \ud835\udc4c \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 subscript \ud835\udc4c \ud835\udc99 \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \\displaystyle\\hat{\\tau}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{%\n.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{%\n\\mathtt{infl}}=Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})-Y_{-{\\color[rgb]{%\n0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%\n0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}}) over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (14) We formalise this estimator and its statistical estimand in \u00a7 B.2 .\nNotably, Zheng and Jiang ( 2022 ) use a similar approach to estimate memorisation in a classification setting. Report issue for preceding element",
    "masked_text": "Influence functions [CITATION] estimate\u2014without re-training a model\u2014how its parameters would change if an instance \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x was removed from the training set. Specifically, the new set of parameters can be approximated as follows [CITATION]:Report issue for preceding element \ud835\udf3d\u2212\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\u2248\ud835\udf3d\\scaleto\u2062T\u20624\u2062p\u2062t+1/N\u2062H\ud835\udf3d\u22121\u2062\u2207\ud835\udf3d\u2112\u2062(\ud835\udf3d\\scaleto\u2062T\u20624\u2062p\u2062t,\ud835\udc99)subscript\ud835\udf3d\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61subscript\ud835\udf3d\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc611\ud835\udc41subscriptsuperscriptH1\ud835\udf3dsubscript\u2207\ud835\udf3d\u2112subscript\ud835\udf3d\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99\\displaystyle\\bm{\\theta}_{-{\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{% .75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}\\approx% \\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}+\\nicefrac{{1}}{{N}}\\,\\mathrm{H}^{% \\mathop{-1}}_{\\bm{\\theta}}\\,\\nabla_{\\bm{\\theta}}\\mathcal{L}(\\bm{\\theta}_{% \\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}T}}{4pt}},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})bold_italic_\u03b8 start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT \u2248 bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT + / start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT \u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , bold_italic_x ) (13) where H\ud835\udf3dsubscriptH\ud835\udf3d\\mathrm{H}_{\\bm{\\theta}}roman_H start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT is the hessian of \u2112\u2112\\mathcal{L}caligraphic_L evaluated at \ud835\udf3d\\scaleto\u2062T\u20624\u2062p\u2062tsubscript\ud835\udf3d\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT. This approximation is based on a first-order Taylor expansion of the training objective around \ud835\udf3d\\scaleto\u2062T\u20624\u2062p\u2062tsubscript\ud835\udf3d\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT, and should lead to small errors under the following assumptions: (i) the loss function is strictly convex in \ud835\udf3d\ud835\udf3d\\bm{\\theta}bold_italic_\u03b8, (ii) H\ud835\udf3dsubscriptH\ud835\udf3d\\mathrm{H}_{\\bm{\\theta}}roman_H start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (i.e., the gradient is zero). Given these assumptions, influence functions can be used to efficiently estimate the counterfactual term Y\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99;\u221e)subscript\ud835\udc4c\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) in the definition of \u03c4\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062tsubscript\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\\tau_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}italic_\u03c4 start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT. Specifically, we can measure the model\u2019s performance using the updated parameters, Y\u2212\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99)=def\u03b3\u2062(\ud835\udf3d\u2212\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t;\ud835\udc99)superscriptdefsubscript\ud835\udc4c\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99\ud835\udefesubscript\ud835\udf3d\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99Y_{-{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})\\mathrel{\\stackrel{{% \\scriptstyle\\textnormal{def}}}{{=}}}\\gamma(\\bm{\\theta}_{-{\\color[rgb]{0,0.88,0% }\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0% .91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},% \\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}T}}{4pt}};{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_\u03b3 ( bold_italic_\u03b8 start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ), and equate Y\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99;\u221e)=Y\u2212\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99)subscript\ud835\udc4c\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99subscript\ud835\udc4c\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{% .75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}% \\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\\infty})=Y_{-{\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ). The influence function estimator of memorisation can then be written as:Report issue for preceding element \u03c4^\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\ud835\ude92\ud835\ude97\ud835\ude8f\ud835\ude95=Y\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99)\u2212Y\u2212\ud835\udc99,\\scaleto\u2062T\u20624\u2062p\u2062t\u2062(\ud835\udc99)superscriptsubscript^\ud835\udf0f\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\ude92\ud835\ude97\ud835\ude8f\ud835\ude95subscript\ud835\udc4c\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99subscript\ud835\udc4c\ud835\udc99\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\ud835\udc99\\displaystyle\\hat{\\tau}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{% .75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{% \\mathtt{infl}}=Y_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})-Y_{-{\\color[rgb]{% 0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}},\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{% 0.12}\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (14) We formalise this estimator and its statistical estimand in \u00a7 B.2. Notably, [CITATION] use a similar approach to estimate memorisation in a classification setting.Report issue for preceding element",
    "citations": [
      {
        "tag": "Koh and Liang (2017)",
        "title": "Understanding black-box predictions via influence functions.",
        "authors": "Pang\u00a0Wei Koh and Percy Liang. 2017.",
        "journal": "InProceedings of the 34th International Conference on Machine Learning, pages 1885\u20131894. PMLR."
      },
      {
        "tag": "Hampel (1974)",
        "title": "The influence curve and its role in robust estimation.",
        "authors": "Frank\u00a0R. Hampel. 1974.",
        "journal": "Journal of the American Statistical Association, 69(346):383\u2013393."
      },
      {
        "tag": "Cook and Weisberg (1980)",
        "title": "Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression.",
        "authors": "R.\u00a0Dennis Cook and Sanford Weisberg. 1980.",
        "journal": "Technometrics, 22(4):495\u2013508."
      },
      {
        "tag": "Zheng and Jiang (2022)",
        "title": "An empirical study of memorization in NLP.",
        "authors": "Xiaosen Zheng and Jing Jiang. 2022.",
        "journal": "InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6265\u20136278, Dublin, Ireland. Association for Computational Linguistics."
      }
    ]
  },
  "S5.SS2.p2": {
    "text": "Influence functions thus provide a computationally efficient method to estimate instance-level counterfactual memorisation.\nHowever, none of the required assumptions above is typically satisfied for LMs, which can lead to strong biases in this estimator (Basu et\u00a0al., 2020 ; Bae et\u00a0al., 2022 ; Schioppa et\u00a0al., 2023 ) .\nMoreover, assumptions (ii) and (iii) require \ud835\udf3d \\scaleto \u2062 T \u2062 4 \u2062 p \u2062 t subscript \ud835\udf3d \\scaleto \ud835\udc47 4 \ud835\udc5d \ud835\udc61 \\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{%\npgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT to be locally optimal, meaning that this approach is not applicable for studying c < T \ud835\udc50 \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c < italic_T .\nWe therefore cannot use it to study how memorisation interacts with training dynamics. Report issue for preceding element",
    "masked_text": "Influence functions thus provide a computationally efficient method to estimate instance-level counterfactual memorisation. However, none of the required assumptions above is typically satisfied for LMs, which can lead to strong biases in this estimator [CITATION]. Moreover, assumptions (ii) and (iii) require \ud835\udf3d\\scaleto\u2062T\u20624\u2062p\u2062tsubscript\ud835\udf3d\\scaleto\ud835\udc474\ud835\udc5d\ud835\udc61\\bm{\\theta}_{\\scaleto{{\\color[rgb]{.75,0,.25}\\definecolor[named]{% pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT to be locally optimal, meaning that this approach is not applicable for studying c<T\ud835\udc50\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}italic_c < italic_T. We therefore cannot use it to study how memorisation interacts with training dynamics.Report issue for preceding element",
    "citations": [
      {
        "tag": "Bae et\u00a0al. (2022)",
        "title": "If influence functions are the answer, then what is the question?",
        "authors": "Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger\u00a0B. Grosse. 2022.",
        "journal": "Advances in Neural Information Processing Systems, 35:17953\u201317967."
      },
      {
        "tag": "Basu et\u00a0al. (2020)",
        "title": "Influence functions in deep learning are fragile.",
        "authors": "Samyadeep Basu, Phil Pope, and Soheil Feizi. 2020.",
        "journal": "InInternational Conference on Learning Representations."
      },
      {
        "tag": "Schioppa et\u00a0al. (2023)",
        "title": "Theoretical and practical perspectives on what influence functions do.",
        "authors": "Andrea Schioppa, Katja Filippova, Ivan Titov, and Polina Zablotskaia. 2023.",
        "journal": "Advances in Neural Information Processing Systems, 36:27560\u201327581."
      }
    ]
  },
  "S5.SS3.p1": {
    "text": "Carlini et\u00a0al. ( 2023 ) defines memorisation as ( k , \u2113 ) \ud835\udc58 \u2113 (k,\\!\\ell) ( italic_k , roman_\u2113 ) -extractability; a string is ( k , \u2113 ) \ud835\udc58 \u2113 (k,\\!\\ell) ( italic_k , roman_\u2113 ) -extractable if the model correctly predicts \u2113 \u2113 \\ell roman_\u2113 of its tokens given a prefix of k \ud835\udc58 k italic_k tokens.\nThis definition has recently gained much interest because of its relevance to copyright infringement and data protection.\nDespite being seemingly different, we argue that extractable memorisation is an estimator for counterfactual memorisation.\nConcretely, let performance \u03b3 \ud835\udefe \\gamma italic_\u03b3 be measured as whether a string is ( k , \u2113 ) \ud835\udc58 \u2113 (k,\\!\\ell) ( italic_k , roman_\u2113 ) -extractable.\nNow, assume that a string is not ( k , \u2113 ) \ud835\udc58 \u2113 (k,\\!\\ell) ( italic_k , roman_\u2113 ) -extractable in the absence of training, i.e., Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2062 = 0 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 0 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0 .\nGiven this assumption, we can define an extractable memorisation estimator as: Report issue for preceding element \u03c4 ^ \ud835\udc99 , c \ud835\ude8e\ud835\udea1\ud835\ude9d\ud835\ude9b = Y c \u2062 ( \ud835\udc99 ) superscript subscript ^ \ud835\udf0f \ud835\udc99 \ud835\udc50 \ud835\ude8e\ud835\udea1\ud835\ude9d\ud835\ude9b subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 \\displaystyle\\widehat{\\tau}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{%\npgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}%\n\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\\mathtt{extr}}=Y_{{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%\n\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (15) We formalise this estimator and its statistical estimand in \u00a7 B.3 . Report issue for preceding element",
    "masked_text": "[CITATION] defines memorisation as (k,\u2113)\ud835\udc58\u2113(k,\\!\\ell)( italic_k , roman_\u2113 )-extractability; a string is (k,\u2113)\ud835\udc58\u2113(k,\\!\\ell)( italic_k , roman_\u2113 )-extractable if the model correctly predicts \u2113\u2113\\ellroman_\u2113 of its tokens given a prefix of k\ud835\udc58kitalic_k tokens. This definition has recently gained much interest because of its relevance to copyright infringement and data protection. Despite being seemingly different, we argue that extractable memorisation is an estimator for counterfactual memorisation. Concretely, let performance \u03b3\ud835\udefe\\gammaitalic_\u03b3 be measured as whether a string is (k,\u2113)\ud835\udc58\u2113(k,\\!\\ell)( italic_k , roman_\u2113 )-extractable. Now, assume that a string is not (k,\u2113)\ud835\udc58\u2113(k,\\!\\ell)( italic_k , roman_\u2113 )-extractable in the absence of training, i.e., Yc\u2062(\ud835\udc99;\u221e)\u2062=0subscript\ud835\udc4c\ud835\udc50\ud835\udc990Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\mathop{=}0italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0. Given this assumption, we can define an extractable memorisation estimator as:Report issue for preceding element \u03c4^\ud835\udc99,c\ud835\ude8e\ud835\udea1\ud835\ude9d\ud835\ude9b=Yc\u2062(\ud835\udc99)superscriptsubscript^\ud835\udf0f\ud835\udc99\ud835\udc50\ud835\ude8e\ud835\udea1\ud835\ude9d\ud835\ude9bsubscript\ud835\udc4c\ud835\udc50\ud835\udc99\\displaystyle\\widehat{\\tau}_{{\\color[rgb]{0,0.88,0}\\definecolor[named]{% pgfstrokecolor}{rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}},{\\color[rgb]{.75,0,.25}% \\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\\mathtt{extr}}=Y_{{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({% \\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})over^ start_ARG italic_\u03c4 end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (15) We formalise this estimator and its statistical estimand in \u00a7 B.3.Report issue for preceding element",
    "citations": [
      {
        "tag": "Carlini et\u00a0al. (2023)",
        "title": "Quantifying memorization across neural language models.",
        "authors": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023.",
        "journal": "InThe Eleventh International Conference on Learning Representations."
      }
    ]
  },
  "S5.SS3.p2": {
    "text": "Extractable memorisation thus implicitly assumes that Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2062 = 0 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 0 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0 .\nCounterfactual memorisation, on the other hand, controls for this counterfactual.\nNotably, assuming Y c \u2062 ( \ud835\udc99 ; \u221e ) \u2062 = 0 subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 0 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}\\infty})\\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0 may be reasonable when a string is long and complex; in this case, the chance that \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x would be in the model\u2019s top- 1 1 1 1 beam (the output of greedy decoding) approaches zero.\nHowever, this assumption may not be reasonable for shorter or less complex sequences. Rather, it may cause the resulting estimate to conflate memorisation with the intrinsic difficulty of predicting a string. Report issue for preceding element",
    "masked_text": "Extractable memorisation thus implicitly assumes that Yc\u2062(\ud835\udc99;\u221e)\u2062=0subscript\ud835\udc4c\ud835\udc50\ud835\udc990Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\mathop{=}0italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0. Counterfactual memorisation, on the other hand, controls for this counterfactual. Notably, assuming Yc\u2062(\ud835\udc99;\u221e)\u2062=0subscript\ud835\udc4c\ud835\udc50\ud835\udc990Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}};{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}\\infty})\\mathop{=}0italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; \u221e ) = 0 may be reasonable when a string is long and complex; in this case, the chance that \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x would be in the model\u2019s top-1111 beam (the output of greedy decoding) approaches zero. However, this assumption may not be reasonable for shorter or less complex sequences. Rather, it may cause the resulting estimate to conflate memorisation with the intrinsic difficulty of predicting a string.Report issue for preceding element",
    "citations": []
  },
  "S6.p1": {
    "text": "While our method applies to any model trained with a single pass on its training data, we focus on quantifying memorisation in pretrained LMs, which are characterised by the use of architectures with a large number of parameters and large datasets.\nDue to the costs of training such models from scratch, we take advantage of open-source pretrained models whose intermediate checkpoints and preprocessed data are publicly available.\nIn this section, we detail the models and data used and describe how we collect the observed outcomes Y c \u2062 ( \ud835\udc99 ) subscript \ud835\udc4c \ud835\udc50 \ud835\udc99 Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%\n}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) . Report issue for preceding element",
    "masked_text": "While our method applies to any model trained with a single pass on its training data, we focus on quantifying memorisation in pretrained LMs, which are characterised by the use of architectures with a large number of parameters and large datasets. Due to the costs of training such models from scratch, we take advantage of open-source pretrained models whose intermediate checkpoints and preprocessed data are publicly available. In this section, we detail the models and data used and describe how we collect the observed outcomes Yc\u2062(\ud835\udc99)subscript\ud835\udc4c\ud835\udc50\ud835\udc99Y_{{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c% }}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}})italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ).Report issue for preceding element",
    "citations": []
  },
  "S6.SS0.SSS0.Px1.p1": {
    "text": "We use the publicly available Pythia model suite 10 10 10 Both preprocessed data and intermediate checkpoints are publicly available at github.com/EleutherAI/pythia . (Biderman et\u00a0al., 2023b ) , composed of 8 8 8 8 transformers of sizes ranging from \\qty [mode=math]70 to \\qty [mode=math]12 parameters.\nThese models were trained on the Pile dataset (Gao et\u00a0al., 2020 ; Biderman et\u00a0al., 2022 ) , a \\qty [mode=math]300-token curated collection of English documents.\nAll models are trained using the same data.\nSpecifically, the dataset is shuffled and \u201cpacked\u201d into sequences of 2 , 049 2049 2,049 2 , 049 tokens; 11 11 11 Since target tokens are the right-shifted input tokens, to compute the loss on 2 , 048 2048 2,048 2 , 048 tokens the Pythia authors added a token to the context. each of these sequences corresponds to an instance \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x .\nTraining was performed using a cosine learning rate schedule with warm-up, and using a batch size of 1 , 024 1024 1,024 1 , 024 sequences, resulting in exactly \\qty [mode=math]143 optimisation steps.\nWe use the model versions trained on the deduplicated Pile dataset to reduce the risk of finding spurious memorisation patterns due to duplication.\nThe deduplicated dataset has \\qty [mode=math]207 tokens, thus models using this version are trained for circa 1.5 1.5 1.5 1.5 epochs to keep an equal token count relative to the non-deduplicated versions.\nWe consider the checkpoints relative to the first epoch (i.e., up to step \\qty [mode=math]95). 12 12 12 For completeness, we report the second half-epoch (steps \\qty [mode=math]96- \\qty [mode=math]143) analysis in App. D . More details are in App. C . Report issue for preceding element",
    "masked_text": "We use the publicly available Pythia model suite101010Both preprocessed data and intermediate checkpoints are publicly available at github.com/EleutherAI/pythia. [CITATION], composed of 8888 transformers of sizes ranging from \\qty[mode=math]70 to \\qty[mode=math]12 parameters. These models were trained on the Pile dataset [CITATION], a \\qty[mode=math]300-token curated collection of English documents. All models are trained using the same data. Specifically, the dataset is shuffled and \u201cpacked\u201d into sequences of 2,04920492,0492 , 049 tokens;111111Since target tokens are the right-shifted input tokens, to compute the loss on 2,04820482,0482 , 048 tokens the Pythia authors added a token to the context. each of these sequences corresponds to an instance \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x. Training was performed using a cosine learning rate schedule with warm-up, and using a batch size of 1,02410241,0241 , 024 sequences, resulting in exactly \\qty[mode=math]143 optimisation steps. We use the model versions trained on the deduplicated Pile dataset to reduce the risk of finding spurious memorisation patterns due to duplication. The deduplicated dataset has \\qty[mode=math]207 tokens, thus models using this version are trained for circa 1.51.51.51.5 epochs to keep an equal token count relative to the non-deduplicated versions. We consider the checkpoints relative to the first epoch (i.e., up to step \\qty[mode=math]95).121212For completeness, we report the second half-epoch (steps \\qty[mode=math]96-\\qty[mode=math]143) analysis in App. D. More details are in App. C.Report issue for preceding element",
    "citations": [
      {
        "tag": "Biderman et\u00a0al. (2023b)",
        "title": "Pythia: A suite for analyzing large language models across training and scaling.",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der\u00a0Wal. 2023b.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, ICML\u201923."
      },
      {
        "tag": "Biderman et\u00a0al. (2022)",
        "title": "Datasheet for the Pile.",
        "authors": "Stella Biderman, Kieran Bicheno, and Leo Gao. 2022.",
        "journal": "arXiv preprint 2201.07311."
      },
      {
        "tag": "Gao et\u00a0al. (2020)",
        "title": "The Pile: An 800GB dataset of diverse text for language modeling.",
        "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020.",
        "journal": "arXiv preprint 2101.00027."
      }
    ]
  },
  "S6.SS0.SSS0.Px2.p1": {
    "text": "Ideally, we would collect performance metrics for each instance \ud835\udc99 \ud835\udc99 {\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%\n\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}%\n{0.88}{0.12}\\bm{x}} bold_italic_x and for every checkpoint step c \ud835\udc50 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .\nHowever, given the size of the Pile dataset, it is computationally infeasible to collect evaluations for all instances; thus we resort to subsampling this data.\nFurthermore, the granularity of the available checkpoints (i.e., every \\qty [mode=math]1 timesteps) does not allow us to consider each timestep; thus we consider timesteps c \u2208 { 0 , \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 1 \u2062 \u2026 , \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 95 } \ud835\udc50 0 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 1 \u2026 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 95 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n\\in\\{0,\\qty[mode=math]{1}{}...,\\qty[mode=math]{95}{}\\} italic_c \u2208 { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 \u2026 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } and treatment timesteps g \u2208 { \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 1 , \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 2 \u2062 \u2026 , \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 95 } \ud835\udc54 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 1 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 2 \u2026 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 95 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{%\n\\qty[mode=math]{1}{},\\qty[mode=math]{2}{}...,\\qty[mode=math]{95}{}\\} italic_g \u2208 { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 2 \u2026 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } .\nTo match the checkpoint frequency, we consider all instances between two checkpoints (i.e., \\qty [mode=math]1 batches) as if they were seen by the model at the same timestep.\nWe term these groups of batches macro-batches 13 13 13 In econometrics group of instances that undergo treatment at the same time are typically termed cohorts . and formally define them as \ud835\udca2 g = \u22c3 g \u2212 \\qty \u2062 [ m \u2062 o \u2062 d \u2062 e = m \u2062 a \u2062 t \u2062 h ] \u2062 1 < t \u2264 g \u212c t subscript \ud835\udca2 \ud835\udc54 subscript \ud835\udc54 \\qty delimited-[] \ud835\udc5a \ud835\udc5c \ud835\udc51 \ud835\udc52 \ud835\udc5a \ud835\udc4e \ud835\udc61 \u210e 1 \ud835\udc61 \ud835\udc54 subscript \u212c \ud835\udc61 \\mathcal{G}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}g}}=\\bigcup_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{1,.5,0}g}-\\qty[mode=math]{1}{}<t\\leq{\\color[rgb]{1,.5,0}\\definecolor[%\nnamed]{pgfstrokecolor}{rgb}{1,.5,0}g}}\\mathcal{B}_{t} caligraphic_G start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = \u22c3 start_POSTSUBSCRIPT italic_g - [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 < italic_t \u2264 italic_g end_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .\nTo obtain enough evaluations for each macro-batch, we sample instances from the training set in two steps: we randomly choose 10 10 10 10 batches for each macro-batch and sample 10 10 10 10 instances from each.\nThis process results in \\qty [mode=math]14.3 analysed training instances.\nAdditionally, we sample \\qty [mode=math]2 instances from the validation set to create \ud835\udca2 \u221e subscript \ud835\udca2 \\mathcal{G}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n1,.5,0}\\infty}} caligraphic_G start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT .\nThis process returns a panel of \\qty [mode=math]16.3 instances evaluated at 96 96 96 96 timesteps. 14 14 14 Our data and experimental artefacts are publicly available at huggingface.co/collections/pietrolesci/memorisation-profiles . As our performance metric we use the sequence-level log-likelihood: \u03b3 \u2062 ( \ud835\udf3d , \ud835\udc99 ) = log \u2061 p \\scaleto \u2062 \ud835\udf3d \u2062 4 \u2062 p \u2062 t \u2062 ( \ud835\udc99 ) \ud835\udefe \ud835\udf3d \ud835\udc99 subscript \ud835\udc5d \\scaleto \ud835\udf3d 4 \ud835\udc5d \ud835\udc61 \ud835\udc99 \\gamma(\\bm{\\theta},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{%\nrgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%\n\\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})=\\log p_{\\scaleto{\\bm{%\n\\theta}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{%\n0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill%\n{0.91}{0}{0.88}{0.12}\\bm{x}}) italic_\u03b3 ( bold_italic_\u03b8 , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 15 15 15 Results using different metrics are reported in App. D . Report issue for preceding element",
    "masked_text": "Ideally, we would collect performance metrics for each instance \ud835\udc99\ud835\udc99{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}% \\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill{0.91}{0}% {0.88}{0.12}\\bm{x}}bold_italic_x and for every checkpoint step c\ud835\udc50{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_c. However, given the size of the Pile dataset, it is computationally infeasible to collect evaluations for all instances; thus we resort to subsampling this data. Furthermore, the granularity of the available checkpoints (i.e., every \\qty[mode=math]1 timesteps) does not allow us to consider each timestep; thus we consider timesteps c\u2208{0,\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u20621\u2062\u2026,\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u206295}\ud835\udc500\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e1\u2026\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e95{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% \\in\\{0,\\qty[mode=math]{1}{}...,\\qty[mode=math]{95}{}\\}italic_c \u2208 { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 \u2026 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } and treatment timesteps g\u2208{\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u20621,\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u20622\u2062\u2026,\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u206295}\ud835\udc54\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e1\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e2\u2026\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e95{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\in\\{% \\qty[mode=math]{1}{},\\qty[mode=math]{2}{}...,\\qty[mode=math]{95}{}\\}italic_g \u2208 { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 2 \u2026 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 }. To match the checkpoint frequency, we consider all instances between two checkpoints (i.e., \\qty[mode=math]1 batches) as if they were seen by the model at the same timestep. We term these groups of batches macro-batches131313In econometrics group of instances that undergo treatment at the same time are typically termed cohorts. and formally define them as \ud835\udca2g=\u22c3g\u2212\\qty\u2062[m\u2062o\u2062d\u2062e=m\u2062a\u2062t\u2062h]\u20621<t\u2264g\u212ctsubscript\ud835\udca2\ud835\udc54subscript\ud835\udc54\\qtydelimited-[]\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc61\u210e1\ud835\udc61\ud835\udc54subscript\u212c\ud835\udc61\\mathcal{G}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}g}}=\\bigcup_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{1,.5,0}g}-\\qty[mode=math]{1}{}<t\\leq{\\color[rgb]{1,.5,0}\\definecolor[% named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\\mathcal{B}_{t}caligraphic_G start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = \u22c3 start_POSTSUBSCRIPT italic_g - [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 < italic_t \u2264 italic_g end_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. To obtain enough evaluations for each macro-batch, we sample instances from the training set in two steps: we randomly choose 10101010 batches for each macro-batch and sample 10101010 instances from each. This process results in \\qty[mode=math]14.3 analysed training instances. Additionally, we sample \\qty[mode=math]2 instances from the validation set to create \ud835\udca2\u221esubscript\ud835\udca2\\mathcal{G}_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 1,.5,0}\\infty}}caligraphic_G start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT. This process returns a panel of \\qty[mode=math]16.3 instances evaluated at 96969696 timesteps.141414Our data and experimental artefacts are publicly available at huggingface.co/collections/pietrolesci/memorisation-profiles. As our performance metric we use the sequence-level log-likelihood: \u03b3\u2062(\ud835\udf3d,\ud835\udc99)=log\u2061p\\scaleto\u2062\ud835\udf3d\u20624\u2062p\u2062t\u2062(\ud835\udc99)\ud835\udefe\ud835\udf3d\ud835\udc99subscript\ud835\udc5d\\scaleto\ud835\udf3d4\ud835\udc5d\ud835\udc61\ud835\udc99\\gamma(\\bm{\\theta},{\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{% rgb}{0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}% \\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\\bm{x}})=\\log p_{\\scaleto{\\bm{% \\theta}}{4pt}}({\\color[rgb]{0,0.88,0}\\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0.88,0}\\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\\pgfsys@color@cmyk@fill% {0.91}{0}{0.88}{0.12}\\bm{x}})italic_\u03b3 ( bold_italic_\u03b8 , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ). 151515Results using different metrics are reported in App. D.Report issue for preceding element",
    "citations": []
  },
  "S6.SS0.SSS0.Px3.p1": {
    "text": "To compute statistical significance, we use the Simple Multiplier Bootstrap procedure of Callaway and Sant\u2019Anna ( 2021 ) which returns simultaneous confidence intervals for all memorisation estimates, accounting for dependencies across macro-batches and checkpoint steps and thus avoiding multiple-testing issues. Report issue for preceding element",
    "masked_text": "To compute statistical significance, we use the Simple Multiplier Bootstrap procedure of [CITATION] which returns simultaneous confidence intervals for all memorisation estimates, accounting for dependencies across macro-batches and checkpoint steps and thus avoiding multiple-testing issues.Report issue for preceding element",
    "citations": [
      {
        "tag": "Callaway and Sant\u2019Anna (2021)",
        "title": "Difference-in-Differences with multiple time periods.",
        "authors": "Brantly Callaway and Pedro H.\u00a0C. Sant\u2019Anna. 2021.",
        "journal": "Journal of Econometrics, 225(2):200\u2013230."
      }
    ]
  },
  "S7.p1": {
    "text": "We report the memorisation profiles of all Pythia sizes in Fig. 2 . Below, we use these memorisation profiles to describe different types of memorisation. Report issue for preceding element",
    "masked_text": "We report the memorisation profiles of all Pythia sizes in Fig. 2. Below, we use these memorisation profiles to describe different types of memorisation.Report issue for preceding element",
    "citations": []
  },
  "S7.SS0.SSS0.Px1.p1": {
    "text": "Instantaneous memorisation estimates (defined in \u00a7 3 as \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g \u2062 = c \ud835\udc54 \ud835\udc50 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{%\n=}{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g = italic_c ) are depicted as the diagonal entries in the memorisation profiles in Fig. 2 , and are also presented in Fig. 3 .\nFrom these estimates, we can clearly observe the effect of the treatment step on memorisation: instantaneous memorisation is stronger earlier in training than later.\nInterestingly (but perhaps unsurprisingly), instantaneous memorisation correlates with the cosine learning rate schedule: it is stronger after the warm-up period (around timestep \\qty [mode=math]1.5) than before it.\nFurthermore, and as expected, instantaneous memorisation increases with model size. 16 16 16 Notably, we expect instantaneous memorisation to always be present in normally-trained LMs (albeit with a potentially small value). It could thus be used for power analysis (Cohen, 1992 ) : choosing the number of instances to sample per macro-batch which provides sufficient statistical power to correctly detect memorisation. Report issue for preceding element",
    "masked_text": "Instantaneous memorisation estimates (defined in \u00a7 3 as \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g\u2062=c\ud835\udc54\ud835\udc50{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\\mathop{% =}{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_g = italic_c) are depicted as the diagonal entries in the memorisation profiles in Fig. 2, and are also presented in Fig. 3. From these estimates, we can clearly observe the effect of the treatment step on memorisation: instantaneous memorisation is stronger earlier in training than later. Interestingly (but perhaps unsurprisingly), instantaneous memorisation correlates with the cosine learning rate schedule: it is stronger after the warm-up period (around timestep \\qty[mode=math]1.5) than before it. Furthermore, and as expected, instantaneous memorisation increases with model size.161616Notably, we expect instantaneous memorisation to always be present in normally-trained LMs (albeit with a potentially small value). It could thus be used for power analysis [CITATION]: choosing the number of instances to sample per macro-batch which provides sufficient statistical power to correctly detect memorisation.Report issue for preceding element",
    "citations": [
      {
        "tag": "Cohen (1992)",
        "title": "Statistical power analysis.",
        "authors": "Jacob Cohen. 1992.",
        "journal": "Current Directions in Psychological Science, 1(3):98\u2013101."
      }
    ]
  },
  "S7.SS0.SSS0.Px2.p1": {
    "text": "Persistent memorisation estimates (defined in \u00a7 3 as \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g > c \ud835\udc54 \ud835\udc50 {\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>{\\color%\n[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g > italic_c ) are depicted as the off-diagonal entries in the memorisation profiles in Fig. 2 . Fig. 4 shows the average persistent memorisation at a specific number of timesteps after treatment; in this figure, \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT were averaged across macro-batches for each c \u2062 - g \ud835\udc50 \ud835\udc54 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%\n\\mathop{-}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c - italic_g . 17 17 17 By averaging across macro-batches, variance is lower and more estimates become statistically significant. This way of aggregating the memorisation profile allows us to summarise the general memorisation patterns of a model.\nSmaller models have lower persistent memorisation, with \\qty [mode=math]70 having no persistent memorisation.\nInterestingly, persistent memorisation plateaus after \\qty [mode=math]25 timesteps.\nThis result has implications for data ordering during training.\nFor example, if there are particular instances that we do not want the model to memorise, but which we still want to use during training, they should be included in earlier batches. 18 18 18 We note that our results differ from Biderman et\u00a0al. \u2019s ( 2023b ), who find no differences in memorisation due to an instance\u2019s treatment step. We hypothesise that this discrepancy stems from the differences in metrics used to quantify memorisation and the statistical approaches adopted. Report issue for preceding element",
    "masked_text": "Persistent memorisation estimates (defined in \u00a7 3 as \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g>c\ud835\udc54\ud835\udc50{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>{\\color% [rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}italic_g > italic_c) are depicted as the off-diagonal entries in the memorisation profiles in Fig. 2. Fig. 4 shows the average persistent memorisation at a specific number of timesteps after treatment; in this figure, \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT were averaged across macro-batches for each c\u2062-g\ud835\udc50\ud835\udc54{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}% \\mathop{-}{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}italic_c - italic_g.171717By averaging across macro-batches, variance is lower and more estimates become statistically significant. This way of aggregating the memorisation profile allows us to summarise the general memorisation patterns of a model. Smaller models have lower persistent memorisation, with \\qty[mode=math]70 having no persistent memorisation. Interestingly, persistent memorisation plateaus after \\qty[mode=math]25 timesteps. This result has implications for data ordering during training. For example, if there are particular instances that we do not want the model to memorise, but which we still want to use during training, they should be included in earlier batches.181818We note that our results differ from [CITATION]\u2019s ( [CITATION]), who find no differences in memorisation due to an instance\u2019s treatment step. We hypothesise that this discrepancy stems from the differences in metrics used to quantify memorisation and the statistical approaches adopted.Report issue for preceding element",
    "citations": [
      {
        "tag": "Biderman et\u00a0al. (2023b)",
        "title": "Pythia: A suite for analyzing large language models across training and scaling.",
        "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad\u00a0Aflah Khan, Shivanshu Purohit, USVSN\u00a0Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der\u00a0Wal. 2023b.",
        "journal": "InProceedings of the 40th International Conference on Machine Learning, ICML\u201923."
      }
    ]
  },
  "S7.SS0.SSS0.Px3.p1": {
    "text": "Residual memorisation estimates (defined in \u00a7 3 as \u03c4 g , c subscript \ud835\udf0f \ud835\udc54 \ud835\udc50 \\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when c = T \ud835\udc50 \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%\n\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T ) are depicted as the final-column entries in Fig. 2 , and are also presented in Fig. 5 (we consider T \ud835\udc47 {\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T to be the end of the first epoch here, i.e., timestep \\qty [mode=math]95).\nInterestingly, while all macro-batches undergo some degree of instantaneous memorisation, it appears that many are then forgotten by the end of the first epoch, as shown by the statistically insignificant residual memorisation estimates.\nFurthermore, in line with our persistent memorisation results, residual memorisation shows a recency effect: the final macro-batches are the most memorised.\nWe hypothesise that this recency effect can be explained by the learning rate schedule.\nSpecifically, when the learning rate is high, the optimisation process moves model parameters further in the locally optimal direction, thus \u201coverwriting\u201d previous information with new information; this results in higher instantaneous and lower residual memorisation.\nOn the contrary, towards the end of the training process, when the learning rate is lower, previous information is \u201cforgotten\u201d less as the updates are smaller (in expectation), resulting in higher residual and lower instantaneous memorisation. Report issue for preceding element",
    "masked_text": "Residual memorisation estimates (defined in \u00a7 3 as \u03c4g,csubscript\ud835\udf0f\ud835\udc54\ud835\udc50\\tau_{{\\color[rgb]{1,.5,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}italic_\u03c4 start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when c=T\ud835\udc50\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={% \\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}italic_c = italic_T) are depicted as the final-column entries in Fig. 2, and are also presented in Fig. 5 (we consider T\ud835\udc47{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}italic_T to be the end of the first epoch here, i.e., timestep \\qty[mode=math]95). Interestingly, while all macro-batches undergo some degree of instantaneous memorisation, it appears that many are then forgotten by the end of the first epoch, as shown by the statistically insignificant residual memorisation estimates. Furthermore, in line with our persistent memorisation results, residual memorisation shows a recency effect: the final macro-batches are the most memorised. We hypothesise that this recency effect can be explained by the learning rate schedule. Specifically, when the learning rate is high, the optimisation process moves model parameters further in the locally optimal direction, thus \u201coverwriting\u201d previous information with new information; this results in higher instantaneous and lower residual memorisation. On the contrary, towards the end of the training process, when the learning rate is lower, previous information is \u201cforgotten\u201d less as the updates are smaller (in expectation), resulting in higher residual and lower instantaneous memorisation.Report issue for preceding element",
    "citations": []
  },
  "S7.SS0.SSS0.Px4.p1": {
    "text": "Due to the cost of training large LMs, it is highly desirable to be able to make predictions about a trained model\u2019s characteristics before undertaking training. One strategy is to derive insights from smaller models to inform the design of larger ones (Rae et\u00a0al., 2021 ; Black et\u00a0al., 2022 ; Le\u00a0Scao et\u00a0al., 2022 ) . 19 19 19 Scaling laws for other notions of memorisation ( \u00a7 5 ) have been studied in Biderman et\u00a0al. ( 2023a ) . Predictability across scales is visually apparent in Fig. 3 and 4 where there are similar trends across model sizes.\nWe formalise this intuition in Fig. 6 , where we report the Pearson correlation between the memorisation profiles of different models.\nInterestingly, memorisation for larger models (e.g., \\qty [mode=math]12) is predictable from smaller ones (e.g., \\qty [mode=math]410).\nWe note that \\qty [mode=math]70 and \\qty [mode=math]160 are less predictive of the memorisation in \\qty [mode=math]12.\nHowever, prior work has shown that both these models suffer from training instability (Godey et\u00a0al., 2024 ) ; the reduction in predictability with these smaller models might thus be specific to the Pythia suite. Report issue for preceding element",
    "masked_text": "Due to the cost of training large LMs, it is highly desirable to be able to make predictions about a trained model\u2019s characteristics before undertaking training. One strategy is to derive insights from smaller models to inform the design of larger ones [CITATION].191919Scaling laws for other notions of memorisation (\u00a7 5) have been studied in [CITATION]. Predictability across scales is visually apparent in Fig. 3 and 4 where there are similar trends across model sizes. We formalise this intuition in Fig. 6, where we report the Pearson correlation between the memorisation profiles of different models. Interestingly, memorisation for larger models (e.g., \\qty[mode=math]12) is predictable from smaller ones (e.g., \\qty[mode=math]410). We note that \\qty[mode=math]70 and \\qty[mode=math]160 are less predictive of the memorisation in \\qty[mode=math]12. However, prior work has shown that both these models suffer from training instability [CITATION]; the reduction in predictability with these smaller models might thus be specific to the Pythia suite.Report issue for preceding element",
    "citations": [
      {
        "tag": "Rae et\u00a0al. (2021)",
        "title": "Scaling language models: Methods, analysis & insights from training gopher.",
        "authors": "Jack\u00a0W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et\u00a0al. 2021.",
        "journal": "arXiv preprint 2112.11446."
      },
      {
        "tag": "Black et\u00a0al. (2022)",
        "title": "GPT-NeoX-20B: An open-source autoregressive language model.",
        "authors": "Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn\u00a0Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.",
        "journal": "InProceedings of BigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pages 95\u2013136, virtual+Dublin. Association for Computational Linguistics."
      },
      {
        "tag": "Le\u00a0Scao et\u00a0al. (2022)",
        "title": "What language model to train if you have one million GPU hours?",
        "authors": "Teven Le\u00a0Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M\u00a0Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng\u00a0Xin Yong, Julien Launay, and Iz\u00a0Beltagy. 2022.",
        "journal": "InFindings of the Association for Computational Linguistics: EMNLP 2022, pages 765\u2013782, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
      },
      {
        "tag": "Biderman et\u00a0al. (2023a)",
        "title": "Emergent and predictable memorization in large language models.",
        "authors": "Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023a.",
        "journal": "Advances in Neural Information Processing Systems, 36:28072\u201328090."
      },
      {
        "tag": "Godey et\u00a0al. (2024)",
        "title": "Why do small language models underperform? Studying language model saturation via the softmax bottleneck.",
        "authors": "Nathan Godey, \u00c9ric de la Clergerie, and Beno\u00eet Sagot. 2024.",
        "journal": "arXiv preprint 2404.07647."
      }
    ]
  },
  "S8.p1": {
    "text": "The memorisation of training data by neural networks has critical implications for privacy, copyright, and security. Thus, well-founded quantifications of memorisation, and corresponding accurate and efficient methods for their estimation are of great importance.\nThis work presents one such quantification and builds on the econometrics literature to derive an unbiased and efficient estimator of memorisation based on the difference-in-differences design.\nWe use this estimator to study the memorisation profiles of the Pythia model suite and find that memorisation is stronger and more persistent in larger models, determined by data order and learning rate, and stable across model sizes. Report issue for preceding element",
    "masked_text": "The memorisation of training data by neural networks has critical implications for privacy, copyright, and security. Thus, well-founded quantifications of memorisation, and corresponding accurate and efficient methods for their estimation are of great importance. This work presents one such quantification and builds on the econometrics literature to derive an unbiased and efficient estimator of memorisation based on the difference-in-differences design. We use this estimator to study the memorisation profiles of the Pythia model suite and find that memorisation is stronger and more persistent in larger models, determined by data order and learning rate, and stable across model sizes.Report issue for preceding element",
    "citations": []
  },
  "Sx1.p1": {
    "text": "This work estimates counterfactual memorisation in pretrained LMs.\nUnfortunately, due to the costs associated with running large pretrained LMs\u2014even in inference mode\u2014we experimented with a limited number of models (the Pythia suite) trained in a single language (English).\nInvestigating whether other model architectures, training procedures, and natural languages result in similar memorisation profiles would be important to strengthen our conclusions.\nFurthermore, when collecting the panel data needed to estimate memorisation, we subsampled the number of evaluated instances; this can significantly increase our estimators\u2019 variance. Report issue for preceding element",
    "masked_text": "This work estimates counterfactual memorisation in pretrained LMs. Unfortunately, due to the costs associated with running large pretrained LMs\u2014even in inference mode\u2014we experimented with a limited number of models (the Pythia suite) trained in a single language (English). Investigating whether other model architectures, training procedures, and natural languages result in similar memorisation profiles would be important to strengthen our conclusions. Furthermore, when collecting the panel data needed to estimate memorisation, we subsampled the number of evaluated instances; this can significantly increase our estimators\u2019 variance.Report issue for preceding element",
    "citations": []
  },
  "Sx2.p1": {
    "text": "Pietro and Andreas received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 Research and Innovation programme grant AVeriTeC (Grant agreement No. 865958).\nClara is funded by a Google PhD Fellowship.\nWe thank the anonymous reviewers, for their helpful questions and comments that helped us improve the paper.\nWe also thank Machel Reid for early discussions on the topic;\nSidak Pal Singh, Gregor Bachmann, and Ornella Darova for their feedback on earlier versions of this paper; and\nDavide Lesci and Marco Lesci for proofreading the final version of the paper. Report issue for preceding element",
    "masked_text": "Pietro and Andreas received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 Research and Innovation programme grant AVeriTeC (Grant agreement No. 865958). Clara is funded by a Google PhD Fellowship. We thank the anonymous reviewers, for their helpful questions and comments that helped us improve the paper. We also thank Machel Reid for early discussions on the topic; Sidak Pal Singh, Gregor Bachmann, and Ornella Darova for their feedback on earlier versions of this paper; and Davide Lesci and Marco Lesci for proofreading the final version of the paper.Report issue for preceding element",
    "citations": []
  }
}