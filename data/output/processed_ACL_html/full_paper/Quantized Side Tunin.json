{
  "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning ofQuantized Large Language Models",
  "text": "Recent advancements in large language models (LLMs), including GPT (Brown et\u00a0al., 2020 ; Floridi and Chiriatti, 2020 ; OpenAI, 2023 ) , PaLM (Chowdhery et\u00a0al., 2022 ) , OPT (Zhang et\u00a0al., 2022 ) , and LLaMA (Touvron et\u00a0al., 2023 ) , have showcased remarkable task-generalization capabilities across diverse applications (Stiennon et\u00a0al., 2020 ; Dosovitskiy et\u00a0al., 2020 ) .\nThe ongoing evolution of LLMs\u2019 capabilities is accompanied by exponential increases in LLMs\u2019 sizes, with some models encompassing 100 billion parameters (Raffel et\u00a0al., 2020 ; Scao et\u00a0al., 2022 ) .\nFinetuning pre-trained LLMs (Min et\u00a0al., 2021 ; Wang et\u00a0al., 2022b , a ; Liu et\u00a0al., 2022 ) for customized downstream tasks provides an effective approach to introducing desired behaviors, mitigating undesired ones, and thus boosting the LLMs\u2019 performance (Ouyang et\u00a0al., 2022 ; Askell et\u00a0al., 2021 ; Bai et\u00a0al., 2022 ) .\nNevertheless, the process of LLM finetuning is characterized by its substantial memory demands. For instance, finetuning a 16-bit LLaMA model with 65 billion parameters requires more than 780GB of memory (Dettmers et\u00a0al., 2023 ) . Report issue for preceding element\nTo reduce the computational requirement of LLM finetuning, recent work introduces parameter-efficient finetuning (PEFT), which updates a subset of trainable parameters from an LLM or introduces a small number of new parameters into the LLM while keeping the vast majority of the original LLM parameters frozen (Houlsby et\u00a0al., 2019 ; Li and Liang, 2021 ; Pfeiffer et\u00a0al., 2020 ; Hu et\u00a0al., 2021 ; He et\u00a0al., 2021 ; Lester et\u00a0al., 2021 ) .\nPEFT methods achieve comparable performance as full finetuning while enabling fast adaption to new tasks without suffering from catastrophic forgetting (Pfeiffer et\u00a0al., 2020 ) .\nHowever, PEFT methods necessitate caching intermediate activations during forward processing, since these activations are needed to update trainable parameters during backward propagation.\nAs a result, PEFT methods require saving more than 70% of activations and almost the same training time compared to full finetuning (Liao et\u00a0al., 2023 ; Sung et\u00a0al., 2022 ) .\nConcisely, existing PEFT techniques cannot effectively reduce the memory footprint of LLM finetuning, restricting their applications in numerous real-world memory-constrained scenarios. Report issue for preceding element\nRecent work has also introduced approaches to combining PEFT and quantization. For example, QLoRA (Dettmers et\u00a0al., 2023 ) quantizes an LLM\u2019s weights to 4-bit and leverages low-rank adaption (LoRA) (He et\u00a0al., 2021 ) to finetune the quantized LLM.\nQLoRA reduces the memory footprint of an LLM\u2019s weights and optimizer states, and as a result, finetuning a 65B LLM requires less than 48 GB of memory.\nHowever, QLoRA does not consider the memory footprint of intermediate activations, which can be particularly large when using a large batch size for finetuning.\nAs a result, QLoRA only supports small-batch training (e.g. a batch size of 1 1 1 1 ), and finetuning a 65B LLM requires checkpointing gradients Chen et\u00a0al. ( 2016 ) to fit the LLM on a single 48GB GPU, resulting in long training time.\nBesides, our evaluation also reveals that the performance of QLoRA becomes unstable when using 16-bit floating points. Sung et\u00a0al. ( 2022 ) and Zhang et\u00a0al. ( 2020 ) propose to use a side network to reduce the memory footprint of intermediate activations by avoiding backpropagation of the LLM on natural language processing (NLP) and computer vision (CV) tasks, respectively.\nEven with the adoption of a side network, the inherent model size of the LLM remains a challenge. Meanwhile, these approaches focus on small models (i.e., less than 3 billion parameters), and their applicability and efficacy for larger models remain unexplored. Report issue for preceding element\nIn this paper, we propose a fast, memory-efficient LLM finetuning framework, called Q uantized S ide- T uning (QST), which operates through a dual-stage process as shown in Figure 2 .\nFirst, QST quantizes an LLM into 4-bit to reduce the memory footprint of its model weights.\nSecond, QST introduces a side network separating from\nthe quantized LLM to avoid performing backward propagation for the quantized LLM, thus saving the memory footprint of intermediate activations.\nDuring the training phase of QST, the input to each layer of the side network is formed by combining (1) the downsampled output of the corresponding quantized LLM layer and (2) the output of the previous layer of the side network.\nA larger LLM usually has a larger model depth (i.e., the number of layers) and width (the hidden size of each layer), which in turn requires more trainable parameters for the downsampling layers.\nUnlike Sung et\u00a0al. ( 2022 ) that leverages linear layer to perform downsampling, QST uses several low-rank adapter methods (He et\u00a0al., 2021 ; Edalati et\u00a0al., 2022 ) such as MaxPooling (LeCun et\u00a0al., 1998 ) and AvgPooling, significantly reducing the required trainable parameters and the memory footprint for the optimizer states.\nAfter that, we use a learnable parameter to assign weights and subsequently aggregate the hidden states of the quantized LLM and the side network.\nFinally, we reuse the LLM head or classifier to predict.\nCombined with 4-bit quantization and side tuning, QST significantly reduces all three main contributors of the memory footprint and training time during the training phase. Besides, QST does not increase inference latency since the LLM and side network can be computed in parallel. Figure 1 compares the memory footprint of QST and existing parameter-efficient fine-tuning methods, including QLoRA and LST. Report issue for preceding element\nTo validate the effectiveness of our QST, we conduct extensive evaluations for different types of LLMs (e.g., OPT, LLaMA 2), with 1.3B to 70B parameters, on various benchmarks. Experiment results show that QST can reduce the total memory footprint by up to 2.3 \u00d7 \\times \u00d7 and speed up the finetuning process by up to 3 \u00d7 \\times \u00d7 while achieving competent performance compared with the state-of-the-art.\nOur codes are released to the GitHub 1 1 1 https://github.com/YouAreSpecialToMe/QST . Report issue for preceding element\nFinetuning allows an LLM to adapt to specialized domains and tasks (Devlin et\u00a0al., 2018 ; Radford et\u00a0al., 2019 ; Brown et\u00a0al., 2020 ) . However, fully finetuning an LLM comes with high computation costs due to the rapidly increasing LLM sizes. Parameter-efficient finetuning (PEFT) methods are proposed to solve this issue. Drawing inspiration from the pronounced sensitivity of LLMs to prompts as highlighted in Schick and Sch\u00fctze ( 2020 ) , a series of studies introduce trainable prompt embeddings prepended to the input text or attention components while preserving the original LLM parameters Liu et\u00a0al. ( 2023 ); Li and Liang ( 2021 ); Lester et\u00a0al. ( 2021 ) . Rusu et\u00a0al. ( 2016 ) and Houlsby et\u00a0al. ( 2019 ) propose adapter modules to introduce new task-specific parameters, which are inserted into the Transformer layers inside the LLM. LoRA Hu et\u00a0al. ( 2021 ) leverages the low-rank decomposition concept to construct trainable parameters inserted into the original LLM weights. (IA) 3 3 {}^{3} start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT Liu et\u00a0al. ( 2022 ) proposes to scale the pre-trained model weights of an LLM with a trainable vector. Of late, there has been a surge in the proposal of unified approaches that amalgamate various PEFT methods by leveraging human heuristics He et\u00a0al. ( 2021 ) or employing neural architecture search Zhou et\u00a0al. ( 2023 ); Zoph and Le ( 2016 ); Mao et\u00a0al. ( 2021 ) . Existing PEFT approaches focus on optimizing model performance while minimizing trainable parameters. However, a reduction in the number of trainable parameters does not inherently imply a corresponding reduction in memory footprint. Report issue for preceding element\nMemory-efficient training and finetuning aims to reduce the memory footprint during the LLM training and/or finetuning phase. Reversible neural networks Gomez et\u00a0al. ( 2017 ); Kitaev et\u00a0al. ( 2020 ); Mangalam et\u00a0al. ( 2022 ) allow the intermediate activations of each layer to be recomputed from the activation of its next layer, thus exempting the need to save intermediate activations. Gradient checkpointing Chen et\u00a0al. ( 2016 ) offers an optimization strategy that balances computational resources against memory footprint. Specifically, it reduces memory requirement by selectively discarding certain intermediate activations, which are subsequently recomputed through an additional forward pass when needed. Another line to enhancing memory efficiency involves network compression, that is, the original LLM is reduced to a more compact form, thereby making both the training and inference phases more computationally economical. Network pruning and distillation are the most prevalent strategies for network compression. Network distillation Hinton et\u00a0al. ( 2015 ); Koratana et\u00a0al. ( 2019 ) involves the creation of a student network that is trained to approximate the output distribution of a teacher network across a specified dataset. Network pruning Frankle and Carbin ( 2018 ); Frankle et\u00a0al. ( 2020 ) aims to streamline models by ascertaining the significance of individual parameters and subsequently eliminating those deemed non-essential. Compared with PEFT methods, network compression yields models optimized for expedited inference, whereas PEFT methods may achieve superior performance by updating a small set of trainable parameters. Report issue for preceding element\nRecently, QLoRA Dettmers et\u00a0al. ( 2023 ) quantizes the LLM to 4-bit and then adds LoRA to finetune the quantized LLM. QLoRA significantly reduces the memory footprint of weights and optimizer states compared with full finetuning while retaining similar performance. QLoRA does not consider the memory footprint of intermediate activations, and thus falls short in finetuning the LLM with a large batch size, resulting in a long training time.\nIn the context of NLP and CV tasks, the studies by Sung et\u00a0al. ( 2022 ) and Zhang et\u00a0al. ( 2020 ) introduce the concept of employing a side network. The side network aims to obviate the need for backpropagation through the LLM, thereby reducing the memory footprint associated with intermediate activations. Despite incorporating the side network, the inherent model size (i.e., the memory footprint of weights) of the LLM still poses computational challenges. Hence, both methods can only focus exclusively on models with fewer than 3 billion parameters, and fail to finetune models with more parameters. Report issue for preceding element\nIn this section, we first describe the process of quantizing an LLM into 4-bit, and then introduce our design of the side network for side tuning. Report issue for preceding element\nQuantization is the process of converting a data type with more bits (e.g., 32- or 16-bit floating points) into another data type with fewer bits (e.g., 8-bit integers or 4-bit floating points). QST first quantizes an LLM from 16-bit into 4-bit, formulated as follows. Report issue for preceding element X 4 \u2062 b \u2062 i \u2062 t = superscript \ud835\udc4b 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 absent \\displaystyle X^{4bit}= italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT = round \u2062 ( M 4 \u2062 b \u2062 i \u2062 t Absmax \u2062 ( X 16 \u2062 b \u2062 i \u2062 t ) \u2062 X 16 \u2062 b \u2062 i \u2062 t ) round subscript \ud835\udc40 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 Absmax superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 \\displaystyle\\text{round}\\left(\\frac{M_{4bit}}{\\text{Absmax}(X^{16bit})}X^{16%\nbit}\\right) round ( divide start_ARG italic_M start_POSTSUBSCRIPT 4 italic_b italic_i italic_t end_POSTSUBSCRIPT end_ARG start_ARG Absmax ( italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) end_ARG italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) (1) = \\displaystyle= = round \u2062 ( c 16 \u2062 b \u2062 i \u2062 t \u22c5 X 16 \u2062 b \u2062 i \u2062 t ) , round \u22c5 superscript \ud835\udc50 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 \\displaystyle\\text{round}\\left(c^{16bit}\\cdot X^{16bit}\\right), round ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT \u22c5 italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) , (2) where X 4 \u2062 b \u2062 i \u2062 t superscript \ud835\udc4b 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 X^{4bit} italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT and X 16 \u2062 b \u2062 i \u2062 t superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 X^{16bit} italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT are tensors in 4- and 16-bit, respectively. M 4 \u2062 b \u2062 i \u2062 t subscript \ud835\udc40 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 M_{4bit} italic_M start_POSTSUBSCRIPT 4 italic_b italic_i italic_t end_POSTSUBSCRIPT is the maximum value of the 4-bit data type. For example, M N \u2062 F \u2062 4 = 1 subscript \ud835\udc40 \ud835\udc41 \ud835\udc39 4 1 M_{NF4}=1 italic_M start_POSTSUBSCRIPT italic_N italic_F 4 end_POSTSUBSCRIPT = 1 , where NF4 is an information-theoretically optimal data type that ensures each quantization bin has an equal number of values assigned from the input tensor. QST considers both NF4 and FP4 to quantize an LLM. We empirically demonstrate that NF4 performs the best in our experiments (see Section 4 ). c 16 \u2062 b \u2062 i \u2062 t superscript \ud835\udc50 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 c^{16bit} italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT is the quantization constant (or quantization scale) of the 16-bit data type. Correspondingly, dequantization is given by Report issue for preceding element d \u2062 e \u2062 q \u2062 u \u2062 a \u2062 n \u2062 t \u2062 ( c 16 \u2062 b \u2062 i \u2062 t , X 4 \u2062 b \u2062 i \u2062 t ) = X 4 \u2062 b \u2062 i \u2062 t c 16 \u2062 b \u2062 i \u2062 t = X 16 \u2062 b \u2062 i \u2062 t . \ud835\udc51 \ud835\udc52 \ud835\udc5e \ud835\udc62 \ud835\udc4e \ud835\udc5b \ud835\udc61 superscript \ud835\udc50 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc50 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 \\displaystyle dequant(c^{16bit},X^{4bit})=\\frac{X^{4bit}}{c^{16bit}}=X^{16bit}. italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT , italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) = divide start_ARG italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT end_ARG start_ARG italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT end_ARG = italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT . (3) The key limitation of this method arises when the input tensor contains values with very large magnitudes, commonly referred to as outliers .\nSuch outliers can result in under-utilization of the quantization bins, leading to sparsely populated or even empty bins in some instances. To address this issue, a prevalent strategy involves partitioning the input tensor into discrete blocks, each subjected to independent quantization with its own associated quantization constant. As a result, the input tensor X \u2208 \u211d b \u00d7 h \ud835\udc4b superscript \u211d \ud835\udc4f \u210e X\\in\\mathbb{R}^{b\\times h} italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_b \u00d7 italic_h end_POSTSUPERSCRIPT is decomposed into n \ud835\udc5b n italic_n contiguous blocks, each comprising B \ud835\udc35 B italic_B elements. This decomposition is facilitated by flattening X \ud835\udc4b X italic_X into a 1-dimensional array, which is then partitioned into n = ( b \u00d7 h ) B \ud835\udc5b \ud835\udc4f \u210e \ud835\udc35 n=\\frac{(b\\times h)}{B} italic_n = divide start_ARG ( italic_b \u00d7 italic_h ) end_ARG start_ARG italic_B end_ARG individual blocks. Then, we can leverage E.q. (1) to independently quantize these n \ud835\udc5b n italic_n blocks using different quantization constants. Typically, minimizing the error associated with 4-bit quantization would necessitate the utilization of smaller block sizes. This is attributed to the reduced influence of outliers on other weights. However, using a small block size leads to high memory overhead since we need to allocate more memory for these quantization constants. To reduce the memory footprint of quantization constants, we can use the same quantization strategy to quantize these quantization constants (Dettmers et\u00a0al., 2023 ) . In this paper, we use 8-bit float points to quantize the quantization constants, and the forward pass of a single linear layer in the LLM is defined as Y 16 \u2062 b \u2062 i \u2062 t = d \u2062 e \u2062 q \u2062 u \u2062 a \u2062 n \u2062 t \u2062 ( d \u2062 e \u2062 q \u2062 u \u2062 a \u2062 n \u2062 t \u2062 ( c 2 16 \u2062 b \u2062 i \u2062 t , c 1 8 \u2062 b \u2062 i \u2062 t ) , W 4 \u2062 b \u2062 i \u2062 t ) \u2062 X 16 \u2062 b \u2062 i \u2062 t superscript \ud835\udc4c 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 \ud835\udc51 \ud835\udc52 \ud835\udc5e \ud835\udc62 \ud835\udc4e \ud835\udc5b \ud835\udc61 \ud835\udc51 \ud835\udc52 \ud835\udc5e \ud835\udc62 \ud835\udc4e \ud835\udc5b \ud835\udc61 subscript superscript \ud835\udc50 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 2 subscript superscript \ud835\udc50 8 \ud835\udc4f \ud835\udc56 \ud835\udc61 1 superscript \ud835\udc4a 4 \ud835\udc4f \ud835\udc56 \ud835\udc61 superscript \ud835\udc4b 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 Y^{16bit}=dequant(dequant(c^{16bit}_{2},c^{8bit}_{1}),W^{4bit})X^{16bit} italic_Y start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT = italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_c start_POSTSUPERSCRIPT 8 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_W start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT .\n4-bit quantization can significantly reduce the memory footprint of weights, facilitating easier storage and deployment of LLMs. Besides, low-precision floating numbers are faster to execute on modern accelerators such as GPUs, leading to faster model training and inference.\nNonetheless, the high to low precision data type conversion process during quantization can lead to accuracy degradation, attributable to the inherent information loss. Report issue for preceding element\nWe now analyze the memory footprint of LLM training and then introduce the neural architecture of the side network, which reduces the inherent information loss and minimizes accuracy drop during quantization. Report issue for preceding element\nFor a given LLM with N \ud835\udc41 N italic_N layers, let y i = f i \u2062 ( W i , x i ) subscript \ud835\udc66 \ud835\udc56 subscript \ud835\udc53 \ud835\udc56 subscript \ud835\udc4a \ud835\udc56 subscript \ud835\udc65 \ud835\udc56 y_{i}=f_{i}(W_{i},x_{i}) italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) denotes the i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT transformer layer of the LLM, where x i subscript \ud835\udc65 \ud835\udc56 x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the input to the i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer (i.e., x i = y i \u2212 1 subscript \ud835\udc65 \ud835\udc56 subscript \ud835\udc66 \ud835\udc56 1 x_{i}=y_{i-1} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ).\nThe memory required during the training phase of the LLM predominantly comprises three main contributors: M1 ) weights of the LLM { W i } i = 1 N superscript subscript subscript \ud835\udc4a \ud835\udc56 \ud835\udc56 1 \ud835\udc41 \\{W_{i}\\}_{i=1}^{N} { italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT , M2 ) the optimizer state, which is threefold the size of the trainable parameters when employing the Adam optimizer (Kingma and Ba, 2014 ) (one for gradient and two for moments), and M3 ) the intermediate activations { y i \u2032 } i = 1 N superscript subscript superscript subscript \ud835\udc66 \ud835\udc56 \u2032 \ud835\udc56 1 \ud835\udc41 \\{y_{i}^{\\prime}\\}_{i=1}^{N} { italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT . The memory footprint of intermediate activations is related to model depth, width, and several training settings, e.g., batch size and sequence length.\nQLoRA reduces the memory footprint of an LLM\u2019s weights and optimizer states (M1 and M2) but fails to reduce intermediate activations (M3).\nWhen finetuning an LLM with a large batch size and/or long sequence length, the memory footprint of QLoRA increases significantly. However, using a small batch size results in long training time. Sung et\u00a0al. ( 2022 ) only reduces the memory footprint of intermediate activations (M3), thus it struggles to finetune a model with more than 3 billion parameters. Report issue for preceding element\nOur side network g \ud835\udc54 g italic_g serves as a lightweight version of the quantized LLM f \ud835\udc53 f italic_f . The hidden state and weight dimension of g \ud835\udc54 g italic_g are r \ud835\udc5f r italic_r times smaller than those of f \ud835\udc53 f italic_f , where r \ud835\udc5f r italic_r is the reduction factor.\nDuring the forward pass, the hidden state of the i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of the side network h g i subscript \u210e subscript \ud835\udc54 \ud835\udc56 h_{g_{i}} italic_h start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT is formulated by h g i 16 \u2062 b \u2062 i \u2062 t = ( 1 \u2212 \u03b2 i ) * d \u2062 o \u2062 w \u2062 n \u2062 s \u2062 a \u2062 m \u2062 p \u2062 l \u2062 e i \u2062 ( h f i 16 \u2062 b \u2062 i \u2062 t ) + \u03b2 i * h g i \u2212 1 16 \u2062 b \u2062 i \u2062 t , subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc54 \ud835\udc56 1 subscript \ud835\udefd \ud835\udc56 \ud835\udc51 \ud835\udc5c \ud835\udc64 \ud835\udc5b \ud835\udc60 \ud835\udc4e \ud835\udc5a \ud835\udc5d \ud835\udc59 subscript \ud835\udc52 \ud835\udc56 subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc53 \ud835\udc56 subscript \ud835\udefd \ud835\udc56 subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc54 \ud835\udc56 1 h^{16bit}_{g_{i}}=(1-\\beta_{i})*downsample_{i}(h^{16bit}_{f_{i}})+\\beta_{i}*h^%\n{16bit}_{g_{i-1}}, italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( 1 - italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) * italic_d italic_o italic_w italic_n italic_s italic_a italic_m italic_p italic_l italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) + italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT * italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , where h f i 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc53 \ud835\udc56 h^{16bit}_{f_{i}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT is the hidden state of the i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of f \ud835\udc53 f italic_f and can be computed using E.q. (3). The illustration of i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of our QST is shown in Figure 3 . Note that we use the output of the embedding layer and the downsampled embedding layer as h f 0 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc53 0 h^{16bit}_{f_{0}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and h g 0 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc54 0 h^{16bit}_{g_{0}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT . \u03b2 i = s \u2062 i \u2062 g \u2062 m \u2062 o \u2062 i \u2062 d \u2062 ( \u03b3 i ) subscript \ud835\udefd \ud835\udc56 \ud835\udc60 \ud835\udc56 \ud835\udc54 \ud835\udc5a \ud835\udc5c \ud835\udc56 \ud835\udc51 subscript \ud835\udefe \ud835\udc56 \\beta_{i}=sigmoid(\\gamma_{i}) italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_s italic_i italic_g italic_m italic_o italic_i italic_d ( italic_\u03b3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is a learned gate parameter of i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer, where \u03b3 i subscript \ud835\udefe \ud835\udc56 \\gamma_{i} italic_\u03b3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a learnable zero-initialized scalar. d \u2062 o \u2062 w \u2062 n \u2062 s \u2062 a \u2062 m \u2062 p \u2062 l \u2062 e i \ud835\udc51 \ud835\udc5c \ud835\udc64 \ud835\udc5b \ud835\udc60 \ud835\udc4e \ud835\udc5a \ud835\udc5d \ud835\udc59 subscript \ud835\udc52 \ud835\udc56 downsample_{i} italic_d italic_o italic_w italic_n italic_s italic_a italic_m italic_p italic_l italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the downsample module of the i t \u2062 h superscript \ud835\udc56 \ud835\udc61 \u210e i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer to reduce the hidden state dimension of f \ud835\udc53 f italic_f by r \ud835\udc5f r italic_r times. Prior work leverages linear projections to downsample (i.e., \u00d7 1 r absent 1 \ud835\udc5f \\times\\frac{1}{r} \u00d7 divide start_ARG 1 end_ARG start_ARG italic_r end_ARG ) the high-dimensional hidden states of f \ud835\udc53 f italic_f to the low-dimensional hidden states of g \ud835\udc54 g italic_g .\nHowever, an LLM typically comprises plenty of layers with substantially high-dimensional hidden states, particularly when the number of parameters exceeds 3 billion. Using linear projections to downsample involves a significant amount of trainable parameters, requiring a high memory footprint for the parameters and their optimizer states. For example, if the LLM has 24 layers, the dimension of its hidden state is 2048 and the reduction factor r \ud835\udc5f r italic_r is 4, the downsample module consumes about 50% of the overall trainable parameters. Report issue for preceding element\nTo address this problem, we leverage several different downsample methods, including LoRA He et\u00a0al. ( 2021 ) , Adapter Edalati et\u00a0al. ( 2022 ) , MaxPooling LeCun et\u00a0al. ( 1998 ) and AvgPooling.\nLoRA augments a linear projection through an additional factorized projection, which can be formulated as W = L 1 \u2062 L 2 \ud835\udc4a subscript \ud835\udc3f 1 subscript \ud835\udc3f 2 W=L_{1}L_{2} italic_W = italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , where W \u2208 \u211d d i \u2062 n \u00d7 d o \u2062 u \u2062 t \ud835\udc4a superscript \u211d subscript \ud835\udc51 \ud835\udc56 \ud835\udc5b subscript \ud835\udc51 \ud835\udc5c \ud835\udc62 \ud835\udc61 W\\in\\mathbb{R}^{d_{in}\\times d_{out}} italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , L 1 \u2208 \u211d d i \u2062 n \u00d7 d r subscript \ud835\udc3f 1 superscript \u211d subscript \ud835\udc51 \ud835\udc56 \ud835\udc5b subscript \ud835\udc51 \ud835\udc5f L_{1}\\in\\mathbb{R}^{d_{in}\\times d_{r}} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and L 2 \u2208 \u211d d r \u00d7 d o \u2062 u \u2062 t subscript \ud835\udc3f 2 superscript \u211d subscript \ud835\udc51 \ud835\udc5f subscript \ud835\udc51 \ud835\udc5c \ud835\udc62 \ud835\udc61 L_{2}\\in\\mathbb{R}^{d_{r}\\times d_{out}} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .\nAdapter is similar to LoRA but introduces an extra non-linear function between L 1 subscript \ud835\udc3f 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and L 2 subscript \ud835\udc3f 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .\nUsing LoRA or Adapter can reduce the ratio of the trainable parameters of these downsample modules from 56% to 8%.\nMaxPooling and AvgPooling do not introduce extra trainable parameters. We empirically demonstrate that the Adapter performs the best in our experiments. Report issue for preceding element\nFinally, we upsample (i.e., \u00d7 r absent \ud835\udc5f \\times r \u00d7 italic_r ) from low-dimensional hidden states of g \ud835\udc54 g italic_g to high-dimensional hidden states of f \ud835\udc53 f italic_f .\nPrior works have only evaluated side tuning methods (e.g., LST) in the context of classification tasks. We observe that LST suffers from repetition when generating long-sequence texts, which renders it incapable of producing extensive and high-quality texts. This limitation stems from LST\u2019s utilization of the hidden states of the side network for prediction, which causes an initialization position far removed from the pre-trained model at the onset of finetuning.\nAs illustrated in MEFT (Liao et\u00a0al., 2023 ) , the initialization step emerges as a critical factor influencing the efficacy of fine-tuning methods.\nTo resolve this issue, QST combines the output of the LLM\u2019s last layer (i.e., h f N 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc53 \ud835\udc41 h^{16bit}_{f_{N}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) with the output of the side network (i.e., h g N 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc54 \ud835\udc41 h^{16bit}_{g_{N}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT ), and sends the weighted sum h N 16 \u2062 b \u2062 i \u2062 t = \u03b1 \u2062 h f N 16 \u2062 b \u2062 i \u2062 t + ( 1 \u2212 \u03b1 ) \u2062 h g N 16 \u2062 b \u2062 i \u2062 t subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 \ud835\udc41 \ud835\udefc subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc53 \ud835\udc41 1 \ud835\udefc subscript superscript \u210e 16 \ud835\udc4f \ud835\udc56 \ud835\udc61 subscript \ud835\udc54 \ud835\udc41 h^{16bit}_{N}=\\alpha h^{16bit}_{f_{N}}+(1-\\alpha)h^{16bit}_{g_{N}} italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = italic_\u03b1 italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( 1 - italic_\u03b1 ) italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT to the LM head, where \u03b1 \ud835\udefc \\alpha italic_\u03b1 is a learnable parameter. We initialize \u03b1 \ud835\udefc \\alpha italic_\u03b1 to 1 to preserve the starting point from the pre-trained model at the beginning of finetuning, which is consistent with the initialization of LoRA.\nWith this design, when switching across different downstream tasks, QST can fulfil the necessary adjustments by altering the side network alone, and thus obviating the need for redeploying the LLM. Report issue for preceding element\nQST only updates the parameters of the side network g \ud835\udc54 g italic_g , but not the 4-bit weights in the LLM f \ud835\udc53 f italic_f . Unlike QLoRA, the calculation of the gradient \u2202 L \u2202 g \ud835\udc3f \ud835\udc54 \\frac{\\partial L}{\\partial g} divide start_ARG \u2202 italic_L end_ARG start_ARG \u2202 italic_g end_ARG does not entail the calculation of \u2202 L \u2202 f \ud835\udc3f \ud835\udc53 \\frac{\\partial L}{\\partial f} divide start_ARG \u2202 italic_L end_ARG start_ARG \u2202 italic_f end_ARG , thus avoiding the extensive computational costs of performing backpropagation on f \ud835\udc53 f italic_f , which ultimately reduces the memory footprint of intermediate activations and speeds up finetuning. Report issue for preceding element\nIn summary, QST leverages a 4-bit data type to store an LLM\u2019s model weights, thus reducing the memory footprint of weights (M1). In addition, QST leverages a 16-bit computation data type for the forward pass and backpropagation computation and only computes the gradient of weights in g \ud835\udc54 g italic_g (M3). Finally, QST leverages several factorized projection and gradient-free downsample methods to reduce the trainable parameters (M2). These techniques together allow QST to reduce the memory requirement for all three factors, resulting in fast and memory-efficient finetuning with a nearly 1% performance drop. Report issue for preceding element\nIn this section, we empirically validate the effectiveness of our QST method by examining its performance for LLMs with different types (e.g., OPT and LLaMA 2), sizes (from 1.3B to 70B), and benchmarks. Report issue for preceding element\nDatasets. We evaluate the performance of QST and several baselines on natural language understanding (NLU) and natural language generation tasks. For NLU experiments, we use the GLUE Wang et\u00a0al. ( 2018 ) (General Language Understanding Evaluation) and MMLU Hendrycks et\u00a0al. ( 2020 ) (Massively Multitask Language Understanding) benchmarks. The GLUE benchmark provides a comprehensive evaluation of models across a range of linguistic tasks. These tasks encompass linguistic acceptability as examined in CoLA Warstadt et\u00a0al. ( 2019 ) , sentiment analysis as portrayed in SST2 Socher et\u00a0al. ( 2013 ) , tasks probing similarity and paraphrase distinctions such as MRPC Dolan and Brockett ( 2005 ) , QQP Iyer ( 2017 ) , and STS-B Cer et\u00a0al. ( 2017 ) , in addition to natural language inference tasks including MNLI Williams et\u00a0al. ( 2017 ) , QNLI Rajpurkar et\u00a0al. ( 2016 ) , and RTE Bentivogli et\u00a0al. ( 2009 ) . We report accuracy on MNLI, QQP, QNLI, SST-2, MRPC, and RTE, Pearson correlation coefficients on SST-B, and Mathews correlation coefficients Matthews ( 1975 ) on CoLA. The MMLU benchmark consists of 57 tasks including elementary mathematics, US history, computer science, law, and more. We report the average 5-shot test accuracy on the 57 tasks. Report issue for preceding element\nModels. We use decoder-only LLMs such as the OPT series (OPT-1.3B, OPT-2.7B, OPT-6.7B, OPT-13B, OPT-30B, and OPT-66B) and the LLaMA-2 series (LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B). Report issue for preceding element\nBaselines. We compare QST with QLoRA Dettmers et\u00a0al. ( 2023 ) , LST Sung et\u00a0al. ( 2022 ) , LoRA He et\u00a0al. ( 2021 ) , and Adapter Houlsby et\u00a0al. ( 2019 ) . Note that we only compare LST, LoRA, and Adapter when the model size is less than 3B since their memory footprint of weights can be excessively huge beyond that. Report issue for preceding element\nImplementation. We set the reduction factor r \ud835\udc5f r italic_r to 16 by default.\nWe use Adapter as the downsample module, a linear layer as the upsample module, and set the rank of the Adapter to 16.\nWe use the NF4 data type to store the weights of the LLM and bfloat16 as the data type for computation.\nWe adopt the same parameters reported in QLoRA, LST, LoRA, and Adapter to construct the baselines.\nOther hyperparameters are specified in Appendix A and Appendix B . We run each experiment three times under different random seeds and report the average performance.\nWe conduct all the experiments using Pytorch Paszke et\u00a0al. ( 2017 ) and HuggingFace library Wolf et\u00a0al. ( 2019 ) on 4 NVIDIA RTX A5000 GPUs, each with 24GB memory. Report issue for preceding element\nTable 1 shows the performance of different methods on the GLUE benchmark.\nOverall, QST achieves the lowest memory footprint among all methods while attaining competent accuracy.\nParticularly, for relatively small models (i.e., OPT-1.3B and OPT-2.7B), QST reduces the memory footprint by around 2 \u00d7 \\times \u00d7 compared with QLoRA, LoRA, and Adapter, while achieving comparable accuracy. Compared with LST, QST reduces the memory requirement by 3.2GB and 6.3GB for finetuning OPT-1.3B and OPT-2.7B. QST also reduces the trainable parameters by around 10 \u00d7 \\times \u00d7 and 5 \u00d7 \\times \u00d7 compared with QLoRA and the other baselines, respectively. Report issue for preceding element\nFor larger models such as OPT-6.7B, we focus on comparing QST with QLoRA. This is because QLoRA has similar accuracy with the other baselines, but LoRA, Adapter, and LST all have excessively huge memory footprints of weights when it comes to finetuning OPT-6.7B 2 2 2 QLoRA can leverage gradient accumulation to finetune with a batch size of 16 while guaranteeing an affordable memory footprint. .\nCompared with QLoRA, QST reduces the memory footprint and trainable parameters by 2.3 \u00d7 \\times \u00d7 and 5.5 \u00d7 \\times \u00d7 , while only introducing a 1.3% accuracy drop. Report issue for preceding element\nThe experiment results of the MMLU benchmark are shown in Table 2. We set the batch size to 4 and the sequence length to 384. We use the Alpaca dataset (Taori et\u00a0al., 2023 ) to finetune both QLoRA and QST. We compare QST with QLoRA on accuracy and memory requirement over OPT-1.3B, OPT-2.7B, OPT-6.7B, OPT-13B, OPT-30B, OPT-66B, LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. QST improves the accuracy by 0.1% on average while reducing the memory footprint by 1.8 \u00d7 \\times \u00d7 compared with QLoRA. Particularly, QST yields an enhancement of 2.1% in accuracy over QLoRa when finetuning LLaMA-2-13B. When finetuning the OPT-2.7B, OPT-6.7B, and OPT-13B models, QST achieves 0.3%, 0.6%, and 0.3% accuracy improvements, respectively. Report issue for preceding element\nEffects of batch size. Figure 5 LABEL:sub@bs_compare illustrates the effects of batch size for different methods. We use LLaMA-2-70B as the LLM and set the sequence length to 512. While the memory footprint of all methods increases as the batch size increases, QST achieves the lowest memory footprint among all, regardless of the batch size.\nParticularly, the memory footprint of QST is only one-third of LoRA and Adapter.\nBesides, the memory footprint of both QST and LST grows less drastically than QLoRa, Adapter, and LoRa as the batch size increases.\nThis is because both LST and QST use side tuning to reduce the hidden dimension of the intermediate activations, thereby alleviating the growth of memory footprint induced by intermediate activations. QST also achieves an additional reduction of approximately 100GB in memory footprint compared to LST, thanks to the 4-bit quantization design that effectively compresses the memory footprint of the weights and well design of the downsample modules to reduce the optimizer states. Report issue for preceding element\nEffects of the model size. Figure 5 LABEL:sub@ms_compare shows the effects of the total model bits on different methods. We use the OPT model series and\nset the batch size to 4. Due to the 4-bit quantization, QST and QLoRA reduce the memory footprint compared with the other baselines.\nThe memory footprint gap further widens as the model size increases. Besides, QST achieves around 2 times reduction in memory footprint compared with QLoRA thanks to its small volume of trainable parameters and intermediate activations. Report issue for preceding element\nEffects of sequence length. Figure 5 LABEL:sub@seq_compare shows the effects of sequence length on different methods. We use LLaMA-2-70B and set the batch size to 4. Similar to the effect of batch size, LST and QST alleviate the growth rate of memory footprint of intermediate activations, while QST further achieves around 100GB reduction in memory footprint compared with LST. Report issue for preceding element\nTable 3 shows the training throughput of different methods, measured by FLOPS per token (the lower the better), on LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. While the FLOPS per token of all methods increases as the model size grows, QST achieves the lowest FLOPS per token among all. Particularly, QST achieves around 2.5 \u00d7 \\times \u00d7 speed up compared with the baselines. LST suffers from the highest FLOPS per token. The FLOPS per token of QLoRA is slightly higher than LoRA and Adapter since QLoRA adds more LoRA components. Report issue for preceding element\nEffects of reduction factor r \ud835\udc5f r italic_r . We conduct experiments using LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B to verify the effects of reduction factor r \ud835\udc5f r italic_r (from 2 to 64) on memory footprint, MMLU accuracy, and throughput. We set the batch size to 4 and the sequence length to 384.\nThe MMLU accuracy changes slightly as r \ud835\udc5f r italic_r varies as shown in Figure 5a . QST achieves the best accuracy of finetuning LLaMA-2-7B and LLaMA-2-13B when r \ud835\udc5f r italic_r is set to 16.\nAs shown in Figure 5b and 5c , the memory footprint and the FLOPS per token decrease drastically when r \ud835\udc5f r italic_r varies from 2 to 16 for finetuning all the models. The memory footprint and the FLOPS per token decrease slightly when r \ud835\udc5f r italic_r varies from 16 to 64.\nTherefore, we use r \ud835\udc5f r italic_r to 16 in our experiments as default. Report issue for preceding element\nEffects of 4-bit data types. We evaluate two 4-bit data types: FP4 and NF4 using the LLaMA-2 model series and the MMLU benchmark. As shown in Table 4, NF4 improves the average accuracy by about 0.8% compared with FP4. Therefore, we use NF4 as the default 4-bit data type in our experiments. Report issue for preceding element\nEffects of computation data types. We analyze the effects of two computation data types: BF16 (results shown in Table 1) and FP16 (results shown in Table 5). As can be seen, QST retains similar results using FP16 and BF16. On the other hand, QLoRA is unstable using FP16 as the computation data type. We finetune OPT-6.7B on the GLUE benchmark and discover that QLoRA fails to finetune on the MRPC and QNLI datasets. We run each dataset under three different random seeds and QLoRA fails on two of them. Report issue for preceding element\nEffects of downsample modules. We conduct experiments on different downsample modules: Linear, LoRA, Adapter, MaxPooling, and AvgPooling using LLaMA-2-7B and the MMLU benchmark. As shown in Table 6, using Adapter as the downsample module achieves the best performance among all baselines, and reduces the trainable parameters and memory footprint. Report issue for preceding element\nWe conduct experiments on Chatbot performance using MT-benchmark (Zheng et\u00a0al., 2023 ) . MT-benchmark is a set of challenging multi-turn open-ended questions for evaluating the chat assistant\u2019s performance in writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities categories. In our experiments, we use GPT-4 to act as judges and assess the quality of the responses of the model finetuned by QLoRA and QST. We finetune LLaMA-2-70B using a variant of OASST1 (Dettmers et\u00a0al., 2023 ) . Table 7 shows the experiment results of QLoRA and QST on the total training time, memory footprint, and the average MT-Bench score over 8 categories. QST speeds up the training by 3.2 \u00d7 \\times \u00d7 and reduces memory footprint by 1.7 \u00d7 \\times \u00d7 , with even an improved score of 0.46 compared with QLoRA. Notably, QST\u2019s chatbot performance outperforms the original LLaMA-2-70B, achieving an improvement of 0.21. Report issue for preceding element\nFigure 6 compares QST, QLoRA, and the original LLaMA-2-70B in different categories using MT-Bench. Out of the 8 categories, QST-70B outperforms QLoRA and LLaMA-2-70B in four of them: STEM, Extraction, Coding, and Roleplay. In the Humanities category, all models demonstrate comparable proficiency. For the remaining three categories, QLoRA takes the lead in Reasoning and Writing, and the original LLaMA-2-70B shows superior performance in Mathematics. Report issue for preceding element\nWe present the results of several case studies that assess the performance of QST-70B across various domains. While this analysis may not be exhaustive, as controlling for all variables in such a concise qualitative study exceeds our scope, our endeavor is to present a selection of representative samples that, in our belief, add meaningful insights of QST to this paper. Report issue for preceding element\nQST can generate high-quality writing samples on various domains. For example: Report issue for preceding element\nUser Write a brief summary of the latest advancements in artificial intelligence and their potential impacts on the industry. \\speak QST-70B 2023 has seen significant advancements in artificial intelligence, particularly in the areas of machine learning, computer vision, and natural language processing. These advancements have potential impacts on various industries, including: Report issue for preceding element\n1. Healthcare: AI-powered diagnosis, drug discovery, and precision medicine are transforming patient care. An AI-assisted cancer diagnosis can predict patient outcomes and suggest personalized treatments. AI-driven drug discovery can accelerate the development of new medicines. Report issue for preceding element\n\u2026 Report issue for preceding element\n5. Energy: AI can optimize energy consumption, predict energy demand, and improve the efficiency of renewable energy sources. AI-based smart grids can optimize energy distribution and reduce waste. AI-driven energy management systems can help buildings and homes reduce energy consumption and costs. \\speak User Create a short story about a time traveler visiting ancient Egypt. \\speak QST-70B A time traveler arrived in ancient Egypt, curious about the bustling society he had heard so much about. As he wandered the desert sands, he stumbled upon a construction site where workers were building a colossal pyramid. He marveled at the sheer size of the edifice and decided to approach one of the workers. \u201cExcuse me,\u201d the time traveler said. \u201cWhat\u2019s the purpose of this structure?\u201d Report issue for preceding element\nThe worker gave him a perplexed look. \u201cYou must be a foreigner. Everyone knows that we build pyramids to bury our pharaohs. These magnificent structures ensure their safe passage into the afterlife.\u201d Report issue for preceding element\nFascinated, the time traveler asked, \u201cWhat\u2019s the secret to building such colossal structures?\u201d Report issue for preceding element\n\u201cIt\u2019s simple,\u201d the worker replied with a smile. \u201cWe work together as one. Each of us has a specific task, and we combine our efforts to create something grand.\u201d \u2026 Report issue for preceding element\nQST-70B can easily answer questions such as \u201cWhat is the capital of France?\". For example, Report issue for preceding element\nUser What is the capital of France? \\speak QST-70B The capital of France is Paris. Report issue for preceding element\nQST-70B can still answer some questions that are obscure: Report issue for preceding element\nUser When was the painter who created the \u2018Mona Lisa\u2019 born? \\speak QST-70B The Italian artist who painted the \u2018Mona Lisa,\u2019 Leonardo da Vinci, was born on April 15, 1452. Report issue for preceding element\nQST-70B gave an incorrect answer to the following question as mentioned in Wei et\u00a0al. ( 2022 ); Zheng et\u00a0al. ( 2023 ) : Report issue for preceding element\nUser In what year was the singer who popularized \u2018I\u2019ll Keep the Lovelight Burning\u2019 born? \\speak QST-70B The singer who popularized \u201cI\u2019ll Keep the Lovelight Burning\u201d is Dolly Parton. Dolly Parton was born on January 19, 1946. Report issue for preceding element\nDolly Parton is a very famous country music singer, born on January 19, 1946, but she was not the singer who made the song \u201cI\u2019ll Keep the Lovelight Burning\u201d popular.\nGPT-4\u2019s answer is uncertain about the exact singer who popularized \u201cI\u2019ll Keep the Lovelight Burning\u201d. Report issue for preceding element\nThe performance of QST in mathematical tasks varies depending on the nature of the questions.\nQST can correctly answer basic arithmetic questions. For example: Report issue for preceding element\nUser What is the sum of 15 and 27? \\speak QST-70B 15 plus 27 is equal to 42. \\speak User What is the multiply of 3 and 45? \\speak QST-70B 3 multiplied by 45 is 135. Report issue for preceding element\nQST can give the correct answers to percent calculations. For example: {dialogue} \\speak User If a shirt costs $60 and is on sale for 25% off, what is the sale price? \\speak QST-70B Great question! Let\u2019s break down the process to find the sale price: Report issue for preceding element\n1. Calculate 25% of the original price by multiplying $60 by 0.25, which gives us $15. Report issue for preceding element\n2. Subtract $15 from the original price of $60 to find the sale price: $60 - $15 = $45. Report issue for preceding element\nTherefore, the sale price of the shirt would be $45. Report issue for preceding element\nSurprisingly, QST is also capable of leveraging the theory of mind to break down the process and calculation for math problems step by step. QST can correctly demonstrate the step-by-step process of solving algebraic equations and geometry problems. For example: Report issue for preceding element\nUser Solve the equation for x: 2x + 3 = 19. \\speak QST-70B 2x + 3 = 19, Report issue for preceding element\n2x = 19 - 3, Report issue for preceding element\n2x = 16, Report issue for preceding element\nx = 16/2, Report issue for preceding element\nx = 8. \\speak User A right triangle has one leg that is 8 cm long and another leg that is 15 cm long. What is the length of the hypotenuse? \\speak QST-70B Great question! The Pythagorean theorem is needed to solve this problem. Here is how to go about it: Report issue for preceding element\nc^2 = 8^2 + 15^2 Report issue for preceding element\nc^2 = 64 + 225 Report issue for preceding element\nc^2 = 289 Report issue for preceding element\nNow, to find the hypotenuse (c), we take the square root of both sides:\nc = 289 289 \\sqrt{289} square-root start_ARG 289 end_ARG , c = 17.\nThus, the length of the hypotenuse of the right triangle is approximately 17 cm. Report issue for preceding element\nCalculate the integral of the function f(x) = x^2 from x = 0 to x = 3. Report issue for preceding element\nSimilar to Dettmers et\u00a0al. ( 2023 ) , QST may make mistakes on some questions. For example: Report issue for preceding element\nUser Please factorize 1833. \\speak QST-70B 1833 = 33 \u00d7 55. Report issue for preceding element\nApparently, 1833 can not be factorized as 33 \u00d7 55, the correct answer is 3 \u00d7 17 \u00d7 43. We still get the wrong answer if we ask QST to calculate step by step: Report issue for preceding element\nUser Please factorize 1833 step by step. \\speak QST-70B 1833 can be factored as follows: 1833 = 3 \u00d7 311. Here\u2019s how: 1833 = 3 \u00d7 3 \u00d7 111, 3 \u00d7 3 = 9, 9 \u00d7 111 = 1833. So, the prime factorization of 1833 is 1833 = 3 \u00d7 3 \u00d7 111. Report issue for preceding element\nWhether QST can give correct answers to logical reasoning math problems depends. Sometimes, a more instructive input prompt, e.g., prompt it to analyze the problem step by step, may lead to the correct answers. Take a question from Zheng et\u00a0al. ( 2023 ) as an example: Report issue for preceding element\nUser Some people got on a bus at the terminal. At the first bus stop, half of the people got down and 4 more people got in. Then at the second bus stop, 6 people got down and 8 more got in. If there were a total of 25 people heading to the third stop, how many people got on the bus at the terminal? \\speak QST-70B 20 people got on the bus at the terminal. \\speak User Please analyze it in detail. \\speak QST-70B To find the total number of people who boarded the bus at the terminal, we\u2019ll analyze the information step by step: Report issue for preceding element\n1. Half of the people who boarded the bus at the terminal got down at the first bus stop. This means if x people boarded the bus at the terminal, x/2 people got down at the first stop. Report issue for preceding element\n2. 4 more people got on the bus at the first stop. So, the total number of people on the bus after the first stop is x/2 + 4. Report issue for preceding element\n3. At the second bus stop, 6 people got down. Now, the number of people on the bus is x/2 + 4 - 6. Report issue for preceding element\n4. 8 more people got on the bus at the second stop. The total number of people on the bus heading to the third stop is x/2 + 4 - 6 + 8 = x/2 + 12 - 6 = x/2 + 6. Report issue for preceding element\n5. There were a total of 25 people heading to the third stop. So, x/2 + 6 = 25. Report issue for preceding element\n6. Solving for x, we get: Report issue for preceding element\nx/2 + 6 = 25, Report issue for preceding element\nx/2 = 25 - 6, Report issue for preceding element\nx/2 = 19, Report issue for preceding element\nx = 2 x 19, Report issue for preceding element\nx = 38. Report issue for preceding element\nSo, there were 38 people who boarded the bus at the terminal. Report issue for preceding element\nThe correct answer is 38. QST generates the correct answer for this question after being instructed to analyze the problem step by step. Report issue for preceding element\nQST is good at generating codes for simple algorithms. Take a question in Zheng et\u00a0al. ( 2023 ) as an example: Report issue for preceding element\nUser Write a C++ program to find the nth Fibonacci number using recursion. Report issue for preceding element\nQST-70B Here\u2019s an example of a C++ program to find the nth Fibonacci number using recursion: Report issue for preceding element \u2b07 1 int fibonacci ( int n ) { 2 if ( n == 0 || n == 1) { 3 return n ; 4 } 5 else { 6 return fibonacci ( n -1) + fibonacci ( n -2); 7 } 8 } This program uses the recursive formula of the Fibonacci sequence, where the nth Fibonacci number is the sum of the previous two numbers. The program also includes a base case, where the function directly returns the values for n=0 and n=1 to avoid an infinite loop. Report issue for preceding element\nUser Implement a program to find the common elements in two arrays without using any extra data structures. \\speak QST-70B To find common elements in two arrays without using any extra data structures, you can follow these steps: Report issue for preceding element\n1. Sort both arrays. Report issue for preceding element\n2. Loop through the first array and for each element, loop through the second array. Report issue for preceding element\n3. If you find the same element in both arrays, store it in a variable or an array. Report issue for preceding element\n4. At the end of both loops, the common elements will be in the variable or array. Report issue for preceding element\nHere\u2019s an example of pseudo code for the above-described method: Report issue for preceding element \u2b07 1 def findCommonElements ( arr1 , arr2 ) 2 1. sort ( arr1 ); 3 2. sort ( arr2 ); 4 3. commonElements = []; 5 4. for ( var i = 0; i < arr1 . length ; i ++) 6 for ( var j = 0; j < arr2 . length ; j ++) 7 if ( arr1 [ i ] === arr2 [ j ]) 8 commonElements . push ( arr1 [ i ]); 9 5. return commonElements ; This solution time complexity is O(n^2), where n is the length of the arrays. Report issue for preceding element\nPlease note that this is one of the ways to find the common elements, and it is not the most efficient algorithm, especially for large datasets. If you need to handle large datasets, it is better to use a more efficient algorithm like sorting both arrays and then using two pointers to find the common elements. Report issue for preceding element\nThe generated algorithm is current. Although we claim in the question that do not use extra data structure. QST still uses an array \u201ccommonElements\u201d. For complex algorithms (e.g. dynamic programming), QST can make mistakes. Report issue for preceding element\nQST is good at roleplay. For example: Report issue for preceding element\nUser Suppose that you are a doctor. A patient complains of frequent headaches. What would you say? Report issue for preceding element\nQST-70B\nAs a doctor, if a patient were to complain of frequent headaches, I would ask several questions to better understand their symptoms and medical history in order to provide an accurate diagnosis and recommend appropriate treatment. Report issue for preceding element\nHere\u2019s an example conversation: Report issue for preceding element\nAssistant: Good morning, how can I assist you today? Report issue for preceding element\nPatient: Hi, doctor. I\u2019ve been experiencing frequent headaches, and I\u2019m not sure why. They\u2019re really frustrating and disrupting my daily life. Report issue for preceding element\nAssistant: Sorry to hear that. Can you tell me more about your headaches? What\u2019s the location and duration of the pain? Have you noticed any patterns or triggers? Report issue for preceding element\nPatient: Well, the pain is usually on both sides of my head and can last anywhere from a few hours to the entire day. It\u2019s been happening on and off for the past few months. Sometimes, I\u2019ll feel a dull ache behind my eyes or sharp pains in the back of my neck. I can\u2019t seem to connect the headaches to any specific food or activity. Report issue for preceding element\nAssistant: Thank you for that information. Have you experienced any other symptoms lately, such as blurred vision, nausea, or dizziness? Report issue for preceding element\nPatient: No, nothing like that. Just the headaches. Report issue for preceding element\nAssistant: Let\u2019s start with some general screening questions. Do you have a history of allergies, or are there any known allergies in your family?\nPatient: I don\u2019t have any allergies that I know of, but my dad has seasonal allergies. Report issue for preceding element\nAssistant: Great. How about any previous illnesses, surgeries, or injuries?\nPatient: I had a tonsillectomy when I was a kid and a sprained ankle a few years ago, but that\u2019s it. Report issue for preceding element\nAssistant: Thanks. Report issue for preceding element\nAnother example: Report issue for preceding element\nUser Suppose you are a mathematician and poet. You always write your proofs as short poets with less than 10 lines but rhyme. Prove the square root of 2 is an irrational number. Report issue for preceding element\nQST-70B Pi is an odd thing, oh so very strange Report issue for preceding element\nIt cannot be expressed by a simple range Report issue for preceding element\nAn irrational number, it just won\u2019t behave Report issue for preceding element\nIt\u2019s a math mystery, that\u2019s hard to enslave Report issue for preceding element\nIts decimal points stretch on and on Report issue for preceding element\nNever repeating, never done Report issue for preceding element\nA constant that\u2019s found in every sphere Report issue for preceding element\nMath\u2019s magic, that\u2019s crystal clear Report issue for preceding element\nQST is good at STEM. For example: Report issue for preceding element\nUser Explain the basic principle of quantum computing and how it differs from classical computing. Report issue for preceding element\nQST-70B Quantum computing is a different approach to computing that uses the properties of quantum mechanics to perform operations on data. Unlike classical computing, which uses bits to represent data as either a 0 or a 1, quantum computing uses quantum bits, or qubits, which can exist as both a 0 and a 1 at the same time. This allows quantum computers to process a much larger number of possible solutions simultaneously, making them well-suited for certain types of complex problems. Another key difference is that quantum computers use quantum entanglement, which allows qubits to become \u201centangled\u201d so that the state of one qubit can instantly affect the state of another, regardless of the distance between them. This property allows quantum computers to perform certain types of calculations much faster than classical computers. Quantum computing is still a developing field, and there are many technical challenges to overcome before it can be widely adopted. Report issue for preceding element\nAnother example: Report issue for preceding element\nUser Explain the basic principle of quantum computing and how it differs from classical computing. Report issue for preceding element\nQST-70B Key Challenges of Manned Space Exploration to Mars: Report issue for preceding element\n1. Radiation Exposure: Mars lacks a strong magnetic field to protect against harmful cosmic and solar radiation, which poses a significant health risk for human travelers. Report issue for preceding element\n2. Isolation and Psychological Stress: A trip to Mars takes about 6-9 months, followed by a long-term stay on the planet. This prolonged isolation can lead to psychological strain, including depression, anxiety, and sleep disorders. Report issue for preceding element\n3. Muscle and Bone Loss: In microgravity environments, the human body experiences muscle and bone atrophy due to a lack of resistance and weight-bearing activities. Prolonged exposure can result in severe health consequences. Report issue for preceding element\n4. Technological and Communication Limitations: The vast distance between Earth and Mars creates communication challenges, making real-time communication and technological support difficult. Equipment failures and repair needs may require prolonged troubleshooting and resolution. Report issue for preceding element\nBenefits of Manned Space Exploration to Mars: Report issue for preceding element\n1. Scientific Discovery: Humans can perform more intricate and nuanced experiments and gather more comprehensive data compared to robotic missions, including the search for life on Mars. Report issue for preceding element\n2. Technological Advancement: The challenges of sending humans to Mars drive innovation in areas such as propulsion, life support systems, radiation shielding, and in-situ resource utilization. Report issue for preceding element\n3. Inspiration and Educational Value: Mars missions captivate the public and inspire the next generation of scientists and engineers, fostering a greater interest in STEM education and motivating people to pursue careers in these fields. Report issue for preceding element\n\u2026 Report issue for preceding element\nQST can accurately extract information from the input text. For example: {dialogue} \\speak User Read the following descriptions of historical events and extract the year, location, and main figure involved. Present the information in a tabular JSON format. a) In 1963, in Washington D.C., Martin Luther King Jr. delivered his famous \"I Have a Dream\" speech. b) The first man to walk on the moon was Neil Armstrong in 1969, during the Apollo 11 mission. c) In 1989, the Berlin Wall, a symbol of the Cold War, was torn down by the citizens of East and West Berlin. Report issue for preceding element\nQST-70B { Report issue for preceding element\n\"a\": { Report issue for preceding element\n\"year\": 1963, Report issue for preceding element\n\"location\": \"Washington D.C.\", Report issue for preceding element\n\"main figure\": \"Martin Luther King Jr.\" Report issue for preceding element\n}, Report issue for preceding element\n\"b\": { Report issue for preceding element\n\"year\": 1969, Report issue for preceding element\n\"location\": \"The Moon\", Report issue for preceding element\n\"main figure\": \"Neil Armstrong\" Report issue for preceding element\n}, Report issue for preceding element\n\"c\": { Report issue for preceding element\n\"year\": 1989, Report issue for preceding element\n\"location\": \"Berlin\", Report issue for preceding element\n\"main figure\": \"The citizens of East and West Berlin\" Report issue for preceding element\n} Report issue for preceding element\n} Report issue for preceding element\nAnother example: Report issue for preceding element\nUser Interpret the subtle sentiments in these TV show reviews and categorize them. Present the results in a structured YAML format. Report issue for preceding element\na) \u201cThe series finale, although not universally satisfying, did tie up several key storylines in a somewhat unconventional manner.\u201d Report issue for preceding element\nb) \u201cThis season\u2019s narrative arc is less compelling than previous ones, but the stellar performances keep it watchable.\u201d Report issue for preceding element\nc) \u201cThe latest episode took a different turn, which might not align with everyone\u2019s taste but certainly adds a unique flavor to the overall series.\u201d Report issue for preceding element\nQST-70B a) Ambivalent b) Neutral c) Positive Report issue for preceding element\nQST is good at answering questions about humanities. For example: Report issue for preceding element\nUser Analyze the character development of Elizabeth Bennet in Jane Austen\u2019s \u201cPride and Prejudice.\u201d Discuss how her interactions with other characters contribute to the central themes of the novel. Report issue for preceding element\nQST-70B Elizabeth Bennet is the protagonist of Jane Austen\u2019s \u201cPride and Prejudice,\u201d and her character development plays a crucial role in the novel\u2019s exploration of themes such as love, class, and social hierarchy. Report issue for preceding element\nAt the beginning of the novel, Elizabeth is portrayed as an intelligent, witty, and strong-willed young woman who rejects societal norms and expectations. Her interactions with other characters, particularly Mr. Darcy and Mr. Wickham, serve to challenge her beliefs and values. Report issue for preceding element\nThrough her conversations and debates with Mr. Darcy, Elizabeth begins to question her initial prejudices against him and learns to appreciate his integrity and sincerity. Their interactions also highlight the central theme of pride and prejudice, as they both struggle to overcome their respective biases. Report issue for preceding element\nElizabeth\u2019s interactions with Mr. Wickham, on the other hand, serve to illustrate her tendency to make hasty judgments and her willingness to believe in someone\u2019s flaws. Her later realization of his true character serves as a lesson for both her and the reader. Report issue for preceding element\nMoreover, Elizabeth\u2019s relationships with her family members and friends add depth to her character and the central themes. Her sister Jane\u2019s kindness and caution provide a contrast to Elizabeth\u2019s impulsiveness, while her best friend Charlotte\u2019s practicality and contentment with her situation demonstrate the societal pressure on women to marry for security. Report issue for preceding element\nIn conclusion, Elizabeth Bennet\u2019s character development is integral to the central themes of \u201cPride and Prejudice.\u201d Her interactions with other characters challenge her beliefs, illustrate the danger of pride and prejudice, and emphasize the importance of understanding and respect in relationships. Report issue for preceding element\nAnother example: Report issue for preceding element\nUser Critique the use of color and light in Vincent Van Gogh\u2019s \u201cStarry Night\u201d and discuss how these elements contribute to the overall emotional impact of the painting. Report issue for preceding element\nQST-70B \u201cStarry Night\u201d by Vincent Van Gogh is a masterpiece that showcases the artist\u2019s unique perspective on color and light. The painting\u2019s vibrant and dynamic use of color creates a powerful emotional response in the viewer, drawing them into a swirling vortex of blue, yellow, and green. The painting\u2019s composition revolves around the interaction between light and dark. The central cypress tree stands tall, its branches stretching toward the sky, while the darker, moonlit clouds in the background form a swirling pattern that echoes the movement of the tree\u2019s branches. This juxtaposition of light and dark creates a sense of energy and dynamism, pulling the viewer\u2019s eye between the different sections of the painting. Van Gogh\u2019s use of color further accentuates the emotional impact of the painting. The stars in the night sky are depicted in bold, swirling brushstrokes of yellow and white, which contrast with the darker blues and greens of the surrounding sky. This stark contrast creates a sense of depth and dimensionality as if the stars are bursting forth from the canvas. The yellow and orange hues of the stars and moon also evoke feelings of hope and joy, capturing the sense of wonder and awe that comes from gazing up at a starry night sky. In addition, the artist\u2019s deliberate brushwork adds texture and a tangible quality to the painting. The thick brushstrokes and heavy impasto give the painting a raised, three-dimensional feel as if the stars and trees are reaching out to touch the viewer. This tactile quality intensifies the emotional connection between the painting and the viewer, drawing them into the swirling vortex of light and color. Report issue for preceding element\nIn this paper, we propose Quantized Side Tuing (QST), a novel fast and memory-efficient finetuning framework. QST operates through a dual-stage process: first, QST quantizes the LLM into 4-bit to reduce the memory footprint of the weights in LLM; then QST introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. QST can significantly reduce the memory footprint of LLM finetuning compared to existing approaches. In particular, experiments show that QST can reduce the total memory footprint by up to 2.3 \u00d7 \\times \u00d7 and speed up the finetuning process by up to 3 \u00d7 \\times \u00d7 while achieving comparable performance compared with the state-of-the-art. Report issue for preceding element",
  "masked_text": "Recent advancements in large language models (LLMs), including GPT [CITATION], PaLM [CITATION], OPT [CITATION], and LLaMA [CITATION], have showcased remarkable task-generalization capabilities across diverse applications [CITATION]. The ongoing evolution of LLMs\u2019 capabilities is accompanied by exponential increases in LLMs\u2019 sizes, with some models encompassing 100 billion parameters [CITATION]. Finetuning pre-trained LLMs [CITATION] for customized downstream tasks provides an effective approach to introducing desired behaviors, mitigating undesired ones, and thus boosting the LLMs\u2019 performance [CITATION]. Nevertheless, the process of LLM finetuning is characterized by its substantial memory demands. For instance, finetuning a 16-bit LLaMA model with 65 billion parameters requires more than 780GB of memory [CITATION]. Report issue for preceding element\nTo reduce the computational requirement of LLM finetuning, recent work introduces parameter-efficient finetuning (PEFT), which updates a subset of trainable parameters from an LLM or introduces a small number of new parameters into the LLM while keeping the vast majority of the original LLM parameters frozen [CITATION]. PEFT methods achieve comparable performance as full finetuning while enabling fast adaption to new tasks without suffering from catastrophic forgetting [CITATION]. However, PEFT methods necessitate caching intermediate activations during forward processing, since these activations are needed to update trainable parameters during backward propagation. As a result, PEFT methods require saving more than 70% of activations and almost the same training time compared to full finetuning [CITATION]. Concisely, existing PEFT techniques cannot effectively reduce the memory footprint of LLM finetuning, restricting their applications in numerous real-world memory-constrained scenarios.Report issue for preceding element\nRecent work has also introduced approaches to combining PEFT and quantization. For example, QLoRA [CITATION] quantizes an LLM\u2019s weights to 4-bit and leverages low-rank adaption (LoRA) [CITATION] to finetune the quantized LLM. QLoRA reduces the memory footprint of an LLM\u2019s weights and optimizer states, and as a result, finetuning a 65B LLM requires less than 48 GB of memory. However, QLoRA does not consider the memory footprint of intermediate activations, which can be particularly large when using a large batch size for finetuning. As a result, QLoRA only supports small-batch training (e.g. a batch size of 1111), and finetuning a 65B LLM requires checkpointing gradients [CITATION] to fit the LLM on a single 48GB GPU, resulting in long training time. Besides, our evaluation also reveals that the performance of QLoRA becomes unstable when using 16-bit floating points. [CITATION] and [CITATION] propose to use a side network to reduce the memory footprint of intermediate activations by avoiding backpropagation of the LLM on natural language processing (NLP) and computer vision (CV) tasks, respectively. Even with the adoption of a side network, the inherent model size of the LLM remains a challenge. Meanwhile, these approaches focus on small models (i.e., less than 3 billion parameters), and their applicability and efficacy for larger models remain unexplored. Report issue for preceding element\nIn this paper, we propose a fast, memory-efficient LLM finetuning framework, called Quantized Side-Tuning (QST), which operates through a dual-stage process as shown in Figure 2. First, QST quantizes an LLM into 4-bit to reduce the memory footprint of its model weights. Second, QST introduces a side network separating from the quantized LLM to avoid performing backward propagation for the quantized LLM, thus saving the memory footprint of intermediate activations. During the training phase of QST, the input to each layer of the side network is formed by combining (1) the downsampled output of the corresponding quantized LLM layer and (2) the output of the previous layer of the side network. A larger LLM usually has a larger model depth (i.e., the number of layers) and width (the hidden size of each layer), which in turn requires more trainable parameters for the downsampling layers. Unlike [CITATION] that leverages linear layer to perform downsampling, QST uses several low-rank adapter methods [CITATION] such as MaxPooling [CITATION] and AvgPooling, significantly reducing the required trainable parameters and the memory footprint for the optimizer states. After that, we use a learnable parameter to assign weights and subsequently aggregate the hidden states of the quantized LLM and the side network. Finally, we reuse the LLM head or classifier to predict. Combined with 4-bit quantization and side tuning, QST significantly reduces all three main contributors of the memory footprint and training time during the training phase. Besides, QST does not increase inference latency since the LLM and side network can be computed in parallel. Figure 1 compares the memory footprint of QST and existing parameter-efficient fine-tuning methods, including QLoRA and LST.Report issue for preceding element\nTo validate the effectiveness of our QST, we conduct extensive evaluations for different types of LLMs (e.g., OPT, LLaMA 2), with 1.3B to 70B parameters, on various benchmarks. Experiment results show that QST can reduce the total memory footprint by up to 2.3 \u00d7\\times\u00d7 and speed up the finetuning process by up to 3 \u00d7\\times\u00d7 while achieving competent performance compared with the state-of-the-art. Our codes are released to the GitHub 111https://github.com/YouAreSpecialToMe/QST .Report issue for preceding element\nFinetuning allows an LLM to adapt to specialized domains and tasks [CITATION]. However, fully finetuning an LLM comes with high computation costs due to the rapidly increasing LLM sizes. Parameter-efficient finetuning (PEFT) methods are proposed to solve this issue. Drawing inspiration from the pronounced sensitivity of LLMs to prompts as highlighted in [CITATION], a series of studies introduce trainable prompt embeddings prepended to the input text or attention components while preserving the original LLM parameters [CITATION]. [CITATION] and [CITATION] propose adapter modules to introduce new task-specific parameters, which are inserted into the Transformer layers inside the LLM. LoRA [CITATION] leverages the low-rank decomposition concept to construct trainable parameters inserted into the original LLM weights. (IA)33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT [CITATION] proposes to scale the pre-trained model weights of an LLM with a trainable vector. Of late, there has been a surge in the proposal of unified approaches that amalgamate various PEFT methods by leveraging human heuristics [CITATION] or employing neural architecture search [CITATION]. Existing PEFT approaches focus on optimizing model performance while minimizing trainable parameters. However, a reduction in the number of trainable parameters does not inherently imply a corresponding reduction in memory footprint. Report issue for preceding element\nMemory-efficient training and finetuning aims to reduce the memory footprint during the LLM training and/or finetuning phase. Reversible neural networks [CITATION] allow the intermediate activations of each layer to be recomputed from the activation of its next layer, thus exempting the need to save intermediate activations. Gradient checkpointing [CITATION] offers an optimization strategy that balances computational resources against memory footprint. Specifically, it reduces memory requirement by selectively discarding certain intermediate activations, which are subsequently recomputed through an additional forward pass when needed. Another line to enhancing memory efficiency involves network compression, that is, the original LLM is reduced to a more compact form, thereby making both the training and inference phases more computationally economical. Network pruning and distillation are the most prevalent strategies for network compression. Network distillation [CITATION] involves the creation of a student network that is trained to approximate the output distribution of a teacher network across a specified dataset. Network pruning [CITATION] aims to streamline models by ascertaining the significance of individual parameters and subsequently eliminating those deemed non-essential. Compared with PEFT methods, network compression yields models optimized for expedited inference, whereas PEFT methods may achieve superior performance by updating a small set of trainable parameters.Report issue for preceding element\nRecently, QLoRA [CITATION] quantizes the LLM to 4-bit and then adds LoRA to finetune the quantized LLM. QLoRA significantly reduces the memory footprint of weights and optimizer states compared with full finetuning while retaining similar performance. QLoRA does not consider the memory footprint of intermediate activations, and thus falls short in finetuning the LLM with a large batch size, resulting in a long training time. In the context of NLP and CV tasks, the studies by [CITATION] and [CITATION] introduce the concept of employing a side network. The side network aims to obviate the need for backpropagation through the LLM, thereby reducing the memory footprint associated with intermediate activations. Despite incorporating the side network, the inherent model size (i.e., the memory footprint of weights) of the LLM still poses computational challenges. Hence, both methods can only focus exclusively on models with fewer than 3 billion parameters, and fail to finetune models with more parameters. Report issue for preceding element\nIn this section, we first describe the process of quantizing an LLM into 4-bit, and then introduce our design of the side network for side tuning.Report issue for preceding element\nQuantization is the process of converting a data type with more bits (e.g., 32- or 16-bit floating points) into another data type with fewer bits (e.g., 8-bit integers or 4-bit floating points). QST first quantizes an LLM from 16-bit into 4-bit, formulated as follows.Report issue for preceding element X4\u2062b\u2062i\u2062t=superscript\ud835\udc4b4\ud835\udc4f\ud835\udc56\ud835\udc61absent\\displaystyle X^{4bit}=italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT = round\u2062(M4\u2062b\u2062i\u2062tAbsmax\u2062(X16\u2062b\u2062i\u2062t)\u2062X16\u2062b\u2062i\u2062t)roundsubscript\ud835\udc404\ud835\udc4f\ud835\udc56\ud835\udc61Absmaxsuperscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61\\displaystyle\\text{round}\\left(\\frac{M_{4bit}}{\\text{Absmax}(X^{16bit})}X^{16% bit}\\right)round ( divide start_ARG italic_M start_POSTSUBSCRIPT 4 italic_b italic_i italic_t end_POSTSUBSCRIPT end_ARG start_ARG Absmax ( italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) end_ARG italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) (1) =\\displaystyle== round\u2062(c16\u2062b\u2062i\u2062t\u22c5X16\u2062b\u2062i\u2062t),round\u22c5superscript\ud835\udc5016\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61\\displaystyle\\text{round}\\left(c^{16bit}\\cdot X^{16bit}\\right),round ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT \u22c5 italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) , (2) where X4\u2062b\u2062i\u2062tsuperscript\ud835\udc4b4\ud835\udc4f\ud835\udc56\ud835\udc61X^{4bit}italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT and X16\u2062b\u2062i\u2062tsuperscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61X^{16bit}italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT are tensors in 4- and 16-bit, respectively. M4\u2062b\u2062i\u2062tsubscript\ud835\udc404\ud835\udc4f\ud835\udc56\ud835\udc61M_{4bit}italic_M start_POSTSUBSCRIPT 4 italic_b italic_i italic_t end_POSTSUBSCRIPT is the maximum value of the 4-bit data type. For example, MN\u2062F\u20624=1subscript\ud835\udc40\ud835\udc41\ud835\udc3941M_{NF4}=1italic_M start_POSTSUBSCRIPT italic_N italic_F 4 end_POSTSUBSCRIPT = 1, where NF4 is an information-theoretically optimal data type that ensures each quantization bin has an equal number of values assigned from the input tensor. QST considers both NF4 and FP4 to quantize an LLM. We empirically demonstrate that NF4 performs the best in our experiments (see Section 4). c16\u2062b\u2062i\u2062tsuperscript\ud835\udc5016\ud835\udc4f\ud835\udc56\ud835\udc61c^{16bit}italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT is the quantization constant (or quantization scale) of the 16-bit data type. Correspondingly, dequantization is given by Report issue for preceding element d\u2062e\u2062q\u2062u\u2062a\u2062n\u2062t\u2062(c16\u2062b\u2062i\u2062t,X4\u2062b\u2062i\u2062t)=X4\u2062b\u2062i\u2062tc16\u2062b\u2062i\u2062t=X16\u2062b\u2062i\u2062t.\ud835\udc51\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc4e\ud835\udc5b\ud835\udc61superscript\ud835\udc5016\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b4\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b4\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc5016\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61\\displaystyle dequant(c^{16bit},X^{4bit})=\\frac{X^{4bit}}{c^{16bit}}=X^{16bit}.italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT , italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) = divide start_ARG italic_X start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT end_ARG start_ARG italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT end_ARG = italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT . (3) The key limitation of this method arises when the input tensor contains values with very large magnitudes, commonly referred to as outliers. Such outliers can result in under-utilization of the quantization bins, leading to sparsely populated or even empty bins in some instances. To address this issue, a prevalent strategy involves partitioning the input tensor into discrete blocks, each subjected to independent quantization with its own associated quantization constant. As a result, the input tensor X\u2208\u211db\u00d7h\ud835\udc4bsuperscript\u211d\ud835\udc4f\u210eX\\in\\mathbb{R}^{b\\times h}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_b \u00d7 italic_h end_POSTSUPERSCRIPT is decomposed into n\ud835\udc5bnitalic_n contiguous blocks, each comprising B\ud835\udc35Bitalic_B elements. This decomposition is facilitated by flattening X\ud835\udc4bXitalic_X into a 1-dimensional array, which is then partitioned into n=(b\u00d7h)B\ud835\udc5b\ud835\udc4f\u210e\ud835\udc35n=\\frac{(b\\times h)}{B}italic_n = divide start_ARG ( italic_b \u00d7 italic_h ) end_ARG start_ARG italic_B end_ARG individual blocks. Then, we can leverage E.q. (1) to independently quantize these n\ud835\udc5bnitalic_n blocks using different quantization constants. Typically, minimizing the error associated with 4-bit quantization would necessitate the utilization of smaller block sizes. This is attributed to the reduced influence of outliers on other weights. However, using a small block size leads to high memory overhead since we need to allocate more memory for these quantization constants. To reduce the memory footprint of quantization constants, we can use the same quantization strategy to quantize these quantization constants [CITATION]. In this paper, we use 8-bit float points to quantize the quantization constants, and the forward pass of a single linear layer in the LLM is defined as Y16\u2062b\u2062i\u2062t=d\u2062e\u2062q\u2062u\u2062a\u2062n\u2062t\u2062(d\u2062e\u2062q\u2062u\u2062a\u2062n\u2062t\u2062(c216\u2062b\u2062i\u2062t,c18\u2062b\u2062i\u2062t),W4\u2062b\u2062i\u2062t)\u2062X16\u2062b\u2062i\u2062tsuperscript\ud835\udc4c16\ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc4e\ud835\udc5b\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc4e\ud835\udc5b\ud835\udc61subscriptsuperscript\ud835\udc5016\ud835\udc4f\ud835\udc56\ud835\udc612subscriptsuperscript\ud835\udc508\ud835\udc4f\ud835\udc56\ud835\udc611superscript\ud835\udc4a4\ud835\udc4f\ud835\udc56\ud835\udc61superscript\ud835\udc4b16\ud835\udc4f\ud835\udc56\ud835\udc61Y^{16bit}=dequant(dequant(c^{16bit}_{2},c^{8bit}_{1}),W^{4bit})X^{16bit}italic_Y start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT = italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_d italic_e italic_q italic_u italic_a italic_n italic_t ( italic_c start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_c start_POSTSUPERSCRIPT 8 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_W start_POSTSUPERSCRIPT 4 italic_b italic_i italic_t end_POSTSUPERSCRIPT ) italic_X start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT. 4-bit quantization can significantly reduce the memory footprint of weights, facilitating easier storage and deployment of LLMs. Besides, low-precision floating numbers are faster to execute on modern accelerators such as GPUs, leading to faster model training and inference. Nonetheless, the high to low precision data type conversion process during quantization can lead to accuracy degradation, attributable to the inherent information loss.Report issue for preceding element\nWe now analyze the memory footprint of LLM training and then introduce the neural architecture of the side network, which reduces the inherent information loss and minimizes accuracy drop during quantization.Report issue for preceding element\nFor a given LLM with N\ud835\udc41Nitalic_N layers, let yi=fi\u2062(Wi,xi)subscript\ud835\udc66\ud835\udc56subscript\ud835\udc53\ud835\udc56subscript\ud835\udc4a\ud835\udc56subscript\ud835\udc65\ud835\udc56y_{i}=f_{i}(W_{i},x_{i})italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) denotes the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT transformer layer of the LLM, where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the input to the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer (i.e., xi=yi\u22121subscript\ud835\udc65\ud835\udc56subscript\ud835\udc66\ud835\udc561x_{i}=y_{i-1}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT). The memory required during the training phase of the LLM predominantly comprises three main contributors: M1) weights of the LLM {Wi}i=1Nsuperscriptsubscriptsubscript\ud835\udc4a\ud835\udc56\ud835\udc561\ud835\udc41\\{W_{i}\\}_{i=1}^{N}{ italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, M2) the optimizer state, which is threefold the size of the trainable parameters when employing the Adam optimizer [CITATION] (one for gradient and two for moments), and M3) the intermediate activations {yi\u2032}i=1Nsuperscriptsubscriptsuperscriptsubscript\ud835\udc66\ud835\udc56\u2032\ud835\udc561\ud835\udc41\\{y_{i}^{\\prime}\\}_{i=1}^{N}{ italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT. The memory footprint of intermediate activations is related to model depth, width, and several training settings, e.g., batch size and sequence length. QLoRA reduces the memory footprint of an LLM\u2019s weights and optimizer states (M1 and M2) but fails to reduce intermediate activations (M3). When finetuning an LLM with a large batch size and/or long sequence length, the memory footprint of QLoRA increases significantly. However, using a small batch size results in long training time. [CITATION] only reduces the memory footprint of intermediate activations (M3), thus it struggles to finetune a model with more than 3 billion parameters.Report issue for preceding element\nOur side network g\ud835\udc54gitalic_g serves as a lightweight version of the quantized LLM f\ud835\udc53fitalic_f. The hidden state and weight dimension of g\ud835\udc54gitalic_g are r\ud835\udc5fritalic_r times smaller than those of f\ud835\udc53fitalic_f, where r\ud835\udc5fritalic_r is the reduction factor. During the forward pass, the hidden state of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of the side network hgisubscript\u210esubscript\ud835\udc54\ud835\udc56h_{g_{i}}italic_h start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT is formulated by hgi16\u2062b\u2062i\u2062t=(1\u2212\u03b2i)*d\u2062o\u2062w\u2062n\u2062s\u2062a\u2062m\u2062p\u2062l\u2062ei\u2062(hfi16\u2062b\u2062i\u2062t)+\u03b2i*hgi\u2212116\u2062b\u2062i\u2062t,subscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc54\ud835\udc561subscript\ud835\udefd\ud835\udc56\ud835\udc51\ud835\udc5c\ud835\udc64\ud835\udc5b\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59subscript\ud835\udc52\ud835\udc56subscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc53\ud835\udc56subscript\ud835\udefd\ud835\udc56subscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc54\ud835\udc561h^{16bit}_{g_{i}}=(1-\\beta_{i})*downsample_{i}(h^{16bit}_{f_{i}})+\\beta_{i}*h^% {16bit}_{g_{i-1}},italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( 1 - italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) * italic_d italic_o italic_w italic_n italic_s italic_a italic_m italic_p italic_l italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) + italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT * italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , where hfi16\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc53\ud835\udc56h^{16bit}_{f_{i}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT is the hidden state of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of f\ud835\udc53fitalic_f and can be computed using E.q. (3). The illustration of it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of our QST is shown in Figure 3. Note that we use the output of the embedding layer and the downsampled embedding layer as hf016\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc530h^{16bit}_{f_{0}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and hg016\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc540h^{16bit}_{g_{0}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. \u03b2i=s\u2062i\u2062g\u2062m\u2062o\u2062i\u2062d\u2062(\u03b3i)subscript\ud835\udefd\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51subscript\ud835\udefe\ud835\udc56\\beta_{i}=sigmoid(\\gamma_{i})italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_s italic_i italic_g italic_m italic_o italic_i italic_d ( italic_\u03b3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is a learned gate parameter of it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer, where \u03b3isubscript\ud835\udefe\ud835\udc56\\gamma_{i}italic_\u03b3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a learnable zero-initialized scalar. d\u2062o\u2062w\u2062n\u2062s\u2062a\u2062m\u2062p\u2062l\u2062ei\ud835\udc51\ud835\udc5c\ud835\udc64\ud835\udc5b\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59subscript\ud835\udc52\ud835\udc56downsample_{i}italic_d italic_o italic_w italic_n italic_s italic_a italic_m italic_p italic_l italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the downsample module of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer to reduce the hidden state dimension of f\ud835\udc53fitalic_f by r\ud835\udc5fritalic_r times. Prior work leverages linear projections to downsample (i.e., \u00d71rabsent1\ud835\udc5f\\times\\frac{1}{r}\u00d7 divide start_ARG 1 end_ARG start_ARG italic_r end_ARG) the high-dimensional hidden states of f\ud835\udc53fitalic_f to the low-dimensional hidden states of g\ud835\udc54gitalic_g. However, an LLM typically comprises plenty of layers with substantially high-dimensional hidden states, particularly when the number of parameters exceeds 3 billion. Using linear projections to downsample involves a significant amount of trainable parameters, requiring a high memory footprint for the parameters and their optimizer states. For example, if the LLM has 24 layers, the dimension of its hidden state is 2048 and the reduction factor r\ud835\udc5fritalic_r is 4, the downsample module consumes about 50% of the overall trainable parameters.Report issue for preceding element\nTo address this problem, we leverage several different downsample methods, including LoRA [CITATION], Adapter [CITATION], MaxPooling [CITATION] and AvgPooling. LoRA augments a linear projection through an additional factorized projection, which can be formulated as W=L1\u2062L2\ud835\udc4asubscript\ud835\udc3f1subscript\ud835\udc3f2W=L_{1}L_{2}italic_W = italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, where W\u2208\u211ddi\u2062n\u00d7do\u2062u\u2062t\ud835\udc4asuperscript\u211dsubscript\ud835\udc51\ud835\udc56\ud835\udc5bsubscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61W\\in\\mathbb{R}^{d_{in}\\times d_{out}}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, L1\u2208\u211ddi\u2062n\u00d7drsubscript\ud835\udc3f1superscript\u211dsubscript\ud835\udc51\ud835\udc56\ud835\udc5bsubscript\ud835\udc51\ud835\udc5fL_{1}\\in\\mathbb{R}^{d_{in}\\times d_{r}}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and L2\u2208\u211ddr\u00d7do\u2062u\u2062tsubscript\ud835\udc3f2superscript\u211dsubscript\ud835\udc51\ud835\udc5fsubscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61L_{2}\\in\\mathbb{R}^{d_{r}\\times d_{out}}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. Adapter is similar to LoRA but introduces an extra non-linear function between L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Using LoRA or Adapter can reduce the ratio of the trainable parameters of these downsample modules from 56% to 8%. MaxPooling and AvgPooling do not introduce extra trainable parameters. We empirically demonstrate that the Adapter performs the best in our experiments. Report issue for preceding element\nFinally, we upsample (i.e., \u00d7rabsent\ud835\udc5f\\times r\u00d7 italic_r) from low-dimensional hidden states of g\ud835\udc54gitalic_g to high-dimensional hidden states of f\ud835\udc53fitalic_f. Prior works have only evaluated side tuning methods (e.g., LST) in the context of classification tasks. We observe that LST suffers from repetition when generating long-sequence texts, which renders it incapable of producing extensive and high-quality texts. This limitation stems from LST\u2019s utilization of the hidden states of the side network for prediction, which causes an initialization position far removed from the pre-trained model at the onset of finetuning. As illustrated in MEFT [CITATION], the initialization step emerges as a critical factor influencing the efficacy of fine-tuning methods. To resolve this issue, QST combines the output of the LLM\u2019s last layer (i.e., hfN16\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc53\ud835\udc41h^{16bit}_{f_{N}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT) with the output of the side network (i.e., hgN16\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc54\ud835\udc41h^{16bit}_{g_{N}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT), and sends the weighted sum hN16\u2062b\u2062i\u2062t=\u03b1\u2062hfN16\u2062b\u2062i\u2062t+(1\u2212\u03b1)\u2062hgN16\u2062b\u2062i\u2062tsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc41\ud835\udefcsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc53\ud835\udc411\ud835\udefcsubscriptsuperscript\u210e16\ud835\udc4f\ud835\udc56\ud835\udc61subscript\ud835\udc54\ud835\udc41h^{16bit}_{N}=\\alpha h^{16bit}_{f_{N}}+(1-\\alpha)h^{16bit}_{g_{N}}italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = italic_\u03b1 italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT + ( 1 - italic_\u03b1 ) italic_h start_POSTSUPERSCRIPT 16 italic_b italic_i italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT to the LM head, where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a learnable parameter. We initialize \u03b1\ud835\udefc\\alphaitalic_\u03b1 to 1 to preserve the starting point from the pre-trained model at the beginning of finetuning, which is consistent with the initialization of LoRA. With this design, when switching across different downstream tasks, QST can fulfil the necessary adjustments by altering the side network alone, and thus obviating the need for redeploying the LLM.Report issue for preceding element\nQST only updates the parameters of the side network g\ud835\udc54gitalic_g, but not the 4-bit weights in the LLM f\ud835\udc53fitalic_f. Unlike QLoRA, the calculation of the gradient \u2202L\u2202g\ud835\udc3f\ud835\udc54\\frac{\\partial L}{\\partial g}divide start_ARG \u2202 italic_L end_ARG start_ARG \u2202 italic_g end_ARG does not entail the calculation of \u2202L\u2202f\ud835\udc3f\ud835\udc53\\frac{\\partial L}{\\partial f}divide start_ARG \u2202 italic_L end_ARG start_ARG \u2202 italic_f end_ARG, thus avoiding the extensive computational costs of performing backpropagation on f\ud835\udc53fitalic_f, which ultimately reduces the memory footprint of intermediate activations and speeds up finetuning.Report issue for preceding element\nIn summary, QST leverages a 4-bit data type to store an LLM\u2019s model weights, thus reducing the memory footprint of weights (M1). In addition, QST leverages a 16-bit computation data type for the forward pass and backpropagation computation and only computes the gradient of weights in g\ud835\udc54gitalic_g (M3). Finally, QST leverages several factorized projection and gradient-free downsample methods to reduce the trainable parameters (M2). These techniques together allow QST to reduce the memory requirement for all three factors, resulting in fast and memory-efficient finetuning with a nearly 1% performance drop.Report issue for preceding element\nIn this section, we empirically validate the effectiveness of our QST method by examining its performance for LLMs with different types (e.g., OPT and LLaMA 2), sizes (from 1.3B to 70B), and benchmarks.Report issue for preceding element\nDatasets. We evaluate the performance of QST and several baselines on natural language understanding (NLU) and natural language generation tasks. For NLU experiments, we use the GLUE [CITATION] (General Language Understanding Evaluation) and MMLU [CITATION] (Massively Multitask Language Understanding) benchmarks. The GLUE benchmark provides a comprehensive evaluation of models across a range of linguistic tasks. These tasks encompass linguistic acceptability as examined in CoLA [CITATION], sentiment analysis as portrayed in SST2 [CITATION], tasks probing similarity and paraphrase distinctions such as MRPC [CITATION], QQP [CITATION], and STS-B [CITATION], in addition to natural language inference tasks including MNLI [CITATION], QNLI [CITATION], and RTE [CITATION]. We report accuracy on MNLI, QQP, QNLI, SST-2, MRPC, and RTE, Pearson correlation coefficients on SST-B, and Mathews correlation coefficients [CITATION] on CoLA. The MMLU benchmark consists of 57 tasks including elementary mathematics, US history, computer science, law, and more. We report the average 5-shot test accuracy on the 57 tasks. Report issue for preceding element\nModels. We use decoder-only LLMs such as the OPT series (OPT-1.3B, OPT-2.7B, OPT-6.7B, OPT-13B, OPT-30B, and OPT-66B) and the LLaMA-2 series (LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B). Report issue for preceding element\nBaselines. We compare QST with QLoRA [CITATION], LST [CITATION], LoRA [CITATION], and Adapter [CITATION]. Note that we only compare LST, LoRA, and Adapter when the model size is less than 3B since their memory footprint of weights can be excessively huge beyond that. Report issue for preceding element\nImplementation. We set the reduction factor r\ud835\udc5fritalic_r to 16 by default. We use Adapter as the downsample module, a linear layer as the upsample module, and set the rank of the Adapter to 16. We use the NF4 data type to store the weights of the LLM and bfloat16 as the data type for computation. We adopt the same parameters reported in QLoRA, LST, LoRA, and Adapter to construct the baselines. Other hyperparameters are specified in Appendix A and Appendix B. We run each experiment three times under different random seeds and report the average performance. We conduct all the experiments using Pytorch [CITATION] and HuggingFace library [CITATION] on 4 NVIDIA RTX A5000 GPUs, each with 24GB memory.Report issue for preceding element\nTable 1 shows the performance of different methods on the GLUE benchmark. Overall, QST achieves the lowest memory footprint among all methods while attaining competent accuracy. Particularly, for relatively small models (i.e., OPT-1.3B and OPT-2.7B), QST reduces the memory footprint by around 2\u00d7\\times\u00d7 compared with QLoRA, LoRA, and Adapter, while achieving comparable accuracy. Compared with LST, QST reduces the memory requirement by 3.2GB and 6.3GB for finetuning OPT-1.3B and OPT-2.7B. QST also reduces the trainable parameters by around 10\u00d7\\times\u00d7 and 5\u00d7\\times\u00d7 compared with QLoRA and the other baselines, respectively.Report issue for preceding element\nFor larger models such as OPT-6.7B, we focus on comparing QST with QLoRA. This is because QLoRA has similar accuracy with the other baselines, but LoRA, Adapter, and LST all have excessively huge memory footprints of weights when it comes to finetuning OPT-6.7B222QLoRA can leverage gradient accumulation to finetune with a batch size of 16 while guaranteeing an affordable memory footprint.. Compared with QLoRA, QST reduces the memory footprint and trainable parameters by 2.3\u00d7\\times\u00d7 and 5.5\u00d7\\times\u00d7, while only introducing a 1.3% accuracy drop. Report issue for preceding element\nThe experiment results of the MMLU benchmark are shown in Table 2. We set the batch size to 4 and the sequence length to 384. We use the Alpaca dataset [CITATION] to finetune both QLoRA and QST. We compare QST with QLoRA on accuracy and memory requirement over OPT-1.3B, OPT-2.7B, OPT-6.7B, OPT-13B, OPT-30B, OPT-66B, LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. QST improves the accuracy by 0.1% on average while reducing the memory footprint by 1.8\u00d7\\times\u00d7 compared with QLoRA. Particularly, QST yields an enhancement of 2.1% in accuracy over QLoRa when finetuning LLaMA-2-13B. When finetuning the OPT-2.7B, OPT-6.7B, and OPT-13B models, QST achieves 0.3%, 0.6%, and 0.3% accuracy improvements, respectively. Report issue for preceding element\nEffects of batch size. Figure 5LABEL:sub@bs_compare illustrates the effects of batch size for different methods. We use LLaMA-2-70B as the LLM and set the sequence length to 512. While the memory footprint of all methods increases as the batch size increases, QST achieves the lowest memory footprint among all, regardless of the batch size. Particularly, the memory footprint of QST is only one-third of LoRA and Adapter. Besides, the memory footprint of both QST and LST grows less drastically than QLoRa, Adapter, and LoRa as the batch size increases. This is because both LST and QST use side tuning to reduce the hidden dimension of the intermediate activations, thereby alleviating the growth of memory footprint induced by intermediate activations. QST also achieves an additional reduction of approximately 100GB in memory footprint compared to LST, thanks to the 4-bit quantization design that effectively compresses the memory footprint of the weights and well design of the downsample modules to reduce the optimizer states.Report issue for preceding element\nEffects of the model size. Figure 5LABEL:sub@ms_compare shows the effects of the total model bits on different methods. We use the OPT model series and set the batch size to 4. Due to the 4-bit quantization, QST and QLoRA reduce the memory footprint compared with the other baselines. The memory footprint gap further widens as the model size increases. Besides, QST achieves around 2 times reduction in memory footprint compared with QLoRA thanks to its small volume of trainable parameters and intermediate activations.Report issue for preceding element\nEffects of sequence length. Figure 5LABEL:sub@seq_compare shows the effects of sequence length on different methods. We use LLaMA-2-70B and set the batch size to 4. Similar to the effect of batch size, LST and QST alleviate the growth rate of memory footprint of intermediate activations, while QST further achieves around 100GB reduction in memory footprint compared with LST. Report issue for preceding element\nTable 3 shows the training throughput of different methods, measured by FLOPS per token (the lower the better), on LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B. While the FLOPS per token of all methods increases as the model size grows, QST achieves the lowest FLOPS per token among all. Particularly, QST achieves around 2.5\u00d7\\times\u00d7 speed up compared with the baselines. LST suffers from the highest FLOPS per token. The FLOPS per token of QLoRA is slightly higher than LoRA and Adapter since QLoRA adds more LoRA components.Report issue for preceding element\nEffects of reduction factor r\ud835\udc5fritalic_r. We conduct experiments using LLaMA-2-7B, LLaMA-2-13B, and LLaMA-2-70B to verify the effects of reduction factor r\ud835\udc5fritalic_r (from 2 to 64) on memory footprint, MMLU accuracy, and throughput. We set the batch size to 4 and the sequence length to 384. The MMLU accuracy changes slightly as r\ud835\udc5fritalic_r varies as shown in Figure 5a. QST achieves the best accuracy of finetuning LLaMA-2-7B and LLaMA-2-13B when r\ud835\udc5fritalic_r is set to 16. As shown in Figure 5b and 5c, the memory footprint and the FLOPS per token decrease drastically when r\ud835\udc5fritalic_r varies from 2 to 16 for finetuning all the models. The memory footprint and the FLOPS per token decrease slightly when r\ud835\udc5fritalic_r varies from 16 to 64. Therefore, we use r\ud835\udc5fritalic_r to 16 in our experiments as default.Report issue for preceding element\nEffects of 4-bit data types. We evaluate two 4-bit data types: FP4 and NF4 using the LLaMA-2 model series and the MMLU benchmark. As shown in Table 4, NF4 improves the average accuracy by about 0.8% compared with FP4. Therefore, we use NF4 as the default 4-bit data type in our experiments. Report issue for preceding element\nEffects of computation data types. We analyze the effects of two computation data types: BF16 (results shown in Table 1) and FP16 (results shown in Table 5). As can be seen, QST retains similar results using FP16 and BF16. On the other hand, QLoRA is unstable using FP16 as the computation data type. We finetune OPT-6.7B on the GLUE benchmark and discover that QLoRA fails to finetune on the MRPC and QNLI datasets. We run each dataset under three different random seeds and QLoRA fails on two of them.Report issue for preceding element\nEffects of downsample modules. We conduct experiments on different downsample modules: Linear, LoRA, Adapter, MaxPooling, and AvgPooling using LLaMA-2-7B and the MMLU benchmark. As shown in Table 6, using Adapter as the downsample module achieves the best performance among all baselines, and reduces the trainable parameters and memory footprint.Report issue for preceding element\nWe conduct experiments on Chatbot performance using MT-benchmark [CITATION]. MT-benchmark is a set of challenging multi-turn open-ended questions for evaluating the chat assistant\u2019s performance in writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities categories. In our experiments, we use GPT-4 to act as judges and assess the quality of the responses of the model finetuned by QLoRA and QST. We finetune LLaMA-2-70B using a variant of OASST1 [CITATION]. Table 7 shows the experiment results of QLoRA and QST on the total training time, memory footprint, and the average MT-Bench score over 8 categories. QST speeds up the training by 3.2 \u00d7\\times\u00d7 and reduces memory footprint by 1.7 \u00d7\\times\u00d7, with even an improved score of 0.46 compared with QLoRA. Notably, QST\u2019s chatbot performance outperforms the original LLaMA-2-70B, achieving an improvement of 0.21.Report issue for preceding element\nFigure 6 compares QST, QLoRA, and the original LLaMA-2-70B in different categories using MT-Bench. Out of the 8 categories, QST-70B outperforms QLoRA and LLaMA-2-70B in four of them: STEM, Extraction, Coding, and Roleplay. In the Humanities category, all models demonstrate comparable proficiency. For the remaining three categories, QLoRA takes the lead in Reasoning and Writing, and the original LLaMA-2-70B shows superior performance in Mathematics. Report issue for preceding element\nWe present the results of several case studies that assess the performance of QST-70B across various domains. While this analysis may not be exhaustive, as controlling for all variables in such a concise qualitative study exceeds our scope, our endeavor is to present a selection of representative samples that, in our belief, add meaningful insights of QST to this paper.Report issue for preceding element\nQST can generate high-quality writing samples on various domains. For example:Report issue for preceding element\nUser Write a brief summary of the latest advancements in artificial intelligence and their potential impacts on the industry. \\speakQST-70B 2023 has seen significant advancements in artificial intelligence, particularly in the areas of machine learning, computer vision, and natural language processing. These advancements have potential impacts on various industries, including:Report issue for preceding element\n1. Healthcare: AI-powered diagnosis, drug discovery, and precision medicine are transforming patient care. An AI-assisted cancer diagnosis can predict patient outcomes and suggest personalized treatments. AI-driven drug discovery can accelerate the development of new medicines.Report issue for preceding element\n\u2026Report issue for preceding element\n5. Energy: AI can optimize energy consumption, predict energy demand, and improve the efficiency of renewable energy sources. AI-based smart grids can optimize energy distribution and reduce waste. AI-driven energy management systems can help buildings and homes reduce energy consumption and costs. \\speakUser Create a short story about a time traveler visiting ancient Egypt. \\speakQST-70B A time traveler arrived in ancient Egypt, curious about the bustling society he had heard so much about. As he wandered the desert sands, he stumbled upon a construction site where workers were building a colossal pyramid. He marveled at the sheer size of the edifice and decided to approach one of the workers. \u201cExcuse me,\u201d the time traveler said. \u201cWhat\u2019s the purpose of this structure?\u201dReport issue for preceding element\nThe worker gave him a perplexed look. \u201cYou must be a foreigner. Everyone knows that we build pyramids to bury our pharaohs. These magnificent structures ensure their safe passage into the afterlife.\u201dReport issue for preceding element\nFascinated, the time traveler asked, \u201cWhat\u2019s the secret to building such colossal structures?\u201dReport issue for preceding element\n\u201cIt\u2019s simple,\u201d the worker replied with a smile. \u201cWe work together as one. Each of us has a specific task, and we combine our efforts to create something grand.\u201d \u2026Report issue for preceding element\nQST-70B can easily answer questions such as \u201cWhat is the capital of France?\". For example,Report issue for preceding element\nUser What is the capital of France? \\speakQST-70B The capital of France is Paris. Report issue for preceding element\nQST-70B can still answer some questions that are obscure:Report issue for preceding element\nUser When was the painter who created the \u2018Mona Lisa\u2019 born? \\speakQST-70B The Italian artist who painted the \u2018Mona Lisa,\u2019 Leonardo da Vinci, was born on April 15, 1452.Report issue for preceding element\nQST-70B gave an incorrect answer to the following question as mentioned in [CITATION]:Report issue for preceding element\nUser In what year was the singer who popularized \u2018I\u2019ll Keep the Lovelight Burning\u2019 born? \\speakQST-70B The singer who popularized \u201cI\u2019ll Keep the Lovelight Burning\u201d is Dolly Parton. Dolly Parton was born on January 19, 1946.Report issue for preceding element\nDolly Parton is a very famous country music singer, born on January 19, 1946, but she was not the singer who made the song \u201cI\u2019ll Keep the Lovelight Burning\u201d popular. GPT-4\u2019s answer is uncertain about the exact singer who popularized \u201cI\u2019ll Keep the Lovelight Burning\u201d.Report issue for preceding element\nThe performance of QST in mathematical tasks varies depending on the nature of the questions. QST can correctly answer basic arithmetic questions. For example:Report issue for preceding element\nUser What is the sum of 15 and 27? \\speakQST-70B 15 plus 27 is equal to 42. \\speakUser What is the multiply of 3 and 45? \\speakQST-70B 3 multiplied by 45 is 135.Report issue for preceding element\nQST can give the correct answers to percent calculations. For example: {dialogue} \\speakUser If a shirt costs $60 and is on sale for 25% off, what is the sale price? \\speakQST-70B Great question! Let\u2019s break down the process to find the sale price:Report issue for preceding element\n1. Calculate 25% of the original price by multiplying $60 by 0.25, which gives us $15.Report issue for preceding element\n2. Subtract $15 from the original price of $60 to find the sale price: $60 - $15 = $45.Report issue for preceding element\nTherefore, the sale price of the shirt would be $45.Report issue for preceding element\nSurprisingly, QST is also capable of leveraging the theory of mind to break down the process and calculation for math problems step by step. QST can correctly demonstrate the step-by-step process of solving algebraic equations and geometry problems. For example:Report issue for preceding element\nUser Solve the equation for x: 2x + 3 = 19. \\speakQST-70B 2x + 3 = 19,Report issue for preceding element\n2x = 19 - 3,Report issue for preceding element\n2x = 16,Report issue for preceding element\nx = 16/2,Report issue for preceding element\nx = 8. \\speakUser A right triangle has one leg that is 8 cm long and another leg that is 15 cm long. What is the length of the hypotenuse? \\speakQST-70B Great question! The Pythagorean theorem is needed to solve this problem. Here is how to go about it:Report issue for preceding element\nc^2 = 8^2 + 15^2Report issue for preceding element\nc^2 = 64 + 225Report issue for preceding element\nc^2 = 289Report issue for preceding element\nNow, to find the hypotenuse (c), we take the square root of both sides: c = 289289\\sqrt{289}square-root start_ARG 289 end_ARG, c = 17. Thus, the length of the hypotenuse of the right triangle is approximately 17 cm.Report issue for preceding element\nCalculate the integral of the function f(x) = x^2 from x = 0 to x = 3.Report issue for preceding element\nSimilar to [CITATION], QST may make mistakes on some questions. For example:Report issue for preceding element\nUser Please factorize 1833. \\speakQST-70B 1833 = 33 \u00d7 55.Report issue for preceding element\nApparently, 1833 can not be factorized as 33 \u00d7 55, the correct answer is 3 \u00d7 17 \u00d7 43. We still get the wrong answer if we ask QST to calculate step by step:Report issue for preceding element\nUser Please factorize 1833 step by step. \\speakQST-70B 1833 can be factored as follows: 1833 = 3 \u00d7 311. Here\u2019s how: 1833 = 3 \u00d7 3 \u00d7 111, 3 \u00d7 3 = 9, 9 \u00d7 111 = 1833. So, the prime factorization of 1833 is 1833 = 3 \u00d7 3 \u00d7 111.Report issue for preceding element\nWhether QST can give correct answers to logical reasoning math problems depends. Sometimes, a more instructive input prompt, e.g., prompt it to analyze the problem step by step, may lead to the correct answers. Take a question from [CITATION] as an example:Report issue for preceding element\nUser Some people got on a bus at the terminal. At the first bus stop, half of the people got down and 4 more people got in. Then at the second bus stop, 6 people got down and 8 more got in. If there were a total of 25 people heading to the third stop, how many people got on the bus at the terminal? \\speakQST-70B 20 people got on the bus at the terminal. \\speakUser Please analyze it in detail. \\speakQST-70B To find the total number of people who boarded the bus at the terminal, we\u2019ll analyze the information step by step:Report issue for preceding element\n1. Half of the people who boarded the bus at the terminal got down at the first bus stop. This means if x people boarded the bus at the terminal, x/2 people got down at the first stop.Report issue for preceding element\n2. 4 more people got on the bus at the first stop. So, the total number of people on the bus after the first stop is x/2 + 4.Report issue for preceding element\n3. At the second bus stop, 6 people got down. Now, the number of people on the bus is x/2 + 4 - 6.Report issue for preceding element\n4. 8 more people got on the bus at the second stop. The total number of people on the bus heading to the third stop is x/2 + 4 - 6 + 8 = x/2 + 12 - 6 = x/2 + 6.Report issue for preceding element\n5. There were a total of 25 people heading to the third stop. So, x/2 + 6 = 25.Report issue for preceding element\n6. Solving for x, we get:Report issue for preceding element\nx/2 + 6 = 25,Report issue for preceding element\nx/2 = 25 - 6,Report issue for preceding element\nx/2 = 19,Report issue for preceding element\nx = 2 x 19,Report issue for preceding element\nx = 38.Report issue for preceding element\nSo, there were 38 people who boarded the bus at the terminal.Report issue for preceding element\nThe correct answer is 38. QST generates the correct answer for this question after being instructed to analyze the problem step by step.Report issue for preceding element\nQST is good at generating codes for simple algorithms. Take a question in [CITATION] as an example:Report issue for preceding element\nUser Write a C++ program to find the nth Fibonacci number using recursion.Report issue for preceding element\nQST-70B Here\u2019s an example of a C++ program to find the nth Fibonacci number using recursion:Report issue for preceding element \u2b07 1int fibonacci(int n) { 2if (n == 0 || n == 1) { 3return n; 4} 5else { 6return fibonacci(n-1) + fibonacci(n-2); 7} 8} This program uses the recursive formula of the Fibonacci sequence, where the nth Fibonacci number is the sum of the previous two numbers. The program also includes a base case, where the function directly returns the values for n=0 and n=1 to avoid an infinite loop.Report issue for preceding element\nUser Implement a program to find the common elements in two arrays without using any extra data structures. \\speakQST-70B To find common elements in two arrays without using any extra data structures, you can follow these steps:Report issue for preceding element\n1. Sort both arrays.Report issue for preceding element\n2. Loop through the first array and for each element, loop through the second array.Report issue for preceding element\n3. If you find the same element in both arrays, store it in a variable or an array.Report issue for preceding element\n4. At the end of both loops, the common elements will be in the variable or array.Report issue for preceding element\nHere\u2019s an example of pseudo code for the above-described method:Report issue for preceding element \u2b07 1def findCommonElements(arr1, arr2) 2 1. sort(arr1); 3 2. sort(arr2); 4 3. commonElements = []; 5 4. for (var i = 0; i < arr1.length; i++) 6 for (var j = 0; j < arr2.length; j++) 7 if (arr1[i] === arr2[j]) 8 commonElements.push(arr1[i]); 9 5. return commonElements; This solution time complexity is O(n^2), where n is the length of the arrays.Report issue for preceding element\nPlease note that this is one of the ways to find the common elements, and it is not the most efficient algorithm, especially for large datasets. If you need to handle large datasets, it is better to use a more efficient algorithm like sorting both arrays and then using two pointers to find the common elements. Report issue for preceding element\nThe generated algorithm is current. Although we claim in the question that do not use extra data structure. QST still uses an array \u201ccommonElements\u201d. For complex algorithms (e.g. dynamic programming), QST can make mistakes.Report issue for preceding element\nQST is good at roleplay. For example:Report issue for preceding element\nUser Suppose that you are a doctor. A patient complains of frequent headaches. What would you say?Report issue for preceding element\nQST-70B As a doctor, if a patient were to complain of frequent headaches, I would ask several questions to better understand their symptoms and medical history in order to provide an accurate diagnosis and recommend appropriate treatment.Report issue for preceding element\nHere\u2019s an example conversation:Report issue for preceding element\nAssistant: Good morning, how can I assist you today?Report issue for preceding element\nPatient: Hi, doctor. I\u2019ve been experiencing frequent headaches, and I\u2019m not sure why. They\u2019re really frustrating and disrupting my daily life.Report issue for preceding element\nAssistant: Sorry to hear that. Can you tell me more about your headaches? What\u2019s the location and duration of the pain? Have you noticed any patterns or triggers?Report issue for preceding element\nPatient: Well, the pain is usually on both sides of my head and can last anywhere from a few hours to the entire day. It\u2019s been happening on and off for the past few months. Sometimes, I\u2019ll feel a dull ache behind my eyes or sharp pains in the back of my neck. I can\u2019t seem to connect the headaches to any specific food or activity.Report issue for preceding element\nAssistant: Thank you for that information. Have you experienced any other symptoms lately, such as blurred vision, nausea, or dizziness? Report issue for preceding element\nPatient: No, nothing like that. Just the headaches.Report issue for preceding element\nAssistant: Let\u2019s start with some general screening questions. Do you have a history of allergies, or are there any known allergies in your family? Patient: I don\u2019t have any allergies that I know of, but my dad has seasonal allergies.Report issue for preceding element\nAssistant: Great. How about any previous illnesses, surgeries, or injuries? Patient: I had a tonsillectomy when I was a kid and a sprained ankle a few years ago, but that\u2019s it.Report issue for preceding element\nAssistant: Thanks.Report issue for preceding element\nAnother example:Report issue for preceding element\nUser Suppose you are a mathematician and poet. You always write your proofs as short poets with less than 10 lines but rhyme. Prove the square root of 2 is an irrational number.Report issue for preceding element\nQST-70B Pi is an odd thing, oh so very strangeReport issue for preceding element\nIt cannot be expressed by a simple rangeReport issue for preceding element\nAn irrational number, it just won\u2019t behaveReport issue for preceding element\nIt\u2019s a math mystery, that\u2019s hard to enslave Report issue for preceding element\nIts decimal points stretch on and onReport issue for preceding element\nNever repeating, never doneReport issue for preceding element\nA constant that\u2019s found in every sphereReport issue for preceding element\nMath\u2019s magic, that\u2019s crystal clearReport issue for preceding element\nQST is good at STEM. For example:Report issue for preceding element\nUser Explain the basic principle of quantum computing and how it differs from classical computing.Report issue for preceding element\nQST-70B Quantum computing is a different approach to computing that uses the properties of quantum mechanics to perform operations on data. Unlike classical computing, which uses bits to represent data as either a 0 or a 1, quantum computing uses quantum bits, or qubits, which can exist as both a 0 and a 1 at the same time. This allows quantum computers to process a much larger number of possible solutions simultaneously, making them well-suited for certain types of complex problems. Another key difference is that quantum computers use quantum entanglement, which allows qubits to become \u201centangled\u201d so that the state of one qubit can instantly affect the state of another, regardless of the distance between them. This property allows quantum computers to perform certain types of calculations much faster than classical computers. Quantum computing is still a developing field, and there are many technical challenges to overcome before it can be widely adopted.Report issue for preceding element\nAnother example:Report issue for preceding element\nUser Explain the basic principle of quantum computing and how it differs from classical computing. Report issue for preceding element\nQST-70B Key Challenges of Manned Space Exploration to Mars:Report issue for preceding element\n1. Radiation Exposure: Mars lacks a strong magnetic field to protect against harmful cosmic and solar radiation, which poses a significant health risk for human travelers.Report issue for preceding element\n2. Isolation and Psychological Stress: A trip to Mars takes about 6-9 months, followed by a long-term stay on the planet. This prolonged isolation can lead to psychological strain, including depression, anxiety, and sleep disorders.Report issue for preceding element\n3. Muscle and Bone Loss: In microgravity environments, the human body experiences muscle and bone atrophy due to a lack of resistance and weight-bearing activities. Prolonged exposure can result in severe health consequences.Report issue for preceding element\n4. Technological and Communication Limitations: The vast distance between Earth and Mars creates communication challenges, making real-time communication and technological support difficult. Equipment failures and repair needs may require prolonged troubleshooting and resolution.Report issue for preceding element\nBenefits of Manned Space Exploration to Mars:Report issue for preceding element\n1. Scientific Discovery: Humans can perform more intricate and nuanced experiments and gather more comprehensive data compared to robotic missions, including the search for life on Mars.Report issue for preceding element\n2. Technological Advancement: The challenges of sending humans to Mars drive innovation in areas such as propulsion, life support systems, radiation shielding, and in-situ resource utilization.Report issue for preceding element\n3. Inspiration and Educational Value: Mars missions captivate the public and inspire the next generation of scientists and engineers, fostering a greater interest in STEM education and motivating people to pursue careers in these fields.Report issue for preceding element\n\u2026Report issue for preceding element\nQST can accurately extract information from the input text. For example: {dialogue} \\speakUser Read the following descriptions of historical events and extract the year, location, and main figure involved. Present the information in a tabular JSON format. a) In 1963, in Washington D.C., Martin Luther King Jr. delivered his famous \"I Have a Dream\" speech. b) The first man to walk on the moon was Neil Armstrong in 1969, during the Apollo 11 mission. c) In 1989, the Berlin Wall, a symbol of the Cold War, was torn down by the citizens of East and West Berlin.Report issue for preceding element\nQST-70B {Report issue for preceding element\n\"a\": {Report issue for preceding element\n\"year\": 1963,Report issue for preceding element\n\"location\": \"Washington D.C.\",Report issue for preceding element\n\"main figure\": \"Martin Luther King Jr.\"Report issue for preceding element\n},Report issue for preceding element\n\"b\": {Report issue for preceding element\n\"year\": 1969,Report issue for preceding element\n\"location\": \"The Moon\",Report issue for preceding element\n\"main figure\": \"Neil Armstrong\"Report issue for preceding element\n},Report issue for preceding element\n\"c\": { Report issue for preceding element\n\"year\": 1989,Report issue for preceding element\n\"location\": \"Berlin\",Report issue for preceding element\n\"main figure\": \"The citizens of East and West Berlin\"Report issue for preceding element\n}Report issue for preceding element\n}Report issue for preceding element\nAnother example:Report issue for preceding element\nUser Interpret the subtle sentiments in these TV show reviews and categorize them. Present the results in a structured YAML format.Report issue for preceding element\na) \u201cThe series finale, although not universally satisfying, did tie up several key storylines in a somewhat unconventional manner.\u201dReport issue for preceding element\nb) \u201cThis season\u2019s narrative arc is less compelling than previous ones, but the stellar performances keep it watchable.\u201dReport issue for preceding element\nc) \u201cThe latest episode took a different turn, which might not align with everyone\u2019s taste but certainly adds a unique flavor to the overall series.\u201dReport issue for preceding element\nQST-70B a) Ambivalent b) Neutral c) Positive Report issue for preceding element\nQST is good at answering questions about humanities. For example:Report issue for preceding element\nUser Analyze the character development of Elizabeth Bennet in Jane Austen\u2019s \u201cPride and Prejudice.\u201d Discuss how her interactions with other characters contribute to the central themes of the novel.Report issue for preceding element\nQST-70B Elizabeth Bennet is the protagonist of Jane Austen\u2019s \u201cPride and Prejudice,\u201d and her character development plays a crucial role in the novel\u2019s exploration of themes such as love, class, and social hierarchy.Report issue for preceding element\nAt the beginning of the novel, Elizabeth is portrayed as an intelligent, witty, and strong-willed young woman who rejects societal norms and expectations. Her interactions with other characters, particularly Mr. Darcy and Mr. Wickham, serve to challenge her beliefs and values.Report issue for preceding element\nThrough her conversations and debates with Mr. Darcy, Elizabeth begins to question her initial prejudices against him and learns to appreciate his integrity and sincerity. Their interactions also highlight the central theme of pride and prejudice, as they both struggle to overcome their respective biases.Report issue for preceding element\nElizabeth\u2019s interactions with Mr. Wickham, on the other hand, serve to illustrate her tendency to make hasty judgments and her willingness to believe in someone\u2019s flaws. Her later realization of his true character serves as a lesson for both her and the reader.Report issue for preceding element\nMoreover, Elizabeth\u2019s relationships with her family members and friends add depth to her character and the central themes. Her sister Jane\u2019s kindness and caution provide a contrast to Elizabeth\u2019s impulsiveness, while her best friend Charlotte\u2019s practicality and contentment with her situation demonstrate the societal pressure on women to marry for security.Report issue for preceding element\nIn conclusion, Elizabeth Bennet\u2019s character development is integral to the central themes of \u201cPride and Prejudice.\u201d Her interactions with other characters challenge her beliefs, illustrate the danger of pride and prejudice, and emphasize the importance of understanding and respect in relationships.Report issue for preceding element\nAnother example:Report issue for preceding element\nUser Critique the use of color and light in Vincent Van Gogh\u2019s \u201cStarry Night\u201d and discuss how these elements contribute to the overall emotional impact of the painting.Report issue for preceding element\nQST-70B \u201cStarry Night\u201d by Vincent Van Gogh is a masterpiece that showcases the artist\u2019s unique perspective on color and light. The painting\u2019s vibrant and dynamic use of color creates a powerful emotional response in the viewer, drawing them into a swirling vortex of blue, yellow, and green. The painting\u2019s composition revolves around the interaction between light and dark. The central cypress tree stands tall, its branches stretching toward the sky, while the darker, moonlit clouds in the background form a swirling pattern that echoes the movement of the tree\u2019s branches. This juxtaposition of light and dark creates a sense of energy and dynamism, pulling the viewer\u2019s eye between the different sections of the painting. Van Gogh\u2019s use of color further accentuates the emotional impact of the painting. The stars in the night sky are depicted in bold, swirling brushstrokes of yellow and white, which contrast with the darker blues and greens of the surrounding sky. This stark contrast creates a sense of depth and dimensionality as if the stars are bursting forth from the canvas. The yellow and orange hues of the stars and moon also evoke feelings of hope and joy, capturing the sense of wonder and awe that comes from gazing up at a starry night sky. In addition, the artist\u2019s deliberate brushwork adds texture and a tangible quality to the painting. The thick brushstrokes and heavy impasto give the painting a raised, three-dimensional feel as if the stars and trees are reaching out to touch the viewer. This tactile quality intensifies the emotional connection between the painting and the viewer, drawing them into the swirling vortex of light and color.Report issue for preceding element\nIn this paper, we propose Quantized Side Tuing (QST), a novel fast and memory-efficient finetuning framework. QST operates through a dual-stage process: first, QST quantizes the LLM into 4-bit to reduce the memory footprint of the weights in LLM; then QST introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. QST can significantly reduce the memory footprint of LLM finetuning compared to existing approaches. In particular, experiments show that QST can reduce the total memory footprint by up to 2.3 \u00d7\\times\u00d7 and speed up the finetuning process by up to 3 \u00d7\\times\u00d7 while achieving comparable performance compared with the state-of-the-art.Report issue for preceding element",
  "citations": [
    {
      "tag": "Warstadt et\u00a0al. (2019)",
      "title": "Neural network acceptability judgments.",
      "authors": "Alex Warstadt, Amanpreet Singh, and Samuel\u00a0R Bowman. 2019.",
      "journal": "Transactions of the Association for Computational Linguistics, 7:625\u2013641."
    },
    {
      "tag": "Edalati et\u00a0al. (2022)",
      "title": "Krona: Parameter efficient tuning with kronecker adapter.",
      "authors": "Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid\u00a0Partovi Nia, James\u00a0J Clark, and Mehdi Rezagholizadeh. 2022.",
      "journal": "arXiv preprint arXiv:2212.10650."
    },
    {
      "tag": "Hinton et\u00a0al. (2015)",
      "title": "Distilling the knowledge in a neural network.",
      "authors": "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.",
      "journal": "arXiv preprint arXiv:1503.02531."
    },
    {
      "tag": "Liao et\u00a0al. (2023)",
      "title": "Make your pre-trained model reversible: From parameter to memory efficient fine-tuning.",
      "authors": "Baohao Liao, Shaomu Tan, and Christof Monz. 2023.",
      "journal": "arXiv preprint arXiv:2306.00477."
    },
    {
      "tag": "Williams et\u00a0al. (2017)",
      "title": "A broad-coverage challenge corpus for sentence understanding through inference.",
      "authors": "Adina Williams, Nikita Nangia, and Samuel\u00a0R Bowman. 2017.",
      "journal": "arXiv preprint arXiv:1704.05426."
    },
    {
      "tag": "Radford et\u00a0al. (2019)",
      "title": "Language models are unsupervised multitask learners.",
      "authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et\u00a0al. 2019.",
      "journal": "OpenAI blog, 1(8):9."
    },
    {
      "tag": "Bentivogli et\u00a0al. (2009)",
      "title": "The fifth pascal recognizing textual entailment challenge.",
      "authors": "Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009.",
      "journal": "TAC, 7:8."
    },
    {
      "tag": "Dosovitskiy et\u00a0al. (2020)",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale.",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et\u00a0al. 2020.",
      "journal": "arXiv preprint arXiv:2010.11929."
    },
    {
      "tag": "Chowdhery et\u00a0al. (2022)",
      "title": "Palm: Scaling language modeling with pathways.",
      "authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung\u00a0Won Chung, Charles Sutton, Sebastian Gehrmann, et\u00a0al. 2022.",
      "journal": "arXiv preprint arXiv:2204.02311."
    },
    {
      "tag": "Zhang et\u00a0al. (2022)",
      "title": "Opt: Open pre-trained transformer language models.",
      "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi\u00a0Victoria Lin, et\u00a0al. 2022.",
      "journal": "arXiv preprint arXiv:2205.01068."
    },
    {
      "tag": "OpenAI (2023)",
      "title": "GPT-4 technical report.",
      "authors": "OpenAI. 2023.",
      "journal": "CoRR, abs/2303.08774."
    },
    {
      "tag": "Hu et\u00a0al. (2021)",
      "title": "Lora: Low-rank adaptation of large language models.",
      "authors": "Edward\u00a0J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu\u00a0Wang, and Weizhu Chen. 2021.",
      "journal": "arXiv preprint arXiv:2106.09685."
    },
    {
      "tag": "Houlsby et\u00a0al. (2019)",
      "title": "Parameter-efficient transfer learning for nlp.",
      "authors": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De\u00a0Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.",
      "journal": "InInternational Conference on Machine Learning, pages 2790\u20132799. PMLR."
    },
    {
      "tag": "Lester et\u00a0al. (2021)",
      "title": "The power of scale for parameter-efficient prompt tuning.",
      "authors": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.",
      "journal": "arXiv preprint arXiv:2104.08691."
    },
    {
      "tag": "Chen et\u00a0al. (2016)",
      "title": "Training deep nets with sublinear memory cost.",
      "authors": "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.",
      "journal": "arXiv preprint arXiv:1604.06174."
    },
    {
      "tag": "Hendrycks et\u00a0al. (2020)",
      "title": "Measuring massive multitask language understanding.",
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.",
      "journal": "arXiv preprint arXiv:2009.03300."
    },
    {
      "tag": "Sung et\u00a0al. (2022)",
      "title": "Lst: Ladder side-tuning for parameter and memory efficient transfer learning.",
      "authors": "Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.",
      "journal": "Advances in Neural Information Processing Systems, 35:12991\u201313005."
    },
    {
      "tag": "Wang et\u00a0al. (2018)",
      "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding.",
      "authors": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\u00a0R Bowman. 2018.",
      "journal": "arXiv preprint arXiv:1804.07461."
    },
    {
      "tag": "Dolan and Brockett (2005)",
      "title": "Automatically constructing a corpus of sentential paraphrases.",
      "authors": "Bill Dolan and Chris Brockett. 2005.",
      "journal": "InThird International Workshop on Paraphrasing (IWP2005)."
    },
    {
      "tag": "Raffel et\u00a0al. (2020)",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter\u00a0J Liu. 2020.",
      "journal": "The Journal of Machine Learning Research, 21(1):5485\u20135551."
    },
    {
      "tag": "Rusu et\u00a0al. (2016)",
      "title": "Progressive neural networks.",
      "authors": "Andrei\u00a0A Rusu, Neil\u00a0C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016.",
      "journal": "arXiv preprint arXiv:1606.04671."
    },
    {
      "tag": "Taori et\u00a0al. (2023)",
      "title": "Stanford alpaca: An instruction-following llama model.",
      "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori\u00a0B. Hashimoto. 2023.",
      "journal": "https://github.com/tatsu-lab/stanford_alpaca."
    },
    {
      "tag": "Askell et\u00a0al. (2021)",
      "title": "A general language assistant as a laboratory for alignment.",
      "authors": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et\u00a0al. 2021.",
      "journal": "arXiv preprint arXiv:2112.00861."
    },
    {
      "tag": "Wang et\u00a0al. (2022b)",
      "title": "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.",
      "authors": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut\u00a0Selvan Dhanasekaran, Atharva Naik, David Stap, et\u00a0al. 2022b.",
      "journal": "arXiv preprint arXiv:2204.07705."
    },
    {
      "tag": "Liu et\u00a0al. (2022)",
      "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.",
      "authors": "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\u00a0A Raffel. 2022.",
      "journal": "Advances in Neural Information Processing Systems, 35:1950\u20131965."
    },
    {
      "tag": "Bai et\u00a0al. (2022)",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et\u00a0al. 2022.",
      "journal": "arXiv preprint arXiv:2204.05862."
    },
    {
      "tag": "Socher et\u00a0al. (2013)",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank.",
      "authors": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher\u00a0D Manning, Andrew\u00a0Y Ng, and Christopher Potts. 2013.",
      "journal": "InProceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642."
    },
    {
      "tag": "Pfeiffer et\u00a0al. (2020)",
      "title": "Adapterfusion: Non-destructive task composition for transfer learning.",
      "authors": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. 2020.",
      "journal": "arXiv preprint arXiv:2005.00247."
    },
    {
      "tag": "Iyer (2017)",
      "title": "First quora dataset release: Question pairs.",
      "authors": "Shankar Iyer. 2017.",
      "journal": "https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs."
    },
    {
      "tag": "Cer et\u00a0al. (2017)",
      "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation.",
      "authors": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017.",
      "journal": "arXiv preprint arXiv:1708.00055."
    },
    {
      "tag": "Kingma and Ba (2014)",
      "title": "Adam: A method for stochastic optimization.",
      "authors": "Diederik\u00a0P Kingma and Jimmy Ba. 2014.",
      "journal": "arXiv preprint arXiv:1412.6980."
    },
    {
      "tag": "Rajpurkar et\u00a0al. (2016)",
      "title": "Squad: 100,000+ questions for machine comprehension of text.",
      "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.",
      "journal": "arXiv preprint arXiv:1606.05250."
    },
    {
      "tag": "Koratana et\u00a0al. (2019)",
      "title": "Lit: Learned intermediate representation training for model compression.",
      "authors": "Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia. 2019.",
      "journal": "InInternational Conference on Machine Learning, pages 3509\u20133518. PMLR."
    },
    {
      "tag": "Schick and Sch\u00fctze (2020)",
      "title": "Exploiting cloze questions for few shot text classification and natural language inference.",
      "authors": "Timo Schick and Hinrich Sch\u00fctze. 2020.",
      "journal": "arXiv preprint arXiv:2001.07676."
    },
    {
      "tag": "Zhou et\u00a0al. (2023)",
      "title": "Autopeft: Automatic configuration search for parameter-efficient fine-tuning.",
      "authors": "Han Zhou, Xingchen Wan, Ivan Vuli\u0107, and Anna Korhonen. 2023.",
      "journal": "arXiv preprint arXiv:2301.12132."
    },
    {
      "tag": "Stiennon et\u00a0al. (2020)",
      "title": "Learning to summarize with human feedback.",
      "authors": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul\u00a0F Christiano. 2020.",
      "journal": "Advances in Neural Information Processing Systems, 33:3008\u20133021."
    },
    {
      "tag": "Zoph and Le (2016)",
      "title": "Neural architecture search with reinforcement learning.",
      "authors": "Barret Zoph and Quoc\u00a0V Le. 2016.",
      "journal": "arXiv preprint arXiv:1611.01578."
    },
    {
      "tag": "Wolf et\u00a0al. (2019)",
      "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing.",
      "authors": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et\u00a0al. 2019.",
      "journal": "arXiv preprint arXiv:1910.03771."
    },
    {
      "tag": "Paszke et\u00a0al. (2017)",
      "title": "Automatic differentiation in pytorch.",
      "authors": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.",
      "journal": ""
    },
    {
      "tag": "Scao et\u00a0al. (2022)",
      "title": "Bloom: A 176b-parameter open-access multilingual language model.",
      "authors": "Teven\u00a0Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra\u00a0Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et\u00a0al. 2022.",
      "journal": "arXiv preprint arXiv:2211.05100."
    },
    {
      "tag": "Wang et\u00a0al. (2022a)",
      "title": "Self-instruct: Aligning language model with self generated instructions.",
      "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah\u00a0A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a.",
      "journal": "arXiv preprint arXiv:2212.10560."
    },
    {
      "tag": "He et\u00a0al. (2021)",
      "title": "Towards a unified view of parameter-efficient transfer learning.",
      "authors": "Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. 2021.",
      "journal": "arXiv preprint arXiv:2110.04366."
    },
    {
      "tag": "Touvron et\u00a0al. (2023)",
      "title": "Llama: Open and efficient foundation language models.",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al. 2023.",
      "journal": "arXiv preprint arXiv:2302.13971."
    },
    {
      "tag": "Ouyang et\u00a0al. (2022)",
      "title": "Training language models to follow instructions with human feedback.",
      "authors": "Long Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al. 2022.",
      "journal": "Advances in Neural Information Processing Systems, 35:27730\u201327744."
    },
    {
      "tag": "Brown et\u00a0al. (2020)",
      "title": "Language models are few-shot learners.",
      "authors": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\u00a0D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et\u00a0al. 2020.",
      "journal": "Advances in neural information processing systems, 33:1877\u20131901."
    },
    {
      "tag": "Mangalam et\u00a0al. (2022)",
      "title": "Reversible vision transformers.",
      "authors": "Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo\u00a0Xiong, Christoph Feichtenhofer, and Jitendra Malik. 2022.",
      "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10830\u201310840."
    },
    {
      "tag": "Liu et\u00a0al. (2023)",
      "title": "Gpt understands, too.",
      "authors": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023.",
      "journal": "AI Open."
    },
    {
      "tag": "Min et\u00a0al. (2021)",
      "title": "Metaicl: Learning to learn in context.",
      "authors": "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021.",
      "journal": "arXiv preprint arXiv:2110.15943."
    },
    {
      "tag": "Mao et\u00a0al. (2021)",
      "title": "Unipelt: A unified framework for parameter-efficient language model tuning.",
      "authors": "Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2021.",
      "journal": "arXiv preprint arXiv:2110.07577."
    },
    {
      "tag": "Frankle and Carbin (2018)",
      "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks.",
      "authors": "Jonathan Frankle and Michael Carbin. 2018.",
      "journal": "arXiv preprint arXiv:1803.03635."
    },
    {
      "tag": "Kitaev et\u00a0al. (2020)",
      "title": "Reformer: The efficient transformer.",
      "authors": "Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. 2020.",
      "journal": "arXiv preprint arXiv:2001.04451."
    },
    {
      "tag": "Li and Liang (2021)",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation.",
      "authors": "Xiang\u00a0Lisa Li and Percy Liang. 2021.",
      "journal": "arXiv preprint arXiv:2101.00190."
    },
    {
      "tag": "Frankle et\u00a0al. (2020)",
      "title": "Linear mode connectivity and the lottery ticket hypothesis.",
      "authors": "Jonathan Frankle, Gintare\u00a0Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020.",
      "journal": "InInternational Conference on Machine Learning, pages 3259\u20133269. PMLR."
    },
    {
      "tag": "Devlin et\u00a0al. (2018)",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
      "journal": "arXiv preprint arXiv:1810.04805."
    },
    {
      "tag": "Zheng et\u00a0al. (2023)",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
      "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric.\u00a0P Xing, Hao Zhang, Joseph\u00a0E. Gonzalez, and Ion Stoica. 2023.",
      "journal": ""
    },
    {
      "tag": "Gomez et\u00a0al. (2017)",
      "title": "The reversible residual network: Backpropagation without storing activations.",
      "authors": "Aidan\u00a0N Gomez, Mengye Ren, Raquel Urtasun, and Roger\u00a0B Grosse. 2017.",
      "journal": "Advances in neural information processing systems, 30."
    },
    {
      "tag": "LeCun et\u00a0al. (1998)",
      "title": "Gradient-based learning applied to document recognition.",
      "authors": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998.",
      "journal": "Proceedings of the IEEE, 86(11):2278\u20132324."
    },
    {
      "tag": "Dettmers et\u00a0al. (2023)",
      "title": "Qlora: Efficient finetuning of quantized llms.",
      "authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.",
      "journal": "arXiv preprint arXiv:2305.14314."
    },
    {
      "tag": "Zhang et\u00a0al. (2020)",
      "title": "Side-tuning: a baseline for network adaptation via additive side networks.",
      "authors": "Jeffrey\u00a0O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020.",
      "journal": "InComputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 698\u2013714. Springer."
    },
    {
      "tag": "Floridi and Chiriatti (2020)",
      "title": "Gpt-3: Its nature, scope, limits, and consequences.",
      "authors": "Luciano Floridi and Massimo Chiriatti. 2020.",
      "journal": "Minds and Machines, 30:681\u2013694."
    },
    {
      "tag": "Matthews (1975)",
      "title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme.",
      "authors": "Brian\u00a0W Matthews. 1975.",
      "journal": "Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442\u2013451."
    },
    {
      "tag": "Wei et\u00a0al. (2022)",
      "title": "Chain-of-thought prompting elicits reasoning in large language models.",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed\u00a0Chi, Quoc\u00a0V Le, Denny Zhou, et\u00a0al. 2022.",
      "journal": "Advances in Neural Information Processing Systems, 35:24824\u201324837."
    }
  ]
}