{
  "title": "PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training",
  "text": "Multimodal large language models (MLLMs) (Liu et\u00a0al., 2024b ; Zhu et\u00a0al., 2023 ; Dai et\u00a0al., 2023 ; Bai et\u00a0al., 2023 ; Zhang et\u00a0al., 2023 ; Lu et\u00a0al., 2024a ; Zhou et\u00a0al., 2024 ) have achieved significant strides in complex visual tasks by integrating the world knowledge and reasoning capabilities of large language models (LLMs). Nonetheless, hallucination (Liu et\u00a0al., 2023a ; Zhou et\u00a0al., 2023 ; Yin et\u00a0al., 2023 ; Wang et\u00a0al., 2023a ) issue persists in these models, and even the most capable multimodal models often respond with texts that do not accurately reflect the provided visual content. Notably, scaling up model parameters and training data has not proven effective inmitigating this issue for MLLMs, unlike for their unimodal language counterparts. Report issue for preceding element\nIn this paper, we focus on addressing the problem of hallucination in the context of dense image captioning (Liu et\u00a0al., 2024b ; Chen et\u00a0al., 2023 ; 2024 ) , which requires a comprehensive and detailed description of every aspect of an image. Dense image captioning exposes hallucinations more acutely, as models must generate rich and detailed captions for complex scenes while ensuring fidelity to the visual content. Hallucinations undermine the reliability of MLLMs in applications that require precise and faithful visual descriptions.\nTo tackle the hallucination challenge\nin MLLMs, we recognize the need for a robust quantitative metric that accurately reflects caption quality regarding hallucination. Hence, we introduce HalFscore , a novel metric providing a more granular and comprehensive evaluation of hallucinations specific to dense captioning. HalFscore measures both the accuracy and completeness of dense captions by identifying the incorrect elements and assessing the missing details, offering a balanced view of the model\u2019s performance. To achieve this, we propose to build the language graph that captures the main concepts along with their relationships, and compute its discrepancy against the ground truth. HalFscore aggregates the precision and recall to model the accuracy and completeness of dense captioning results. Compared to previous hallucination scores (Li et\u00a0al., 2023b ; Guan et\u00a0al., 2024 ) , the proposed HalFscore offers a more fine-grained and holistic evaluation, serving as a valuable guide when developing the hallucination suppression method. Report issue for preceding element\nWe further analyze the common cases plagued with hallucinations, and conjecture that the issue in MLLMs comes from the over-reliance on the model\u2019s pretrained linguistic knowledge. As shown in Figure 1 , as more texts are produced, the multimodal model gradually deviates from multimodal generation to unimodal mode, which favors text generation based on the preceding language patterns while overlooking the actual visual information. For example, when presented with an image of a green banana, a model has the tendency to describe it as yellow because of the common knowledge that ripe bananas are typically yellow. In fact, concurrent MLLMs are obtained by continuous training from a pre-trained LLM and are generally equipped with a strong language bias. Report issue for preceding element\nInspired by this, we propose a simple and effective training strategy that reduces the model\u2019s heavy dependence on the language prior by incorporating adversarially perturbed text during training. Specifically, we introduce carefully designed perturbations that aligns with the general knowledge but conflict with the visual content, intentionally misleading the model based on its language prior. For example, we might introduce the perturbation \u201cAs bananas ripen, their color gradually turns yellow\u201d before asking about the color of a green banana in the image. This perturbative training enforces the model to scrutinize the image content when predicting every token, rather than hallucinating contents from the text hints. Essentially, our method adjusts the model\u2019s conditional distribution to depend more heavily on the image and less on the perturbation text, which leads to more robust multimodal capability. Report issue for preceding element\nAs opposed to state-of-the-art methods that resort to more advanced decoding strategies (Leng et\u00a0al., 2024 ; Huang et\u00a0al., 2024 ) , the proposed method, PertuboLLaVA , effectively suppresses the hallucinations in MLLMs without incurring additional training or inference costs, making it more suitable for real-world applications.\nFigure 2 shows our method can describe rich image details with less hallucinations.\nOn the other hand, our method is much more efficient, scalable, and easier to adopt compared to RLHF-based methods which require additional human preference data and incur substantial training overhead and complexity.\nAdditionally, we find the proposed method beneficial to general multimodal abilities, bringing boosted performance across all the multimodal benchmarks. Report issue for preceding element\nTo summarize, our contributions are two-fold. First, we introduce a more principled metric computed on the language graph, serving as a comprehensive hallucination measure. Second, we identify the root cause of hallucinations in MLLMs as its inherent language bias, and propose perturbative visual training, enhancing the model\u2019s focus on visual content during training. The proposed method integrates seamlessly into existing training pipelines, introducing minimal additional cost. It provides a scalable, efficient solution to enhance multimodal models\u2019 visual understanding capabilities, excelling over prior compared to state-of-the-art methods across multiple dimensions. Report issue for preceding element\nWith the rapid advancement of large language models (Touvron et\u00a0al., 2023a ; b ; Chiang et\u00a0al., 2023 ; Bi et\u00a0al., 2024 ) , researchers are leveraging their knowledge and reasoning abilities to build multimodal systems for complex tasks. These models are typically constructed with a pretrained vision encoder to process visual information, a language model backbone responsible for reasoning, and a projector to map the visual data into textual space. The training process of multimodal model is generally divided into two stages: pretraining, using image captioning data for modality alignment, and instruction fine-tuning, using question-answer data to enable task handling.\nSeveral leading open-source projects (Bai et\u00a0al., 2023 ; Lu et\u00a0al., 2024b ; OpenGVLab, 2024 ) have amassed large multimodal datasets and developed high-performing models using advanced language model backbones and vision encoders. However, hallucinations remain a persistent challenge in models\u2019 outputs. Report issue for preceding element\nVarious benchmarks assess hallucination in MLLMs, divided into, categorized into close-ended (Li et\u00a0al., 2023b ; Wang et\u00a0al., 2023b ) and open-ended tasks (Sun et\u00a0al., 2023 ; Liu et\u00a0al., 2023b ) .\nClose-ended tasks use yes-or-no or multiple-choice questions to test for hallucinations, focusing on accuracy. The POPE (Li et\u00a0al., 2023b ) benchmark detects non-existent entities, while AMBER Wang et\u00a0al. ( 2023b ) also considers attributes and relationships.\nIn open-ended tasks, such as image captioning or free-form Visual Question Answering (VQA) (Wu et\u00a0al., 2017 ; Jing et\u00a0al., 2020 ) , LLM-free metrics like CHAIR (Rohrbach et\u00a0al., 2018 ) measure the ratio of hallucinated to actual objects in responses.\nOn the other hand, LLM-based metrics, such as MMHalBench (Sun et\u00a0al., 2023 ) and GAVIE (Liu et\u00a0al., 2023b ) , utilize external LLMs like GPT (Achiam et\u00a0al., 2023 ) to assign scores to generated responses and are used in benchmarks. Hallucination evaluation in multi-modal models is more evident in open-ended tasks, as these tasks require a detailed understanding of the image and dense outputs. However, current metrics like object-level CHAIR and caption-level MMHalBench lack fine-grained hallucination analysis. Report issue for preceding element\nCurrent efforts to mitigate hallucinations are mainly divided into training-based and decoding-based strategies (Leng et\u00a0al., 2024 ; Huang et\u00a0al., 2024 ) .\nMainstream training-related approaches (Yu et\u00a0al., 2024 ; Sun et\u00a0al., 2023 ) introduce algorithms like RLHF (Ouyang et\u00a0al., 2022 ) and DPO (Rafailov et\u00a0al., 2024 ) from the LLM area into multimodal models. By constructing hallucination preference data, they train a reward model to provide reward supervision or use DPO to reduce multimodal hallucinations. However, the primary challenge with training-based approaches lies in the substantial computational overhead, as they necessitate training additional reward models or incorporating extra training phases. Decoding strategies include approaches like OPERA (Huang et\u00a0al., 2024 ) , which corrects abnormal attention map, and methods like VCD (Leng et\u00a0al., 2024 ) that decouple language priors causing hallucinations, subtracting them from prediction probabilities. Although decoding strategies have the advantage of being training-free, they do not address the root cause of hallucinations in multimodal models, as these issues originate during training. Moreover, from a practical standpoint, the inference cost for large models often exceeds the training cost, since models are trained once but deployed countless times. In the work, we propose a novel, simple yet effective training strategy that avoids the additional training overhead of previous training-based approaches while offering a more comprehensive solution. Report issue for preceding element\nAn effective hallucination metric should be both fine-grained and comprehensive.\nCurrent metrics fall short of these criteria. For example, CHAIR focuses on matching objects while failing to measure hallucinations about attributes and relationships. MMHalbench uses GPT-4 to produce a single holistic score but lacks detailed analysis. Moreover, prior metrics only measure the degree of hallucination without assessing the comprehensiveness of the image captioning, typically favoring a short but confident answer, which is inconsistent with users\u2019 subjective experience. Report issue for preceding element\nWe introduce HalFscore which measures both hallucination and completeness of dense captions with fine granularity.\nAs illustrated in Figure 3 , we construct graphs for both the MLLM\u2019s output and the ground truth.\nHere, we leverage dense annotations as ground truth, which provides sufficient detailed descriptions\nthat reflect all the aspects of the input images. Specifically, we selected 1,000 images from the Densely\nCaptioned Images (DCI) dataset (Urbanek et\u00a0al., 2023 ) , in which images are manually annotated and densely captioned.\nBy comparing the graphs, we can identify hallucinations\u2014concepts generated by the model that contradict the ground truth, and omissions\u2014concepts present in the ground truth but absent in the model\u2019s captions. We denote the concepts denoted in the generation as \ud835\udc9e gen subscript \ud835\udc9e gen \\mathcal{C}_{\\textnormal{gen}} caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , the concepts corresponding to ground truths as \ud835\udc9e gt subscript \ud835\udc9e gt \\mathcal{C}_{\\textnormal{gt}} caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT , and compute the precision and recall as Report issue for preceding element Precision = | \ud835\udc9e gen \u2229 \ud835\udc9e gt | | \ud835\udc9e gen | = 1 \u2212 | \ud835\udc9e hallucinated | | \ud835\udc9e gen | , absent subscript \ud835\udc9e gen subscript \ud835\udc9e gt subscript \ud835\udc9e gen 1 subscript \ud835\udc9e hallucinated subscript \ud835\udc9e gen \\displaystyle=\\frac{|\\mathcal{C}_{\\textnormal{gen}}\\cap\\mathcal{C}_{%\n\\textnormal{gt}}|}{|\\mathcal{C}_{\\textnormal{gen}}|}=1-\\frac{|\\mathcal{C}_{%\n\\textnormal{hallucinated}}|}{|\\mathcal{C}_{\\textnormal{gen}}|}, = divide start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT \u2229 caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | end_ARG = 1 - divide start_ARG | caligraphic_C start_POSTSUBSCRIPT hallucinated end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | end_ARG , (1) Recall = | \ud835\udc9e gen \u2229 \ud835\udc9e gt | | \ud835\udc9e gt | = 1 \u2212 | \ud835\udc9e omitted | | \ud835\udc9e gt | . absent subscript \ud835\udc9e gen subscript \ud835\udc9e gt subscript \ud835\udc9e gt 1 subscript \ud835\udc9e omitted subscript \ud835\udc9e gt \\displaystyle=\\frac{|\\mathcal{C}_{\\textnormal{gen}}\\cap\\mathcal{C}_{%\n\\textnormal{gt}}|}{|\\mathcal{C}_{\\textnormal{gt}}|}=1-\\frac{|\\mathcal{C}_{%\n\\textnormal{omitted}}|}{|\\mathcal{C}_{\\textnormal{gt}}|}. = divide start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT \u2229 caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG = 1 - divide start_ARG | caligraphic_C start_POSTSUBSCRIPT omitted end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG . (2) Here, the precision reflects the hallucination degree, whereas the recall assesses how well the captions cover image details. Then we compute HalFscore by aggregating these two scores, serving as a single metric that reflects the overall captioning quality: Report issue for preceding element HalFscore = 2 \u00d7 Precision \u00d7 Recall Precision + Recall . HalFscore 2 Precision Recall Precision Recall \\textnormal{HalFscore}=2\\times\\frac{\\textnormal{Precision}\\times\\textnormal{%\nRecall}}{\\textnormal{Precision}+\\textnormal{Recall}}. HalFscore = 2 \u00d7 divide start_ARG Precision \u00d7 Recall end_ARG start_ARG Precision + Recall end_ARG . (3)\nTo derive the above HalFscore, graph construction and matching are pivotal. We propose a novel triplet data structure to represent the information for captions. Specifically, we prompt the GPT-4o model to parse the concepts along with their relative relationships, and represent them by the triplet representation, i.e. , \u27e8 e 1 , e 2 , r 12 \u27e9 subscript \ud835\udc52 1 subscript \ud835\udc52 2 subscript \ud835\udc5f 12 \\langle e_{1},e_{2},r_{12}\\rangle \u27e8 italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT \u27e9 .\nAs shown in Figure 4 , the triplet representation can well model general and rich information, such as relative relationship between instances, e.g. , \u27e8 clock , on , wall \u27e9 clock on wall \\langle\\texttt{clock},\\texttt{on},\\texttt{wall}\\rangle \u27e8 clock , on , wall \u27e9 , or the attribute description for a single instance, e.g. , \u27e8 mirror , is , pink \u27e9 mirror is pink \\langle\\texttt{mirror},\\texttt{is},\\texttt{pink}\\rangle \u27e8 mirror , is , pink \u27e9 . In this way, the caption is transformed to a set of triplets, serving as a structured representation. Report issue for preceding element\nMeanwhile, the triplet can be viewed as two nodes representing entities, e 1 subscript \ud835\udc52 1 e_{1} italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and e 2 subscript \ud835\udc52 2 e_{2} italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , with a relation edge r \ud835\udc5f r italic_r . We then organize the extracted triplets into a concept graph \ud835\udca2 \ud835\udca2 \\mathcal{G} caligraphic_G by matching entity nodes. In this graph, nodes represent entity concepts e i subscript \ud835\udc52 \ud835\udc56 e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and edges represent relational concepts r i \u2062 j subscript \ud835\udc5f \ud835\udc56 \ud835\udc57 r_{ij} italic_r start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT between entities e i subscript \ud835\udc52 \ud835\udc56 e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and e j subscript \ud835\udc52 \ud835\udc57 e_{j} italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . This graph allows us to represent all the information in the caption comprehensively and accurately. Formally, we obtain the graph \ud835\udca2 \ud835\udca2 \\mathcal{G} caligraphic_G with nodes \ud835\udcb1 \ud835\udcb1 \\mathcal{V} caligraphic_V and edges \u2130 \u2130 \\mathcal{E} caligraphic_E as follows: Report issue for preceding element \ud835\udca2 = ( \ud835\udcb1 , \u2130 ) , \ud835\udcb1 = { e 1 , e 2 , \u2026 , e n } , \u2130 = { r 12 , r 23 , \u2026 , r m \u2062 n } . formulae-sequence \ud835\udca2 \ud835\udcb1 \u2130 formulae-sequence \ud835\udcb1 subscript \ud835\udc52 1 subscript \ud835\udc52 2 \u2026 subscript \ud835\udc52 \ud835\udc5b \u2130 subscript \ud835\udc5f 12 subscript \ud835\udc5f 23 \u2026 subscript \ud835\udc5f \ud835\udc5a \ud835\udc5b \\mathcal{G}=(\\mathcal{V},\\mathcal{E}),\\quad\\mathcal{V}=\\{e_{1},e_{2},\\dots,e_{%\nn}\\},\\quad\\mathcal{E}=\\{r_{12},r_{23},\\dots,r_{mn}\\}. caligraphic_G = ( caligraphic_V , caligraphic_E ) , caligraphic_V = { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } , caligraphic_E = { italic_r start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 23 end_POSTSUBSCRIPT , \u2026 , italic_r start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT } . (4)\nBy matching the constructed graphs, we can identify the hallucinated concepts ( \ud835\udc9e hallucinated subscript \ud835\udc9e hallucinated \\mathcal{C}_{\\textnormal{hallucinated}} caligraphic_C start_POSTSUBSCRIPT hallucinated end_POSTSUBSCRIPT ) and omitted concepts ( \ud835\udc9e omitted subscript \ud835\udc9e omitted \\mathcal{C}_{\\textnormal{omitted}} caligraphic_C start_POSTSUBSCRIPT omitted end_POSTSUBSCRIPT ) using GPT-4o. Based on these identified concepts, we proceed to calculate precision and recall according to the equations aforementioned, and then derive the Fscore. Please see the Appendix A.8 for detailed GPT-4o prompts used for triplet extraction and graph matching. Report issue for preceding element\nTo mitigate the over-reliance on language priors in multimodal models, we introduce a novel training framework that introduces adaptive, context-specific perturbations in the textual inputs during training. This approach simulates the effect of language priors and forces the model to adjust its responses based on visual data rather than textual biases. Report issue for preceding element\nSpecifically, during the instruction tuning training, the multimodal model is tasked with predicting the text according to the input image-question tokens ( I , x q ) \ud835\udc3c subscript \ud835\udc65 \ud835\udc5e (I,x_{q}) ( italic_I , italic_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . We introduce a contextually adaptive perturbation text x p subscript \ud835\udc65 \ud835\udc5d x_{p} italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , crafted to mimic misleading language priors. Thus, we obtain the perturbed input tokens as ( I , x p , x q ) \ud835\udc3c subscript \ud835\udc65 \ud835\udc5d subscript \ud835\udc65 \ud835\udc5e (I,x_{p},x_{q}) ( italic_I , italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , as depicted in Figure 5 . This perturbation is integrated seamlessly as part of the input, without any direct loss computation on x p subscript \ud835\udc65 \ud835\udc5d x_{p} italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT itself, thus maintaining the integrity of the model\u2019s original training regime. The purpose of introducing the perturbation text x p subscript \ud835\udc65 \ud835\udc5d x_{p} italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is to induce errors by tempting the model to rely on the perturbative text. This compels the model to focus more on the image content to generate correct responses, thereby reducing its dependency on linguistic cues that are not supported by visual evidence. Report issue for preceding element\nTo ensure effectiveness and naturalness of the perturbations, we adhere to the following principles. 1) Contextual relevance . the perturbation is expected to be contextually relevant to the image content such that it appears to be plausible but misleading. 2) Alignment with pretrained knowledge . The perturbations are designed to resonate with common language priors, ensuring that they are realistic and reflect potential model biases. 3) Semantic variation . We ensure a diverse range of perturbations by varying the structure and thematic elements of x p subscript \ud835\udc65 \ud835\udc5d x_{p} italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , aligning them with common misconceptions or biases. In practice, we use GPT-4o to generate the perturbation text. The GPT-4o model views the image, question and answer, and is instructed to construct strong and diverse perturbations based on the world knowledge as well as certain image details, without disclosing the answer. The GPT-4 instruction prompt is detailed in the appendix. Report issue for preceding element\nOur approach offers significant benefits over prior approaches as summarized in Table 1 . Compared to the training method like RLAIF-V, we avoid the need to collect costly preference data or train an additional reward model. Our method is easy to implement and incurs minimal additional training cost to the multimodal model, making it a nearly \u201cfree lunch\u201d solution. The quantification of the additional training overhead is provided in the Appendix A.3 . Compared to contrastive decoding strategies, our approach more fundamentally mitigates the multimodal model\u2019s excessive reliance on language priors, without introducing any additional inference overhead. Moreover, the effectiveness of our method scales with the quality and diversity of the perturbation texts. Report issue for preceding element\nOur method can be further understood and explained from a mathematical perspective. In a more general sense, it can be interpreted as introducing additional noise or perturbation during the training phase to develop a more robust model.\nThis approach aligns with the ideas proposed in the seminal work (Clark et\u00a0al., 2019 ) .\nThus, we leverage the mathematical reasoning from that work to explain our approach. Similarly, in multimodal tasks, we define the following: x k subscript \ud835\udc65 \ud835\udc58 x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is the k \ud835\udc58 k italic_k -th token predicted by the model, x < k subscript \ud835\udc65 absent \ud835\udc58 x_{<k} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT denotes the preceding tokens in the output, I \ud835\udc3c I italic_I refers to the corresponding image, x < k p superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d x_{<k}^{p} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT indicates the prediction based solely on language priors, while x < k \u2212 p superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d x_{<k}^{-p} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT is the prediction made by the multimodal model without the influence of language prior. Thus we have Report issue for preceding element p \u2062 ( x k | x < k , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc3c \\displaystyle p(x_{k}|x_{<k},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT , italic_I ) = p \u2062 ( x k | x < k p , x < k \u2212 p , I ) absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \\displaystyle=p(x_{k}|x_{<k}^{p},x_{<k}^{-p},I) = italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) (5) = p \u2062 ( x k | x < k \u2212 p , I ) \u2062 p \u2062 ( x < k p | x k , x < k \u2212 p , I ) p \u2062 ( x < k p | x < k \u2212 p , I ) absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \\displaystyle=\\frac{p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},x_{<k}^{-p},I)}{p%\n(x_{<k}^{p}|x_{<k}^{-p},I)} = divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG (6) \u221d p \u2062 ( x k | x < k \u2212 p , I ) \u2062 p \u2062 ( x < k p | x k , x < k \u2212 p , I ) proportional-to absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \\displaystyle\\propto p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},x_{<k}^{-p},I) \u221d italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) (7) = p \u2062 ( x k | x < k \u2212 p , I ) \u2062 p \u2062 ( x < k p | x k , I ) absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 \ud835\udc3c \\displaystyle=p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},I) = italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_I ) (8) = p \u2062 ( x k | x < k \u2212 p , I ) \u2062 p \u2062 ( x k | x < k p , I ) \u2062 p \u2062 ( x < k p ) p \u2062 ( x k ) absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 \\displaystyle=p(x_{k}|x_{<k}^{-p},I)\\frac{p(x_{k}|x_{<k}^{p},I)p(x_{<k}^{p})}{%\np(x_{k})} = italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG (9) \u221d p \u2062 ( x k | x < k \u2212 p , I ) \u2062 p \u2062 ( x k | x < k p , I ) p \u2062 ( x k ) . proportional-to absent \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 \\displaystyle\\propto p(x_{k}|x_{<k}^{-p},I)\\frac{p(x_{k}|x_{<k}^{p},I)}{p(x_{k%\n})}. \u221d italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG . (10) Equations (6) and (9) are applications of Bayes\u2019 theorem. x < k p superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d x_{<k}^{p} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT and x < k \u2212 p superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d x_{<k}^{-p} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT are mutually independent and Equation (8) follows from the conditional independence assumption. Report issue for preceding element\nNow take a look at the Equations (10). p \u2062 ( x k ) \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 p(x_{k}) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) models the probability of the next token. In a sufficiently uniform dataset, p \u2062 ( x k ) \ud835\udc5d subscript \ud835\udc65 \ud835\udc58 p(x_{k}) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) can be ignored. p \u2062 ( x k | x < k \u2212 p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{-p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) represents the behavior we expect from the multimodal model when it predicts the next token based on the image without interference from language priors. However, this probability is difficult to model directly. Instead, we designed a perturbative training that adds perturbed text before the instruction to enhance p \u2062 ( x k | x < k p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) , encouraging the model to learn p \u2062 ( x k | x < k \u2212 p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{-p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) . Here, p \u2062 ( x k | x < k p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) represents the bias introduced by the perturbation text. Since x < k p superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d x_{<k}^{p} italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT as we define it represents predictions based solely on language priors, the term p \u2062 ( x k | x < k p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) can exclude the image and become p \u2062 ( x k | x < k p ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d p(x_{k}|x_{<k}^{p}) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) , which represents the language model\u2019s prediction tendency when presented with the perturbed text. During the training of multimodal models, we can assume that the world knowledge embedded in the language model remains unchanged, so p \u2062 ( x k | x < k p ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d p(x_{k}|x_{<k}^{p}) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) is a perturbation term that cannot be optimized. Therefore, in the training process of multimodal models, our perturbation training method guides the model to optimize towards p \u2062 ( x k | x < k \u2212 p , I ) \ud835\udc5d conditional subscript \ud835\udc65 \ud835\udc58 superscript subscript \ud835\udc65 absent \ud835\udc58 \ud835\udc5d \ud835\udc3c p(x_{k}|x_{<k}^{-p},I) italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) , transforming it into a fully multimodal model that is unaffected by language priors and relies entirely on image information to answer questions. Report issue for preceding element\nTo ensure the reliability and credibility of the results, we conducted experiments on the open-source and widely-used LLaVA1.5 dataset.\nBy utilizing the LLaVA1.5 dataset, we fully replicated LLaVA1.5 training process and results, and use it as the baseline for our experiments.\nTo ensure fairness, we apply perturbations directly to the LLaVA1.5 dataset rather than incorporating additional data.\nWe conduct experiments by generating perturbed text using GPT-4o on the 160k data related to the VQA task in the dataset. Report issue for preceding element\nWe selected three representative methods, each addressing hallucination from different perspectives and demonstrating strong performance: the training-based RLAIF-V (Yu et\u00a0al., 2024 ) , the decoding strategy OPERA (Huang et\u00a0al., 2024 ) and VCD (Leng et\u00a0al., 2024 ) . For RLAIF-V, we use the open-source model weights of RLAIF-7B, which were fine-tuned on LLaVA1.5 (Liu et\u00a0al., 2024b ) . Notably, the reward model used in this setup is LLaVA-Next 34B (Liu et\u00a0al., 2024a ) , which may transfer some of LLaVA-Next 34B\u2019s capabilities into RLAIF-7B, potentially making the comparison with other experimental setups less fair. The hyperparameters for OPERA and VCD are provided in Section A.5 to support the reproducibility of our results. We use beam search as the default decoding strategy, with N beams subscript \ud835\udc41 beams N_{\\textnormal{beams}} italic_N start_POSTSUBSCRIPT beams end_POSTSUBSCRIPT set to 5. Report issue for preceding element\nWe additionally evaluate six best multimodal models available today, Ovis1.6 (Lu et\u00a0al., 2024b ) , Qwen2-VL (Wang et\u00a0al., 2024 ) , LLaVA-OneVision (Li et\u00a0al., 2024 ) , InternVL2 (OpenGVLab, 2024 ) , Idefics3 (Lauren\u00e7on et\u00a0al., 2024 ) , and MiniCPM-2.6 (Hu et\u00a0al., 2024 ) , to further verify the effectiveness of our proposed metric.\nThese models have been trained on vast datasets, comprising hundreds of millions of samples, and required significant computational resources, including extensive GPU hours.\nTheir general performance substantially surpasses that of LLaVA1.5. Report issue for preceding element\nTable 2 shows the our final HalFscore includes precision, recall, and fscore, and further classifies hallucinations into object, attribute, and relation types for more detailed evaluation.\nFrom this metric, we observe that compared to the LLaVA1.5 baseline, the leading methods have achieved substantial and comprehensive improvements in both precision and recall.\nNotably, there is a significant reduction in hallucinations when describing objects and attributes, although there remains room for improvement in relation hallucinations.\nOur method also demonstrates marked improvement over LLaVA1.5, with a +6.2 increase in precision, +0.7 in recall, and +2.4 in Fscore. In comparisons with OPERA, VCD, and RLAIF-V, our approach achieved the best results in precision and fscore, though the improvement in recall is less pronounced.\nOverall, by leveraging these hallucination mitigation techniques, OPERA, VCD, and RLAIF-V have achieved recall scores comparable to MiniCPM-2.6, while our method has even surpassed MiniCPM, further underscoring the effectiveness of our approach. Report issue for preceding element\nTo evaluate caption-based object hallucination, we use the CHAIR metric (Rohrbach et\u00a0al., 2018 ) , which measures the percentage of generated words that correspond to actual objects in the image.\nThe results in Table 3 show that, on this metric, our method significantly reduces hallucinations compared to the LLaVA1.5 baseline and outperforms both OPERA and VCD by a large margin.\nHowever, RLAIF-V performs better on this metric, showing fewer object hallucinations.\nA possible reason is that it uses additional feedback model.\nMoving forward, we plan to design targeted perturbation texts for objects to further enhance performance. Report issue for preceding element\nHallusionBench (Guan et\u00a0al., 2024 ) evaluates how well multi-modal models handle language hallucinations and visual illusions.\nPrevious hallucination assessment mainly focused on dense caption tasks, and by incorporating HallusionBench, we aim to enhance our hallucination evaluation through the addition of multiple-choice and vision reasoning tasks.\nThe results in Table 3 demonstrate a substantial improvement (+0.6) of our method on the HallusionBench metric, whereas the VCD decoding strategy merely maintained the baseline, OPERA led to a modest increase of 0.2.\nThis outcome further confirms that our method is not only effective in dense captioning tasks but also excels in vision reasoning tasks. Report issue for preceding element\nWe assess the models\u2019 general ability on three widely used benchmarks: MMBench (Liu et\u00a0al., 2023c ) , CCBench (Liu et\u00a0al., 2023c ) , and SEEDImage (Li et\u00a0al., 2023a ) , to verify that our model\u2019s general capabilities.\nFrom the Table 3 , we observe that both VCD and RLAIF-V result in some degradation in general performance, while OPERA remains stable.\nIn contrast, our method not only avoids degradation in general metrics but also shows improvements across multiple areas, with gains of +1.6 in MMB, +0.3 in SEEDImage, and +1.2 in CCBench.\nWe attribute this improvement in generalization to perturbation training, which compels the multimodal model to more effectively leverage image information. Report issue for preceding element\nTo assess the effect of perturbation levels on training, we devised three methods of inserting perturbation text, detailed in Table 4 . The first method alerts the model to upcoming text perturbations, instructing it to ignore them. The second focuses the model on image regions without referencing perturbations. The third offers no guidance. These methods progressively increase the level of perturbation and the difficulty of training the multimodal model. As Table 5 shows, increasing perturbation levels enhances training difficulty and reduces hallucinations, as indicated by HalFscore and CHAIR metrics. However, this also leads to shorter captions and lower recall, suggesting a more cautious model behavior. General performance metrics decline consistently, implying that while perturbations decrease hallucinations, excessive perturbation use may impair overall performance. Report issue for preceding element\nTo investigate the impact of relevance of perturbation text, we conducted experiments using randomly sampled texts from a text dataset instead of the carefully designed perturbations used earlier.\nThe results in Table 5 show that introducing random text perturbations, compared to the LLaVA1.5 baseline, still mitigates hallucinations in multimodal models, as evidenced by improvements in HaFscore and HalBench. However, the effectiveness of random perturbations is inferior to that of targeted text-based experiments. We suggest that even random perturbations influence the training of multimodal models, as the training is disrupted by the random text interference. However, more relevant perturbations exert a stronger disruptive effect, prompting the model to focus more on the image to overcome these challenges. Report issue for preceding element\nWe perform the user study to explore whether the proposed HalFscore correlates well with human evaluation.\nWe show an image and ask human raters to compare two paragraphs generated\nand determine which generated text describes the main subject and detailed information of the input image more comprehensively (recall_h), and which text contains less information that is not present in the input image (precision_h).\nFour methods, LLaVA 1.5, MiniCPM-2.6, Internvl2, and our method are selected for human evaluation.\nEach method is compared with other methods for 12 12 12 12 times.\nWe compute the human evaluated scores for the recall and precision and drive the Pearson correlations coefficients between the scores of human evaluation and that of HalFscore.\nThe results are shown in the Table 6 .\nWe observe that our precision score and recall score align with human evaluations.\nSpecifically, we compare our method with another metric, MMhalbench.\nThe correlation coefficient between the metric and the human evaluations of precision scores is relatively smaller than HalFscore.\nThe comparison demonstrates our metric aligns with human evaluations more than directly asking large language models to rate the level of hallucination.\nFurther details about the user study can be found in Section A.7 of the Appendix. Report issue for preceding element\nPerturboLLaVA is a novel training strategy that enhances model performance in the SFT phase.\nIt can be integrated seamlessly with post-SFT optimization RLAIF-V and OPERA during inference. In our tests, using OPERA as a plugin with PerturboLLaVA during inference led to performance boosts, as detailed in Table 3 . PerturboLLaVA with OPERA achieved further gains, improving HalFscore by 0.6, with a 3-point improvement in the CHAIR s . Hallucination reasoning in Hallusionbench also improved by +0.1, while general capability increased by 0.4 in CCBench.\nCompared to applying OPERA on LLaVA1.5, replacing the baseline model with PerturboLLaVA resulted in improved performance in both hallucination reduction and general capability.\nThus, PerturboLLaVA introduces a new optimization direction that complements existing strategies, offering an additional benefit. Report issue for preceding element\nIn our work, we introduce a concept-level HalFscore that enables fine-grained analysis of various hallucinations involving objects, attributes, and relations in dense captions.\nThis metric also reflects the overall captioning capability of multi-modal models. To mitigate hallucinations, we propose a simple yet effective training strategy that guides multi-modal models to reduce reliance on linguistic priors and prioritize image information. Our method outperforms leading state-of-the-art approaches without incurring additional training or inference overhead. Overall, we believe that our proposed metric enhances the evaluation of hallucinations in multi-modal models and that our method effectively mitigates hallucination issues. The proposed method shows great promise of becoming a standard strategy for training robust multimodal models. Report issue for preceding element\nThis work was supported by the National Key R&D Program of China (No.\u00a02022ZD0160101), the Ningbo Science and Technology Bureau\n(Grant Number 2024Z291), and the National Natural Science Foundation of China (No. 62206244).\nCC and ML contributed equally. Part of the work\nwas done when CC was doing an internship at WeChat Group. Report issue for preceding element",
  "masked_text": "Multimodal large language models (MLLMs) [CITATION] have achieved significant strides in complex visual tasks by integrating the world knowledge and reasoning capabilities of large language models (LLMs). Nonetheless, hallucination [CITATION] issue persists in these models, and even the most capable multimodal models often respond with texts that do not accurately reflect the provided visual content. Notably, scaling up model parameters and training data has not proven effective inmitigating this issue for MLLMs, unlike for their unimodal language counterparts.Report issue for preceding element\nIn this paper, we focus on addressing the problem of hallucination in the context of dense image captioning [CITATION], which requires a comprehensive and detailed description of every aspect of an image. Dense image captioning exposes hallucinations more acutely, as models must generate rich and detailed captions for complex scenes while ensuring fidelity to the visual content. Hallucinations undermine the reliability of MLLMs in applications that require precise and faithful visual descriptions. To tackle the hallucination challenge in MLLMs, we recognize the need for a robust quantitative metric that accurately reflects caption quality regarding hallucination. Hence, we introduce HalFscore, a novel metric providing a more granular and comprehensive evaluation of hallucinations specific to dense captioning. HalFscore measures both the accuracy and completeness of dense captions by identifying the incorrect elements and assessing the missing details, offering a balanced view of the model\u2019s performance. To achieve this, we propose to build the language graph that captures the main concepts along with their relationships, and compute its discrepancy against the ground truth. HalFscore aggregates the precision and recall to model the accuracy and completeness of dense captioning results. Compared to previous hallucination scores [CITATION], the proposed HalFscore offers a more fine-grained and holistic evaluation, serving as a valuable guide when developing the hallucination suppression method.Report issue for preceding element\nWe further analyze the common cases plagued with hallucinations, and conjecture that the issue in MLLMs comes from the over-reliance on the model\u2019s pretrained linguistic knowledge. As shown in Figure 1, as more texts are produced, the multimodal model gradually deviates from multimodal generation to unimodal mode, which favors text generation based on the preceding language patterns while overlooking the actual visual information. For example, when presented with an image of a green banana, a model has the tendency to describe it as yellow because of the common knowledge that ripe bananas are typically yellow. In fact, concurrent MLLMs are obtained by continuous training from a pre-trained LLM and are generally equipped with a strong language bias.Report issue for preceding element\nInspired by this, we propose a simple and effective training strategy that reduces the model\u2019s heavy dependence on the language prior by incorporating adversarially perturbed text during training. Specifically, we introduce carefully designed perturbations that aligns with the general knowledge but conflict with the visual content, intentionally misleading the model based on its language prior. For example, we might introduce the perturbation \u201cAs bananas ripen, their color gradually turns yellow\u201d before asking about the color of a green banana in the image. This perturbative training enforces the model to scrutinize the image content when predicting every token, rather than hallucinating contents from the text hints. Essentially, our method adjusts the model\u2019s conditional distribution to depend more heavily on the image and less on the perturbation text, which leads to more robust multimodal capability.Report issue for preceding element\nAs opposed to state-of-the-art methods that resort to more advanced decoding strategies [CITATION], the proposed method, PertuboLLaVA, effectively suppresses the hallucinations in MLLMs without incurring additional training or inference costs, making it more suitable for real-world applications. Figure 2 shows our method can describe rich image details with less hallucinations. On the other hand, our method is much more efficient, scalable, and easier to adopt compared to RLHF-based methods which require additional human preference data and incur substantial training overhead and complexity. Additionally, we find the proposed method beneficial to general multimodal abilities, bringing boosted performance across all the multimodal benchmarks.Report issue for preceding element\nTo summarize, our contributions are two-fold. First, we introduce a more principled metric computed on the language graph, serving as a comprehensive hallucination measure. Second, we identify the root cause of hallucinations in MLLMs as its inherent language bias, and propose perturbative visual training, enhancing the model\u2019s focus on visual content during training. The proposed method integrates seamlessly into existing training pipelines, introducing minimal additional cost. It provides a scalable, efficient solution to enhance multimodal models\u2019 visual understanding capabilities, excelling over prior compared to state-of-the-art methods across multiple dimensions.Report issue for preceding element\nWith the rapid advancement of large language models [CITATION], researchers are leveraging their knowledge and reasoning abilities to build multimodal systems for complex tasks. These models are typically constructed with a pretrained vision encoder to process visual information, a language model backbone responsible for reasoning, and a projector to map the visual data into textual space. The training process of multimodal model is generally divided into two stages: pretraining, using image captioning data for modality alignment, and instruction fine-tuning, using question-answer data to enable task handling. Several leading open-source projects [CITATION] have amassed large multimodal datasets and developed high-performing models using advanced language model backbones and vision encoders. However, hallucinations remain a persistent challenge in models\u2019 outputs.Report issue for preceding element\nVarious benchmarks assess hallucination in MLLMs, divided into, categorized into close-ended [CITATION] and open-ended tasks [CITATION]. Close-ended tasks use yes-or-no or multiple-choice questions to test for hallucinations, focusing on accuracy. The POPE [CITATION] benchmark detects non-existent entities, while AMBER [CITATION] also considers attributes and relationships. In open-ended tasks, such as image captioning or free-form Visual Question Answering (VQA) [CITATION], LLM-free metrics like CHAIR [CITATION] measure the ratio of hallucinated to actual objects in responses. On the other hand, LLM-based metrics, such as MMHalBench [CITATION] and GAVIE [CITATION], utilize external LLMs like GPT [CITATION] to assign scores to generated responses and are used in benchmarks. Hallucination evaluation in multi-modal models is more evident in open-ended tasks, as these tasks require a detailed understanding of the image and dense outputs. However, current metrics like object-level CHAIR and caption-level MMHalBench lack fine-grained hallucination analysis.Report issue for preceding element\nCurrent efforts to mitigate hallucinations are mainly divided into training-based and decoding-based strategies [CITATION]. Mainstream training-related approaches [CITATION] introduce algorithms like RLHF [CITATION] and DPO [CITATION] from the LLM area into multimodal models. By constructing hallucination preference data, they train a reward model to provide reward supervision or use DPO to reduce multimodal hallucinations. However, the primary challenge with training-based approaches lies in the substantial computational overhead, as they necessitate training additional reward models or incorporating extra training phases. Decoding strategies include approaches like OPERA [CITATION], which corrects abnormal attention map, and methods like VCD [CITATION] that decouple language priors causing hallucinations, subtracting them from prediction probabilities. Although decoding strategies have the advantage of being training-free, they do not address the root cause of hallucinations in multimodal models, as these issues originate during training. Moreover, from a practical standpoint, the inference cost for large models often exceeds the training cost, since models are trained once but deployed countless times. In the work, we propose a novel, simple yet effective training strategy that avoids the additional training overhead of previous training-based approaches while offering a more comprehensive solution.Report issue for preceding element\nAn effective hallucination metric should be both fine-grained and comprehensive. Current metrics fall short of these criteria. For example, CHAIR focuses on matching objects while failing to measure hallucinations about attributes and relationships. MMHalbench uses GPT-4 to produce a single holistic score but lacks detailed analysis. Moreover, prior metrics only measure the degree of hallucination without assessing the comprehensiveness of the image captioning, typically favoring a short but confident answer, which is inconsistent with users\u2019 subjective experience.Report issue for preceding element\nWe introduce HalFscore which measures both hallucination and completeness of dense captions with fine granularity. As illustrated in Figure 3, we construct graphs for both the MLLM\u2019s output and the ground truth. Here, we leverage dense annotations as ground truth, which provides sufficient detailed descriptions that reflect all the aspects of the input images. Specifically, we selected 1,000 images from the Densely Captioned Images (DCI) dataset [CITATION], in which images are manually annotated and densely captioned. By comparing the graphs, we can identify hallucinations\u2014concepts generated by the model that contradict the ground truth, and omissions\u2014concepts present in the ground truth but absent in the model\u2019s captions. We denote the concepts denoted in the generation as \ud835\udc9egensubscript\ud835\udc9egen\\mathcal{C}_{\\textnormal{gen}}caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT, the concepts corresponding to ground truths as \ud835\udc9egtsubscript\ud835\udc9egt\\mathcal{C}_{\\textnormal{gt}}caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT, and compute the precision and recall asReport issue for preceding element Precision =|\ud835\udc9egen\u2229\ud835\udc9egt||\ud835\udc9egen|=1\u2212|\ud835\udc9ehallucinated||\ud835\udc9egen|,absentsubscript\ud835\udc9egensubscript\ud835\udc9egtsubscript\ud835\udc9egen1subscript\ud835\udc9ehallucinatedsubscript\ud835\udc9egen\\displaystyle=\\frac{|\\mathcal{C}_{\\textnormal{gen}}\\cap\\mathcal{C}_{% \\textnormal{gt}}|}{|\\mathcal{C}_{\\textnormal{gen}}|}=1-\\frac{|\\mathcal{C}_{% \\textnormal{hallucinated}}|}{|\\mathcal{C}_{\\textnormal{gen}}|},= divide start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT \u2229 caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | end_ARG = 1 - divide start_ARG | caligraphic_C start_POSTSUBSCRIPT hallucinated end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | end_ARG , (1) Recall =|\ud835\udc9egen\u2229\ud835\udc9egt||\ud835\udc9egt|=1\u2212|\ud835\udc9eomitted||\ud835\udc9egt|.absentsubscript\ud835\udc9egensubscript\ud835\udc9egtsubscript\ud835\udc9egt1subscript\ud835\udc9eomittedsubscript\ud835\udc9egt\\displaystyle=\\frac{|\\mathcal{C}_{\\textnormal{gen}}\\cap\\mathcal{C}_{% \\textnormal{gt}}|}{|\\mathcal{C}_{\\textnormal{gt}}|}=1-\\frac{|\\mathcal{C}_{% \\textnormal{omitted}}|}{|\\mathcal{C}_{\\textnormal{gt}}|}.= divide start_ARG | caligraphic_C start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT \u2229 caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG = 1 - divide start_ARG | caligraphic_C start_POSTSUBSCRIPT omitted end_POSTSUBSCRIPT | end_ARG start_ARG | caligraphic_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT | end_ARG . (2) Here, the precision reflects the hallucination degree, whereas the recall assesses how well the captions cover image details. Then we compute HalFscore by aggregating these two scores, serving as a single metric that reflects the overall captioning quality:Report issue for preceding element HalFscore=2\u00d7Precision\u00d7RecallPrecision+Recall.HalFscore2PrecisionRecallPrecisionRecall\\textnormal{HalFscore}=2\\times\\frac{\\textnormal{Precision}\\times\\textnormal{% Recall}}{\\textnormal{Precision}+\\textnormal{Recall}}.HalFscore = 2 \u00d7 divide start_ARG Precision \u00d7 Recall end_ARG start_ARG Precision + Recall end_ARG . (3)\nTo derive the above HalFscore, graph construction and matching are pivotal. We propose a novel triplet data structure to represent the information for captions. Specifically, we prompt the GPT-4o model to parse the concepts along with their relative relationships, and represent them by the triplet representation, i.e., \u27e8e1,e2,r12\u27e9subscript\ud835\udc521subscript\ud835\udc522subscript\ud835\udc5f12\\langle e_{1},e_{2},r_{12}\\rangle\u27e8 italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT \u27e9. As shown in Figure 4, the triplet representation can well model general and rich information, such as relative relationship between instances, e.g., \u27e8clock,on,wall\u27e9clockonwall\\langle\\texttt{clock},\\texttt{on},\\texttt{wall}\\rangle\u27e8 clock , on , wall \u27e9, or the attribute description for a single instance, e.g., \u27e8mirror,is,pink\u27e9mirrorispink\\langle\\texttt{mirror},\\texttt{is},\\texttt{pink}\\rangle\u27e8 mirror , is , pink \u27e9. In this way, the caption is transformed to a set of triplets, serving as a structured representation.Report issue for preceding element\nMeanwhile, the triplet can be viewed as two nodes representing entities, e1subscript\ud835\udc521e_{1}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and e2subscript\ud835\udc522e_{2}italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, with a relation edge r\ud835\udc5fritalic_r. We then organize the extracted triplets into a concept graph \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G by matching entity nodes. In this graph, nodes represent entity concepts eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and edges represent relational concepts ri\u2062jsubscript\ud835\udc5f\ud835\udc56\ud835\udc57r_{ij}italic_r start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT between entities eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ejsubscript\ud835\udc52\ud835\udc57e_{j}italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. This graph allows us to represent all the information in the caption comprehensively and accurately. Formally, we obtain the graph \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G with nodes \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V and edges \u2130\u2130\\mathcal{E}caligraphic_E as follows:Report issue for preceding element \ud835\udca2=(\ud835\udcb1,\u2130),\ud835\udcb1={e1,e2,\u2026,en},\u2130={r12,r23,\u2026,rm\u2062n}.formulae-sequence\ud835\udca2\ud835\udcb1\u2130formulae-sequence\ud835\udcb1subscript\ud835\udc521subscript\ud835\udc522\u2026subscript\ud835\udc52\ud835\udc5b\u2130subscript\ud835\udc5f12subscript\ud835\udc5f23\u2026subscript\ud835\udc5f\ud835\udc5a\ud835\udc5b\\mathcal{G}=(\\mathcal{V},\\mathcal{E}),\\quad\\mathcal{V}=\\{e_{1},e_{2},\\dots,e_{% n}\\},\\quad\\mathcal{E}=\\{r_{12},r_{23},\\dots,r_{mn}\\}.caligraphic_G = ( caligraphic_V , caligraphic_E ) , caligraphic_V = { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } , caligraphic_E = { italic_r start_POSTSUBSCRIPT 12 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 23 end_POSTSUBSCRIPT , \u2026 , italic_r start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT } . (4)\nBy matching the constructed graphs, we can identify the hallucinated concepts (\ud835\udc9ehallucinatedsubscript\ud835\udc9ehallucinated\\mathcal{C}_{\\textnormal{hallucinated}}caligraphic_C start_POSTSUBSCRIPT hallucinated end_POSTSUBSCRIPT) and omitted concepts (\ud835\udc9eomittedsubscript\ud835\udc9eomitted\\mathcal{C}_{\\textnormal{omitted}}caligraphic_C start_POSTSUBSCRIPT omitted end_POSTSUBSCRIPT) using GPT-4o. Based on these identified concepts, we proceed to calculate precision and recall according to the equations aforementioned, and then derive the Fscore. Please see the Appendix A.8 for detailed GPT-4o prompts used for triplet extraction and graph matching.Report issue for preceding element\nTo mitigate the over-reliance on language priors in multimodal models, we introduce a novel training framework that introduces adaptive, context-specific perturbations in the textual inputs during training. This approach simulates the effect of language priors and forces the model to adjust its responses based on visual data rather than textual biases.Report issue for preceding element\nSpecifically, during the instruction tuning training, the multimodal model is tasked with predicting the text according to the input image-question tokens (I,xq)\ud835\udc3csubscript\ud835\udc65\ud835\udc5e(I,x_{q})( italic_I , italic_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ). We introduce a contextually adaptive perturbation text xpsubscript\ud835\udc65\ud835\udc5dx_{p}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, crafted to mimic misleading language priors. Thus, we obtain the perturbed input tokens as (I,xp,xq)\ud835\udc3csubscript\ud835\udc65\ud835\udc5dsubscript\ud835\udc65\ud835\udc5e(I,x_{p},x_{q})( italic_I , italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ), as depicted in Figure 5. This perturbation is integrated seamlessly as part of the input, without any direct loss computation on xpsubscript\ud835\udc65\ud835\udc5dx_{p}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT itself, thus maintaining the integrity of the model\u2019s original training regime. The purpose of introducing the perturbation text xpsubscript\ud835\udc65\ud835\udc5dx_{p}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is to induce errors by tempting the model to rely on the perturbative text. This compels the model to focus more on the image content to generate correct responses, thereby reducing its dependency on linguistic cues that are not supported by visual evidence.Report issue for preceding element\nTo ensure effectiveness and naturalness of the perturbations, we adhere to the following principles. 1) Contextual relevance. the perturbation is expected to be contextually relevant to the image content such that it appears to be plausible but misleading. 2) Alignment with pretrained knowledge. The perturbations are designed to resonate with common language priors, ensuring that they are realistic and reflect potential model biases. 3) Semantic variation. We ensure a diverse range of perturbations by varying the structure and thematic elements of xpsubscript\ud835\udc65\ud835\udc5dx_{p}italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , aligning them with common misconceptions or biases. In practice, we use GPT-4o to generate the perturbation text. The GPT-4o model views the image, question and answer, and is instructed to construct strong and diverse perturbations based on the world knowledge as well as certain image details, without disclosing the answer. The GPT-4 instruction prompt is detailed in the appendix.Report issue for preceding element\nOur approach offers significant benefits over prior approaches as summarized in Table 1. Compared to the training method like RLAIF-V, we avoid the need to collect costly preference data or train an additional reward model. Our method is easy to implement and incurs minimal additional training cost to the multimodal model, making it a nearly \u201cfree lunch\u201d solution. The quantification of the additional training overhead is provided in the Appendix A.3. Compared to contrastive decoding strategies, our approach more fundamentally mitigates the multimodal model\u2019s excessive reliance on language priors, without introducing any additional inference overhead. Moreover, the effectiveness of our method scales with the quality and diversity of the perturbation texts.Report issue for preceding element\nOur method can be further understood and explained from a mathematical perspective. In a more general sense, it can be interpreted as introducing additional noise or perturbation during the training phase to develop a more robust model. This approach aligns with the ideas proposed in the seminal work [CITATION]. Thus, we leverage the mathematical reasoning from that work to explain our approach. Similarly, in multimodal tasks, we define the following: xksubscript\ud835\udc65\ud835\udc58x_{k}italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is the k\ud835\udc58kitalic_k-th token predicted by the model, x<ksubscript\ud835\udc65absent\ud835\udc58x_{<k}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT denotes the preceding tokens in the output, I\ud835\udc3cIitalic_I refers to the corresponding image, x<kpsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dx_{<k}^{p}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT indicates the prediction based solely on language priors, while x<k\u2212psuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dx_{<k}^{-p}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT is the prediction made by the multimodal model without the influence of language prior. Thus we haveReport issue for preceding element p\u2062(xk|x<k,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58subscript\ud835\udc65absent\ud835\udc58\ud835\udc3c\\displaystyle p(x_{k}|x_{<k},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT , italic_I ) =p\u2062(xk|x<kp,x<k\u2212p,I)absent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\\displaystyle=p(x_{k}|x_{<k}^{p},x_{<k}^{-p},I)= italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) (5) =p\u2062(xk|x<k\u2212p,I)\u2062p\u2062(x<kp|xk,x<k\u2212p,I)p\u2062(x<kp|x<k\u2212p,I)absent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\\displaystyle=\\frac{p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},x_{<k}^{-p},I)}{p% (x_{<k}^{p}|x_{<k}^{-p},I)}= divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG (6) \u221dp\u2062(xk|x<k\u2212p,I)\u2062p\u2062(x<kp|xk,x<k\u2212p,I)proportional-toabsent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\\displaystyle\\propto p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},x_{<k}^{-p},I)\u221d italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) (7) =p\u2062(xk|x<k\u2212p,I)\u2062p\u2062(x<kp|xk,I)absent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dsubscript\ud835\udc65\ud835\udc58\ud835\udc3c\\displaystyle=p(x_{k}|x_{<k}^{-p},I)p(x_{<k}^{p}|x_{k},I)= italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_I ) (8) =p\u2062(xk|x<k\u2212p,I)\u2062p\u2062(xk|x<kp,I)\u2062p\u2062(x<kp)p\u2062(xk)absent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc5dsubscript\ud835\udc65\ud835\udc58\\displaystyle=p(x_{k}|x_{<k}^{-p},I)\\frac{p(x_{k}|x_{<k}^{p},I)p(x_{<k}^{p})}{% p(x_{k})}= italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) italic_p ( italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG (9) \u221dp\u2062(xk|x<k\u2212p,I)\u2062p\u2062(xk|x<kp,I)p\u2062(xk).proportional-toabsent\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3c\ud835\udc5dsubscript\ud835\udc65\ud835\udc58\\displaystyle\\propto p(x_{k}|x_{<k}^{-p},I)\\frac{p(x_{k}|x_{<k}^{p},I)}{p(x_{k% })}.\u221d italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) divide start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) end_ARG start_ARG italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG . (10) Equations (6) and (9) are applications of Bayes\u2019 theorem. x<kpsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dx_{<k}^{p}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT and x<k\u2212psuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dx_{<k}^{-p}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT are mutually independent and Equation (8) follows from the conditional independence assumption.Report issue for preceding element\nNow take a look at the Equations (10). p\u2062(xk)\ud835\udc5dsubscript\ud835\udc65\ud835\udc58p(x_{k})italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) models the probability of the next token. In a sufficiently uniform dataset, p\u2062(xk)\ud835\udc5dsubscript\ud835\udc65\ud835\udc58p(x_{k})italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) can be ignored. p\u2062(xk|x<k\u2212p,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{-p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ) represents the behavior we expect from the multimodal model when it predicts the next token based on the image without interference from language priors. However, this probability is difficult to model directly. Instead, we designed a perturbative training that adds perturbed text before the instruction to enhance p\u2062(xk|x<kp,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ), encouraging the model to learn p\u2062(xk|x<k\u2212p,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{-p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ). Here, p\u2062(xk|x<kp,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) represents the bias introduced by the perturbation text. Since x<kpsuperscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dx_{<k}^{p}italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT as we define it represents predictions based solely on language priors, the term p\u2062(xk|x<kp,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT , italic_I ) can exclude the image and become p\u2062(xk|x<kp)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dp(x_{k}|x_{<k}^{p})italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ), which represents the language model\u2019s prediction tendency when presented with the perturbed text. During the training of multimodal models, we can assume that the world knowledge embedded in the language model remains unchanged, so p\u2062(xk|x<kp)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5dp(x_{k}|x_{<k}^{p})italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) is a perturbation term that cannot be optimized. Therefore, in the training process of multimodal models, our perturbation training method guides the model to optimize towards p\u2062(xk|x<k\u2212p,I)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc58superscriptsubscript\ud835\udc65absent\ud835\udc58\ud835\udc5d\ud835\udc3cp(x_{k}|x_{<k}^{-p},I)italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT , italic_I ), transforming it into a fully multimodal model that is unaffected by language priors and relies entirely on image information to answer questions.Report issue for preceding element\nTo ensure the reliability and credibility of the results, we conducted experiments on the open-source and widely-used LLaVA1.5 dataset. By utilizing the LLaVA1.5 dataset, we fully replicated LLaVA1.5 training process and results, and use it as the baseline for our experiments. To ensure fairness, we apply perturbations directly to the LLaVA1.5 dataset rather than incorporating additional data. We conduct experiments by generating perturbed text using GPT-4o on the 160k data related to the VQA task in the dataset. Report issue for preceding element\nWe selected three representative methods, each addressing hallucination from different perspectives and demonstrating strong performance: the training-based RLAIF-V [CITATION], the decoding strategy OPERA [CITATION] and VCD [CITATION]. For RLAIF-V, we use the open-source model weights of RLAIF-7B, which were fine-tuned on LLaVA1.5 [CITATION]. Notably, the reward model used in this setup is LLaVA-Next 34B [CITATION], which may transfer some of LLaVA-Next 34B\u2019s capabilities into RLAIF-7B, potentially making the comparison with other experimental setups less fair. The hyperparameters for OPERA and VCD are provided in Section A.5 to support the reproducibility of our results. We use beam search as the default decoding strategy, with Nbeamssubscript\ud835\udc41beamsN_{\\textnormal{beams}}italic_N start_POSTSUBSCRIPT beams end_POSTSUBSCRIPT set to 5.Report issue for preceding element\nWe additionally evaluate six best multimodal models available today, Ovis1.6 [CITATION], Qwen2-VL [CITATION], LLaVA-OneVision [CITATION], InternVL2 [CITATION], Idefics3 [CITATION], and MiniCPM-2.6 [CITATION], to further verify the effectiveness of our proposed metric. These models have been trained on vast datasets, comprising hundreds of millions of samples, and required significant computational resources, including extensive GPU hours. Their general performance substantially surpasses that of LLaVA1.5.Report issue for preceding element\nTable 2 shows the our final HalFscore includes precision, recall, and fscore, and further classifies hallucinations into object, attribute, and relation types for more detailed evaluation. From this metric, we observe that compared to the LLaVA1.5 baseline, the leading methods have achieved substantial and comprehensive improvements in both precision and recall. Notably, there is a significant reduction in hallucinations when describing objects and attributes, although there remains room for improvement in relation hallucinations. Our method also demonstrates marked improvement over LLaVA1.5, with a +6.2 increase in precision, +0.7 in recall, and +2.4 in Fscore. In comparisons with OPERA, VCD, and RLAIF-V, our approach achieved the best results in precision and fscore, though the improvement in recall is less pronounced. Overall, by leveraging these hallucination mitigation techniques, OPERA, VCD, and RLAIF-V have achieved recall scores comparable to MiniCPM-2.6, while our method has even surpassed MiniCPM, further underscoring the effectiveness of our approach.Report issue for preceding element\nTo evaluate caption-based object hallucination, we use the CHAIR metric [CITATION], which measures the percentage of generated words that correspond to actual objects in the image. The results in Table 3 show that, on this metric, our method significantly reduces hallucinations compared to the LLaVA1.5 baseline and outperforms both OPERA and VCD by a large margin. However, RLAIF-V performs better on this metric, showing fewer object hallucinations. A possible reason is that it uses additional feedback model. Moving forward, we plan to design targeted perturbation texts for objects to further enhance performance.Report issue for preceding element\nHallusionBench [CITATION] evaluates how well multi-modal models handle language hallucinations and visual illusions. Previous hallucination assessment mainly focused on dense caption tasks, and by incorporating HallusionBench, we aim to enhance our hallucination evaluation through the addition of multiple-choice and vision reasoning tasks. The results in Table 3 demonstrate a substantial improvement (+0.6) of our method on the HallusionBench metric, whereas the VCD decoding strategy merely maintained the baseline, OPERA led to a modest increase of 0.2. This outcome further confirms that our method is not only effective in dense captioning tasks but also excels in vision reasoning tasks.Report issue for preceding element\nWe assess the models\u2019 general ability on three widely used benchmarks: MMBench [CITATION], CCBench [CITATION], and SEEDImage [CITATION], to verify that our model\u2019s general capabilities. From the Table 3, we observe that both VCD and RLAIF-V result in some degradation in general performance, while OPERA remains stable. In contrast, our method not only avoids degradation in general metrics but also shows improvements across multiple areas, with gains of +1.6 in MMB, +0.3 in SEEDImage, and +1.2 in CCBench. We attribute this improvement in generalization to perturbation training, which compels the multimodal model to more effectively leverage image information.Report issue for preceding element\nTo assess the effect of perturbation levels on training, we devised three methods of inserting perturbation text, detailed in Table 4. The first method alerts the model to upcoming text perturbations, instructing it to ignore them. The second focuses the model on image regions without referencing perturbations. The third offers no guidance. These methods progressively increase the level of perturbation and the difficulty of training the multimodal model. As Table 5 shows, increasing perturbation levels enhances training difficulty and reduces hallucinations, as indicated by HalFscore and CHAIR metrics. However, this also leads to shorter captions and lower recall, suggesting a more cautious model behavior. General performance metrics decline consistently, implying that while perturbations decrease hallucinations, excessive perturbation use may impair overall performance.Report issue for preceding element\nTo investigate the impact of relevance of perturbation text, we conducted experiments using randomly sampled texts from a text dataset instead of the carefully designed perturbations used earlier. The results in Table 5 show that introducing random text perturbations, compared to the LLaVA1.5 baseline, still mitigates hallucinations in multimodal models, as evidenced by improvements in HaFscore and HalBench. However, the effectiveness of random perturbations is inferior to that of targeted text-based experiments. We suggest that even random perturbations influence the training of multimodal models, as the training is disrupted by the random text interference. However, more relevant perturbations exert a stronger disruptive effect, prompting the model to focus more on the image to overcome these challenges.Report issue for preceding element\nWe perform the user study to explore whether the proposed HalFscore correlates well with human evaluation. We show an image and ask human raters to compare two paragraphs generated and determine which generated text describes the main subject and detailed information of the input image more comprehensively (recall_h), and which text contains less information that is not present in the input image (precision_h). Four methods, LLaVA 1.5, MiniCPM-2.6, Internvl2, and our method are selected for human evaluation. Each method is compared with other methods for 12121212 times. We compute the human evaluated scores for the recall and precision and drive the Pearson correlations coefficients between the scores of human evaluation and that of HalFscore. The results are shown in the Table 6. We observe that our precision score and recall score align with human evaluations. Specifically, we compare our method with another metric, MMhalbench. The correlation coefficient between the metric and the human evaluations of precision scores is relatively smaller than HalFscore. The comparison demonstrates our metric aligns with human evaluations more than directly asking large language models to rate the level of hallucination. Further details about the user study can be found in Section A.7 of the Appendix.Report issue for preceding element\nPerturboLLaVA is a novel training strategy that enhances model performance in the SFT phase. It can be integrated seamlessly with post-SFT optimization RLAIF-V and OPERA during inference. In our tests, using OPERA as a plugin with PerturboLLaVA during inference led to performance boosts, as detailed in Table 3. PerturboLLaVA with OPERA achieved further gains, improving HalFscore by 0.6, with a 3-point improvement in the CHAIRs. Hallucination reasoning in Hallusionbench also improved by +0.1, while general capability increased by 0.4 in CCBench. Compared to applying OPERA on LLaVA1.5, replacing the baseline model with PerturboLLaVA resulted in improved performance in both hallucination reduction and general capability. Thus, PerturboLLaVA introduces a new optimization direction that complements existing strategies, offering an additional benefit.Report issue for preceding element\nIn our work, we introduce a concept-level HalFscore that enables fine-grained analysis of various hallucinations involving objects, attributes, and relations in dense captions. This metric also reflects the overall captioning capability of multi-modal models. To mitigate hallucinations, we propose a simple yet effective training strategy that guides multi-modal models to reduce reliance on linguistic priors and prioritize image information. Our method outperforms leading state-of-the-art approaches without incurring additional training or inference overhead. Overall, we believe that our proposed metric enhances the evaluation of hallucinations in multi-modal models and that our method effectively mitigates hallucination issues. The proposed method shows great promise of becoming a standard strategy for training robust multimodal models.Report issue for preceding element\nThis work was supported by the National Key R&D Program of China (No. 2022ZD0160101), the Ningbo Science and Technology Bureau (Grant Number 2024Z291), and the National Natural Science Foundation of China (No. 62206244). CC and ML contributed equally. Part of the work was done when CC was doing an internship at WeChat Group.Report issue for preceding element",
  "citations": [
    {
      "tag": "Li et\u00a0al. (2023b)",
      "title": "Evaluating object hallucination in large vision-language models.",
      "authors": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne\u00a0Xin Zhao, and Ji-Rong Wen.",
      "journal": "arXiv preprint arXiv:2305.10355, 2023b."
    },
    {
      "tag": "Hu et\u00a0al. (2024)",
      "title": "Minicpm: Unveiling the potential of small language models with scalable training strategies.",
      "authors": "Shengding Hu, Yuge Tu, Xu\u00a0Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2404.06395, 2024."
    },
    {
      "tag": "Rafailov et\u00a0al. (2024)",
      "title": "Direct preference optimization: Your language model is secretly a reward model.",
      "authors": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher\u00a0D Manning, Stefano Ermon, and Chelsea Finn.",
      "journal": "Advances in Neural Information Processing Systems, 36, 2024."
    },
    {
      "tag": "Yin et\u00a0al. (2023)",
      "title": "Woodpecker: Hallucination correction for multimodal large language models.",
      "authors": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke\u00a0Li, Xing Sun, and Enhong Chen.",
      "journal": "arXiv preprint arXiv:2310.16045, 2023."
    },
    {
      "tag": "Bi et\u00a0al. (2024)",
      "title": "Deepseek llm: Scaling open-source language models with longtermism.",
      "authors": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2401.02954, 2024."
    },
    {
      "tag": "Guan et\u00a0al. (2024)",
      "title": "Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models.",
      "authors": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et\u00a0al.",
      "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 14375\u201314385, 2024."
    },
    {
      "tag": "Chiang et\u00a0al. (2023)",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
      "authors": "Wei-Lin Chiang, Zhuohan Li, Zi\u00a0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E Gonzalez, et\u00a0al.",
      "journal": "See https://vicuna. lmsys. org (accessed 14 April 2023), 2023."
    },
    {
      "tag": "Wang et\u00a0al. (2023b)",
      "title": "An llm-free multi-dimensional benchmark for mllms hallucination evaluation.",
      "authors": "Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji\u00a0Zhang, and Jitao Sang.",
      "journal": "arXiv preprint arXiv:2311.07397, 2023b."
    },
    {
      "tag": "Chen et\u00a0al. (2024)",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.",
      "authors": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu\u00a0Qiao, and Jifeng Dai.",
      "journal": "CVPR, 2024."
    },
    {
      "tag": "Liu et\u00a0al. (2023a)",
      "title": "Aligning large multi-modal model with robust instruction tuning.",
      "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.",
      "journal": "arXiv preprint arXiv:2306.14565, 2023a."
    },
    {
      "tag": "Liu et\u00a0al. (2023b)",
      "title": "Mitigating hallucination in large multi-modal models via robust instruction tuning.",
      "authors": "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.",
      "journal": "InThe Twelfth International Conference on Learning Representations, 2023b."
    },
    {
      "tag": "OpenGVLab (2024)",
      "title": "Internvl2: Better than the best\u2014expanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024.",
      "authors": "OpenGVLab.",
      "journal": "URLhttps://internvl.github.io/blog/2024-07-02-InternVL-2.0/."
    },
    {
      "tag": "Li et\u00a0al. (2023a)",
      "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension.",
      "authors": "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.",
      "journal": "arXiv preprint arXiv:2307.16125, 2023a."
    },
    {
      "tag": "Yu et\u00a0al. (2024)",
      "title": "Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness.",
      "authors": "Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da\u00a0Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et\u00a0al.",
      "journal": "arXiv e-prints, pp.\u00a0 arXiv\u20132405, 2024."
    },
    {
      "tag": "Zhou et\u00a0al. (2024)",
      "title": "Mlvu: A comprehensive benchmark for multi-task long video understanding.",
      "authors": "Junjie Zhou, Yan Shu, Bo\u00a0Zhao, Boya Wu, Shitao Xiao, Xi\u00a0Yang, Yongping Xiong, Bo\u00a0Zhang, Tiejun Huang, and Zheng Liu.",
      "journal": "arXiv preprint arXiv:2406.04264, 2024."
    },
    {
      "tag": "Achiam et\u00a0al. (2023)",
      "title": "Gpt-4 technical report.",
      "authors": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2303.08774, 2023."
    },
    {
      "tag": "Rohrbach et\u00a0al. (2018)",
      "title": "Object hallucination in image captioning.",
      "authors": "Anna Rohrbach, Lisa\u00a0Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko.",
      "journal": "InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\u00a0 4035\u20134045, 2018."
    },
    {
      "tag": "Bai et\u00a0al. (2023)",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities.",
      "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.",
      "journal": "arXiv preprint arXiv:2308.12966, 2023."
    },
    {
      "tag": "Chen et\u00a0al. (2023)",
      "title": "Sharegpt4v: Improving large multi-modal models with better captions.",
      "authors": "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.",
      "journal": "arXiv preprint arXiv: 2311.12793, 2023."
    },
    {
      "tag": "Liu et\u00a0al. (2024a)",
      "title": "Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a.",
      "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Bo\u00a0Li, Yuanhan Zhang, Sheng Shen, and Yong\u00a0Jae Lee.",
      "journal": "URLhttps://llava-vl.github.io/blog/2024-01-30-llava-next/."
    },
    {
      "tag": "Liu et\u00a0al. (2024b)",
      "title": "Visual instruction tuning.",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong\u00a0Jae Lee.",
      "journal": "Advances in neural information processing systems, 36, 2024b."
    },
    {
      "tag": "Zhang et\u00a0al. (2023)",
      "title": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.",
      "authors": "Pan Zhang, Xiaoyi Dong\u00a0Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2309.15112, 2023."
    },
    {
      "tag": "Lu et\u00a0al. (2024a)",
      "title": "Deepseek-vl: towards real-world vision-language understanding.",
      "authors": "Haoyu Lu, Wen Liu, Bo\u00a0Zhang, Bingxuan Wang, Kai Dong, Bo\u00a0Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2403.05525, 2024a."
    },
    {
      "tag": "Zhu et\u00a0al. (2023)",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
      "authors": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
      "journal": "arXiv preprint arXiv:2304.10592, 2023."
    },
    {
      "tag": "Wu et\u00a0al. (2017)",
      "title": "Visual question answering: A survey of methods and datasets.",
      "authors": "Qi\u00a0Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van\u00a0den Hengel.",
      "journal": "Computer Vision and Image Understanding, 163:21\u201340, 2017."
    },
    {
      "tag": "Zhou et\u00a0al. (2023)",
      "title": "Analyzing and mitigating object hallucination in large vision-language models.",
      "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.",
      "journal": "arXiv preprint arXiv:2310.00754, 2023."
    },
    {
      "tag": "Li et\u00a0al. (2024)",
      "title": "Llava-onevision: Easy visual task transfer.",
      "authors": "Bo\u00a0Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.",
      "journal": "arXiv preprint arXiv:2408.03326, 2024."
    },
    {
      "tag": "Touvron et\u00a0al. (2023a)",
      "title": "Llama: Open and efficient foundation language models.",
      "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2302.13971, 2023a."
    },
    {
      "tag": "Wang et\u00a0al. (2024)",
      "title": "Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution.",
      "authors": "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2409.12191, 2024."
    },
    {
      "tag": "Sun et\u00a0al. (2023)",
      "title": "Aligning large multimodal models with factually augmented rlhf.",
      "authors": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2309.14525, 2023."
    },
    {
      "tag": "Touvron et\u00a0al. (2023b)",
      "title": "Llama 2: Open foundation and fine-tuned chat models.",
      "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2307.09288, 2023b."
    },
    {
      "tag": "Wang et\u00a0al. (2023a)",
      "title": "Vigc: Visual instruction generation and correction.",
      "authors": "Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2308.12714, 2023a."
    },
    {
      "tag": "Jing et\u00a0al. (2020)",
      "title": "Overcoming language priors in vqa via decomposed linguistic representations.",
      "authors": "Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Jia Yunde, and Qi\u00a0Wu.",
      "journal": "InThirty-Forth AAAI Conference on Artificial Intelligence (AAAI), pp.\u00a0 11181\u201311188, 2020."
    },
    {
      "tag": "Liu et\u00a0al. (2023c)",
      "title": "Mmbench: Is your multi-modal model an all-around player?",
      "authors": "Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo\u00a0Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et\u00a0al.",
      "journal": "arXiv preprint arXiv:2307.06281, 2023c."
    },
    {
      "tag": "Ouyang et\u00a0al. (2022)",
      "title": "Training language models to follow instructions with human feedback.",
      "authors": "Long Ouyang, Jeffrey Wu, Xu\u00a0Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\u00a0al.",
      "journal": "Advances in neural information processing systems, 35:27730\u201327744, 2022."
    },
    {
      "tag": "Lauren\u00e7on et\u00a0al. (2024)",
      "title": "Building and better understanding vision-language models: insights and future directions.",
      "authors": "Hugo Lauren\u00e7on, Andr\u00e9s Marafioti, Victor Sanh, and L\u00e9o Tronchon.",
      "journal": "arXiv preprint arXiv:2408.12637, 2024."
    },
    {
      "tag": "Leng et\u00a0al. (2024)",
      "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding.",
      "authors": "Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing.",
      "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 13872\u201313882, 2024."
    },
    {
      "tag": "Lu et\u00a0al. (2024b)",
      "title": "Ovis: Structural embedding alignment for multimodal large language model.",
      "authors": "Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye.",
      "journal": "arXiv preprint arXiv:2405.20797, 2024b."
    },
    {
      "tag": "Clark et\u00a0al. (2019)",
      "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases.",
      "authors": "Christopher Clark, Mark Yatskar, and Luke Zettlemoyer.",
      "journal": "InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\u00a0 4069\u20134082, 2019."
    },
    {
      "tag": "Dai et\u00a0al. (2023)",
      "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.",
      "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng\u00a0Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.",
      "journal": ""
    },
    {
      "tag": "Huang et\u00a0al. (2024)",
      "title": "Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.",
      "authors": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.",
      "journal": "InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\u00a0 13418\u201313427, 2024."
    },
    {
      "tag": "Urbanek et\u00a0al. (2023)",
      "title": "A picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions.",
      "authors": "Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano.",
      "journal": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\u00a0 26690\u201326699, 2023."
    }
  ]
}