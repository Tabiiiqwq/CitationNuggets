Our work builds upon several active areas of research,
including self-supervised learning, visual pretraining for
robotics, and learning robotic manipulation from demon-
strations.

Self-supervised learning. Self-supervised learning aims to
learn useful representations from unlabeled data by solving
pretext tasks that do not require manual annotation. Early
work in this area focused on designing pretext tasks for 2D
images, such as solving jagsaw puzzles [CITATION_0], constrastive
learning [CITATION_1] or joint embedding approaches [CITATION_2]. Most related to our work is the masked autoencoder
(MAE) approach proposed by He et al. [CITATION_3], which learns
to reconstruct randomly masked patches in an image. MAE
has been shown to learn transferable representations for ob-
ject detection and segmentation tasks. Furthermore, Bach-
mann et al demonstrates MAE pretraining can be extended
to different modalities such as semantics and depth [CITATION_4]. In
this work, we extend the MAE approach to multi-view 3D
scenes, enabling us to learn 3D-aware representations that
are useful for robotic manipulation tasks. Unlike Multi-
MAE which learns semantics and depth through direct su-
pervision, 3D-MVP aims to learn a 3D-aware representa-
tion from multi view images.

Visual pretraining for Robotics. Visual pretraining has
demonstrated impressive generalization ability on computer
vision tasks. Therefore, prior works have explored whether
it works for robotics tasks as well. Specifically, the robotics
community has trended towards learning representations us-
ing state-of-the-art self-supervised vision algorithms on di-
verse interaction datasets [CITATION_5], and finetune the net-
work on robotics tasks [CITATION_6]. 3D-MVP
follows the same procedure. However, existing robotics
pretraining approaches typically learn a 2D visual encoder
(e.g. ResNet [CITATION_7] or ViT [CITATION_8]), we find they are inferior than
manipulation policies which do explicit 3D modeling (e.g.
RVT [CITATION_9], Act3D [CITATION_10]). Migrating a pretrained ViT to 3D
manipulation policies is nontrivial since they do not have
a 2D visual encoder. In this paper, we propose 3D-MVP,
which does 3D-aware pretraining on 3D manipulation poli-
cies, to fill the gap.

Learning manipulation from demonstrations. Recent
work has explored using transformers for multi-task ma-
nipulation policies that predict robot actions from visual
and language inputs [CITATION_11]. End-to-end mod-
els like RT-1 [CITATION_12], GATO [CITATION_13], and InstructRL [CITATION_14] directly
predict 6-DoF end-effector poses but require many demon-

2

strations to learn spatial reasoning and generalize to new
scenes. To better handle 3D scenes, PerAct [CITATION_15] and C2F-
ARM [CITATION_16] voxelize the workspace and detect the 3D voxel
containing the next end-effector pose. However, precise
pose prediction requires high-resolution voxels which are
computationally expensive. Recently, RVT [CITATION_17] proposes
a multi-view transformer that attends over point cloud fea-
tures from multiple camera views to predict actions. This
avoids explicit voxelization and enables faster training and
inference than PerAct. Act3D [CITATION_18] represents the scene as a
continuous 3D feature field and samples points to featurize
with attention, allowing adaptive resolution. GNFactor [CITATION_19]
jointly optimizes a generalizable neural field for reconstruc-
tion and a Perceiver for decision-making. In contrast, our
proposed 3D-MVP learns 3D scene representations through
masked autoencoding pretraining on a large dataset of 3D
object models. This pretraining enables 3D-MVP to build
a rich understanding of 3D geometry and semantics prior
to finetuning on downstream manipulation tasks. Com-
pared to RVT and Act3D which train from scratch on tar-
get tasks, 3D-MVPâ€™s pretraining leads to improved perfor-
mance, sample efficiency and generalization. Unlike GN-
Factor which relies on a pretrained VLM to inject seman-
tics, 3D-MVP directly learns 3D semantic features from ob-
ject models.