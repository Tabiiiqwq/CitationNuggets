2.1. Human Motion Generation

Human motion generation has progressed significantly in
recent years  leveraging a variety of conditioning sig-
nals such as text , actions , speech ,
music , and scenes/objects . Recently, multimodal motion generation has
also gained attention  enabling multiple input
modalities. However, most existing methods focus solely
on generative tasks without supporting estimation. For in-
stance, the method  supports video input but treats it as
a generative task, resulting in motions that loosely imitate
video content rather than precisely matching it. In contrast,
our method jointly handles generation and estimation tasks,
yielding more precise video-conditioned results.

For long-sequence motion generation, existing works
mostly rely on ad-hoc post-processing techniques to stitch
separately generated fixed-length motions . In
contrast, our method introduces a novel diffusion-based ar-
chitecture enabling seamless generation of arbitrary-length
motions conditioned on multiple modalities without com-
plex post-processing.

Existing datasets, such as AMASS , are limited in
size and diversity. To address the scarcity of 3D data,
Motion-X  and MotionBank  augment datasets us-
ing 2D videos and 3D pose estimation models , but
the resulting motions often contain artifacts.
In contrast,
our method directly leverages in-the-wild videos with 2D
annotations without explicit 3D reconstruction, reducing re-
liance on noisy data and enhancing robustness and diversity.

2.2. Human Motion Estimation

Human pose estimation from images , videos , or even sparse marker data  has been
studied extensively in the literature. Recent works focus pri-
marily on estimating global human motion in world-space
coordinates . This is an inherently
ill-posed problem, hence these methods leverage generative
priors and SLAM methods to constrain human and camera
motions, respectively. However, these methods typically in-
volve computationally expensive optimization or separate
post-processing steps.

More recent approaches aim to estimate global human
motion in a feed-forward manner , offer-
ing faster solutions. Our method extends this direction by
jointly modeling generation and estimation within a uni-
fied diffusion framework. This integration leverages shared
representations and generative priors during training to pro-
duce more plausible estimations.