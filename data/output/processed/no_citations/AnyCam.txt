2.1. Foundational Models for Depth and Flow

In the tasks of monocular depth (MDE) and flow estimation,
well-generalizable foundation models have replaced early
deep learning approaches  in the last years. For
MDE, DepthAnything  uses a data engine to construct
a large corpus of automatically annotated data to learn rel-
ative depth estimation. Additional fine-tuning allows for
metric depth estimates. DepthAnythingV2  finetunes
the previous model using synthetic data for better perfor-
mance. Metric3D  and Metric3Dv2  transform im-
ages to canonical camera intrinsics with a fixed focal length.
DepthPro  proposes a two-stage training curriculum with
a second stage solely on synthetic data to sharpen bound-
ary predictions. DepthCrafter  leverages a conditional
diffusion model to predict temporally consistent depth maps

for videos. In this work, we utilize UniDepth  for metric
MDE, which uses a geometric invariance loss on different
image augmentation to enforce consistency.

RAFT  presented the state of the art for optical flow
It improved previous meth-
estimation for a long time.
ods by introducing a recurrent look-up operator on corre-
lation volumes to iteratively refine flow predictions with-
out needing coarse-to-fine flow pyramids. GMFlow 
avoids correlation volumes and instead leverages the prop-
erties of transformers for global matching on feature maps.
This removes the need for iterative steps to improve runtime
performance. UniMatch  extends GMFlow network by
tasks of disparity and depth prediction to enable cross-task
transfer learning of a single transformer network.

We rely on both off-the-shelf MDE and Optical Flow
networks to benefit from strong geometric priors during
training and inference.

2.2. SfM and SLAM

For many decades, the problem of recovering camera pa-
rameters and geometry from images has been formulated
as the Structure-from-Motion (SfM) pipeline .
While many different implementations of the SfM pipeline
exist, COLMAP  has emerged as the standard due to
its robustness and flexibility. One of the drawbacks of SfM
methods is their high computational cost. Simultaneous Lo-
cation and Mapping (SLAM)  approaches em-
ploy a similar pipeline to SfM but focus on the efficient pro-
cessing of consecutive video frames. In recent years, these
classical optimization-based approaches were enhanced by
learned components . However,
relying on epipolar geometry  or photometric consis-
tency  makes them susceptible to high error on highly
dynamic scenes. The strong focus on self driving data pro-
vided datasets with mostly static environments ,
an assumption that does not hold for casual videos.

2.3. Learning Based SfM and SLAM

Largely learning-based methods started to replace classical
SLAM and SfM systems due to improved robustness .
DROID-SLAM extends the framework of RAFT  by an
update operator on both depth and pose estimates. A final
differentiable bundle adjustment (BA) layer produces the fi-
nal pose estimates. ParticleSfM  utilizes dense corre-
spondences inside a BA framework to optimize poses. The
dense correspondences are initialized from optical flow, and
dynamic points are filtered using trajectory-based motion
segmentation. CasualSAM  predicts both depth and
movement from images to get frame-to-frame motion. A
global optimization aligns the scale of the prediction and
refines the poses. Dust3R  is a dense multi-view stereo
method that regresses point coordinates between an im-
age pair. This allows it to be extended to either SfM or

SLAM. FlowMap  proposes to reconstruct a scene by
overfitting a depth network to it and aligning depth maps
via correspondences from flow or point tracking. LEAP-
VO  combines visual and temporal information of video
sequences to improve the tracking accuracy of points and
identify occluded and dynamic points. A sliding window
bundle adjustment then optimizes the poses. The concurrent
work of MonST3R  finetunes Dust3r on mostly syn-
thetic data to generalize it to dynamic scenes. While these
works achieve impressive progress, they generally obtain
poses from aligning depth and point maps or by optimizing
them per-scene. This makes it hard to inject prior informa-
tion about camera motion. In contrast, our method uses a
neural network to predict a trajectory, which can effectively
learn priors over realistic camera motions.