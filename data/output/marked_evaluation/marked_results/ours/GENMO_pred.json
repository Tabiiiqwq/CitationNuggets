{
  "paper_id": "GENMO",
  "pred_citations": [
    [
      "Human motion generation: A survey",
      "A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights",
      "Human motion video generation: A survey"
    ],
    [
      "Make-an-animation: Large-scale text-conditional 3d human motion generation",
      "Motiondiffuse: Text-driven human motion generation with diffusion model",
      "Breaking the limits of text-conditioned 3d motion synthesis with elaborative descriptions"
    ],
    [
      "Action-conditioned on-demand motion generation",
      "Actformer: A gan-based transformer towards general action-conditioned 3d human motion generation",
      "Modiff: Action-conditioned 3d motion generation with denoising diffusion probabilistic models",
      "Action-conditioned 3D human motion synthesis with transformer VAE",
      "Action2motion: Conditioned generation of 3d human motions",
      "Denoising diffusion probabilistic models for action-conditioned 3d motion generation",
      "ASMNet: Action and style-conditioned motion generative network for 3D human motion generation",
      "Learning uncoupled-modulation cvae for 3d action-conditioned human motion synthesis",
      "SINC: Spatial composition of 3D human motions for simultaneous action generation",
      "Multiact: Long-term 3d human motion generation from multiple action labels"
    ],
    [
      "Speech-driven head motion synthesis using neural networks.",
      "DiffMotion: Speech-driven gesture synthesis using denoising diffusion model.",
      "Listen, denoise, action! audio-driven motion synthesis with diffusion models.",
      "Speech-driven head motion synthesis based on a trajectory model."
    ],
    [
      "Music2dance: Dancenet for music\u2010driven dance generation",
      "Self\u2010supervised music motion synchronization learning for music\u2010driven conducting motion generation",
      "Music\u2010driven group choreography",
      "Music\u2010driven dance generation",
      "Taming diffusion models for music\u2010driven conducting motion generation",
      "Keyframe control of music\u2010driven 3D dance generation",
      "GrooveNet: Real\u2010time music\u2010driven dance movement generation using artificial neural networks",
      "Music\u2010driven animation generation of expressive musical gestures",
      "Exploring multi\u2010modal control in music\u2010driven dance generation",
      "Example\u2010based automatic music\u2010driven conventional dance motion synthesis"
    ],
    [
      "Humanise: Language-conditioned human motion generation in 3d scenes",
      "Move-in-2D: 2D-Conditioned Human Motion Generation",
      "Scene and Goal-Conditioned Motion Diffusion: Synthesizing Human Motion in Context-Rich Environments",
      "Laserhuman: language-guided scene-aware human motion generation in free environment",
      "Generating human motion in 3D scenes from text descriptions"
    ],
    [
      "MotionLLM: Multimodal Motion-Language Learning with Large Language Models",
      "Large Motion Model for Unified Multi-modal Motion Generation",
      "FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models",
      "MGPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation",
      "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators",
      "MotionChain: Conversational Motion Controllers via Multimodal Prompts",
      "Combo: Co-speech Holistic 3D Human Motion Generation and Efficient Customizable Adaptation in Harmony"
    ],
    [
      "Video-guided motion synthesis using example motions",
      "Video based motion synthesis by splicing and morphing",
      "Human motion synthesis from 3d video"
    ],
    [
      "Causal Motion Tokenizer for Streaming Motion Generation",
      "MoST: Motion Style Transformer between Diverse Action Contents",
      "Tedi: Temporally-entangled diffusion for long-term motion synthesis",
      "Reconstruct 3D human motion from monocular video using motion library",
      "Beyond imitation: Generative and variational choreography via machine learning"
    ],
    [
      "AMASS: Archive of motion capture as surface shapes"
    ],
    [
      "Motion-X: A large-scale 3d expressive whole-body human motion dataset"
    ],
    [
      "MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations"
    ],
    [
      "Physics-based human motion estimation and synthesis from videos",
      "Motion guided 3d pose estimation from videos"
    ],
    [
      "2d human pose estimation: New benchmark and state of the art analysis",
      "2D Human pose estimation: A survey",
      "2-D human pose estimation from images based on deep learning: a review",
      "Deep learning based 2d human pose estimation: A survey",
      "Vision-based human pose estimation via deep learning: A survey",
      "Human pose estimation from monocular images: A comprehensive survey"
    ],
    [
      "Diffpose: Spatiotemporal diffusion model for video-based human pose estimation",
      "Spatiotemporal learning transformer for video-based human pose estimation",
      "Temporal feature alignment and mutual information maximization for video-based human pose estimation",
      "Human pose estimation in videos",
      "Video-based human pose regression via decoupled space-time aggregation"
    ],
    [
      "MoSh: motion and shape capture from sparse markers",
      "Estimation of missing markers in human motion capture"
    ],
    [
      "Wham: Reconstructing world\u2010grounded humans with accurate 3d motion",
      "World\u2010Grounded Human Motion Recovery via Gravity\u2010View Coordinates",
      "PACE: Human and Camera Motion Estimation from in\u2010the\u2010wild Videos",
      "Synergistic Global\u2010space Camera and Human Reconstruction from Videos",
      "W\u2010HMR: Monocular Human Mesh Recovery in World Space with Weak\u2010Supervised Calibration"
    ],
    [
      "Learning 3D Global Human Motion Estimation from Unpaired, Disjoint Datasets."
    ]
  ],
  "gt_citations": [
    [
      "HP-GAN: Probabilistic 3D human motion prediction via GAN",
      "Implicit neural representations for variable length human motion generation",
      "Chopin, N",
      "Flexible motion in-betweening with diffusion models",
      "Motionlcm: Real-time controllable motion generation via latent consistency model",
      "Momask: Generative masked mod- eling of 3d human motions",
      "A recurrent variational autoen- coder for human motion synthesis",
      "Nemf: Neural motion fields for kinematic ani- mation",
      "MoGlow: Probabilistic and controllable motion synthesis using normalising flows",
      "Mmm: Generative masked motion model",
      "Mmm: Generative masked motion model",
      "Guibas",
      "Human motion gen- eration using wasserstein GAN",
      "Human motion dif- fusion model",
      "Transflower: probabilistic autoregressive dance gen- eration with multimodal attention",
      "ActFormer: A gan-based transformer towards general action-conditioned 3d human motion generation",
      "Motiondif- fuse: Text-driven human motion generation with diffusion model",
      "Re- modiffuse: Retrieval-augmented motion diffusion model",
      "Human motion generation: A survey"
    ],
    [
      "Executing your commands via motion diffusion in latent space",
      "Synthesis of compositional animations from textual descriptions",
      "TM2T: Stochastic and tokenized modeling for the reciprocal gener- ation of 3d human motions and texts",
      "Act as you wish: Fine-grained control of mo- tion diffusion model with hierarchical semantic graphs"
    ],
    [
      "Ac- tion2Motion: Conditioned generation of 3D human mo- In ACM International Conference on Multimedia tions"
    ],
    [
      "Listen, denoise, action! audio-driven motion synthesis with diffusion models",
      "Taming diffusion models for audio-driven co-speech gesture generation"
    ],
    [
      "Ai choreographer: Music conditioned 3d dance generation with aist++",
      "Bailando: 3d dance generation by actor-critic gpt with choreographic In Proceedings of the IEEE/CVF Conference on memory",
      "You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection",
      "Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis",
      "Edge: In Proceedings of Editable dance generation from music",
      "Transflower: probabilistic autoregressive dance gen- eration with multimodal attention"
    ],
    [
      "Stochastic scene-aware motion prediction",
      "NIFTY: Neural object interaction fields for guided human motion synthesis",
      "HUMANISE: Language-conditioned hu- man motion generation in 3d scenes",
      "Generating human interaction motions in scenes with text control",
      "Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo P\u00b4erez Pellitero, and Gerard Pons-Moll"
    ],
    [
      "Motioncraft: Crafting whole-body motion with plug-and-play multimodal controls",
      "M3gpt: An ad- vanced multimodal, multitask framework for motion com- prehension and generation",
      "Large motion model for unified multi-modal motion generation",
      "Ude: A unified driving engine for human motion generation"
    ],
    [
      "Large motion model for unified multi-modal motion generation"
    ],
    [
      "Black, and G\u00a8ul Varol",
      "Black, G\u00a8ul Varol, Xue Bin Peng, and Davis Rempe",
      "Hauptmann, and Jungdam Won",
      "Diffcollage: Parallel generation of large content with diffusion models"
    ],
    [
      "Troje, Ger- ard Pons-Moll, and Michael J"
    ],
    [
      "Motion-x: A large- scale 3d expressive whole-body human motion dataset"
    ],
    [
      "Mo- tionbank: A large-scale video motion benchmark with disen- tangled rule-based annotations, 2024"
    ],
    [
      "Wham: Reconstructing world-grounded humans with accu- rate 3d motion",
      "Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras"
    ],
    [
      "Black, David W",
      "Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation",
      "Neural localizer fields for continuous 3d human pose and shape estimation"
    ],
    [
      "Be- yond static features for temporally consistent 3d human pose and shape from a video",
      "Humans in 4d: Re- In constructing and tracking humans with transformers",
      "VIBE: Video inference for human body pose and shape estimation"
    ],
    [
      "Mocap everyone everywhere: Lightweight motion capture with smartwatches and a head- mounted camera",
      "Sparseposer: Real-time full- body motion reconstruction from sparse data",
      "Transpose: Real-time 3d human translation and pose estimation with six inertial sensors"
    ],
    [
      "Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal",
      "Coin: Control-inpainting diffusion prior for human and camera In ECCV, pages 426\u2013446",
      "Wham: Reconstructing world-grounded humans with accu- rate 3d motion",
      "Tram: Global trajectory and motion of 3d humans from in- the-wild videos",
      "Decoupling human and camera motion from videos in the wild",
      "Glamr: Global occlusion-aware human mesh recov- ery with dynamic cameras"
    ],
    [
      "World-grounded human motion recovery via gravity-view coordinates",
      "Wham: Reconstructing world-grounded humans with accu- rate 3d motion",
      "Tram: Global trajectory and motion of 3d humans from in- the-wild videos",
      "Rohm: Robust human motion reconstruction via diffusion"
    ]
  ]
}