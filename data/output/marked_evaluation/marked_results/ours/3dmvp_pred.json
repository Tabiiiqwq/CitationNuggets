{
  "paper_id": "3dmvp",
  "pred_citations": [
    [
      "Self-supervised learning of pretext-invariant representations: \u2026 An important baseline is the Jigsaw ResNet-50 model\u2026",
      "A Self-Supervised Deep Metric Learning Approach for Jigsaw Puzzle Reconstruction: \u2026 this study, we focus on solving jigsaw puzzles\u2026"
    ],
    [
      "Contrasting contrastive self-supervised representation learning pipelines: \u2026",
      "",
      "RegionCL: Exploring contrastive region pairs for self-supervised representation learning: \u2026"
    ],
    [
      "Self-supervised learning from images with a joint-embedding predictive architecture",
      "Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning",
      "Towards democratizing joint-embedding self-supervised learning"
    ],
    [
      "Masked autoencoders as spatiotemporal learners"
    ],
    [
      "Multimae: Multi-modal multi-task masked autoencoders",
      ""
    ],
    [
      "Grasp2vec: Learning object representations from self-supervised grasping",
      "Combining self-supervised learning and imitation for vision-based rope manipulation",
      "Contextual imagined goals for self-supervised robotic learning"
    ],
    [
      "Self-supervised fine-tuning for human-robot co-painting",
      "Robot fine-tuning made easy: Pre-training rewards and policies for autonomous real-world reinforcement learning",
      "Masked visual pre-training for motor control"
    ],
    [
      "A stereovision-based approach for retrieving variable force feedback in robotic-assisted surgery using modified inception ResNet V2 networks",
      "Deep episodic memory: Encoding, recalling, and predicting episodic experiences for robot action execution",
      "Transformers for one-shot visual imitation",
      "Image transformation and CNNs: A strategy for encoding human locomotor intent for autonomous wearable robots",
      "Res-FLNet: human-robot interaction and collaboration for multi-modal sensing robot autonomous driving tasks based on learning control algorithm",
      "V2cnet: A deep learning framework to translate videos to commands for robotic manipulation",
      "MCS-ResNet: A Generative Robot Grasping Network Based on RGB-D Fusion"
    ],
    [
      "Language-driven representation learning for robotics",
      "ViT-A*: Legged Robot Path Planning using Vision Transformer A"
    ],
    [
      "Rvt: Robotic view transformer for 3d object manipulation"
    ],
    [
      "Act3d: 3d feature field transformers for multi-task robotic manipulation: \u2026"
    ],
    [
      "Perceiver-actor: A multi-task transformer for robotic manipulation",
      "Manipllm: Embodied multimodal large language model for object-centric robotic manipulation",
      "Vima: Robot manipulation with multimodal prompts",
      "Mastering robot manipulation with multimodal prompts through pretraining and multi-task fine-tuning",
      "Embodied bert: A transformer model for embodied, language-guided visual task completion"
    ],
    [
      "Rt-1: Robotics transformer for real-world control at scale: \u2026"
    ],
    [
      "Enhancing robot manipulation skill learning with multi-task capability based on transformer and token reduction",
      "A generalist agent: \u2026 ",
      "Vima: Robot manipulation with multimodal prompts",
      "Jack of all trades, master of some, a multi-purpose transformer agent"
    ],
    [
      "Self-corrected multimodal large language model for end-to-end robot manipulation",
      "LLaKey: Follow My Basic Action Instructions to Your Next Key State"
    ],
    [
      "Voxact\u2010b: Voxel\u2010based acting and stabilizing policy for bimanual manipulation"
    ],
    [
      "Voxact\u2010b: Voxel\u2010based acting and stabilizing policy for bimanual manipulation",
      "Hierarchical diffusion policy for kinematics\u2010aware multi\u2010task robotic manipulation",
      "Rvt: Robotic view transformer for 3d object manipulation",
      "Polarnet: 3d point clouds for language\u2010guided robotic manipulation",
      "Rvt\u20102: Learning precise manipulation from few demonstrations",
      "Super Robot View Transformer"
    ],
    [
      "Rvt: Robotic view transformer for 3d object manipulation"
    ],
    [
      "Act3d: 3d feature field transformers for multi-task robotic manipulation"
    ],
    [
      "GNFactor: Multi-task real robot learning with generalizable neural feature fields",
      ""
    ]
  ],
  "gt_citations": [
    [
      "Unsupervised learning of In Euro- visual representations by solving jigsaw puzzles"
    ],
    [
      "A simple framework for contrastive learning of visual representations",
      "Momentum contrast for unsupervised visual rep- resentation learning"
    ],
    [
      "Masked siamese networks for label-efficient learning",
      "Self-supervised learning from images with a joint-embedding predictive architecture",
      "Unsupervised learning of visual features by contrasting cluster assignments",
      "Emerg- ing properties in self-supervised vision transformers",
      "Bootstrap your own latent-a new approach to self-supervised learning",
      "ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer"
    ],
    [
      "Masked autoencoders are scalable vision learners"
    ],
    [
      "Multimae: Multi-modal multi-task masked autoen- coders"
    ],
    [
      "Scaling egocentric vision: The epic-kitchens dataset",
      "Ego4d: Around the world in 3,000 hours of egocentric video",
      "Understanding human hands in contact at internet scale"
    ],
    [
      "An unbiased look at datasets for visuo-motor pre-training",
      "Vip: Towards universal visual reward and representation via value-implicit pre-training",
      "Where are we in the search for an artificial visual cortex for embodied intelli- gence? Advances in Neural Information Processing Systems, 36, 2024.",
      "representation for robot manipulation",
      "Real-world robot learn- ing with masked visual pre-training",
      "Multi-view masked world In International models for visual robotic manipulation",
      "Masked visual pre-training for motor control"
    ],
    [
      "Deep residual learning for image recognition"
    ],
    [
      "An image is worth 16x16 words: Trans- arXiv preprint formers for image recognition at scale"
    ],
    [
      "Rvt: Robotic view transformer for 3d object manipulation"
    ],
    [
      "Act3d: Infinite resolution action detec- tion transformer for robotic manipulation"
    ],
    [
      "Instruction-driven history-aware policies for robotic manip- ulations",
      "Instruction-following agents with jointly pre-trained vision- language models",
      "Behavior transformers: Cloning k modes with one stone",
      "Perceiver- actor: A multi-task transformer for robotic manipulation",
      "Shelving, stacking, hanging: Relational pose arXiv preprint diffusion for multi-modal rearrangement"
    ],
    [
      "Rt-1: Robotics transformer for real-world control at scale"
    ],
    [
      "A generalist agent"
    ],
    [
      "Instruction-following agents with jointly pre-trained vision- language models"
    ],
    [
      "Perceiver- actor: A multi-task transformer for robotic manipulation"
    ],
    [
      "Coarse-to-fine q-attention: Efficient learn- ing for visual robotic manipulation via discretisation"
    ],
    [
      "Rvt: Robotic view transformer for 3d object manipulation"
    ],
    [
      "Act3d: Infinite resolution action detec- tion transformer for robotic manipulation"
    ],
    [
      "Gnfactor: Multi-task real robot learning In Conference on with generalizable neural feature fields"
    ]
  ]
}