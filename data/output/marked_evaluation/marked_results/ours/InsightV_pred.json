{
  "paper_id": "InsightV",
  "pred_citations": [
    [
      "Mm-llms: Recent advances in multimodal large language models",
      "Evolution and Prospects of Foundation Models: From Large Language Models to Large Multimodal Models",
      "Multimodal large language models: A survey",
      "A comprehensive review of multimodal large language models: Performance and challenges across different tasks",
      "Advancements and Applications of Multimodal Large Language Models: Integration, Challenges, and Future Directions",
      "From large language models to large multimodal models: A literature review",
      "The revolution of multimodal large language models: a survey",
      "A Survey of Multimodel Large Language Models",
      "Surveying the mllm landscape: A meta-review of current surveys",
      "Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning"
    ],
    [
      "Vip-llava: Making large multimodal models understand arbitrary visual prompts",
      "Visual language integration: A survey and open challenges",
      "Vision-language models for vision tasks: A survey"
    ],
    [
      "Multimath: Bridging visual and mathematical reasoning for large language models",
      "Math-llava: Bootstrapping mathematical reasoning for multimodal large language models",
      "A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges"
    ],
    [
      "Towards AI-assisted multiple choice question generation and quality evaluation at scale: Aligning with Bloom's Taxonomy",
      "CLR-Bench: Evaluating Large Language Models in College-level Reasoning",
      "Scemqa: A scientific college entrance level multimodal question answering benchmark",
      "ReadingQuizMaker: a human-NLP collaborative system that supports instructors to design high-quality reading quiz questions",
      "UC-100 Agentic AI Quiz Generation: Personalized Tutoring through Intelligent Retrieval and Adaptive Learning",
      "Teaching college level content and reading comprehension skills simultaneously via an artificially intelligent adaptive computerized instructional system",
      "Leveraging large language models for concept graph recovery and question answering in nlp education"
    ],
    [
      "Coarse-to-fine description for fine-grained visual categorization",
      "Understanding objects in detail with fine-grained attributes",
      "Fine-grained image analysis with deep learning: A survey",
      "Fine-grained visual-textual representation learning",
      "From the whole to detail: Progressively sampling discriminative parts for fine-grained recognition"
    ],
    [
      "Chain-of-thought prompting elicits reasoning in large language models",
      "Faithful chain-of-thought reasoning",
      "Strategic chain-of-thought: Guiding accurate reasoning in llms through strategy elicitation",
      "Chainlm: Empowering large language models with improved chain-of-thought prompting",
      "Challenging big-bench tasks and whether chain-of-thought can solve them"
    ],
    [
      "Chain-of-thought prompting elicits reasoning in large language models",
      "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
      "Automatic chain of thought prompting in large language models",
      "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models"
    ],
    [
      "Reinforcement Learning from Human Feedback: Aligning AI Systems with Human Preferences"
    ],
    [
      "Direct preference optimization: Your language model is secretly a reward model: \u2026"
    ],
    [
      "Iterative length-regularized direct preference optimization: A case study on improving 7b language models to gpt-4 level",
      "ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization",
      "Iterative DPO with An Improvement Model for Fine-tuning Diffusion Models",
      "Accelerated Preference Optimization for Large Language Model Alignment",
      "AIPO: Improving Training Objective for Iterative Preference Optimization"
    ]
  ],
  "gt_citations": [
    [
      "Qwen-vl: A versatile vision-language model for understand- ing, localization, text reading, and beyond",
      "Llava-onevision: Easy visual task transfer",
      "Vila: On pre-training for visual language models, 2023",
      "Improved baselines with visual instruction tuning",
      "Llava-next: Improved reason- ing, ocr, and world knowledge, 2024",
      "Visual instruction tuning",
      "Oryx mllm: On-demand spatial- temporal understanding at arbitrary resolution",
      "Deepseek-vl: towards real-world vision-language understanding",
      "Qwen2-vl: To see the world more clearly"
    ],
    [
      "Vila: On pre-training for visual language models, 2023",
      "Qwen2-vl: To see the world more clearly"
    ],
    [
      "Unimath: A foundational and multimodal mathe- matical reasoner"
    ],
    [
      "Internvl: Scaling up vision foundation mod- els and aligning for generic visual-linguistic tasks"
    ],
    [
      "Mon- key: Image resolution and text label are important things for large multi-modal models",
      "Llava-next: Improved reason- ing, ocr, and world knowledge, 2024",
      "Chain-of-spot: Interactive reasoning improves large vision-language models",
      "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
      "Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images"
    ],
    [
      "G-llava: Solving geometric prob- lem with multi-modal large language model",
      "Mavis: Mathematical visual in- struction tuning",
      "Improve vision language model chain-of-thought reasoning"
    ],
    [
      "Chain-of- thought prompting elicits reasoning in large language models"
    ],
    [
      "Training a helpful and harmless assistant with reinforcement learning from human feedback"
    ],
    [
      "Direct prefer- ence optimization: Your language model is secretly a reward model"
    ],
    [
      "Self-play fine-tuning converts weak lan- guage models to strong language models"
    ]
  ]
}