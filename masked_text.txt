HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. failed: scalerel failed: inconsolata failed: datetime failed: forest failed: fnpct failed: proof-at-the-end failed: tabularray failed: cellspace Authors: achieve the best HTML results from your LaTeX submissions by following these best practices . License: CC BY 4.0 arXiv:2406.04327v1 [cs.LG] 06 Jun 2024 Causal Estimation of Memorisation Profiles Report issue for preceding element Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel {}^{\includegraphics[scale={0.018}]{assets/cambridge_logo.jpg}} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT University of Cambridge, ETH Zürich { pl487 , av308 } @cam.ac.uk { clara.meister , thomas.hofmann , tiago.pimentel } @inf.ethz.ch Report issue for preceding element Abstract Report issue for preceding element Understanding memorisation in language models has practical and societal implications, e.g., studying models’ training dynamics or preventing copyright infringements.
Prior work defines memorisation as the causal effect of training with an instance on the model’s ability to predict that instance.
This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.
Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual.
Further, they often estimate memorisation for a model architecture rather than for a specific model instance.
This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics.
Using this method, we characterise a model’s memorisation profile—its memorisation trends across training—by only observing its behaviour on a small set of instances throughout training.
In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones. Report issue for preceding element \xspaceaddexceptions \csq@thequote@iclose \xspaceaddexceptions Report issue for preceding element Causal Estimation of Memorisation Profiles Report issue for preceding element Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel {}^{\includegraphics[scale={0.018}]{assets/cambridge_logo.jpg}} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT University of Cambridge, ETH Zürich { pl487 , av308 } @cam.ac.uk { clara.meister , thomas.hofmann , tiago.pimentel } @inf.ethz.ch Report issue for preceding element {tblr} colspec = Q[c,m]|X[l,m], stretch = 0 & pietrolesci/memorisation-profiles Report issue for preceding element 1 Introduction Report issue for preceding element Large language models (LMs) are often pretrained with a single pass on web-scale datasets (Raffel et al., 2020 ; Gao et al., 2020 ; Penedo et al., 2023 , inter alia ) .
Given the colossal size of these training sets, one may expect each individual instance to have little impact on the final model.
Yet, LMs can still reproduce entire sequences from their training set verbatim (Carlini et al., 2021 ) , suggesting that models can store, or memorise , precise knowledge about individual training instances.
In the era of large LMs, measuring memorisation is crucial for NLP practitioners; it has implications for copyright and data protection (Hu et al., 2022 ; Vyas et al., 2023 ; Lee et al., 2023 ) , for how models encode factual information (Cao et al., 2022 ; Tirumala et al., 2022 ) , and for understanding their training dynamics Arpit et al. ( 2017 ); Chang and Bergen ( 2024 ) . Report issue for preceding element Figure 1: Memorisation profile (top) and path (bottom) of Pythia \qty [mode=math]6.9.
Each entry represents the expected counterfactual memorisation of instances trained on at a specific timestep ( “Treatment Step” ) across model checkpoints ( “Checkpoint Step” ). The dashed vertical line indicates the end of the first epoch. Report issue for preceding element One line of prior work has adopted a causal definition of memorisation : it is the causal effect of observing an instance during training on a model’s ability to correctly predict that instance (Feldman, 2020 ) .
Despite being an intuitive concept, quantification of this definition is not straightforward as it requires knowledge of a counterfactual : 1 1 1 Counterfactuals are thought experiments about what would have happened if a present condition were changed while keeping everything else unchanged (McCloskey, 2016 ) . we must know how our model would have performed on a training instance had the model not been trained on it.
To overcome this challenge, prior work has proposed a variety of methods to estimate memorisation.
Some estimate it by training a model multiple times on different subsets of the data (e.g., Feldman and Zhang, 2020 ; Zheng and Jiang, 2022 ) , while others implicitly assume this counterfactual’s value to be negligible (Carlini et al., 2021 ) .
Both these approaches, however, have drawbacks:
the first computes memorisation for an architecture rather than a specific model, while the second relies on a strong assumption (we discuss this in detail in § 5 ). Report issue for preceding element In this paper, we first formalise counterfactual memorisation as the difference between two potential outcomes; notably, our formalisation generalises prior definitions of memorisation, allowing us to compare them within a unified framework.
We then draw from the econometrics literature (Callaway and Sant’Anna, 2021 ) and propose a new method which estimates memorisation using only observational data;
our method simply needs a model’s performance measurements (e.g., log-likelihood) on a subset of the training data throughout training.
The output of our method is what we term a memorisation profile : a model’s memorisation of training batches over the course of training. Report issue for preceding element Empirically, we use this method to analyse memorisation for models in the Pythia suite (Biderman et al., 2023b ) and characterise their memorisation profiles; e.g., Fig. 1 reports the memorisation profile of Pythia \qty [mode=math]6.9.
By studying these memorisation profiles we find that memorisation is stronger and more persistent in larger models.
Furthermore, both the learning rate and the position of an instance in the training set considerably impact how strongly that instance is memorised.
Finally, memorisation profiles are stable across model sizes; thus, we can predict memorisation in larger models from the memorisation observed in smaller ones. Report issue for preceding element 2 Background Report issue for preceding element In this section, we introduce some background on language modelling and on causal analysis which will be required throughout our paper. Report issue for preceding element 2.1 Language Modelling Report issue for preceding element We start by providing some background on language modelling. 2 2 2 We frame our exposition in terms of language models, but our framework can be trivially applied to any neural model and input modality (e.g., images). Let p \scaleto ⁢ 𝜽 ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) subscript 𝑝 \scaleto 𝜽 4 𝑝 𝑡 𝒙 p_{\scaleto{\bm{\theta}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_p start_POSTSUBSCRIPT bold_italic_θ 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) be a language model with parameters 𝜽 ∈ ℝ d 𝜽 superscript ℝ 𝑑 \bm{\theta}\in\mathbb{R}^{d} bold_italic_θ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .
This model defines a probability distribution over 𝒙 ∈ 𝒱 ∗ 𝒙 superscript 𝒱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{V}^{*} bold_italic_x ∈ caligraphic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , the set of all finite sequences that can be constructed from elements in the alphabet 𝒱 𝒱 \mathcal{V} caligraphic_V .
To train this model, we start with a set of randomly selected initial parameters, 𝜽 0 subscript 𝜽 0 \bm{\theta}_{0} bold_italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .
We then learn parameters 𝜽 𝜽 \bm{\theta} bold_italic_θ using a dataset 𝒟 𝒟 \mathcal{D} caligraphic_D and an optimisation procedure defined with respect to a loss function ℒ ℒ \mathcal{L} caligraphic_L . Report issue for preceding element Specifically, let 𝒟 = { 𝒙 n } n = 1 N 𝒟 superscript subscript subscript 𝒙 𝑛 𝑛 1 𝑁 \mathcal{D}=\{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}_{n}\}_{n=1}^{N} caligraphic_D = { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be a dataset whose instances 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x are sequences drawn from a target (unknown) distribution p ⁢ ( 𝒙 ) 𝑝 𝒙 p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ) .
These instances are typically assumed to be sampled i.i.d., and are shuffled using a permutation function σ : { 1 , … , N } → { 1 , … , N } : 𝜎 → 1 … 𝑁 1 … 𝑁 \sigma\colon\{1,...,N\}\!\to\!\{1,...,N\} italic_σ : { 1 , … , italic_N } → { 1 , … , italic_N } .
For a given batch size B 𝐵 B italic_B , we then split this dataset into T ≤ ⌊ N / B ⌋ 𝑇 𝑁 𝐵 T\!\leq\!\lfloor\nicefrac{{N}}{{B}}\rfloor italic_T ≤ ⌊ / start_ARG italic_N end_ARG start_ARG italic_B end_ARG ⌋ batches ℬ t subscript ℬ 𝑡 \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
We iterate through these batches performing gradient updates on the model parameters: Report issue for preceding element 𝜽 t = 𝜽 t − 1 − η ⁢ ∇ 𝜽 ℒ ⁢ ( 𝜽 t − 1 , ℬ t ) subscript 𝜽 𝑡 subscript 𝜽 𝑡 1 𝜂 subscript ∇ 𝜽 ℒ subscript 𝜽 𝑡 1 subscript ℬ 𝑡 \displaystyle\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}t}}=\bm{\theta}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}t-1}}-\eta\,\nabla_{\bm{%
\theta}}\mathcal{L}(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}t-1}},\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}t}}) bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_θ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_η ∇ start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (1) where η ∈ ℝ 𝜂 ℝ \eta\in\mathbb{R} italic_η ∈ blackboard_R is a learning rate. 3 3 3 The learning rate can be a function of other quantities (e.g., Duchi et al., 2011 ; Kingma and Ba, 2015 , inter alia ) . Notably, this procedure consists of a single pass on the training set and is standard for recent LMs (e.g., Touvron et al., 2023 ; Jiang et al., 2023 ; Dey et al., 2023 ) . Report issue for preceding element At each iteration, we use a batch ℬ t subscript ℬ 𝑡 \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to obtain a new model checkpoint, 𝜽 t subscript 𝜽 𝑡 \bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}t}} bold_italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
We introduce a notation which distinguishes the indexing of checkpoints and batches.
We use c ∈ { 0 , 1 , … , T } 𝑐 0 1 … 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,1,...,T\} italic_c ∈ { 0 , 1 , … , italic_T } to denote a checkpoint step (e.g., 𝜽 c subscript 𝜽 𝑐 \bm{\theta}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c} bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ). Further, we use g ∈ { 1 , … , T } ∪ { ∞ } 𝑔 1 … 𝑇 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{1,.%
..,T\}\cup\{\infty\} italic_g ∈ { 1 , … , italic_T } ∪ { ∞ } to denote the timestep at which a batch is used for training (e.g., ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ); we term this a treatment step , borrowing this terminology from the econometrics literature.
We denote as g = ∞ 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\!=\!\infty italic_g = ∞ a batch composed of instances that are not used for training and which form a validation set. Report issue for preceding element 2.2 Causal Analysis Report issue for preceding element Causal estimation is typically split into three steps.
First, we define a causal estimand , the target quantity we want to estimate.
Second, we state the assumptions needed to rewrite this causal estimand in terms of observable data, thus defining a statistical estimand ; this process is called identification.
Finally, we define an estimator , a statistical procedure to approximate the statistical estimand. Report issue for preceding element To formally define memorisation as a causal estimand, we will use the potential outcomes framework of Rubin ( 1974 , 2005 ) . 4 4 4 For a comparison of causal frameworks, see Ibeling and Icard ( 2023 ) . For an introduction to causal inference, see Pearl ( 2009 ) and Imbens and Rubin ( 2015 ) . This framework allows us to formally describe the causal effect of an intervention, or treatment , on some target quantity, or outcome .
In § 1 , we defined memorisation as the causal effect of training on an instance on a model’s ability to predict it.
Thus, the act of training on 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x defines the treatment, while the model’s ability to predict an instance defines the outcome. Report issue for preceding element Since training is performed iteratively over batches, instances are treated at different timesteps.
Thus, we use a treatment assignment variable G ⁢ ( 𝒙 ) 𝐺 𝒙 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) to denote the step g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g an instance is trained on.
Further, to quantify the ability of a model with parameters 𝜽 c subscript 𝜽 𝑐 \bm{\theta}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c} bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT to predict 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x , we use a performance function γ 𝛾 \gamma italic_γ .
We then define the outcome variable as Y c ⁢ ( 𝒙 ) = def γ ⁢ ( 𝜽 c , 𝒙 ) superscript def subscript 𝑌 𝑐 𝒙 𝛾 subscript 𝜽 𝑐 𝒙 Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_γ ( bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) , and, unless noted otherwise, we set this performance function to be the log-likelihood of 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x under p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}} italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT : γ ⁢ ( 𝜽 c , 𝒙 ) = log ⁡ p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) 𝛾 subscript 𝜽 𝑐 𝒙 subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 𝒙 \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_γ ( bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 5 5 5 We experiment with other functions in the appendix. Report issue for preceding element To define memorisation we need to represent both observed—i.e., Y c ⁢ ( 𝒙 ) subscript 𝑌 𝑐 𝒙 Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) —and counterfactual outcomes—i.e., the performance of the model on 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x had we not trained on it.
The potential outcomes notation (Splawa-Neyman, 1923 ) enables us to represent both types of outcomes consistently. Report issue for preceding element Definition 1 . Report issue for preceding element The potential outcome of an instance 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c under treatment assignment g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , denoted as Y c ⁢ ( 𝐱 ; g ) subscript 𝑌 𝑐 𝐱 𝑔 Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , is the value that the outcome would have taken if G ⁢ ( 𝐱 ) 𝐺 𝐱 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) was equal to g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element Since we only observe a single permutation of the data, we only see one specific treatment step for each instance, i.e., G ⁢ ( 𝒙 ) 𝐺 𝒙 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) .
Thus, the potential outcome of an instance is observed only for the actual treatment assignment g = G ⁢ ( 𝒙 ) 𝑔 𝐺 𝒙 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\!=\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_g = italic_G ( bold_italic_x ) .
In this case, we can equate potential and observed outcomes, that is Y c ⁢ ( 𝒙 ; g ) = Y c ⁢ ( 𝒙 ) subscript 𝑌 𝑐 𝒙 𝑔 subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})=Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) , a property called consistency (Cole and Frangakis, 2009 ) .
For any other treatment step g ⁢ ≠ G ⁢ ( 𝒙 ) 𝑔 𝐺 𝒙 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
\neq}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}}) italic_g ≠ italic_G ( bold_italic_x ) , the potential outcome is counterfactual and, thus, unobservable from the data. Report issue for preceding element 3 Counterfactual Memorisation Report issue for preceding element Intuitively, counterfactual memorisation can be understood as the answer to the question: how would the model’s performance on instance 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c be different if we had not trained on it at timestep g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g ?
Using the potential outcomes notation, we formalise this definition as follows. Report issue for preceding element Definition 2 . Report issue for preceding element Counterfactual memorisation is the causal effect of using instance 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x for training at the observed timestep G ⁢ ( 𝐱 ) ⁢ = g G 𝐱 g G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g on the model’s performance on this same instance at timestep c c {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : Report issue for preceding element τ 𝒙 , c = def Y c ⁢ ( 𝒙 ; g ) ⏟ performance on 𝒙 when trained with 𝒙 − Y c ⁢ ( 𝒙 ; ∞ ) ⏟ performance on 𝒙 when not trained with 𝒙 superscript def subscript 𝜏 𝒙 𝑐 subscript ⏟ subscript 𝑌 𝑐 𝒙 𝑔 performance on 𝒙 when trained with 𝒙 subscript ⏟ subscript 𝑌 𝑐 𝒙 performance on 𝒙 when not trained with 𝒙 \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}\,\,\,\mathrel{\stackrel%
{{\scriptstyle\textnormal{def}}}{{=}}}\underbrace{Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})}_{\begin{%
subarray}{c}\text{performance on ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\\
\text{when trained with ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\end{subarray}}-%
\underbrace{Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})}_{\begin{subarray}{c}\text{performance on%
 ${\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}$}\\
\text{when not trained with ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\end{subarray}}\!\!\!\! italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP under⏟ start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT - under⏟ start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when not trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT (2) In econometrics, eq. 2 is called an individual treatment effect (ITE).
Notably, the first potential outcome in this equation, Y c ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑐 𝒙 𝑔 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , can be observed from the data since, by definition, we trained on 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep G ⁢ ( 𝒙 ) ⁢ = g 𝐺 𝒙 𝑔 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g .
However, the second term, Y c ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) , is counterfactual.
To compute the ITE, we would need to estimate this counterfactual outcome for a specific instance, which is challenging due to unobserved factors and heterogeneity 6 6 6 I.e., non-random variability across instances. (Lu et al., 2018 ) .
While we would ideally estimate memorisation at the instance level, we focus on average effects instead, as is common in the econometrics literature (Angrist and Pischke, 2015 ) . Report issue for preceding element Definition 3 . Report issue for preceding element Expected counterfactual memorisation is the average causal effect of using instances for training at timestep g g {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g on the model’s performance on these same instances at timestep c c {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : 7 7 7 In econometrics, eq. 3 is called an average treatment effect on the treated (ATT), as it is defined in terms of an expectation over 𝐱 ∼ p ⁢ ( 𝐱 ⁢ ∣ G ⁢ ( 𝐱 ) ⁢ = g ) similar-to 𝐱 p 𝐱 ∣ G 𝐱 g {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\sim p({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}) bold_italic_x ∼ italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = italic_g ) .
In other words, this expectation is taken with respect to the instance distribution conditioned on it being selected for training at step g g {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g .
Assuming the training set is sampled i.i.d. and that its permutation is random (as discussed in § 2.1 ), then p ⁢ ( 𝐱 ⁢ ∣ G ⁢ ( 𝐱 ) ⁢ = g ) = p ⁢ ( 𝐱 ) p 𝐱 ∣ G 𝐱 g p 𝐱 p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ) .
Given these assumptions, eq. 3 would also be an average treatment effect (ATE), which would allow us to make causal claims about the entire population. Report issue for preceding element τ g , c = def 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) − Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = g ] superscript def subscript 𝜏 𝑔 𝑐 subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 𝑔 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\Big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\Big{]} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] (3) Together, the τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT form a matrix which we term the model’s memorisation profile ;
each row therein is the memorisation path of a batch.
Memorisation profiles and paths allow us to analyse a model’s memorisation patterns across different treatment and checkpoint steps.
Notably, there cannot be memorisation whenever c < g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g , as the instances have not been seen by the model yet, so τ g , c = 0 subscript 𝜏 𝑔 𝑐 0 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}=0 italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = 0 in those cases.
We term τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT as instantaneous memorisation when c = g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c = italic_g , as persistent memorisation when c > g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}>{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c > italic_g , and as residual memorisation when c = T 𝑐 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T . Report issue for preceding element 4 Estimating Memorisation Report issue for preceding element The practical implication of defining memorisation at a treatment level g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g is that we can only make causal claims for groups of instances treated at the same timestep (i.e., a batch ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ), rather than for individual instances.
However, even though this approach simplifies the problem, estimating eq. 3 still poses major challenges as it contains a counterfactual.
A simple decomposition makes this counterfactual explicit: Report issue for preceding element τ g , c = subscript 𝜏 𝑔 𝑐 absent \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}= italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = (4) 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ] ⏟ 1 Report issue for preceding element ⁢ - 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ] ⏟ 2 Report issue for preceding element subscript ⏟ subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 𝑔 ∣ 𝐺 𝒙 𝑔 1 Report issue for preceding element subscript ⏟ subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 ∣ 𝐺 𝒙 𝑔 2 Report issue for preceding element \displaystyle\underbrace{\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\!\big{[%
}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}%
c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}\!}_{%
\leavevmode\hbox to8.9pt{\vbox to8.9pt{\pgfpicture\makeatletter\hbox{\hskip 4.%
45195pt\lower-4.45195pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }%
\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}%
\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox to0.0pt{%
\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%
}{}{}{{}\pgfsys@moveto{4.25195pt}{0.0pt}\pgfsys@curveto{4.25195pt}{2.34831pt}{%
2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\pgfsys@curveto{-2.34831pt}{4.25195pt}{%
-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\pgfsys@curveto{-4.25195pt}{-2.34831%
pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\pgfsys@curveto{2.34831pt}{-4.25%
195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto%
{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1%
.0}{-1.75pt}{-2.25555pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{%
rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{{1}}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}%
\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}%
\lxSVG@closescope\endpgfpicture}}}\mathop{-}\underbrace{\operatorname*{\mathbb%
{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\!\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}\!}_{\leavevmode\hbox to8.9pt{\vbox to8.9%
pt{\pgfpicture\makeatletter\hbox{\hskip 4.45195pt\lower-4.45195pt\hbox to0.0pt%
{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}%
\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}%
{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox
to%
0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%
}{}{}{{}\pgfsys@moveto{4.25195pt}{0.0pt}\pgfsys@curveto{4.25195pt}{2.34831pt}{%
2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\pgfsys@curveto{-2.34831pt}{4.25195pt}{%
-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\pgfsys@curveto{-4.25195pt}{-2.34831%
pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\pgfsys@curveto{2.34831pt}{-4.25%
195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto%
{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1%
.0}{-1.75pt}{-2.25555pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{%
rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{{2}}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}%
\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}%
\lxSVG@closescope\endpgfpicture}}} under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - under⏟ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Expectation 1 Report issue for preceding element can be directly estimated from the data because batch ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT contains a set of examples 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x for which G ⁢ ( 𝒙 ) ⁢ = g 𝐺 𝒙 𝑔 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g and, thus, we can invoke the consistency assumption to equate Y c ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑐 𝒙 𝑔 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) with the observed outcome Y c ⁢ ( 𝒙 ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) .
Let us define the mean across instances in a batch as: Report issue for preceding element Y ¯ c ⁢ ( g ) = def 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g Y c ⁢ ( 𝒙 ) superscript def subscript ¯ 𝑌 𝑐 𝑔 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 \displaystyle\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})\mathrel{\stackrel{{\scriptstyle\textnormal{def%
}}}{{=}}}\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named%
]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (5) We can thus use eq. 5 as an estimator for expectation 1 Report issue for preceding element . Expectation 2 Report issue for preceding element , however, is counterfactual: we cannot observe the potential outcome Y c ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) for instances treated at timestep g ⁢ ≠ ∞ 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
\neq}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty} italic_g ≠ ∞ .
The presence of counterfactual potential outcomes in causal estimands creates challenges for their estimation, being known as the fundamental problem of causal inference (Holland, 1986 ) .
The goal of causal methods is then to estimate these counterfactual outcomes from observed ones, using comparable groups of instances.
Thus far, we have defined our causal estimand.
In this section, we perform steps two and three of causal estimation ( § 2.2 ): we derive two statistical estimands for our causal estimand (identifying it under specific assumptions), and provide concrete estimators for them. Report issue for preceding element 4.1 The Difference Estimator Report issue for preceding element Our first approach to estimate memorisation is straightforward and only requires the observed outcomes of a held-out validation set.
However, it relies on a strong identification assumption. Report issue for preceding element Assumption 1 (I.i.d. Dataset Sampling) . Report issue for preceding element Instances 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x are independently and identically distributed, following p ⁢ ( 𝐱 ) 𝑝 𝐱 p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ) , and are randomly assigned to treatment groups g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element Under this assumption, we have that p ⁢ ( 𝒙 ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ) = p ⁢ ( 𝒙 ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ) = p ⁢ ( 𝒙 ) 𝑝 𝒙 ∣ 𝐺 𝒙 𝑔 𝑝 𝒙 ∣ 𝐺 𝒙 𝑝 𝒙 p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!=\!p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\!=\!p({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = ∞ ) = italic_p ( bold_italic_x ) .
Thus, the following statistical estimand identifies τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element τ g , c 𝚍𝚒𝚏𝚏 = 𝔼 𝒙 [ \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}}=\operatorname*{\mathbb{E}}_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ Y c ( 𝒙 ; g ) ∣ G ( 𝒙 ) = g ] \displaystyle Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] (6) − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ] subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 \displaystyle-\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid\mathop{G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\big{]} - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ start_BIGOP italic_G ( bold_italic_x ) = ∞ end_BIGOP ] (See Lemma 1 in § A.1 for a proof.)
Note that, unlike eq. 3 , the second term in this estimand is not counterfactual: it is the expected observed outcome of validation instances, ℬ ∞ subscript ℬ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT .
This statistical estimand can then be estimated as follows. Report issue for preceding element Estimator 1 . Report issue for preceding element The difference estimator , defined as: Report issue for preceding element τ ^ g , c 𝚍𝚒𝚏𝚏 = Y ¯ c ⁢ ( g ) − Y ¯ c ⁢ ( ∞ ) superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑐 \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}}=\overline{Y}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) (7) is an unbiased estimator of τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element See Lemma 2 in § A.1 for a proof.
∎ Report issue for preceding element Notably, Assump. 1 is satisfied by the training procedure we described in § 2.1 , and is commonly true in machine learning.
However, it might not hold in general, as the train and validation distributions may not match exactly.
For example, NLP practitioners might deduplicate their training set but not validation (Biderman et al., 2023b ) or might use challenge sets for validation (Kiela et al., 2021 ) .
Moreover, even when Assump. 1 holds, eq. 7 is low-variance only if we have large enough samples to compute Y ¯ c ⁢ ( g ) subscript ¯ 𝑌 𝑐 𝑔 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y ¯ c ⁢ ( ∞ ) subscript ¯ 𝑌 𝑐 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) .
Unfortunately, given the size of state-of-the-art LMs and their datasets, it can be expensive—both in terms of computation and memory usage—to extract the performance measures Y c ⁢ ( 𝒙 ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) for all instances 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and checkpoints c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .
Even with unlimited compute, Y ¯ c ⁢ ( g ) subscript ¯ 𝑌 𝑐 𝑔 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) can only be estimated using instances in ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , which lower bounds the variance of this estimator. Report issue for preceding element While the difference estimator is a first step towards a principled estimator of counterfactual memorisation, we can do better.
In the next section, we describe an estimator that has lower variance and requires weaker assumptions. Report issue for preceding element 4.2 The Difference-in-Differences Estimator Report issue for preceding element We now introduce another causal estimator based on the difference-in-differences (DiD) design.
The intuition behind DiD is to use the time dimension to help with identification; DiD identifies a causal estimand using the difference in the trends over time of the outcome on treated vs. untreated instances.
In our specific setting, DiD relies on the assumption that changes in model performance over time would follow similar trends in different batches if they had not been used for training.
We formalise this assumption as follows. Report issue for preceding element Assumption 2 (Parallel Trends) . Report issue for preceding element In the absence of training, the expected change in model performance across checkpoints would be the same regardless of treatment.
That is, for all c , c ′ ≥ g ⁢ - 1 𝑐 superscript 𝑐 ′ 𝑔 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{%
\prime}}\geq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}\mathop{-}1 italic_c , italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≥ italic_g - 1 : Report issue for preceding element 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y c ′ ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = g ] subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 superscript 𝑐 ′ 𝒙 𝐺 𝒙 𝑔 \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] (8) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y c ′ ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ] absent subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 superscript 𝑐 ′ 𝒙 𝐺 𝒙 \displaystyle\qquad=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid\mathop{G({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ start_BIGOP italic_G ( bold_italic_x ) = ∞ end_BIGOP ] We need a second assumption before we can apply the DiD design to our setting. Report issue for preceding element Assumption 3 (No Anticipation) . Report issue for preceding element Training has no effect before it happens.
That is, for all c < g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g : Report issue for preceding element 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) ⁢ = g ] subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] (9) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = g ] absent subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle\qquad\qquad\quad=\operatorname*{\mathbb{E}}_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] Given these two assumptions, we can now follow Callaway and Sant’Anna ( 2021 ) in identifying our target statistical estimand. 8 8 8 The DiD design was originally proposed for the case with only two treatment and checkpoint steps (i.e., g ∈ { 1 , ∞ } 𝑔 1 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{1,{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\} italic_g ∈ { 1 , ∞ } and c ∈ { 0 , 1 } 𝑐 0 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,1\} italic_c ∈ { 0 , 1 } ). Previous work has shown the challenges of extending DiD to multiple timesteps, especially when allowing for heterogeneous treatment effects (Roth et al., 2023 ) . Callaway and Sant’Anna ( 2021 ) propose an extension which identifies eq. 3 while allowing for treatment effect heterogeneity across checkpoint and treatment steps. Combined, these assumptions allow us to rewrite expectation 2 Report issue for preceding element in eq. 4 as a function of potential outcomes that are observable: Y c ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) , Y g − 1 ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 𝑔 1 𝒙 Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) given G ⁢ ( 𝒙 ) ⁢ = ∞ 𝐺 𝒙 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty} italic_G ( bold_italic_x ) = ∞ are observable on a held-out validation set, while Y g − 1 ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑔 1 𝒙 𝑔 Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) given G ⁢ ( 𝒙 ) ⁢ = g 𝐺 𝒙 𝑔 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g is observable on the training set.
The following statistical estimand thus identifies τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element τ g , c 𝚍𝚒𝚍 = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) − Y g − 1 ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) ⁢ = g ] superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚍 subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 𝑔 conditional subscript 𝑌 𝑔 1 𝒙 𝑔 𝐺 𝒙 𝑔 \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}}=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0%
}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0%
.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}[Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\!-\!Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}]\! italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] (10) − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ] subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 \displaystyle\qquad\,\,\,-\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}[Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!-\!Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]%
{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\!\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (See Lemma 3 in § A.2 for a proof.)
This leads to the following DiD estimator. Report issue for preceding element Estimator 2 . Report issue for preceding element The difference-in-differences estimator (DiD), defined as: Report issue for preceding element τ ^ g , c 𝚍𝚒𝚍 = ( Y ¯ c ⁢ ( g ) ⁢ - Y ¯ g − 1 ⁢ ( g ) ) ⏟ diff in trained − ( Y ¯ c ⁢ ( ∞ ) ⁢ - Y ¯ g − 1 ⁢ ( ∞ ) ) ⏟ diff in untrained superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚍 subscript ⏟ subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑔 1 𝑔 diff in trained subscript ⏟ subscript ¯ 𝑌 𝑐 subscript ¯ 𝑌 𝑔 1 diff in untrained \displaystyle\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}}=\underbrace{\!\Big{(}%
\overline{Y}_{\!\!{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g})\mathop{-}\overline{Y}_{\!\!\mathop{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}\!}_{\emph{diff in %
trained}}-\underbrace{\!\Big{(}\overline{Y}_{\!\!{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{-}\overline{Y}%
_{\!\!\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty})\Big{)}\!}_{\emph{diff in untrained}} over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = under⏟ start_ARG ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( italic_g ) ) end_ARG start_POSTSUBSCRIPT diff in trained end_POSTSUBSCRIPT - under⏟ start_ARG ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( ∞ ) ) end_ARG start_POSTSUBSCRIPT diff in untrained end_POSTSUBSCRIPT (11) is an unbiased estimator of τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 . Report issue for preceding element Proof. Report issue for preceding element See Lemma 4 in § A.2 for a proof.
∎ Report issue for preceding element The DiD estimator depends on weaker assumptions and has a lower variance (under mild assumptions, see § A.3 ) than the difference estimator in eq. 7 .
Specifically, the parallel trends assumption ( Assump. 2 ) is strictly weaker than the i.i.d. one ( Assump. 1 ): if p ⁢ ( 𝒙 ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ) = p ⁢ ( 𝒙 ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ) 𝑝 𝒙 ∣ 𝐺 𝒙 𝑔 𝑝 𝒙 ∣ 𝐺 𝒙 p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = ∞ ) , then it is trivial that performances should present parallel trends.
Moreover, Assump. 2 only requires that the training and validation sets follow similar trends on average, which might be true even in the case of, e.g., challenge validation sets or deduplicated training data.
Therefore, the assumptions underpinning DiD are more likely to hold in practice and we will use it to estimate memorisation here. Report issue for preceding element In practice, the difference-in-differences estimation procedure includes two steps.
First, we compute the model’s performance on a subset of analysed instances—i.e., samples from the training and validation sets—using the available checkpoints; thus forming a panel of observed outcomes, as it is usually termed in econometrics.
Then, we use this panel to compute the estimates in eq. 11 . Report issue for preceding element 5 Prior Notions of Memorisation Report issue for preceding element Memorisation has recently received much attention (Arpit et al., 2017 ; Carlini et al., 2019 , 2021 ; Anagnostidis et al., 2023 , inter alia ) . 9 9 9 See Hartmann et al. ( 2023 ) or Ishihara ( 2023 ) for surveys. Prior work has studied how model architecture and training choices influence memorisation (Tirumala et al., 2022 ; Kandpal et al., 2022 ; Lee et al., 2022 ; Biderman et al., 2023a ) , and where memorised instances are stored within a model (Maini et al., 2023 ; Stoehr et al., 2024 ) .
In this section, we use our framework to discuss three prior notions of memorisation which we consider most relevant to our paper: previous operationalisations of counterfactual memorisation (e.g., Feldman, 2020 ) , influence functions (Zheng and Jiang, 2022 ) , and extractable memorisation (Carlini et al., 2023 ) . Report issue for preceding element 5.1 Previous Operationalisations of Counterfactual Memorisation Report issue for preceding element As mentioned before, estimating an instance’s memorisation τ 𝒙 , c subscript 𝜏 𝒙 𝑐 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT is non-trivial due to the counterfactual component in its definition.
We avoid this issue by estimating expected memorisation τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT instead.
Prior work (Feldman, 2020 ; Feldman and Zhang, 2020 ; Zhang et al., 2023 ; Lukasik et al., 2024 ) takes a different approach, comparing the performance of models trained with and without that instance.
In doing so, they average performance across training runs, measuring what we term architectural counterfactual memorisation . Report issue for preceding element Formally, let 𝝍 𝝍 \bm{\psi} bold_italic_ψ be a vector of variables responsible for training variance.
This includes, e.g., the data permutation induced by σ 𝜎 \sigma italic_σ and the initial model parameters 𝜽 0 subscript 𝜽 0 \bm{\theta}_{0} bold_italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .
By defining a distribution p ⁢ ( 𝝍 ) 𝑝 𝝍 p(\bm{\psi}) italic_p ( bold_italic_ψ ) over these variables, architectural memorisation can be defined as follows. Report issue for preceding element Definition 4 . Report issue for preceding element Architectural counterfactual memorisation is the counterfactual memorisation τ 𝐱 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript τ 𝐱 \scaleto T 4 p t \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT when marginalising over training variables 𝛙 𝛙 \bm{\psi} bold_italic_ψ : Report issue for preceding element τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT = def 𝔼 𝝍 [ τ 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ ≠ ∞ ] superscript def absent subscript 𝔼 𝝍 subscript 𝜏 𝒙 \scaleto 𝑇 4 𝑝 𝑡 ∣ 𝐺 𝒙 \displaystyle\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[\tau_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto%
{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}%
{4pt}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{\neq}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ∣ italic_G ( bold_italic_x ) ≠ ∞ ] (12) = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) ⁢ - Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ ≠ ∞ ] absent subscript 𝔼 𝝍 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 ∣ 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mathop{-}Y_{\scaleto{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4%
pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{\neq}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] where G ⁢ ( 𝐱 ) 𝐺 𝐱 G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) in the first potential outcome depends on which batch the shuffling function σ 𝜎 \sigma italic_σ puts 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in. Report issue for preceding element Prior work has proposed a number of methods to estimate this value (Bachmann et al., 2022 ; Lin et al., 2022 ; Ilyas et al., 2022 ; Park et al., 2023 ) .
The simplest of these is to train several models while including or not 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in the training set; these models are then used to approximate the expectation above.
We describe a statistical estimand and estimator for τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT in § B.1 , discussing the assumptions needed by this approach. Report issue for preceding element Notably, this operationalisation has the advantage of estimating memorisation at the instance level.
However, it also has drawbacks—beyond just being computationally expensive to estimate.
These become apparent upon closer inspection of the definition of τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT .
First, it does not provide insights into the effect of the checkpoint step or treatment step on memorisation;
this is because T 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T is hard-coded into τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT ’s definition and because it marginalises over permutations of the data.
While it is trivial to generalise this definition to other checkpoint steps c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c or to specific g 𝑔 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , prior work has mainly focused on these choices, overlooking the impact of training dynamics on memorisation.
Further, and perhaps more importantly, marginalising over p ⁢ ( 𝝍 ) 𝑝 𝝍 p(\bm{\psi}) italic_p ( bold_italic_ψ ) means that this metric quantifies memorisation for a model architecture, rather than for a specific model. Report issue for preceding element 5.2 Influence Functions Report issue for preceding element Influence functions (Hampel, 1974 ; Cook and Weisberg, 1980 ) estimate—without re-training a model—how its parameters would change if an instance 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x was removed from the training set.
Specifically, the new set of parameters can be approximated as follows (Koh and Liang, 2017 ) : Report issue for preceding element 𝜽 − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ≈ 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t + 1 / N ⁢ H 𝜽 − 1 ⁢ ∇ 𝜽 ℒ ⁢ ( 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t , 𝒙 ) subscript 𝜽 𝒙 \scaleto 𝑇 4 𝑝 𝑡 subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 1 𝑁 subscript superscript H 1 𝜽 subscript ∇ 𝜽 ℒ subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle\bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}\approx%
\bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}+\nicefrac{{1}}{{N}}\,\mathrm{H}^{%
\mathop{-1}}_{\bm{\theta}}\,\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta}_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) bold_italic_θ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ≈ bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT + / start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ∇ start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , bold_italic_x ) (13) where H 𝜽 subscript H 𝜽 \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT is the hessian of ℒ ℒ \mathcal{L} caligraphic_L evaluated at 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
This approximation is based on a first-order Taylor expansion of the training objective around 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , and should lead to small errors under the following assumptions:
(i) the loss function is strictly convex in 𝜽 𝜽 \bm{\theta} bold_italic_θ , (ii) H 𝜽 subscript H 𝜽 \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (i.e., the gradient is zero).
Given these assumptions, influence functions can be used to efficiently estimate the counterfactual term Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) in the definition of τ 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑇 4 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
Specifically, we can measure the model’s performance using the updated parameters, Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) = def γ ⁢ ( 𝜽 − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ; 𝒙 ) superscript def subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝛾 subscript 𝜽 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathrel{\stackrel{{%
\scriptstyle\textnormal{def}}}{{=}}}\gamma(\bm{\theta}_{-{\color[rgb]{0,0.88,0%
}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0%
.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}};{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_γ ( bold_italic_θ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ) , and equate Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) = Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) .
The influence function estimator of memorisation can then be written as: Report issue for preceding element τ ^ 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t 𝚒𝚗𝚏𝚕 = Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) − Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) superscript subscript ^ 𝜏 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝚒𝚗𝚏𝚕 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle\hat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{%
\mathtt{infl}}=Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (14) We formalise this estimator and its statistical estimand in § B.2 .
Notably, Zheng and Jiang ( 2022 ) use a similar approach to estimate memorisation in a classification setting. Report issue for preceding element Influence functions thus provide a computationally efficient method to estimate instance-level counterfactual memorisation.
However, none of the required assumptions above is typically satisfied for LMs, which can lead to strong biases in this estimator (Basu et al., 2020 ; Bae et al., 2022 ; Schioppa et al., 2023 ) .
Moreover, assumptions (ii) and (iii) require 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT to be locally optimal, meaning that this approach is not applicable for studying c < T 𝑐 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c < italic_T .
We therefore cannot use it to study how memorisation interacts with training dynamics. Report issue for preceding element 5.3 Extractable Memorisation Report issue for preceding element Carlini et al. ( 2023 ) defines memorisation as ( k , ℓ ) 𝑘 ℓ (k,\!\ell) ( italic_k , roman_ℓ ) -extractability; a string is ( k , ℓ ) 𝑘 ℓ (k,\!\ell) ( italic_k , roman_ℓ ) -extractable if the model correctly predicts ℓ ℓ \ell roman_ℓ of its tokens given a prefix of k 𝑘 k italic_k tokens.
This definition has recently gained much interest because of its relevance to copyright infringement and data protection.
Despite being seemingly different, we argue that extractable memorisation is an estimator for counterfactual memorisation.
Concretely, let performance γ 𝛾 \gamma italic_γ be measured as whether a string is ( k , ℓ ) 𝑘 ℓ (k,\!\ell) ( italic_k , roman_ℓ ) -extractable.
Now, assume that a string is not ( k , ℓ ) 𝑘 ℓ (k,\!\ell) ( italic_k , roman_ℓ ) -extractable in the absence of training, i.e., Y c ⁢ ( 𝒙 ; ∞ ) ⁢ = 0 subscript 𝑌 𝑐 𝒙 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = 0 .
Given this assumption, we can define an extractable memorisation estimator as: Report issue for preceding element τ ^ 𝒙 , c 𝚎𝚡𝚝𝚛 = Y c ⁢ ( 𝒙 ) superscript subscript ^ 𝜏 𝒙 𝑐 𝚎𝚡𝚝𝚛 subscript 𝑌 𝑐 𝒙 \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (15) We formalise this estimator and its statistical estimand in § B.3 . Report issue for preceding element Extractable memorisation thus implicitly assumes that Y c ⁢ ( 𝒙 ; ∞ ) ⁢ = 0 subscript 𝑌 𝑐 𝒙 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = 0 .
Counterfactual memorisation, on the other hand, controls for this counterfactual.
Notably, assuming Y c ⁢ ( 𝒙 ; ∞ ) ⁢ = 0 subscript 𝑌 𝑐 𝒙 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = 0 may be reasonable when a string is long and complex; in this case, the chance that 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x would be in the model’s top- 1 1 1 1 beam (the output of greedy decoding) approaches zero.
However, this assumption may not be reasonable for shorter or less complex sequences. Rather, it may cause the resulting estimate to conflate memorisation with the intrinsic difficulty of predicting a string. Report issue for preceding element 6 Experiments Report issue for preceding element While our method applies to any model trained with a single pass on its training data, we focus on quantifying memorisation in pretrained LMs, which are characterised by the use of architectures with a large number of parameters and large datasets.
Due to the costs of training such models from scratch, we take advantage of open-source pretrained models whose intermediate checkpoints and preprocessed data are publicly available.
In this section, we detail the models and data used and describe how we collect the observed outcomes Y c ⁢ ( 𝒙 ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) . Report issue for preceding element The Pythia Suite. Report issue for preceding element We use the publicly available Pythia model suite 10 10 10 Both preprocessed data and intermediate checkpoints are publicly available at github.com/EleutherAI/pythia . (Biderman et al., 2023b ) , composed of 8 8 8 8 transformers of sizes ranging from \qty [mode=math]70 to \qty [mode=math]12 parameters.
These models were trained on the Pile dataset (Gao et al., 2020 ; Biderman et al., 2022 ) , a \qty [mode=math]300-token curated collection of English documents.
All models are trained using the same data.
Specifically, the dataset is shuffled and “packed” into sequences of 2 , 049 2049 2,049 2 , 049 tokens; 11 11 11 Since target tokens are the right-shifted input tokens, to compute the loss on 2 , 048 2048 2,048 2 , 048 tokens the Pythia authors added a token to the context. each of these sequences corresponds to an instance 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x .
Training was performed using a cosine learning rate schedule with warm-up, and using a batch size of 1 , 024 1024 1,024 1 , 024 sequences, resulting in exactly \qty [mode=math]143 optimisation steps.
We use the model versions trained on the deduplicated Pile dataset to reduce the risk of finding spurious memorisation patterns due to duplication.
The deduplicated dataset has \qty [mode=math]207 tokens, thus models using this version are trained for circa 1.5 1.5 1.5 1.5 epochs to keep an equal token count relative to the non-deduplicated versions.
We consider the checkpoints relative to the first epoch (i.e., up to step \qty [mode=math]95). 12 12 12 For completeness, we report the second half-epoch (steps \qty [mode=math]96- \qty [mode=math]143) analysis in App. D . More details are in App. C . Report issue for preceding element Constructing the Panel. Report issue for preceding element Ideally, we would collect performance metrics for each instance 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and for every checkpoint step c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .
However, given the size of the Pile dataset, it is computationally infeasible to collect evaluations for all instances; thus we resort to subsampling this data.
Furthermore, the granularity of the available checkpoints (i.e., every \qty [mode=math]1 timesteps) does not allow us to consider each timestep; thus we consider timesteps c ∈ { 0 , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 ⁢ … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 95 } 𝑐 0 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,\qty[mode=math]{1}{}...,\qty[mode=math]{95}{}\} italic_c ∈ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } and treatment timesteps g ∈ { \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 2 ⁢ … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 95 } 𝑔 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 2 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 95 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},\qty[mode=math]{2}{}...,\qty[mode=math]{95}{}\} italic_g ∈ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 2 … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } .
To match the checkpoint frequency, we consider all instances between two checkpoints (i.e., \qty [mode=math]1 batches) as if they were seen by the model at the same timestep.
We term these groups of batches macro-batches 13 13 13 In econometrics group of instances that undergo treatment at the same time are typically termed cohorts . and formally define them as 𝒢 g = ⋃ g − \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 < t ≤ g ℬ t subscript 𝒢 𝑔 subscript 𝑔 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 𝑡 𝑔 subscript ℬ 𝑡 \mathcal{G}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}=\bigcup_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}-\qty[mode=math]{1}{}<t\leq{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\mathcal{B}_{t} caligraphic_G start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = ⋃ start_POSTSUBSCRIPT italic_g - [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 < italic_t ≤ italic_g end_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
To obtain enough evaluations for each macro-batch, we sample instances from the training set in two steps: we randomly choose 10 10 10 10 batches for each macro-batch and sample 10 10 10 10 instances from each.
This process results in \qty [mode=math]14.3 analysed training instances.
Additionally, we sample \qty [mode=math]2 instances from the validation set to create 𝒢 ∞ subscript 𝒢 \mathcal{G}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_G start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT .
This process returns a panel of \qty [mode=math]16.3 instances evaluated at 96 96 96 96 timesteps. 14 14 14 Our data and experimental artefacts are publicly available at huggingface.co/collections/pietrolesci/memorisation-profiles . As our performance metric we use the sequence-level log-likelihood: γ ⁢ ( 𝜽 , 𝒙 ) = log ⁡ p \scaleto ⁢ 𝜽 ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) 𝛾 𝜽 𝒙 subscript 𝑝 \scaleto 𝜽 4 𝑝 𝑡 𝒙 \gamma(\bm{\theta},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}) italic_γ ( bold_italic_θ , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 15 15 15 Results using different metrics are reported in App. D . Report issue for preceding element Figure 2: Memorisation profiles ( τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT ). We only show statistically significant entries. Report issue for preceding element Statistical Inference. Report issue for preceding element To compute statistical significance, we use the Simple Multiplier Bootstrap procedure of Callaway and Sant’Anna ( 2021 ) which returns simultaneous confidence intervals for all memorisation estimates, accounting for dependencies across macro-batches and checkpoint steps and thus avoiding multiple-testing issues. Report issue for preceding element 7 Results Report issue for preceding element We report the memorisation profiles of all Pythia sizes in Fig. 2 . Below, we use these memorisation profiles to describe different types of memorisation. Report issue for preceding element Figure 3: Instantaneous memorisation ( τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT for g = c 𝑔 𝑐 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}={\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g = italic_c ). We only show statistically significant estimates. Report issue for preceding element Instantaneous Memorisation. Report issue for preceding element Instantaneous memorisation estimates (defined in § 3 as τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g ⁢ = c 𝑔 𝑐 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
=}{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g = italic_c ) are depicted as the diagonal entries in the memorisation profiles in Fig. 2 , and are also presented in Fig. 3 .
From these estimates, we can clearly observe the effect of the treatment step on memorisation: instantaneous memorisation is stronger earlier in training than later.
Interestingly (but perhaps unsurprisingly), instantaneous memorisation correlates with the cosine learning rate schedule: it is stronger after the warm-up period (around timestep \qty [mode=math]1.5) than before it.
Furthermore, and as expected, instantaneous memorisation increases with model size. 16 16 16 Notably, we expect instantaneous memorisation to always be present in normally-trained LMs (albeit with a potentially small value). It could thus be used for power analysis (Cohen, 1992 ) : choosing the number of instances to sample per macro-batch which provides sufficient statistical power to correctly detect memorisation. Report issue for preceding element Persistent Memorisation. Report issue for preceding element Persistent memorisation estimates (defined in § 3 as τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g > c 𝑔 𝑐 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g > italic_c ) are depicted as the off-diagonal entries in the memorisation profiles in Fig. 2 . Fig. 4 shows the average persistent memorisation at a specific number of timesteps after treatment; in this figure, τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT were averaged across macro-batches for each c ⁢ - g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{-}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c - italic_g . 17 17 17 By averaging across macro-batches, variance is lower and more estimates become statistically significant. This way of aggregating the memorisation profile allows us to summarise the general memorisation patterns of a model.
Smaller models have lower persistent memorisation, with \qty [mode=math]70 having no persistent memorisation.
Interestingly, persistent memorisation plateaus after \qty [mode=math]25 timesteps.
This result has implications for data ordering during training.
For example, if there are particular instances that we do not want the model to memorise, but which we still want to use during training, they should be included in earlier batches. 18 18 18 We note that our results differ from Biderman et al. ’s ( 2023b ), who find no differences in memorisation due to an instance’s treatment step. We hypothesise that this discrepancy stems from the differences in metrics used to quantify memorisation and the statistical approaches adopted. Report issue for preceding element Figure 4: Average persistent memorisation ( τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT averaged per timestep after treatment, i.e., c ⁢ - g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{-}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c - italic_g ). We only show statistically significant estimates. Report issue for preceding element Figure 5: Residual memorisation ( τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT for c = T = \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 95 𝑐 𝑇 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}=%
\qty[mode=math]{95}{} italic_c = italic_T = [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 ). Stronger colour intensity indicates statistical significance. Report issue for preceding element Figure 6: Pearson correlation between the memorisation profile of different models. Report issue for preceding element Residual Memorisation. Report issue for preceding element Residual memorisation estimates (defined in § 3 as τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when c = T 𝑐 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T ) are depicted as the final-column entries in Fig. 2 , and are also presented in Fig. 5 (we consider T 𝑇 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T to be the end of the first epoch here, i.e., timestep \qty [mode=math]95).
Interestingly, while all macro-batches undergo some degree of instantaneous memorisation, it appears that many are then forgotten by the end of the first epoch, as shown by the statistically insignificant residual memorisation estimates.
Furthermore, in line with our persistent memorisation results, residual memorisation shows a recency effect: the final macro-batches are the most memorised.
We hypothesise that this recency effect can be explained by the learning rate schedule.
Specifically, when the learning rate is high, the optimisation process moves model parameters further in the locally optimal direction, thus “overwriting” previous information with new information; this results in higher instantaneous and lower residual memorisation.
On the contrary, towards the end of the training process, when the learning rate is lower, previous information is “forgotten” less as the updates are smaller (in expectation), resulting in higher residual and lower instantaneous memorisation. Report issue for preceding element Memorisation Across Scales. Report issue for preceding element Due to the cost of training large LMs, it is highly desirable to be able to make predictions about a trained model’s characteristics before undertaking training. One strategy is to derive insights from smaller models to inform the design of larger ones (Rae et al., 2021 ; Black et al., 2022 ; Le Scao et al., 2022 ) . 19 19 19 Scaling laws for other notions of memorisation ( § 5 ) have been studied in Biderman et al. ( 2023a ) . Predictability across scales is visually apparent in Fig. 3 and 4 where there are similar trends across model sizes.
We formalise this intuition in Fig. 6 , where we report the Pearson correlation between the memorisation profiles of different models.
Interestingly, memorisation for larger models (e.g., \qty [mode=math]12) is predictable from smaller ones (e.g., \qty [mode=math]410).
We note that \qty [mode=math]70 and \qty [mode=math]160 are less predictive of the memorisation in \qty [mode=math]12.
However, prior work has shown that both these models suffer from training instability (Godey et al., 2024 ) ; the reduction in predictability with these smaller models might thus be specific to the Pythia suite. Report issue for preceding element 8 Conclusions Report issue for preceding element The memorisation of training data by neural networks has critical implications for privacy, copyright, and security. Thus, well-founded quantifications of memorisation, and corresponding accurate and efficient methods for their estimation are of great importance.
This work presents one such quantification and builds on the econometrics literature to derive an unbiased and efficient estimator of memorisation based on the difference-in-differences design.
We use this estimator to study the memorisation profiles of the Pythia model suite and find that memorisation is stronger and more persistent in larger models, determined by data order and learning rate, and stable across model sizes. Report issue for preceding element Limitations Report issue for preceding element This work estimates counterfactual memorisation in pretrained LMs.
Unfortunately, due to the costs associated with running large pretrained LMs—even in inference mode—we experimented with a limited number of models (the Pythia suite) trained in a single language (English).
Investigating whether other model architectures, training procedures, and natural languages result in similar memorisation profiles would be important to strengthen our conclusions.
Furthermore, when collecting the panel data needed to estimate memorisation, we subsampled the number of evaluated instances; this can significantly increase our estimators’ variance. Report issue for preceding element Acknowledgements Report issue for preceding element Report issue for preceding element Pietro and Andreas received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation programme grant AVeriTeC (Grant agreement No. 865958).
Clara is funded by a Google PhD Fellowship.
We thank the anonymous reviewers, for their helpful questions and comments that helped us improve the paper.
We also thank Machel Reid for early discussions on the topic;
Sidak Pal Singh, Gregor Bachmann, and Ornella Darova for their feedback on earlier versions of this paper; and
Davide Lesci and Marco Lesci for proofreading the final version of the paper. Report issue for preceding element References Report issue for preceding element Anagnostidis et al. (2023) ↑ Sotiris Anagnostidis, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. 2023. The curious case of benign memorization . In The Eleventh International Conference on Learning Representations . Angrist and Pischke (2015) ↑ Joshua Angrist and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect . Princeton University Press. Arpit et al. (2017) ↑ Devansh Arpit, Stanisław Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017. A closer look at memorization in deep networks . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML’17, page 233–242. JMLR.org. Bachmann et al. (2022) ↑ Gregor Bachmann, Thomas Hofmann, and Aurelien Lucchi. 2022. Generalization through the lens of leave-one-out error . In International Conference on Learning Representations . Bae et al. (2022) ↑ Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B. Grosse. 2022. If influence functions are the answer, then what is the question? Advances in Neural Information Processing Systems , 35:17953–17967. Basu et al. (2020) ↑ Samyadeep Basu, Phil Pope, and Soheil Feizi. 2020. Influence functions in deep learning are fragile . In International Conference on Learning Representations . Biderman et al. (2022) ↑ Stella Biderman, Kieran Bicheno, and Leo Gao. 2022. Datasheet for the Pile . arXiv preprint 2201.07311 . Biderman et al. (2023a) ↑ Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023a. Emergent and predictable memorization in large language models . Advances in Neural Information Processing Systems , 36:28072–28090. Biderman et al. (2023b) ↑ Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023b. Pythia: A suite for analyzing large language models across training and scaling . In Proceedings of the 40th International Conference on Machine Learning , ICML’23. Black et al. (2022) ↑ Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model . In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models , pages 95–136, virtual+Dublin. Association for Computational Linguistics. Callaway and Sant’Anna (2021) ↑ Brantly Callaway and Pedro H. C. Sant’Anna. 2021. Difference-in-Differences with multiple time periods . Journal of Econometrics , 225(2):200–230. Cao et al. (2022) ↑ Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. 2022. Benign overfitting in two-layer convolutional neural networks . Advances in Neural Information Processing Systems , 35:25237–25250. Carlini et al. (2023) ↑ Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models . In The Eleventh International Conference on Learning Representations . Carlini et al. (2019) ↑ Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The Secret Sharer: Evaluating and testing unintended memorization in neural networks . In Proceedings of the 28th USENIX Conference on Security Symposium , SEC’19, pages 267––284, USA. USENIX Association. Carlini et al. (2021) ↑ Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models . In 30th USENIX Security Symposium (USENIX Security 21) , pages 2633–2650. USENIX Association. Chang and Bergen (2024) ↑ Tyler A. Chang and Benjamin K. Bergen. 2024. Language model behavior: A comprehensive survey . Computational Linguistics , pages 1–58. Cohen (1992) ↑ Jacob Cohen. 1992. Statistical power analysis . Current Directions in Psychological Science , 1(3):98–101. Cole and Frangakis (2009) ↑ Stephen R. Cole and Constantine E. Frangakis. 2009. The consistency statement in causal inference: A definition or an assumption? Epidemiology , 20(1). Cook and Weisberg (1980) ↑ R. Dennis Cook and Sanford Weisberg. 1980. Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression . Technometrics , 22(4):495–508. Dey et al. (2023) ↑ Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open compute-optimal language models trained on the cerebras wafer-scale cluster . arXiv preprint 2304.03208 . Duchi et al. (2011) ↑ John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization . Journal of Machine Learning Research , 12(61):2121–2159. Feldman (2020) ↑ Vitaly Feldman. 2020. Does learning require memorization? A short tale about a long tail . In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , STOC 2020, pages 954–959, New York, NY, USA. Association for Computing Machinery. Feldman and Zhang (2020) ↑ Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation . In Advances in Neural Information Processing Systems , volume 33, pages 2881–2891. Curran Associates, Inc. Gao et al. (2020) ↑ Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling . arXiv preprint 2101.00027 . Godey et al. (2024) ↑ Nathan Godey, Éric de la Clergerie, and Benoît Sagot. 2024. Why do small language models underperform? Studying language model saturation via the softmax bottleneck . arXiv preprint 2404.07647 . Hampel (1974) ↑ Frank R. Hampel. 1974. The influence curve and its role in robust estimation . Journal of the American Statistical Association , 69(346):383–393. Hartmann et al. (2023) ↑ Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, and Robert West. 2023. SoK: Memorization in general-purpose large language models . arXiv preprint 2310.18362 . Holland (1986) ↑ Paul W. Holland. 1986. Statistics and causal inference . Journal of the American Statistical Association , 81(396):945–960. Hu et al. (2022) ↑ Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S. Yu, and Xuyun Zhang. 2022. Membership inference attacks on machine learning: A survey . ACM Comput. Surv. , 54(11s). Ibeling and Icard (2023) ↑ Duligur Ibeling and Thomas Icard. 2023. Comparing causal frameworks: Potential outcomes, structural models, graphs, and abstractions . Advances in Neural Information Processing Systems , 36:80130–80141. Ilyas et al. (2022) ↑ Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022. Datamodels: Understanding predictions with data and data with predictions . In Proceedings of the 39th International Conference on Machine Learning , pages 9525–9587. PMLR. Imbens and Rubin (2015) ↑ Guido W. Imbens and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction . Cambridge University Press. Ishihara (2023) ↑ Shotaro Ishihara. 2023. Training data extraction from pre-trained language models: A survey . In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023) , pages 260–275, Toronto, Canada. Association for Computational Linguistics. Jiang et al. (2023) ↑ Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B . arXiv preprint 2310.06825 . Kandpal et al. (2022) ↑ Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates privacy risks in language models . In International Conference on Machine Learning , pages 10697–10707. PMLR. Kiela et al. (2021) ↑ Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4110–4124, Online. Association for Computational Linguistics. Kingma and Ba (2015) ↑ Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization . In 3rd International Conference on Learning Representations . Koh and Liang (2017) ↑ Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions . In Proceedings of the 34th International Conference on Machine Learning , pages 1885–1894. PMLR. Le Scao et al. (2022) ↑ Teven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. 2022. What language model to train if you have one million GPU hours? In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 765–782, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Lee et al. (2023) ↑ Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. 2023. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023 , WWW ’23, page 3637–3647, New York, NY, USA. Association for Computing Machinery. Lee et al. (2022) ↑ Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8424–8445, Dublin, Ireland. Association for Computational Linguistics. Lin et al. (2022) ↑ Jinkun Lin, Anqi Zhang, Mathias Lécuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. 2022. Measuring the effect of training data on deep learning predictions via randomized experiments . In Proceedings of the 39th International Conference on Machine Learning , pages 13468–13504. PMLR. Lu et al. (2018) ↑ Min Lu, Saad Sadiq, Daniel J Feaster, and Hemant Ishwaran. 2018. Estimating individual treatment effect in observational data using random forest methods . Journal of Computational and Graphical Statistics , 27(1):209–219. Lukasik et al. (2024) ↑ Michal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. 2024. What do larger image classifiers memorise? Transactions on Machine Learning Research . Maini et al. (2023) ↑ Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, and Chiyuan Zhang. 2023. Can neural network memorization be localized? In Proceedings of the 40th International Conference on Machine Learning , ICML’23. JMLR.org. McCloskey (2016) ↑ Donald N. McCloskey. 2016. Counterfactuals , pages 1–5. Palgrave Macmillan UK, London. Park et al. (2023) ↑ Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. TRAK: Attributing model behavior at scale . In Proceedings of the 40th International Conference on Machine Learning , pages 27074–27113. PMLR. Paszke et al. (2019) ↑ Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library . In Advances in Neural Information Processing Systems , volume 32. Curran Associates, Inc. Pearl (2009) ↑ Judea Pearl. 2009. Causality: Models, Reasoning and Inference , 2nd edition. Cambridge University Press, USA. Penedo et al. (2023) ↑ Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only . arXiv preprint 2306.01116 . Rae et al. (2021) ↑ Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher . arXiv preprint 2112.11446 . Raffel et al. (2020) ↑ Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer . Journal of Machine Learning Research , 21(140):1–67. Roth et al. (2023) ↑ Jonathan Roth, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. What’s trending in difference-in-differences? A synthesis of the recent econometrics literature . Journal of Econometrics , 235(2):2218–2244. Rubin (1974) ↑ Donald B. Rubin. 1974. Estimating causal effects of treatments in randomized and nonrandomized studies . Journal of Educational Psychology , 66(5):688–701. Rubin (2005) ↑ Donald B. Rubin. 2005. Causal inference using potential outcomes . Journal of the American Statistical Association , 100(469):322–331. Schioppa et al. (2023) ↑ Andrea Schioppa, Katja Filippova, Ivan Titov, and Polina Zablotskaia. 2023. Theoretical and practical perspectives on what influence functions do . Advances in Neural Information Processing Systems , 36:27560–27581. Splawa-Neyman (1923) ↑ Jerzy Splawa-Neyman. 1923. On the application of probability theory to agricultural experiments. Essay on principles. Annals of Agricultural Sciences , pages 1–51. Stoehr et al. (2024) ↑ Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. 2024. Localizing paragraph memorization in language models . arxiv preprint 2403.19851 . Tirumala et al. (2022) ↑ Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization without overfitting: Analyzing the training dynamics of large language models . In Advances in Neural Information Processing Systems . Touvron et al. (2023) ↑ Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models . arXiv preprint 2307.09288 . Vyas et al. (2023) ↑ Nikhil Vyas, Sham M. Kakade, and Boaz Barak. 2023. On provable copyright protection for generative models . In International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 35277–35299. PMLR. Wolf et al. (2020) ↑ Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online. Association for Computational Linguistics. Zhang et al. (2023) ↑ Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. 2023. Counterfactual memorization in neural language models . In Thirty-seventh Conference on Neural Information Processing Systems . Zheng and Jiang (2022) ↑ Xiaosen Zheng and Jing Jiang. 2022. An empirical study of memorization in NLP . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6265–6278, Dublin, Ireland. Association for Computational Linguistics. Appendix A Proofs Report issue for preceding element A.1 Difference Estimand and Estimator Report issue for preceding element Lemma 1 (Identification of the Difference Estimand) . Report issue for preceding element The difference estimand, defined in eq. 6 , identifies expected counterfactual memorisation (i.e., the causal estimand in eq. 3 ) under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element For this proof, we start with the definition of the difference estimand and, via algebraic manipulation, show its equality to expected counterfactual memorisation: Report issue for preceding element τ g , c 𝚍𝚒𝚏𝚏 superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (16a) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] absent subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] By Assump. 1 (16b) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) − Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] absent subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 𝑔 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] By linearity of expectations (16c) = τ g , c absent subscript 𝜏 𝑔 𝑐 \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}} = italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT (16d) This completes the proof.
∎ Report issue for preceding element Lemma 2 (Unbiasedness of the Difference Estimator) . Report issue for preceding element The difference estimator, defined in Estimator 1 , is an unbiased estimator of expected counterfactual memorisation τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element To prove this estimator is unbiased, let us first define the probability of sampling a batch ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT : Report issue for preceding element p ⁢ ( ℬ g ) = ∏ 𝒙 ∈ ℬ g p ⁢ ( 𝒙 ∣ G ⁢ ( 𝒙 ) = g ) 𝑝 subscript ℬ 𝑔 subscript product 𝒙 subscript ℬ 𝑔 𝑝 conditional 𝒙 𝐺 𝒙 𝑔 \displaystyle p(\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})=\prod_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}p({\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) italic_p ( caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) = ∏ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x ∣ italic_G ( bold_italic_x ) = italic_g ) (17) Taking the expectation of our estimator with respect to the batches used for its estimation we see that: Report issue for preceding element 𝔼 ℬ g , ℬ ∞ [ τ ^ g , c 𝚍𝚒𝚏𝚏 ] subscript 𝔼 subscript ℬ 𝑔 subscript ℬ superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \displaystyle\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\widehat{%
\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^%
{\mathtt{diff}}\right] blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT ] = 𝔼 ℬ g , ℬ ∞ [ Y ¯ c ⁢ ( g ) − Y ¯ c ⁢ ( ∞ ) ] absent subscript 𝔼 subscript ℬ 𝑔 subscript ℬ subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑐 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\overline%
{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25%
}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-%
\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) ] (18a) = 𝔼 ℬ g , ℬ ∞ [ 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g Y c ⁢ ( 𝒙 ) − 1 | ℬ ∞ | ⁢ ∑ 𝒙 ∈ ℬ ∞ Y c ⁢ ( 𝒙 ) ] absent subscript 𝔼 subscript ℬ 𝑔 subscript ℬ 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 1 subscript ℬ subscript 𝒙 subscript ℬ subscript 𝑌 𝑐 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{%
|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in%
\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] (18b) = 𝔼 ℬ g [ 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g Y c ⁢ ( 𝒙 ) ] − 𝔼 ℬ ∞ [ 1 | ℬ ∞ | ⁢ ∑ 𝒙 ∈ ℬ ∞ Y c ⁢ ( 𝒙 ) ] absent subscript 𝔼 subscript ℬ 𝑔 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 subscript 𝔼 subscript ℬ 1 subscript ℬ subscript 𝒙 subscript ℬ subscript 𝑌 𝑐 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\left[\frac{1}{|\mathcal{B}%
_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_%
{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right]-\operatorname*{%
\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{|\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] - blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] (18c) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (18d) = τ g , c 𝚍𝚒𝚏𝚏 absent superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}^{\mathtt{diff}} = italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT (18e) where eq. 18c follows because the sampling of ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and ℬ ∞ subscript ℬ \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty} caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT are independent; eq. 18d holds due to eq. 17 and the unbiasedness of Monte Carlo estimators.
We can now invoke Lemma 1 , which states that τ g , c 𝚍𝚒𝚏𝚏 superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{%
\mathtt{diff}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT identifies τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under the i.i.d. assumption ( Assump. 1 ).
Thus, we have that the expected value of our estimator is equal to τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT , finishing the proof.
∎ Report issue for preceding element A.2 Difference-in-Differences Estimand and Estimator Report issue for preceding element Lemma 3 (Identification of the Difference-in-Differences Estimand) . Report issue for preceding element The DiD estimand, defined in eq. 10 , identifies expected counterfactual memorisation (i.e., the causal estimand in eq. 3 ) under Assumps. 2 and 3 for all c ≥ g 𝑐 𝑔 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\geq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c ≥ italic_g . Report issue for preceding element Proof. Report issue for preceding element To prove this lemma, we first note that by the no anticipations assumption ( Assump. 3 ): Report issue for preceding element 𝔼 𝒙 [ Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] = 𝔼 𝒙 [ Y g − 1 ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] subscript 𝔼 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝑔 𝐺 𝒙 𝑔 \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\left[Y_{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\right]=\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\left[Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\right] blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] (19) Furthermore, by the parallel trends assumption ( Assump. 2 ) and linearity of expectations: Report issue for preceding element 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y c ′ ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y c ′ ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 superscript 𝑐 ′ 𝒙 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 superscript 𝑐 ′ 𝒙 𝐺 𝒙 \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]}=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (20) ⟹ 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ] = 𝔼 𝒙 [ Y c ′ ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = g ] − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y c ′ ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ] absent subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 ∣ 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 subscript 𝑌 superscript 𝑐 ′ 𝒙 ∣ 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 subscript 𝑌 superscript 𝑐 ′ 𝒙 ∣ 𝐺 𝒙 \displaystyle\,\,\implies\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}=\operatorname%
*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{%
\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{\mathbb{E}}_{{\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} ⟹ blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] As in Lemma 1 , we now start with the definition of the DiD estimand and, via algebraic manipulation, show its equivalence to expected counterfactual memorisation: Report issue for preceding element τ g , c 𝚍𝚒𝚍 superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚍 \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = 𝔼 [ Y c ⁢ ( 𝒙 ; g ) − Y g − 1 ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent 𝔼 subscript 𝑌 𝑐 𝒙 𝑔 conditional subscript 𝑌 𝑔 1 𝒙 𝑔 𝐺 𝒙 𝑔 𝔼 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor%
}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (21a) = 𝔼 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 [ Y g − 1 ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] ⏟ no anticipation − 𝔼 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent 𝔼 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript ⏟ 𝔼 conditional subscript 𝑌 𝑔 1 𝒙 𝑔 𝐺 𝒙 𝑔 no anticipation 𝔼 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\underbrace{\operatorname*{\mathbb{E}}%
\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}}_{\text{{\color[rgb]%
{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}%
\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5} no anticipation}}}-%
\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]%
{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - under⏟ start_ARG blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT no anticipation end_POSTSUBSCRIPT - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (21b) = 𝔼 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 [ Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y g − 1 ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] ⏟ parallel trends absent 𝔼 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript ⏟ 𝔼 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 𝑔 𝔼 subscript 𝑌 𝑐 𝒙 conditional subscript 𝑌 𝑔 1 𝒙 𝐺 𝒙 parallel trends \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\underbrace{\operatorname*{\mathbb{E}}%
\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor%
}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]}}_{\text{{\color%
[rgb]{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}%
\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5} parallel trends}}} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - under⏟ start_ARG blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] end_ARG start_POSTSUBSCRIPT parallel trends end_POSTSUBSCRIPT (21c) = 𝔼 [ Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 [ Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] absent 𝔼 conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 𝔼 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]}-\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] (21d) = 𝔼 [ Y c ⁢ ( 𝒙 ; g ) − Y c ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = g ] absent 𝔼 subscript 𝑌 𝑐 𝒙 𝑔 conditional subscript 𝑌 𝑐 𝒙 𝐺 𝒙 𝑔 \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = italic_g ] (21e) = τ g , c absent subscript 𝜏 𝑔 𝑐 \displaystyle\quad=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}} = italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT (21f) where we replace the terms in eqs. 21b and 21c using their equivalences given in eq. 19 and eq. 20 , respectively.
This completes the proof.
We note that a similar proof is available in Lemma A.1 in Callaway and Sant’Anna ( 2021 ) .
∎ Report issue for preceding element Lemma 4 (Unbiasedness of the Difference-in-Differences Estimator) . Report issue for preceding element The difference-in-differences estimator, defined in Estimator 2 , is an unbiased estimator of expected counterfactual memorisation τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 . Report issue for preceding element Proof. Report issue for preceding element We can follow the same logic as in Lemma 2 because the same properties hold (i.e., the sampling of ℬ g subscript ℬ 𝑔 \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and ℬ ∞ subscript ℬ \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty} caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT are independent,
the joint probability of a set is the product of the probability of sampling individual instances, and the unbiasedness of Monte Carlo estimators). We then arrive at the following equivalence: Report issue for preceding element 𝔼 ℬ g , ℬ ∞ [ τ ^ g , c 𝚍𝚒𝚍 ] subscript 𝔼 subscript ℬ 𝑔 subscript ℬ superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚍 \displaystyle\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\!\!\!\!\left[%
\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}}\right] blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ] = 𝔼 ℬ g , ℬ ∞ [ Y ¯ c ⁢ ( g ) − Y ¯ g − 1 ⁢ ( g ) − Y ¯ c ⁢ ( ∞ ) + Y ¯ g − 1 ⁢ ( ∞ ) ] absent subscript 𝔼 subscript ℬ 𝑔 subscript ℬ subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑔 1 𝑔 subscript ¯ 𝑌 𝑐 subscript ¯ 𝑌 𝑔 1 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\bigg{[}%
\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g})-\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})+\overline{Y}_{{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\bigg{]} = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) + over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( ∞ ) ] (22a) = 𝔼 ℬ g , ℬ ∞ [ 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g ( Y c ⁢ ( 𝒙 ) − Y g − 1 ⁢ ( 𝒙 ) ) − 1 | ℬ ∞ | ⁢ ∑ 𝒙 ∈ ℬ ∞ ( Y c ⁢ ( 𝒙 ) − Y g − 1 ⁢ ( 𝒙 ) ) ] absent subscript 𝔼 subscript ℬ 𝑔 subscript ℬ 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 subscript 𝑌 𝑔 1 𝒙 1 subscript ℬ subscript 𝒙 subscript ℬ subscript 𝑌 𝑐 𝒙 subscript 𝑌 𝑔 1 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{%
|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\Big{(}Y_{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\Big{)}-\frac{1}{|\mathcal%
{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty%
}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\Big{(}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\Big{)}\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) - divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] (22b) = 𝔼 ℬ g [ 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g ( Y c ⁢ ( 𝒙 ) − Y g − 1 ⁢ ( 𝒙 ) ) ] − 𝔼 ℬ ∞ [ 1 | ℬ ∞ | ⁢ ∑ 𝒙 ∈ ℬ ∞ ( Y c ⁢ ( 𝒙 ) − Y g − 1 ⁢ ( 𝒙 ) ) ] absent subscript 𝔼 subscript ℬ 𝑔 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 subscript 𝑌 𝑔 1 𝒙 subscript 𝔼 subscript ℬ 1 subscript ℬ subscript 𝒙 subscript ℬ subscript 𝑌 𝑐 𝒙 subscript 𝑌 𝑔 1 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\left[\frac{1}{|\mathcal{B}%
_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_%
{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\Big{)}\right]-\operatorname*{\mathbb{E}}_{\mathcal{B}_{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}%
\left[\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\Big{(}Y_%
{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\Big{)}\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] - blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] (22c) = 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; g ) − Y g − 1 ⁢ ( 𝒙 ; g ) ⁢ ∣ G ⁢ ( 𝒙 ) = g ] − 𝔼 𝒙 [ Y c ⁢ ( 𝒙 ; ∞ ) − Y g − 1 ⁢ ( 𝒙 ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 𝑔 subscript 𝑌 𝑔 1 𝒙 𝑔 ∣ 𝐺 𝒙 𝑔 subscript 𝔼 𝒙 subscript 𝑌 𝑐 𝒙 subscript 𝑌 𝑔 1 𝒙 ∣ 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-%
\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (22d) = τ g , c 𝚍𝚒𝚍 absent superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚍 \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}^{\mathtt{did}} = italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT (22e) Finally, we invoke Lemma 3 which proves that τ g , c 𝚍𝚒𝚍 superscript subscript 𝜏 𝑔 𝑐 𝚍𝚒𝚍 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{%
\mathtt{did}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT identifies τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 .
∎ Report issue for preceding element A.3 Variances of Estimators Report issue for preceding element We assume that all potential outcomes Y c ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑐 𝒙 𝑔 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) have the same variance σ 2 superscript 𝜎 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .
We now first look at the variance of the difference estimator.
To this end, let’s consider the variance of Y ¯ c ⁢ ( g ) subscript ¯ 𝑌 𝑐 𝑔 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) : Report issue for preceding element Var ⁢ ( Y ¯ c ⁢ ( g ) ) Var subscript ¯ 𝑌 𝑐 𝑔 \displaystyle\mathrm{Var}\Big{(}\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)} roman_Var ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) ) = Var ⁢ ( 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g Y c ⁢ ( 𝒙 ) ) = 1 | ℬ g | 2 ⁢ ∑ 𝒙 ∈ ℬ g Var ⁢ ( Y c ⁢ ( 𝒙 ; g ) ∣ G ⁢ ( 𝒙 ) = g ) = | ℬ g | ⁢ σ 2 | ℬ g | 2 = σ 2 | ℬ g | absent Var 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 1 superscript subscript ℬ 𝑔 2 subscript 𝒙 subscript ℬ 𝑔 Var conditional subscript 𝑌 𝑐 𝒙 𝑔 𝐺 𝒙 𝑔 subscript ℬ 𝑔 superscript 𝜎 2 superscript subscript ℬ 𝑔 2 superscript 𝜎 2 subscript ℬ 𝑔 \displaystyle=\mathrm{Var}\left(\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right)=\frac{1}{|\mathcal%
{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|^%
{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\mathrm{Var}\Big{(}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\Big{)}=\frac{|\mathcal{B}_{%
{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|\,%
\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}=\frac{\sigma^{2}}{|\mathcal{B}_{{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|} = roman_Var ( divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ) = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Var ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ∣ italic_G ( bold_italic_x ) = italic_g ) = divide start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = divide start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG (23) This is simply the variance of estimating an expectation using the mean of | ℬ g | subscript ℬ 𝑔 |\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}| | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | i.i.d. random variables, each with variance σ 2 superscript 𝜎 2 \sigma^{2} italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We can similarly derive the variance of Y ¯ c ⁢ ( ∞ ) subscript ¯ 𝑌 𝑐 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) .
The variance of τ ^ g , c 𝚍𝚒𝚏𝚏 superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT is then: Report issue for preceding element Var ⁢ ( τ ^ g , c 𝚍𝚒𝚏𝚏 ) = σ 2 | ℬ g | + σ 2 | ℬ ∞ | − 2 ⁢ Cov ⁢ ( Y ¯ c ⁢ ( g ) , Y ¯ c ⁢ ( ∞ ) ) Var superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 superscript 𝜎 2 subscript ℬ 𝑔 superscript 𝜎 2 subscript ℬ 2 Cov subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑐 \mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{diff}})=\frac{\sigma^{2}}{|%
\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}|}+\frac{\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}-2\,\mathrm{Cov}(\overline{Y}_{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}),%
\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty})) roman_Var ( over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT ) = divide start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG + divide start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG - 2 roman_Cov ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) , over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) ) (24) Assuming batches ℬ g subscript ℬ 𝑔 \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and ℬ ∞ subscript ℬ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_B start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT were drawn independently,
then the estimators Y ¯ c ⁢ ( g ) subscript ¯ 𝑌 𝑐 𝑔 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y ¯ c ⁢ ( ∞ ) subscript ¯ 𝑌 𝑐 \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) should also be independent.
Thus, Cov ⁢ ( Y ¯ c ⁢ ( g ) , Y ¯ c ⁢ ( ∞ ) ) = 0 Cov subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑐 0 \mathrm{Cov}(\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}),\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}))=0 roman_Cov ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) , over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) ) = 0 . Report issue for preceding element We now look at the variance of the difference-in-differences estimator.
Let the correlation between Y c ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑐 𝒙 𝑔 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) and Y g − 1 ⁢ ( 𝒙 ; g ) subscript 𝑌 𝑔 1 𝒙 𝑔 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) be ρ 𝜌 \rho italic_ρ .
These are, respectively, the potential outcomes of our model on a specific instance 𝒙 𝒙 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x before and after training on it.
For shorthand, let Δ ⁢ Y ¯ g = Y ¯ c ⁢ ( g ) − Y ¯ g − 1 ⁢ ( g ) Δ subscript ¯ 𝑌 𝑔 subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑔 1 𝑔 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g}=\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) and Δ ⁢ Y ¯ ∞ = Y ¯ c ⁢ ( ∞ ) − Y ¯ g − 1 ⁢ ( ∞ ) Δ subscript ¯ 𝑌 subscript ¯ 𝑌 𝑐 subscript ¯ 𝑌 𝑔 1 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty}=\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})-\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT = over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ∞ ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( ∞ ) .
We can show that: Report issue for preceding element Var ⁢ ( Δ ⁢ Y ¯ g ) Var Δ subscript ¯ 𝑌 𝑔 \displaystyle\mathrm{Var}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}) roman_Var ( roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) = Var ⁢ ( Y ¯ c ⁢ ( g ) − Y ¯ g − 1 ⁢ ( g ) ) absent Var subscript ¯ 𝑌 𝑐 𝑔 subscript ¯ 𝑌 𝑔 1 𝑔 \displaystyle=\mathrm{Var}\Big{(}\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)} = roman_Var ( over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) ) (25a) = Var ⁢ ( 1 | ℬ g | ⁢ ∑ 𝒙 ∈ ℬ g ( Y c ⁢ ( 𝒙 ; g ) − Y g − 1 ⁢ ( 𝒙 ; g ) ) ) absent Var 1 subscript ℬ 𝑔 subscript 𝒙 subscript ℬ 𝑔 subscript 𝑌 𝑐 𝒙 𝑔 subscript 𝑌 𝑔 1 𝒙 𝑔 \displaystyle=\mathrm{Var}\left(\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}{\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}}\right) = roman_Var ( divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ) ) (25b) = 1 | ℬ g | 2 ⁢ ∑ 𝒙 ∈ ℬ g ( σ 2 + σ 2 − 2 ⁢ Cov ⁢ ( Y c ⁢ ( 𝒙 ; g ) , Y g − 1 ⁢ ( 𝒙 ; g ) ) ) absent 1 superscript subscript ℬ 𝑔 2 subscript 𝒙 subscript ℬ 𝑔 superscript 𝜎 2 superscript 𝜎 2 2 Cov subscript 𝑌 𝑐 𝒙 𝑔 subscript 𝑌 𝑔 1 𝒙 𝑔 \displaystyle=\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\bigg{(}\sigma%
^{2}+\sigma^{2}-2\,\mathrm{Cov}\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}),Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}\bigg{)} = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 2 roman_Cov ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ) ) (25c) = 1 | ℬ g | 2 ⁢ ∑ 𝒙 ∈ ℬ g ( 2 ⁢ σ 2 − 2 ⁢ ρ ⁢ σ 2 ) = 2 ⁢ σ 2 | ℬ g | ⁢ ( 1 − ρ ) absent 1 superscript subscript ℬ 𝑔 2 subscript 𝒙 subscript ℬ 𝑔 2 superscript 𝜎 2 2 𝜌 superscript 𝜎 2 2 superscript 𝜎 2 subscript ℬ 𝑔 1 𝜌 \displaystyle=\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}{(2\sigma^{2}-%
2\rho\sigma^{2})}=\frac{2\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}(1-\rho) = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT bold_italic_x ∈ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 2 italic_ρ italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = divide start_ARG 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_ρ ) (25d) We can derive the variance for Δ ⁢ Y ¯ ∞ Δ subscript ¯ 𝑌 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty} roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT in the exact same manner.
We thus have that: Report issue for preceding element Var ⁢ ( τ ^ g , c 𝚍𝚒𝚍 ) = Var ⁢ ( Δ ⁢ Y ¯ g ) + Var ⁢ ( Δ ⁢ Y ¯ ∞ ) − 2 ⁢ C ⁢ o ⁢ v ⁢ ( Δ ⁢ Y ¯ g , Δ ⁢ Y ¯ ∞ ) Var superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚍 Var Δ subscript ¯ 𝑌 𝑔 Var Δ subscript ¯ 𝑌 2 C o v Δ subscript ¯ 𝑌 𝑔 Δ subscript ¯ 𝑌 \displaystyle\mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}})=\mathrm{Var}(\Delta%
\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g})+\mathrm{Var}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-2\mathrm{Cov}(\Delta\overline{Y}_{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\Delta%
\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) roman_Var ( over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ) = roman_Var ( roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) + roman_Var ( roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ) - 2 roman_C roman_o roman_v ( roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ) (26) Note that Δ ⁢ Y ¯ g Δ subscript ¯ 𝑌 𝑔 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g} roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Δ ⁢ Y ¯ ∞ Δ subscript ¯ 𝑌 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty} roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT are estimated with independent samples, and thus, Cov ⁢ ( Δ ⁢ Y ¯ g , Δ ⁢ Y ¯ ∞ ) = 0 Cov Δ subscript ¯ 𝑌 𝑔 Δ subscript ¯ 𝑌 0 \mathrm{Cov}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},\Delta\overline{Y}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=0 roman_Cov ( roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Δ over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ) = 0 .
We can thus rewrite this estimator’s variance as: Report issue for preceding element Var ⁢ ( τ ^ g , c 𝚍𝚒𝚍 ) = 2 ⁢ σ 2 | ℬ g | ⁢ ( 1 − ρ g ) + 2 ⁢ σ 2 | ℬ g | ⁢ ( 1 − ρ ∞ ) Var superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚍 2 superscript 𝜎 2 subscript ℬ 𝑔 1 subscript 𝜌 𝑔 2 superscript 𝜎 2 subscript ℬ 𝑔 1 subscript 𝜌 \displaystyle\mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}})=\frac{2\sigma^{2}}{|%
\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}|}(1-\rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})+\frac{2\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}(1-\rho_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) roman_Var ( over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ) = divide start_ARG 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_ρ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) + divide start_ARG 2 italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_ρ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ) (27) If we have ρ g > 0.5 subscript 𝜌 𝑔 0.5 \rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>0.5 italic_ρ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT > 0.5 and ρ ∞ > 0.5 subscript 𝜌 0.5 \rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}>0.5 italic_ρ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT > 0.5 , then the variance of τ ^ g , c 𝚍𝚒𝚍 superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚍 \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}} over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT should be lower than that of the τ ^ g , c 𝚍𝚒𝚏𝚏 superscript subscript ^ 𝜏 𝑔 𝑐 𝚍𝚒𝚏𝚏 \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT .
This is a reasonable assumption since—for fixed timesteps g ⁢ - 1 𝑔 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{-%
}1} italic_g - 1 and c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c —there should be a strong relationship between a model’s performance on an instance before ( g ⁢ - 1 𝑔 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{-%
}1} italic_g - 1 ) and after ( c 𝑐 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c ) it has been trained on due to factors such as vocabulary richness and grammatical structure. Report issue for preceding element Appendix B Statistical Estimands and Estimators in Prior Work Report issue for preceding element In this section, we formalise prior works’ estimators of memorisation using our formalisation of counterfactual memorisation. Report issue for preceding element B.1 Architectural Counterfactual Memorisation Report issue for preceding element In this section, we describe one potential estimator for architectural counterfactual memorisation τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT (in Defn. 4 ).
First, we need the following assumption in order to identify the causal estimand for this quantity. Report issue for preceding element Assumption 4 (Negligible training effect) . Report issue for preceding element In expectation, the effect of having a specific instance in the training set is negligible on any validation instance. That is, for any two instances 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and 𝐱 ′ superscript 𝐱 ′ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime} bold_italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT : Report issue for preceding element 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ′ ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ = ∞ ] = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ′ ; ∞ ) ⁢ ∣ G ⁢ ( 𝒙 ) ⁢ ≠ ∞ ] subscript 𝔼 𝝍 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 superscript 𝒙 ′ ∣ 𝐺 𝒙 subscript 𝔼 𝝍 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 superscript 𝒙 ′ ∣ 𝐺 𝒙 \displaystyle\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{%
=}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}%
\right]=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{%
\neq}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}\right] blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ; ∞ ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] (28) Given this assumption, we can identify the following statistical estimand for τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT : Report issue for preceding element τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 superscript subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] − 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 𝐺 𝒙 subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (29) We now define the architectural estimator, associated with this statistical estimand. Report issue for preceding element Estimator 3 . Report issue for preceding element The architectural estimator , defined as: 20 20 20 We note that prior work has proposed more efficient estimators of the above ( Bachmann et al. , 2022 ; Lin et al. , 2022 ; Ilyas et al. , 2022 ; Park et al. , 2023 ) . However, these estimators remain computationally expensive for large LMs. Report issue for preceding element τ ^ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 = 1 | Θ g | ⁢ ∑ 𝜽 ∈ Θ g Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) − 1 | Θ ∞ | ⁢ ∑ 𝜽 ∈ Θ ∞ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) superscript subscript ^ 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 1 subscript Θ 𝑔 subscript 𝜽 subscript Θ 𝑔 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 1 subscript Θ subscript 𝜽 subscript Θ subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}}=\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}}Y_{\scaleto{{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]%
{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_{\scaleto{%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{%
4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (30) is an unbiased estimator of τ 𝐱 , \scaleto ⁢ p ⁢ ( 𝛙 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝐱 \scaleto 𝑝 𝛙 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT under Assump. 4 .
In this equation, Θ g subscript Θ 𝑔 \Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}} roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Θ ∞ subscript Θ \Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}} roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT are sets of model parameters trained independently with or without 𝐱 𝐱 {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in the training set. Report issue for preceding element Proof. Report issue for preceding element First, we prove that the statistical estimand τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 superscript subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}}^{\mathtt{arch}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT identifies τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT : Report issue for preceding element τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 superscript subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] − 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 𝐺 𝒙 subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (31a) = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] − 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] absent subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 𝐺 𝒙 subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] By Assump. 4 (31b) = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) − Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] absent subscript 𝔼 𝝍 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))-Y_{\scaleto{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] Linearity of expectations (31c) = τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t absent subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 \displaystyle=\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} = italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT (31d) We now prove the estimator above is unbiased: Report issue for preceding element τ ^ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 superscript subscript ^ 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = 𝔼 Θ g , Θ ∞ [ 1 | Θ g | ⁢ ∑ 𝜽 ∈ Θ g Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) − 1 | Θ ∞ | ⁢ ∑ 𝜽 ∈ Θ ∞ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) ] absent subscript 𝔼 subscript Θ 𝑔 subscript Θ 1 subscript Θ 𝑔 subscript 𝜽 subscript Θ 𝑔 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 1 subscript Θ subscript 𝜽 subscript Θ subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}},\Theta_{{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}\left[\frac{1}%
{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}}Y_{\scaleto{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]%
{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_{\scaleto{%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{%
4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] (32a) = 𝔼 Θ g [ 1 | Θ g | ⁢ ∑ 𝜽 ∈ Θ g Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) ] − 𝔼 Θ ∞ [ 1 | Θ ∞ | ⁢ ∑ 𝜽 ∈ Θ ∞ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) ] absent subscript 𝔼 subscript Θ 𝑔 1 subscript Θ 𝑔 subscript 𝜽 subscript Θ 𝑔 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 subscript 𝔼 subscript Θ 1 subscript Θ subscript 𝜽 subscript Θ subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}}\left[\frac{1}{|\Theta_{{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{%
\bm{\theta}\in\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}}}Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right]-\operatorname%
*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{1,.5,0}\infty}}}\left[\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_%
{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] - blackboard_E start_POSTSUBSCRIPT roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT bold_italic_θ ∈ roman_Θ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] (32b) = 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; G ⁢ ( 𝒙 ) ) ∣ G ⁢ ( 𝒙 ) ≠ ∞ ] − 𝔼 𝝍 [ Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) ∣ G ⁢ ( 𝒙 ) = ∞ ] absent subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 𝐺 𝒙 subscript 𝔼 𝝍 conditional subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝐺 𝒙 \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) ∣ italic_G ( bold_italic_x ) ≠ ∞ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_ψ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) ∣ italic_G ( bold_italic_x ) = ∞ ] (32c) = τ 𝒙 , \scaleto ⁢ p ⁢ ( 𝝍 ) ⁢ 6 ⁢ p ⁢ t 𝚊𝚛𝚌𝚑 absent superscript subscript 𝜏 𝒙 \scaleto 𝑝 𝝍 6 𝑝 𝑡 𝚊𝚛𝚌𝚑 \displaystyle=\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} = italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_ψ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT (32d) This completes the proof.
∎ Report issue for preceding element B.2 Influence Functions Report issue for preceding element As mentioned in § 5.2 , influence functions approximate 𝜽 − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 𝒙 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT using a first-order Taylor expansion of the training objective around 𝜽 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
This should lead to small errors under the following assumptions:
(i) the loss function is strictly convex in 𝜽 𝜽 \bm{\theta} bold_italic_θ , (ii) H 𝜽 subscript H 𝜽 \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (Koh and Liang, 2017 ) .
We make these assumptions explicit now. Report issue for preceding element Assumption 5 (Strict Convexity) . Report issue for preceding element The loss function ℒ ℒ \mathcal{L} caligraphic_L is strictly convex with respect to the parameters 𝛉 𝛉 \bm{\theta} bold_italic_θ . Report issue for preceding element Assumption 6 (Local Optimality) . Report issue for preceding element The parameters 𝛉 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝛉 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT locally minimise the loss function ℒ ℒ \mathcal{L} caligraphic_L , meaning that the H 𝛉 subscript H 𝛉 \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT is a positive-definite matrix and that gradient of the loss with respect to the parameters 𝛉 \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝛉 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT is zero. Report issue for preceding element Given these assumptions, we can estimate the counterfactual term in τ 𝒙 , c subscript 𝜏 𝒙 𝑐 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT (in eq. 2 ) by computing the performance using the updated parameters 𝜽 − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜽 𝒙 \scaleto 𝑇 4 𝑝 𝑡 \bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_θ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
As mentioned in the main text, we thus define Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) = γ ⁢ ( 𝜽 − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ; 𝒙 ) subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 𝛾 subscript 𝜽 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\gamma(\bm{\theta}_{%
-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}};{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) = italic_γ ( bold_italic_θ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ) and equate Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) = Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . Report issue for preceding element Estimator 4 . Report issue for preceding element The influence function estimator , defined as: Report issue for preceding element τ ^ 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t 𝚒𝚗𝚏𝚕 = Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) − Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) superscript subscript ^ 𝜏 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝚒𝚗𝚏𝚕 subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 \displaystyle\hat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{%
\mathtt{infl}}=Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (33) is an unbiased estimator of τ 𝐱 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t subscript 𝜏 𝐱 \scaleto 𝑇 4 𝑝 𝑡 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT under Assumps. 5 and 6 . Report issue for preceding element Proof. Report issue for preceding element See Cook and Weisberg ( 1980 ) or Koh and Liang ( 2017 ) for derivations of how Y − 𝒙 , \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) subscript 𝑌 𝒙 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) approximates the counterfactual Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) under the assumptions above. The estimator then follows trivially from replacing Y \scaleto ⁢ T ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 \scaleto 𝑇 4 𝑝 𝑡 𝒙 Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) in eq. 2 .
∎ Report issue for preceding element B.3 Extractable Memorisation Report issue for preceding element As mentioned in § 5.3 , extractable memorisation assumes zero-valued counterfactual performances Y c ⁢ ( 𝒙 ; ∞ ) = 0 subscript 𝑌 𝑐 𝒙 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})=0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = 0 .
We formalise this assumption, and the associated statistical estimand and estimator in this section. Report issue for preceding element Assumption 7 (Negligible counterfactual) . Report issue for preceding element In the absence of training, performance on a string should be zero: Y c ⁢ ( 𝐱 ; ∞ ) = 0 subscript 𝑌 𝑐 𝐱 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})=0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) = 0 . Report issue for preceding element Given this assumption, we can trivially identify counterfactual memorisation as being equivalent to the statistical estimand: τ 𝒙 , c 𝚎𝚡𝚝𝚛 = Y c ⁢ ( 𝒙 ; g ) superscript subscript 𝜏 𝒙 𝑐 𝚎𝚡𝚝𝚛 subscript 𝑌 𝑐 𝒙 𝑔 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) .
We can now define the ( k , ℓ ) 𝑘 ℓ (k,\!\ell) ( italic_k , roman_ℓ ) -extractable memorisation estimator under our framework. Report issue for preceding element Estimator 5 . Report issue for preceding element The ( 𝐤 , ℓ ) 𝐤 bold-ℓ \bm{(k,\ell)} bold_( bold_italic_k bold_, bold_ℓ bold_) -extractable memorisation estimator, defined as: Report issue for preceding element τ ^ 𝒙 , c 𝚎𝚡𝚝𝚛 = Y c ⁢ ( 𝒙 ) superscript subscript ^ 𝜏 𝒙 𝑐 𝚎𝚡𝚝𝚛 subscript 𝑌 𝑐 𝒙 \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_τ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (34) is an unbiased estimator of τ 𝐱 , c subscript 𝜏 𝐱 𝑐 \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT under Assump. 7 . Report issue for preceding element Proof. Report issue for preceding element This follows trivially from replacing Y c ⁢ ( 𝒙 ; ∞ ) subscript 𝑌 𝑐 𝒙 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; ∞ ) with 0 in eq. 2 .
∎ Report issue for preceding element Appendix C Implementation Details Report issue for preceding element We implement all experiments using the PyTorch framework (Paszke et al., 2019 ) .
We use the Pythia models as available through the transformers library (Wolf et al., 2020 ) .
For a consistent evaluation between scales, we load every model using bfloat16 precision, which is needed for the larger versions. We control randomness using CUDA deterministic operations and seeding the pseudo-random number generators at every level of the stack and for each multi-processing worker.
We use the implementation of the Callaway and Sant’Anna ( 2021 ) estimator as available in the differences library 21 21 21 github.com/bernardodionisi/differences . . Report issue for preceding element C.1 The Pythia Suite Report issue for preceding element We use the publicly available Pythia model suite (Biderman et al., 2023b ) , which was trained on the Pile (Gao et al., 2020 ; Biderman et al., 2022 ) . Both the preprocessed training data and intermediate checkpoints are publicly available. 22 22 22 github.com/EleutherAI/pythia . Report issue for preceding element Data. Report issue for preceding element The Pile is a \qty [mode=math]300-token curated collection of English documents.
The deduplicated version of the dataset is obtained by applying a near-deduplication method based on MinHashLSH and has \qty [mode=math]207 tokens.
Before being used for training, the dataset is shuffled, tokenised, and “packed” into sequences of 2 , 049 2049 2,049 2 , 049 tokens with no end-of-document token. 23 23 23 github.com/EleutherAI/pythia/issues/123 . By design, each sequence can pack multiple documents and tokens can attend across document boundaries.
Noticeably, the packing process implies that the second half-epoch of deduplicated data contains the same documents but not necessarily the same sequences.
There does not exist an official validation set for Pythia models. However, we confirmed with the authors that the original Pile validation set has not been used for training. Report issue for preceding element Models. Report issue for preceding element The Pythia model suite is composed of 16 models: transformers of 8 8 8 8 different sizes trained on the Pile as-is or deduplicated.
All model sizes were trained using a cosine learning rate schedule with warm-up, the same data order, and a batch size of 1 , 024 1024 1,024 1 , 024 sequences, resulting in exactly \qty [mode=math]143 optimisation steps.
The final \qty [mode=math]48 optimisation steps correspond to the second half-epoch.
Thus, we focus on model checkpoints at initialisation (step 0 0 ), and after every \qty [mode=math]1 iterations (steps \qty [mode=math]1- \qty [mode=math]95) resulting in 96 96 96 96 checkpoints evenly spaced throughout training. For completeness, we report the second half-epoch (steps \qty [mode=math]96- \qty [mode=math]143) analysis in App. D . Additionally, log-spaced checkpoints are available for timesteps early in training (timesteps c ∈ { 2 i } i = 0 9 𝑐 superscript subscript superscript 2 𝑖 𝑖 0 9 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{2^{i}\}_{i=0}^{9} italic_c ∈ { 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT ).
We do not consider them to obtain evaluations at evenly spaced intervals.
We use all available model sizes, that is, \qty [mode=math]70, \qty [mode=math]160, \qty [mode=math]410, \qty [mode=math]1.4, \qty [mode=math]6.9, and \qty [mode=math]12, except \qty [mode=math]2.8. We exclude \qty [mode=math]2.8 from the results since we found a potential mismatch between the available checkpoints and the data order used during training. Report issue for preceding element C.2 Hardware Details Report issue for preceding element We use a server with one NVIDIA A100 80GB PCIe , 32 32 32 32 CPUs, and 32 32 32 32 GB of RAM for all experiments. Below, we report a subset of the output of the lscpu command: Report issue for preceding element Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Address sizes:       46 bits physical,
                     48 bits virtual
Byte Order:          Little Endian
CPU(s):              32
On-line CPU(s) list: 0-31
Vendor ID:           GenuineIntel
Model name:          Intel(R) Xeon(R)
                     Silver 4210R CPU
                     @ 2.40GHz
CPU family:          6
Model:               85
Thread(s) per core:  1
Core(s) per socket:  1
Socket(s):           8
Stepping:            7
BogoMIPS:            4800.11 Report issue for preceding element Appendix D Additional Results Report issue for preceding element On the next page in Fig. 7 , we report the memorisation profiles obtained using other metrics besides sequence-level log-likelihood. Specifically, the average token-level accuracy given the true context and the average rank assigned by the model to the correct next token given the true context.
We report the results for the entire training process—i.e., using all the available checkpoints: c ∈ { 0 , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 , … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 143 } 𝑐 0 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 143 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{$0$,\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_c ∈ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } and g ∈ { \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 , … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 143 } 𝑔 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 143 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_g ∈ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } .
We present the metrics from most coarse—i.e., average token accuracy ( Fig. 7a )—to most fine-grained—i.e., sequence log-likelihood ( Fig. 7c ). Report issue for preceding element As shown in Fig. 7 , different performance metrics result in distinct memorisation estimates. Specifically, finer-grained metrics—like sequence log-likelihood—allow us to detect smaller memorisation effects, and vice versa.
For example, average token accuracy, which is the most coarse-grained metric, mostly does not capture instantaneous memorisation for Pythia \qty [mode=math]410. Instead, a finer-grained metric—like average token rank or sequence log-likelihood—detects additional effects.
Depending on the use-case different metrics might be appropriate.
For example, analogously to extractable memorisation (Carlini et al., 2021 ) , average token accuracy could be used to measure memorisation as it matches the specific use-case: detecting whether a model would generate a specific sequence when prompted with some of its tokens.
We chose sequence log-likelihood because it allows us to capture more fine-grained memorisation effects beyond the capability of the model to generate a specific sequence.
In particular, accuracy captures “hard” transitions in the model’s output by determining whether a token is the most likely in the model’s output distribution.
Log-likelihood, on the other hand, captures more nuanced impacts of training on an instance by assessing whether a token is more likely to be generated than it would be otherwise. Report issue for preceding element (a) Average Token Accuracy: , γ ⁢ ( 𝜽 c , 𝒙 ) = 1 | 𝒙 | ⁢ ∑ i = 1 | 𝒙 | 𝟙 ⁢ ( x ^ i = x i ) 𝛾 subscript 𝜽 𝑐 𝒙 1 𝒙 superscript subscript 𝑖 1 𝒙 1 subscript ^ 𝑥 𝑖 subscript 𝑥 𝑖 \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\frac{1}{{|{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\sum_{i=1}^{{|{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}|}}\mathbbm{1}(\hat{x}_{i}=%
{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}) italic_γ ( bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG | bold_italic_x | end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT blackboard_1 ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , where x ^ i = argmax x ∈ 𝒱 p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( x ∣ 𝒙 < i ) subscript ^ 𝑥 𝑖 subscript argmax 𝑥 𝒱 subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 conditional 𝑥 subscript 𝒙 absent 𝑖 \hat{x}_{i}=\operatorname*{argmax}_{x\in\mathcal{V}}p_{\scaleto{\bm{\theta}_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}}{%
4pt}}(x\mid{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_argmax start_POSTSUBSCRIPT italic_x ∈ caligraphic_V end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x ∣ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) is the predicted token at position i 𝑖 i italic_i computed using the correct previous tokens as context and | 𝒙 | 𝒙 {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element (b) Average Rank of the True Token: γ ⁢ ( 𝜽 c , 𝒙 ) = 1 | 𝒙 | ⁢ ∑ i = 1 | 𝒙 | rank ⁢ ( x i ) 𝛾 subscript 𝜽 𝑐 𝒙 1 𝒙 superscript subscript 𝑖 1 𝒙 rank subscript 𝑥 𝑖 \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\frac{1}{{|{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\sum_{i=1}^{{|{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}|}}\mathrm{rank}({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}) italic_γ ( bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG | bold_italic_x | end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT roman_rank ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , where the function rank ⁢ ( ⋅ ) rank ⋅ \mathrm{rank}(\cdot) roman_rank ( ⋅ ) returns the rank of the true token at position i 𝑖 i italic_i computed from the probabilities assigned by the model using the correct previous tokens as context, i.e. p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( x ∣ 𝒙 < i ) subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 conditional 𝑥 subscript 𝒙 absent 𝑖 p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}(x\mid{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x ∣ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) , and | 𝒙 | 𝒙 {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element (c) Sequence Log-Likelihood: γ ⁢ ( 𝜽 c , 𝒙 ) = log ⁡ p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) 𝛾 subscript 𝜽 𝑐 𝒙 subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 𝒙 \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_γ ( bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) , where log ⁡ p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( 𝒙 ) = ∑ i = 1 | 𝒙 | log ⁡ p \scaleto ⁢ 𝜽 c ⁢ 4 ⁢ p ⁢ t ⁢ ( x i ∣ 𝒙 < i ) subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 𝒙 superscript subscript 𝑖 1 𝒙 subscript 𝑝 \scaleto subscript 𝜽 𝑐 4 𝑝 𝑡 conditional subscript 𝑥 𝑖 subscript 𝒙 absent 𝑖 \log p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\sum_{i=1}^{{|{%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\log p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}\mid{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∣ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) and | 𝒙 | 𝒙 {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element Figure 7: Memorisation profiles ( τ g , c subscript 𝜏 𝑔 𝑐 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_τ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT ) computed using different performance metrics γ 𝛾 \gamma italic_γ using all the available checkpoints—i.e., c ∈ { 0 , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 , … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 143 } 𝑐 0 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 143 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{$0$,\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_c ∈ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } and g ∈ { \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 1 , … , \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 143 } 𝑔 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 1 … \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 143 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_g ∈ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , … , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } . The dashed vertical line indicates the end of the first epoch ( c ⁢ = \qty ⁢ [ m ⁢ o ⁢ d ⁢ e = m ⁢ a ⁢ t ⁢ h ] ⁢ 95 𝑐 \qty delimited-[] 𝑚 𝑜 𝑑 𝑒 𝑚 𝑎 𝑡 ℎ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{=}\qty[mode=math]{95}{} italic_c = [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 ). We only show statistically significant entries. Report issue for preceding element