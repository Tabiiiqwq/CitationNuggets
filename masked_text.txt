HTML conversions sometimes display errors due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on. failed: scalerel failed: inconsolata failed: datetime failed: forest failed: fnpct failed: proof-at-the-end failed: tabularray failed: cellspace Authors: achieve the best HTML results from your LaTeX submissions by following these best practices . License: CC BY 4.0 arXiv:2406.04327v1 [cs.LG] 06 Jun 2024 Causal Estimation of Memorisation Profiles Report issue for preceding element Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel {}^{\includegraphics[scale={0.018}]{assets/cambridge_logo.jpg}} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT University of Cambridge, ETH ZÃ¼rich { pl487 , av308 } @cam.ac.uk { clara.meister , thomas.hofmann , tiago.pimentel } @inf.ethz.ch Report issue for preceding element Abstract Report issue for preceding element Understanding memorisation in language models has practical and societal implications, e.g., studying modelsâ€™ training dynamics or preventing copyright infringements.
Prior work defines memorisation as the causal effect of training with an instance on the modelâ€™s ability to predict that instance.
This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.
Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual.
Further, they often estimate memorisation for a model architecture rather than for a specific model instance.
This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics.
Using this method, we characterise a modelâ€™s memorisation profileâ€”its memorisation trends across trainingâ€”by only observing its behaviour on a small set of instances throughout training.
In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones. Report issue for preceding element \xspaceaddexceptions \csq@thequote@iclose \xspaceaddexceptions Report issue for preceding element Causal Estimation of Memorisation Profiles Report issue for preceding element Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel {}^{\includegraphics[scale={0.018}]{assets/cambridge_logo.jpg}} start_FLOATSUPERSCRIPT end_FLOATSUPERSCRIPT University of Cambridge, ETH ZÃ¼rich { pl487 , av308 } @cam.ac.uk { clara.meister , thomas.hofmann , tiago.pimentel } @inf.ethz.ch Report issue for preceding element {tblr} colspec = Q[c,m]|X[l,m], stretch = 0 & pietrolesci/memorisation-profiles Report issue for preceding element 1 Introduction Report issue for preceding element Large language models (LMs) are often pretrained with a single pass on web-scale datasets (Raffel etÂ al., 2020 ; Gao etÂ al., 2020 ; Penedo etÂ al., 2023 , inter alia ) .
Given the colossal size of these training sets, one may expect each individual instance to have little impact on the final model.
Yet, LMs can still reproduce entire sequences from their training set verbatim (Carlini etÂ al., 2021 ) , suggesting that models can store, or memorise , precise knowledge about individual training instances.
In the era of large LMs, measuring memorisation is crucial for NLP practitioners; it has implications for copyright and data protection (Hu etÂ al., 2022 ; Vyas etÂ al., 2023 ; Lee etÂ al., 2023 ) , for how models encode factual information (Cao etÂ al., 2022 ; Tirumala etÂ al., 2022 ) , and for understanding their training dynamics Arpit etÂ al. ( 2017 ); Chang and Bergen ( 2024 ) . Report issue for preceding element Figure 1: Memorisation profile (top) and path (bottom) of Pythia \qty [mode=math]6.9.
Each entry represents the expected counterfactual memorisation of instances trained on at a specific timestep ( â€œTreatment Stepâ€ ) across model checkpoints ( â€œCheckpoint Stepâ€ ). The dashed vertical line indicates the end of the first epoch. Report issue for preceding element One line of prior work has adopted a causal definition of memorisation : it is the causal effect of observing an instance during training on a modelâ€™s ability to correctly predict that instance (Feldman, 2020 ) .
Despite being an intuitive concept, quantification of this definition is not straightforward as it requires knowledge of a counterfactual : 1 1 1 Counterfactuals are thought experiments about what would have happened if a present condition were changed while keeping everything else unchanged (McCloskey, 2016 ) . we must know how our model would have performed on a training instance had the model not been trained on it.
To overcome this challenge, prior work has proposed a variety of methods to estimate memorisation.
Some estimate it by training a model multiple times on different subsets of the data (e.g., Feldman and Zhang, 2020 ; Zheng and Jiang, 2022 ) , while others implicitly assume this counterfactualâ€™s value to be negligible (Carlini etÂ al., 2021 ) .
Both these approaches, however, have drawbacks:
the first computes memorisation for an architecture rather than a specific model, while the second relies on a strong assumption (we discuss this in detail in Â§ 5 ). Report issue for preceding element In this paper, we first formalise counterfactual memorisation as the difference between two potential outcomes; notably, our formalisation generalises prior definitions of memorisation, allowing us to compare them within a unified framework.
We then draw from the econometrics literature (Callaway and Santâ€™Anna, 2021 ) and propose a new method which estimates memorisation using only observational data;
our method simply needs a modelâ€™s performance measurements (e.g., log-likelihood) on a subset of the training data throughout training.
The output of our method is what we term a memorisation profile : a modelâ€™s memorisation of training batches over the course of training. Report issue for preceding element Empirically, we use this method to analyse memorisation for models in the Pythia suite (Biderman etÂ al., 2023b ) and characterise their memorisation profiles; e.g., Fig. 1 reports the memorisation profile of Pythia \qty [mode=math]6.9.
By studying these memorisation profiles we find that memorisation is stronger and more persistent in larger models.
Furthermore, both the learning rate and the position of an instance in the training set considerably impact how strongly that instance is memorised.
Finally, memorisation profiles are stable across model sizes; thus, we can predict memorisation in larger models from the memorisation observed in smaller ones. Report issue for preceding element 2 Background Report issue for preceding element In this section, we introduce some background on language modelling and on causal analysis which will be required throughout our paper. Report issue for preceding element 2.1 Language Modelling Report issue for preceding element We start by providing some background on language modelling. 2 2 2 We frame our exposition in terms of language models, but our framework can be trivially applied to any neural model and input modality (e.g., images). Let p \scaleto â¢ ğœ½ â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) subscript ğ‘ \scaleto ğœ½ 4 ğ‘ ğ‘¡ ğ’™ p_{\scaleto{\bm{\theta}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_p start_POSTSUBSCRIPT bold_italic_Î¸ 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) be a language model with parameters ğœ½ âˆˆ â„ d ğœ½ superscript â„ ğ‘‘ \bm{\theta}\in\mathbb{R}^{d} bold_italic_Î¸ âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .
This model defines a probability distribution over ğ’™ âˆˆ ğ’± âˆ— ğ’™ superscript ğ’± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{V}^{*} bold_italic_x âˆˆ caligraphic_V start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , the set of all finite sequences that can be constructed from elements in the alphabet ğ’± ğ’± \mathcal{V} caligraphic_V .
To train this model, we start with a set of randomly selected initial parameters, ğœ½ 0 subscript ğœ½ 0 \bm{\theta}_{0} bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .
We then learn parameters ğœ½ ğœ½ \bm{\theta} bold_italic_Î¸ using a dataset ğ’Ÿ ğ’Ÿ \mathcal{D} caligraphic_D and an optimisation procedure defined with respect to a loss function â„’ â„’ \mathcal{L} caligraphic_L . Report issue for preceding element Specifically, let ğ’Ÿ = { ğ’™ n } n = 1 N ğ’Ÿ superscript subscript subscript ğ’™ ğ‘› ğ‘› 1 ğ‘ \mathcal{D}=\{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}_{n}\}_{n=1}^{N} caligraphic_D = { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be a dataset whose instances ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x are sequences drawn from a target (unknown) distribution p â¢ ( ğ’™ ) ğ‘ ğ’™ p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ) .
These instances are typically assumed to be sampled i.i.d., and are shuffled using a permutation function Ïƒ : { 1 , â€¦ , N } â†’ { 1 , â€¦ , N } : ğœ â†’ 1 â€¦ ğ‘ 1 â€¦ ğ‘ \sigma\colon\{1,...,N\}\!\to\!\{1,...,N\} italic_Ïƒ : { 1 , â€¦ , italic_N } â†’ { 1 , â€¦ , italic_N } .
For a given batch size B ğµ B italic_B , we then split this dataset into T â‰¤ âŒŠ N / B âŒ‹ ğ‘‡ ğ‘ ğµ T\!\leq\!\lfloor\nicefrac{{N}}{{B}}\rfloor italic_T â‰¤ âŒŠ / start_ARG italic_N end_ARG start_ARG italic_B end_ARG âŒ‹ batches â„¬ t subscript â„¬ ğ‘¡ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
We iterate through these batches performing gradient updates on the model parameters: Report issue for preceding element ğœ½ t = ğœ½ t âˆ’ 1 âˆ’ Î· â¢ âˆ‡ ğœ½ â„’ â¢ ( ğœ½ t âˆ’ 1 , â„¬ t ) subscript ğœ½ ğ‘¡ subscript ğœ½ ğ‘¡ 1 ğœ‚ subscript âˆ‡ ğœ½ â„’ subscript ğœ½ ğ‘¡ 1 subscript â„¬ ğ‘¡ \displaystyle\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}t}}=\bm{\theta}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}t-1}}-\eta\,\nabla_{\bm{%
\theta}}\mathcal{L}(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}t-1}},\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}t}}) bold_italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Î¸ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_Î· âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (1) where Î· âˆˆ â„ ğœ‚ â„ \eta\in\mathbb{R} italic_Î· âˆˆ blackboard_R is a learning rate. 3 3 3 The learning rate can be a function of other quantities (e.g., Duchi etÂ al., 2011 ; Kingma and Ba, 2015 , inter alia ) . Notably, this procedure consists of a single pass on the training set and is standard for recent LMs (e.g., Touvron etÂ al., 2023 ; Jiang etÂ al., 2023 ; Dey etÂ al., 2023 ) . Report issue for preceding element At each iteration, we use a batch â„¬ t subscript â„¬ ğ‘¡ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}t}} caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to obtain a new model checkpoint, ğœ½ t subscript ğœ½ ğ‘¡ \bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}t}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
We introduce a notation which distinguishes the indexing of checkpoints and batches.
We use c âˆˆ { 0 , 1 , â€¦ , T } ğ‘ 0 1 â€¦ ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,1,...,T\} italic_c âˆˆ { 0 , 1 , â€¦ , italic_T } to denote a checkpoint step (e.g., ğœ½ c subscript ğœ½ ğ‘ \bm{\theta}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c} bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ). Further, we use g âˆˆ { 1 , â€¦ , T } âˆª { âˆ } ğ‘” 1 â€¦ ğ‘‡ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{1,.%
..,T\}\cup\{\infty\} italic_g âˆˆ { 1 , â€¦ , italic_T } âˆª { âˆ } to denote the timestep at which a batch is used for training (e.g., â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ); we term this a treatment step , borrowing this terminology from the econometrics literature.
We denote as g = âˆ ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\!=\!\infty italic_g = âˆ a batch composed of instances that are not used for training and which form a validation set. Report issue for preceding element 2.2 Causal Analysis Report issue for preceding element Causal estimation is typically split into three steps.
First, we define a causal estimand , the target quantity we want to estimate.
Second, we state the assumptions needed to rewrite this causal estimand in terms of observable data, thus defining a statistical estimand ; this process is called identification.
Finally, we define an estimator , a statistical procedure to approximate the statistical estimand. Report issue for preceding element To formally define memorisation as a causal estimand, we will use the potential outcomes framework of Rubin ( 1974 , 2005 ) . 4 4 4 For a comparison of causal frameworks, see Ibeling and Icard ( 2023 ) . For an introduction to causal inference, see Pearl ( 2009 ) and Imbens and Rubin ( 2015 ) . This framework allows us to formally describe the causal effect of an intervention, or treatment , on some target quantity, or outcome .
In Â§ 1 , we defined memorisation as the causal effect of training on an instance on a modelâ€™s ability to predict it.
Thus, the act of training on ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x defines the treatment, while the modelâ€™s ability to predict an instance defines the outcome. Report issue for preceding element Since training is performed iteratively over batches, instances are treated at different timesteps.
Thus, we use a treatment assignment variable G â¢ ( ğ’™ ) ğº ğ’™ G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) to denote the step g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g an instance is trained on.
Further, to quantify the ability of a model with parameters ğœ½ c subscript ğœ½ ğ‘ \bm{\theta}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c} bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT to predict ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x , we use a performance function Î³ ğ›¾ \gamma italic_Î³ .
We then define the outcome variable as Y c â¢ ( ğ’™ ) = def Î³ â¢ ( ğœ½ c , ğ’™ ) superscript def subscript ğ‘Œ ğ‘ ğ’™ ğ›¾ subscript ğœ½ ğ‘ ğ’™ Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) , and, unless noted otherwise, we set this performance function to be the log-likelihood of ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x under p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}} italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT : Î³ â¢ ( ğœ½ c , ğ’™ ) = log â¡ p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ğ›¾ subscript ğœ½ ğ‘ ğ’™ subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ ğ’™ \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 5 5 5 We experiment with other functions in the appendix. Report issue for preceding element To define memorisation we need to represent both observedâ€”i.e., Y c â¢ ( ğ’™ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) â€”and counterfactual outcomesâ€”i.e., the performance of the model on ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x had we not trained on it.
The potential outcomes notation (Splawa-Neyman, 1923 ) enables us to represent both types of outcomes consistently. Report issue for preceding element Definition 1 . Report issue for preceding element The potential outcome of an instance ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c under treatment assignment g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , denoted as Y c â¢ ( ğ± ; g ) subscript ğ‘Œ ğ‘ ğ± ğ‘” Y_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , is the value that the outcome would have taken if G â¢ ( ğ± ) ğº ğ± G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) was equal to g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element Since we only observe a single permutation of the data, we only see one specific treatment step for each instance, i.e., G â¢ ( ğ’™ ) ğº ğ’™ G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) .
Thus, the potential outcome of an instance is observed only for the actual treatment assignment g = G â¢ ( ğ’™ ) ğ‘” ğº ğ’™ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\!=\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_g = italic_G ( bold_italic_x ) .
In this case, we can equate potential and observed outcomes, that is Y c â¢ ( ğ’™ ; g ) = Y c â¢ ( ğ’™ ) subscript ğ‘Œ ğ‘ ğ’™ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})=Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) , a property called consistency (Cole and Frangakis, 2009 ) .
For any other treatment step g â¢ â‰  G â¢ ( ğ’™ ) ğ‘” ğº ğ’™ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
\neq}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}}) italic_g â‰  italic_G ( bold_italic_x ) , the potential outcome is counterfactual and, thus, unobservable from the data. Report issue for preceding element 3 Counterfactual Memorisation Report issue for preceding element Intuitively, counterfactual memorisation can be understood as the answer to the question: how would the modelâ€™s performance on instance ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c be different if we had not trained on it at timestep g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g ?
Using the potential outcomes notation, we formalise this definition as follows. Report issue for preceding element Definition 2 . Report issue for preceding element Counterfactual memorisation is the causal effect of using instance ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x for training at the observed timestep G â¢ ( ğ± ) â¢ = g G ğ± g G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g on the modelâ€™s performance on this same instance at timestep c c {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : Report issue for preceding element Ï„ ğ’™ , c = def Y c â¢ ( ğ’™ ; g ) âŸ performance on ğ’™ when trained with ğ’™ âˆ’ Y c â¢ ( ğ’™ ; âˆ ) âŸ performance on ğ’™ when not trained with ğ’™ superscript def subscript ğœ ğ’™ ğ‘ subscript âŸ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” performance on ğ’™ when trained with ğ’™ subscript âŸ subscript ğ‘Œ ğ‘ ğ’™ performance on ğ’™ when not trained with ğ’™ \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}\,\,\,\mathrel{\stackrel%
{{\scriptstyle\textnormal{def}}}{{=}}}\underbrace{Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})}_{\begin{%
subarray}{c}\text{performance on ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\\
\text{when trained with ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\end{subarray}}-%
\underbrace{Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})}_{\begin{subarray}{c}\text{performance on%
 ${\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}$}\\
\text{when not trained with ${\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}$}\end{subarray}}\!\!\!\! italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP underâŸ start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT - underâŸ start_ARG italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL performance on bold_italic_x end_CELL end_ROW start_ROW start_CELL when not trained with bold_italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT (2) In econometrics, eq. 2 is called an individual treatment effect (ITE).
Notably, the first potential outcome in this equation, Y c â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘ ğ’™ ğ‘” Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , can be observed from the data since, by definition, we trained on ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x at timestep G â¢ ( ğ’™ ) â¢ = g ğº ğ’™ ğ‘” G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g .
However, the second term, Y c â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) , is counterfactual.
To compute the ITE, we would need to estimate this counterfactual outcome for a specific instance, which is challenging due to unobserved factors and heterogeneity 6 6 6 I.e.,Â non-random variability across instances. (Lu etÂ al., 2018 ) .
While we would ideally estimate memorisation at the instance level, we focus on average effects instead, as is common in the econometrics literature (Angrist and Pischke, 2015 ) . Report issue for preceding element Definition 3 . Report issue for preceding element Expected counterfactual memorisation is the average causal effect of using instances for training at timestep g g {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g on the modelâ€™s performance on these same instances at timestep c c {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c : 7 7 7 In econometrics, eq. 3 is called an average treatment effect on the treated (ATT), as it is defined in terms of an expectation over ğ± âˆ¼ p â¢ ( ğ± â¢ âˆ£ G â¢ ( ğ± ) â¢ = g ) similar-to ğ± p ğ± âˆ£ G ğ± g {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\sim p({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}) bold_italic_x âˆ¼ italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = italic_g ) .
In other words, this expectation is taken with respect to the instance distribution conditioned on it being selected for training at step g g {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g .
Assuming the training set is sampled i.i.d.Â and that its permutation is random (as discussed in Â§ 2.1 ), then p â¢ ( ğ± â¢ âˆ£ G â¢ ( ğ± ) â¢ = g ) = p â¢ ( ğ± ) p ğ± âˆ£ G ğ± g p ğ± p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x ) .
Given these assumptions, eq. 3 would also be an average treatment effect (ATE), which would allow us to make causal claims about the entire population. Report issue for preceding element Ï„ g , c = def ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = g ] superscript def subscript ğœ ğ‘” ğ‘ subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\Big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\Big{]} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (3) Together, the Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT form a matrix which we term the modelâ€™s memorisation profile ;
each row therein is the memorisation path of a batch.
Memorisation profiles and paths allow us to analyse a modelâ€™s memorisation patterns across different treatment and checkpoint steps.
Notably, there cannot be memorisation whenever c < g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g , as the instances have not been seen by the model yet, so Ï„ g , c = 0 subscript ğœ ğ‘” ğ‘ 0 \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}=0 italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = 0 in those cases.
We term Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT as instantaneous memorisation when c = g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c = italic_g , as persistent memorisation when c > g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}>{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c > italic_g , and as residual memorisation when c = T ğ‘ ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T . Report issue for preceding element 4 Estimating Memorisation Report issue for preceding element The practical implication of defining memorisation at a treatment level g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g is that we can only make causal claims for groups of instances treated at the same timestep (i.e., a batch â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ), rather than for individual instances.
However, even though this approach simplifies the problem, estimating eq. 3 still poses major challenges as it contains a counterfactual.
A simple decomposition makes this counterfactual explicit: Report issue for preceding element Ï„ g , c = subscript ğœ ğ‘” ğ‘ absent \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}= italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT = (4) ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ] âŸ 1 Report issue for preceding element â¢ - ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ] âŸ 2 Report issue for preceding element subscript âŸ subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” âˆ£ ğº ğ’™ ğ‘” 1 Report issue for preceding element subscript âŸ subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ âˆ£ ğº ğ’™ ğ‘” 2 Report issue for preceding element \displaystyle\underbrace{\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\!\big{[%
}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}%
c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}\!}_{%
\leavevmode\hbox to8.9pt{\vbox to8.9pt{\pgfpicture\makeatletter\hbox{\hskip 4.%
45195pt\lower-4.45195pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }%
\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}%
\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox to0.0pt{%
\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%
}{}{}{{}\pgfsys@moveto{4.25195pt}{0.0pt}\pgfsys@curveto{4.25195pt}{2.34831pt}{%
2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\pgfsys@curveto{-2.34831pt}{4.25195pt}{%
-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\pgfsys@curveto{-4.25195pt}{-2.34831%
pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\pgfsys@curveto{2.34831pt}{-4.25%
195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto%
{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1%
.0}{-1.75pt}{-2.25555pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{%
rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{{1}}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}%
\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}%
\lxSVG@closescope\endpgfpicture}}}\mathop{-}\underbrace{\operatorname*{\mathbb%
{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\!\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}\!}_{\leavevmode\hbox to8.9pt{\vbox to8.9%
pt{\pgfpicture\makeatletter\hbox{\hskip 4.45195pt\lower-4.45195pt\hbox to0.0pt%
{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}%
\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }\pgfsys@color@rgb@fill{0}{0}%
{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{ }\nullfont\hbox
to%
0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }{
{{}}\hbox{\hbox{{\pgfsys@beginscope\pgfsys@invoke{ }{{}{{{}}}{{}}{}{}{}{}{}{}{%
}{}{}{{}\pgfsys@moveto{4.25195pt}{0.0pt}\pgfsys@curveto{4.25195pt}{2.34831pt}{%
2.34831pt}{4.25195pt}{0.0pt}{4.25195pt}\pgfsys@curveto{-2.34831pt}{4.25195pt}{%
-4.25195pt}{2.34831pt}{-4.25195pt}{0.0pt}\pgfsys@curveto{-4.25195pt}{-2.34831%
pt}{-2.34831pt}{-4.25195pt}{0.0pt}{-4.25195pt}\pgfsys@curveto{2.34831pt}{-4.25%
195pt}{4.25195pt}{-2.34831pt}{4.25195pt}{0.0pt}\pgfsys@closepath\pgfsys@moveto%
{0.0pt}{0.0pt}\pgfsys@stroke\pgfsys@invoke{ }
}{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1%
.0}{-1.75pt}{-2.25555pt}\pgfsys@invoke{ }\hbox{{\definecolor{pgfstrokecolor}{%
rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{ }%
\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\hbox{{{2}}}
}}\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope}}}
}
\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope{{{}}}{}{}\hss}%
\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}%
\lxSVG@closescope\endpgfpicture}}} underâŸ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - underâŸ start_ARG blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Expectation 1 Report issue for preceding element can be directly estimated from the data because batch â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT contains a set of examples ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x for which G â¢ ( ğ’™ ) â¢ = g ğº ğ’™ ğ‘” G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g and, thus, we can invoke the consistency assumption to equate Y c â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘ ğ’™ ğ‘” Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) with the observed outcome Y c â¢ ( ğ’™ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) .
Let us define the mean across instances in a batch as: Report issue for preceding element Y Â¯ c â¢ ( g ) = def 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g Y c â¢ ( ğ’™ ) superscript def subscript Â¯ ğ‘Œ ğ‘ ğ‘” 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ \displaystyle\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})\mathrel{\stackrel{{\scriptstyle\textnormal{def%
}}}{{=}}}\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named%
]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (5) We can thus use eq. 5 as an estimator for expectation 1 Report issue for preceding element . Expectation 2 Report issue for preceding element , however, is counterfactual: we cannot observe the potential outcome Y c â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) for instances treated at timestep g â¢ â‰  âˆ ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
\neq}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty} italic_g â‰  âˆ .
The presence of counterfactual potential outcomes in causal estimands creates challenges for their estimation, being known as the fundamental problem of causal inference (Holland, 1986 ) .
The goal of causal methods is then to estimate these counterfactual outcomes from observed ones, using comparable groups of instances.
Thus far, we have defined our causal estimand.
In this section, we perform steps two and three of causal estimation ( Â§ 2.2 ): we derive two statistical estimands for our causal estimand (identifying it under specific assumptions), and provide concrete estimators for them. Report issue for preceding element 4.1 The Difference Estimator Report issue for preceding element Our first approach to estimate memorisation is straightforward and only requires the observed outcomes of a held-out validation set.
However, it relies on a strong identification assumption. Report issue for preceding element Assumption 1 (I.i.d.Â Dataset Sampling) . Report issue for preceding element Instances ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x are independently and identically distributed, following p â¢ ( ğ± ) ğ‘ ğ± p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x ) , and are randomly assigned to treatment groups g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g . Report issue for preceding element Under this assumption, we have that p â¢ ( ğ’™ â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ) = p â¢ ( ğ’™ â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ) = p â¢ ( ğ’™ ) ğ‘ ğ’™ âˆ£ ğº ğ’™ ğ‘” ğ‘ ğ’™ âˆ£ ğº ğ’™ ğ‘ ğ’™ p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!=\!p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\!=\!p({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = âˆ ) = italic_p ( bold_italic_x ) .
Thus, the following statistical estimand identifies Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element Ï„ g , c ğšğš’ğšğš = ğ”¼ ğ’™ [ \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}}=\operatorname*{\mathbb{E}}_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ Y c ( ğ’™ ; g ) âˆ£ G ( ğ’™ ) = g ] \displaystyle Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (6) âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ] subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ \displaystyle-\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid\mathop{G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\big{]} - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ start_BIGOP italic_G ( bold_italic_x ) = âˆ end_BIGOP ] (See Lemma 1 in Â§ A.1 for a proof.)
Note that, unlike eq. 3 , the second term in this estimand is not counterfactual: it is the expected observed outcome of validation instances, â„¬ âˆ subscript â„¬ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT .
This statistical estimand can then be estimated as follows. Report issue for preceding element Estimator 1 . Report issue for preceding element The difference estimator , defined as: Report issue for preceding element Ï„ ^ g , c ğšğš’ğšğš = Y Â¯ c â¢ ( g ) âˆ’ Y Â¯ c â¢ ( âˆ ) superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğšğš subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘ \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}}=\overline{Y}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) (7) is an unbiased estimator of Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element See Lemma 2 in Â§ A.1 for a proof.
âˆ Report issue for preceding element Notably, Assump. 1 is satisfied by the training procedure we described in Â§ 2.1 , and is commonly true in machine learning.
However, it might not hold in general, as the train and validation distributions may not match exactly.
For example, NLP practitioners might deduplicate their training set but not validation (Biderman etÂ al., 2023b ) or might use challenge sets for validation (Kiela etÂ al., 2021 ) .
Moreover, even when Assump. 1 holds, eq. 7 is low-variance only if we have large enough samples to compute Y Â¯ c â¢ ( g ) subscript Â¯ ğ‘Œ ğ‘ ğ‘” \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y Â¯ c â¢ ( âˆ ) subscript Â¯ ğ‘Œ ğ‘ \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) .
Unfortunately, given the size of state-of-the-art LMs and their datasets, it can be expensiveâ€”both in terms of computation and memory usageâ€”to extract the performance measures Y c â¢ ( ğ’™ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) for all instances ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and checkpoints c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .
Even with unlimited compute, Y Â¯ c â¢ ( g ) subscript Â¯ ğ‘Œ ğ‘ ğ‘” \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) can only be estimated using instances in â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , which lower bounds the variance of this estimator. Report issue for preceding element While the difference estimator is a first step towards a principled estimator of counterfactual memorisation, we can do better.
In the next section, we describe an estimator that has lower variance and requires weaker assumptions. Report issue for preceding element 4.2 The Difference-in-Differences Estimator Report issue for preceding element We now introduce another causal estimator based on the difference-in-differences (DiD) design.
The intuition behind DiD is to use the time dimension to help with identification; DiD identifies a causal estimand using the difference in the trends over time of the outcome on treated vs.Â untreated instances.
In our specific setting, DiD relies on the assumption that changes in model performance over time would follow similar trends in different batches if they had not been used for training.
We formalise this assumption as follows. Report issue for preceding element Assumption 2 (Parallel Trends) . Report issue for preceding element In the absence of training, the expected change in model performance across checkpoints would be the same regardless of treatment.
That is, for all c , c â€² â‰¥ g â¢ - 1 ğ‘ superscript ğ‘ â€² ğ‘” 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{%
\prime}}\geq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}\mathop{-}1 italic_c , italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰¥ italic_g - 1 : Report issue for preceding element ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y c â€² â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = g ] subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ superscript ğ‘ â€² ğ’™ ğº ğ’™ ğ‘” \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (8) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y c â€² â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ] absent subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ superscript ğ‘ â€² ğ’™ ğº ğ’™ \displaystyle\qquad=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid\mathop{G({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ start_BIGOP italic_G ( bold_italic_x ) = âˆ end_BIGOP ] We need a second assumption before we can apply the DiD design to our setting. Report issue for preceding element Assumption 3 (No Anticipation) . Report issue for preceding element Training has no effect before it happens.
That is, for all c < g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c < italic_g : Report issue for preceding element ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) â¢ = g ] subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (9) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = g ] absent subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle\qquad\qquad\quad=\operatorname*{\mathbb{E}}_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] Given these two assumptions, we can now follow Callaway and Santâ€™Anna ( 2021 ) in identifying our target statistical estimand. 8 8 8 The DiD design was originally proposed for the case with only two treatment and checkpoint steps (i.e., g âˆˆ { 1 , âˆ } ğ‘” 1 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{1,{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\} italic_g âˆˆ { 1 , âˆ } and c âˆˆ { 0 , 1 } ğ‘ 0 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,1\} italic_c âˆˆ { 0 , 1 } ). Previous work has shown the challenges of extending DiD to multiple timesteps, especially when allowing for heterogeneous treatment effects (Roth etÂ al., 2023 ) . Callaway and Santâ€™Anna ( 2021 ) propose an extension which identifies eq. 3 while allowing for treatment effect heterogeneity across checkpoint and treatment steps. Combined, these assumptions allow us to rewrite expectation 2 Report issue for preceding element in eq. 4 as a function of potential outcomes that are observable: Y c â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) , Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ ğ‘” 1 ğ’™ Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) given G â¢ ( ğ’™ ) â¢ = âˆ ğº ğ’™ G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty} italic_G ( bold_italic_x ) = âˆ are observable on a held-out validation set, while Y g âˆ’ 1 â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) given G â¢ ( ğ’™ ) â¢ = g ğº ğ’™ ğ‘” G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g} italic_G ( bold_italic_x ) = italic_g is observable on the training set.
The following statistical estimand thus identifies Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT : Report issue for preceding element Ï„ g , c ğšğš’ğš = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) â¢ = g ] superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğš subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” ğº ğ’™ ğ‘” \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}}=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0%
}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0%
.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}[Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\!-\!Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}]\! italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (10) âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ] subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ \displaystyle\qquad\,\,\,-\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}[Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!-\!Y_{\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]%
{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\!\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (See Lemma 3 in Â§ A.2 for a proof.)
This leads to the following DiD estimator. Report issue for preceding element Estimator 2 . Report issue for preceding element The difference-in-differences estimator (DiD), defined as: Report issue for preceding element Ï„ ^ g , c ğšğš’ğš = ( Y Â¯ c â¢ ( g ) â¢ - Y Â¯ g âˆ’ 1 â¢ ( g ) ) âŸ diff in trained âˆ’ ( Y Â¯ c â¢ ( âˆ ) â¢ - Y Â¯ g âˆ’ 1 â¢ ( âˆ ) ) âŸ diff in untrained superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğš subscript âŸ subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘” 1 ğ‘” diff in trained subscript âŸ subscript Â¯ ğ‘Œ ğ‘ subscript Â¯ ğ‘Œ ğ‘” 1 diff in untrained \displaystyle\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}}=\underbrace{\!\Big{(}%
\overline{Y}_{\!\!{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g})\mathop{-}\overline{Y}_{\!\!\mathop{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}\!}_{\emph{diff in %
trained}}-\underbrace{\!\Big{(}\overline{Y}_{\!\!{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{-}\overline{Y}%
_{\!\!\mathop{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty})\Big{)}\!}_{\emph{diff in untrained}} over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = underâŸ start_ARG ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( italic_g ) ) end_ARG start_POSTSUBSCRIPT diff in trained end_POSTSUBSCRIPT - underâŸ start_ARG ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT start_BIGOP italic_g - 1 end_BIGOP end_POSTSUBSCRIPT ( âˆ ) ) end_ARG start_POSTSUBSCRIPT diff in untrained end_POSTSUBSCRIPT (11) is an unbiased estimator of Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 . Report issue for preceding element Proof. Report issue for preceding element See Lemma 4 in Â§ A.2 for a proof.
âˆ Report issue for preceding element The DiD estimator depends on weaker assumptions and has a lower variance (under mild assumptions, see Â§ A.3 ) than the difference estimator in eq. 7 .
Specifically, the parallel trends assumption ( Assump. 2 ) is strictly weaker than the i.i.d.Â one ( Assump. 1 ): if p â¢ ( ğ’™ â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ) = p â¢ ( ğ’™ â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ) ğ‘ ğ’™ âˆ£ ğº ğ’™ ğ‘” ğ‘ ğ’™ âˆ£ ğº ğ’™ p({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})=p({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = italic_g ) = italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = âˆ ) , then it is trivial that performances should present parallel trends.
Moreover, Assump. 2 only requires that the training and validation sets follow similar trends on average, which might be true even in the case of, e.g., challenge validation sets or deduplicated training data.
Therefore, the assumptions underpinning DiD are more likely to hold in practice and we will use it to estimate memorisation here. Report issue for preceding element In practice, the difference-in-differences estimation procedure includes two steps.
First, we compute the modelâ€™s performance on a subset of analysed instancesâ€”i.e., samples from the training and validation setsâ€”using the available checkpoints; thus forming a panel of observed outcomes, as it is usually termed in econometrics.
Then, we use this panel to compute the estimates in eq. 11 . Report issue for preceding element 5 Prior Notions of Memorisation Report issue for preceding element Memorisation has recently received much attention (Arpit etÂ al., 2017 ; Carlini etÂ al., 2019 , 2021 ; Anagnostidis etÂ al., 2023 , inter alia ) . 9 9 9 See Hartmann etÂ al. ( 2023 ) or Ishihara ( 2023 ) for surveys. Prior work has studied how model architecture and training choices influence memorisation (Tirumala etÂ al., 2022 ; Kandpal etÂ al., 2022 ; Lee etÂ al., 2022 ; Biderman etÂ al., 2023a ) , and where memorised instances are stored within a model (Maini etÂ al., 2023 ; Stoehr etÂ al., 2024 ) .
In this section, we use our framework to discuss three prior notions of memorisation which we consider most relevant to our paper: previous operationalisations of counterfactual memorisation (e.g., Feldman, 2020 ) , influence functions (Zheng and Jiang, 2022 ) , and extractable memorisation (Carlini etÂ al., 2023 ) . Report issue for preceding element 5.1 Previous Operationalisations of Counterfactual Memorisation Report issue for preceding element As mentioned before, estimating an instanceâ€™s memorisation Ï„ ğ’™ , c subscript ğœ ğ’™ ğ‘ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT is non-trivial due to the counterfactual component in its definition.
We avoid this issue by estimating expected memorisation Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT instead.
Prior work (Feldman, 2020 ; Feldman and Zhang, 2020 ; Zhang etÂ al., 2023 ; Lukasik etÂ al., 2024 ) takes a different approach, comparing the performance of models trained with and without that instance.
In doing so, they average performance across training runs, measuring what we term architectural counterfactual memorisation . Report issue for preceding element Formally, let ğ ğ \bm{\psi} bold_italic_Ïˆ be a vector of variables responsible for training variance.
This includes, e.g., the data permutation induced by Ïƒ ğœ \sigma italic_Ïƒ and the initial model parameters ğœ½ 0 subscript ğœ½ 0 \bm{\theta}_{0} bold_italic_Î¸ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .
By defining a distribution p â¢ ( ğ ) ğ‘ ğ p(\bm{\psi}) italic_p ( bold_italic_Ïˆ ) over these variables, architectural memorisation can be defined as follows. Report issue for preceding element Definition 4 . Report issue for preceding element Architectural counterfactual memorisation is the counterfactual memorisation Ï„ ğ± , \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript Ï„ ğ± \scaleto T 4 p t \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT when marginalising over training variables ğ›™ ğ›™ \bm{\psi} bold_italic_Ïˆ : Report issue for preceding element Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT = def ğ”¼ ğ [ Ï„ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ âˆ£ G â¢ ( ğ’™ ) â¢ â‰  âˆ ] superscript def absent subscript ğ”¼ ğ subscript ğœ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ âˆ£ ğº ğ’™ \displaystyle\mathrel{\stackrel{{\scriptstyle\textnormal{def}}}{{=}}}%
\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[\tau_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto%
{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}%
{4pt}}\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{\neq}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] (12) = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) â¢ - Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ â‰  âˆ ] absent subscript ğ”¼ ğ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ âˆ£ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mathop{-}Y_{\scaleto{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4%
pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{\neq}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] where G â¢ ( ğ± ) ğº ğ± G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_G ( bold_italic_x ) in the first potential outcome depends on which batch the shuffling function Ïƒ ğœ \sigma italic_Ïƒ puts ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in. Report issue for preceding element Prior work has proposed a number of methods to estimate this value (Bachmann etÂ al., 2022 ; Lin etÂ al., 2022 ; Ilyas etÂ al., 2022 ; Park etÂ al., 2023 ) .
The simplest of these is to train several models while including or not ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in the training set; these models are then used to approximate the expectation above.
We describe a statistical estimand and estimator for Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT in Â§ B.1 , discussing the assumptions needed by this approach. Report issue for preceding element Notably, this operationalisation has the advantage of estimating memorisation at the instance level.
However, it also has drawbacksâ€”beyond just being computationally expensive to estimate.
These become apparent upon closer inspection of the definition of Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT .
First, it does not provide insights into the effect of the checkpoint step or treatment step on memorisation;
this is because T ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T is hard-coded into Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT â€™s definition and because it marginalises over permutations of the data.
While it is trivial to generalise this definition to other checkpoint steps c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c or to specific g ğ‘” {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_g , prior work has mainly focused on these choices, overlooking the impact of training dynamics on memorisation.
Further, and perhaps more importantly, marginalising over p â¢ ( ğ ) ğ‘ ğ p(\bm{\psi}) italic_p ( bold_italic_Ïˆ ) means that this metric quantifies memorisation for a model architecture, rather than for a specific model. Report issue for preceding element 5.2 Influence Functions Report issue for preceding element Influence functions (Hampel, 1974 ; Cook and Weisberg, 1980 ) estimateâ€”without re-training a modelâ€”how its parameters would change if an instance ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x was removed from the training set.
Specifically, the new set of parameters can be approximated as follows (Koh and Liang, 2017 ) : Report issue for preceding element ğœ½ âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â‰ˆ ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t + 1 / N â¢ H ğœ½ âˆ’ 1 â¢ âˆ‡ ğœ½ â„’ â¢ ( ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t , ğ’™ ) subscript ğœ½ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ 1 ğ‘ subscript superscript H 1 ğœ½ subscript âˆ‡ ğœ½ â„’ subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle\bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}\approx%
\bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}+\nicefrac{{1}}{{N}}\,\mathrm{H}^{%
\mathop{-1}}_{\bm{\theta}}\,\nabla_{\bm{\theta}}\mathcal{L}(\bm{\theta}_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) bold_italic_Î¸ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT â‰ˆ bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT + / start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT caligraphic_L ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , bold_italic_x ) (13) where H ğœ½ subscript H ğœ½ \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT is the hessian of â„’ â„’ \mathcal{L} caligraphic_L evaluated at ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
This approximation is based on a first-order Taylor expansion of the training objective around ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT , and should lead to small errors under the following assumptions:
(i) the loss function is strictly convex in ğœ½ ğœ½ \bm{\theta} bold_italic_Î¸ , (ii) H ğœ½ subscript H ğœ½ \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (i.e., the gradient is zero).
Given these assumptions, influence functions can be used to efficiently estimate the counterfactual term Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) in the definition of Ï„ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
Specifically, we can measure the modelâ€™s performance using the updated parameters, Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) = def Î³ â¢ ( ğœ½ âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t ; ğ’™ ) superscript def subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğ›¾ subscript ğœ½ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathrel{\stackrel{{%
\scriptstyle\textnormal{def}}}{{=}}}\gamma(\bm{\theta}_{-{\color[rgb]{0,0.88,0%
}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0%
.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}};{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ) , and equate Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) = Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) .
The influence function estimator of memorisation can then be written as: Report issue for preceding element Ï„ ^ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t ğš’ğš—ğšğš• = Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) âˆ’ Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) superscript subscript ^ ğœ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğš’ğš—ğšğš• subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle\hat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{%
\mathtt{infl}}=Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (14) We formalise this estimator and its statistical estimand in Â§ B.2 .
Notably, Zheng and Jiang ( 2022 ) use a similar approach to estimate memorisation in a classification setting. Report issue for preceding element Influence functions thus provide a computationally efficient method to estimate instance-level counterfactual memorisation.
However, none of the required assumptions above is typically satisfied for LMs, which can lead to strong biases in this estimator (Basu etÂ al., 2020 ; Bae etÂ al., 2022 ; Schioppa etÂ al., 2023 ) .
Moreover, assumptions (ii) and (iii) require ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT to be locally optimal, meaning that this approach is not applicable for studying c < T ğ‘ ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}<{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c < italic_T .
We therefore cannot use it to study how memorisation interacts with training dynamics. Report issue for preceding element 5.3 Extractable Memorisation Report issue for preceding element Carlini etÂ al. ( 2023 ) defines memorisation as ( k , â„“ ) ğ‘˜ â„“ (k,\!\ell) ( italic_k , roman_â„“ ) -extractability; a string is ( k , â„“ ) ğ‘˜ â„“ (k,\!\ell) ( italic_k , roman_â„“ ) -extractable if the model correctly predicts â„“ â„“ \ell roman_â„“ of its tokens given a prefix of k ğ‘˜ k italic_k tokens.
This definition has recently gained much interest because of its relevance to copyright infringement and data protection.
Despite being seemingly different, we argue that extractable memorisation is an estimator for counterfactual memorisation.
Concretely, let performance Î³ ğ›¾ \gamma italic_Î³ be measured as whether a string is ( k , â„“ ) ğ‘˜ â„“ (k,\!\ell) ( italic_k , roman_â„“ ) -extractable.
Now, assume that a string is not ( k , â„“ ) ğ‘˜ â„“ (k,\!\ell) ( italic_k , roman_â„“ ) -extractable in the absence of training, i.e., Y c â¢ ( ğ’™ ; âˆ ) â¢ = 0 subscript ğ‘Œ ğ‘ ğ’™ 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = 0 .
Given this assumption, we can define an extractable memorisation estimator as: Report issue for preceding element Ï„ ^ ğ’™ , c ğšğš¡ğšğš› = Y c â¢ ( ğ’™ ) superscript subscript ^ ğœ ğ’™ ğ‘ ğšğš¡ğšğš› subscript ğ‘Œ ğ‘ ğ’™ \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (15) We formalise this estimator and its statistical estimand in Â§ B.3 . Report issue for preceding element Extractable memorisation thus implicitly assumes that Y c â¢ ( ğ’™ ; âˆ ) â¢ = 0 subscript ğ‘Œ ğ‘ ğ’™ 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = 0 .
Counterfactual memorisation, on the other hand, controls for this counterfactual.
Notably, assuming Y c â¢ ( ğ’™ ; âˆ ) â¢ = 0 subscript ğ‘Œ ğ‘ ğ’™ 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{=}0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = 0 may be reasonable when a string is long and complex; in this case, the chance that ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x would be in the modelâ€™s top- 1 1 1 1 beam (the output of greedy decoding) approaches zero.
However, this assumption may not be reasonable for shorter or less complex sequences. Rather, it may cause the resulting estimate to conflate memorisation with the intrinsic difficulty of predicting a string. Report issue for preceding element 6 Experiments Report issue for preceding element While our method applies to any model trained with a single pass on its training data, we focus on quantifying memorisation in pretrained LMs, which are characterised by the use of architectures with a large number of parameters and large datasets.
Due to the costs of training such models from scratch, we take advantage of open-source pretrained models whose intermediate checkpoints and preprocessed data are publicly available.
In this section, we detail the models and data used and describe how we collect the observed outcomes Y c â¢ ( ğ’™ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) . Report issue for preceding element The Pythia Suite. Report issue for preceding element We use the publicly available Pythia model suite 10 10 10 Both preprocessed data and intermediate checkpoints are publicly available at github.com/EleutherAI/pythia . (Biderman etÂ al., 2023b ) , composed of 8 8 8 8 transformers of sizes ranging from \qty [mode=math]70 to \qty [mode=math]12 parameters.
These models were trained on the Pile dataset (Gao etÂ al., 2020 ; Biderman etÂ al., 2022 ) , a \qty [mode=math]300-token curated collection of English documents.
All models are trained using the same data.
Specifically, the dataset is shuffled and â€œpackedâ€ into sequences of 2 , 049 2049 2,049 2 , 049 tokens; 11 11 11 Since target tokens are the right-shifted input tokens, to compute the loss on 2 , 048 2048 2,048 2 , 048 tokens the Pythia authors added a token to the context. each of these sequences corresponds to an instance ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x .
Training was performed using a cosine learning rate schedule with warm-up, and using a batch size of 1 , 024 1024 1,024 1 , 024 sequences, resulting in exactly \qty [mode=math]143 optimisation steps.
We use the model versions trained on the deduplicated Pile dataset to reduce the risk of finding spurious memorisation patterns due to duplication.
The deduplicated dataset has \qty [mode=math]207 tokens, thus models using this version are trained for circa 1.5 1.5 1.5 1.5 epochs to keep an equal token count relative to the non-deduplicated versions.
We consider the checkpoints relative to the first epoch (i.e., up to step \qty [mode=math]95). 12 12 12 For completeness, we report the second half-epoch (steps \qty [mode=math]96- \qty [mode=math]143) analysis in App. D . More details are in App. C . Report issue for preceding element Constructing the Panel. Report issue for preceding element Ideally, we would collect performance metrics for each instance ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and for every checkpoint step c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c .
However, given the size of the Pile dataset, it is computationally infeasible to collect evaluations for all instances; thus we resort to subsampling this data.
Furthermore, the granularity of the available checkpoints (i.e., every \qty [mode=math]1 timesteps) does not allow us to consider each timestep; thus we consider timesteps c âˆˆ { 0 , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 â¢ â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 95 } ğ‘ 0 \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{0,\qty[mode=math]{1}{}...,\qty[mode=math]{95}{}\} italic_c âˆˆ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } and treatment timesteps g âˆˆ { \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 2 â¢ â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 95 } ğ‘” \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 2 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 95 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},\qty[mode=math]{2}{}...,\qty[mode=math]{95}{}\} italic_g âˆˆ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 2 â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 } .
To match the checkpoint frequency, we consider all instances between two checkpoints (i.e., \qty [mode=math]1 batches) as if they were seen by the model at the same timestep.
We term these groups of batches macro-batches 13 13 13 In econometrics group of instances that undergo treatment at the same time are typically termed cohorts . and formally define them as ğ’¢ g = â‹ƒ g âˆ’ \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 < t â‰¤ g â„¬ t subscript ğ’¢ ğ‘” subscript ğ‘” \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 ğ‘¡ ğ‘” subscript â„¬ ğ‘¡ \mathcal{G}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}=\bigcup_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}-\qty[mode=math]{1}{}<t\leq{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\mathcal{B}_{t} caligraphic_G start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = â‹ƒ start_POSTSUBSCRIPT italic_g - [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 < italic_t â‰¤ italic_g end_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .
To obtain enough evaluations for each macro-batch, we sample instances from the training set in two steps: we randomly choose 10 10 10 10 batches for each macro-batch and sample 10 10 10 10 instances from each.
This process results in \qty [mode=math]14.3 analysed training instances.
Additionally, we sample \qty [mode=math]2 instances from the validation set to create ğ’¢ âˆ subscript ğ’¢ \mathcal{G}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_G start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT .
This process returns a panel of \qty [mode=math]16.3 instances evaluated at 96 96 96 96 timesteps. 14 14 14 Our data and experimental artefacts are publicly available at huggingface.co/collections/pietrolesci/memorisation-profiles . As our performance metric we use the sequence-level log-likelihood: Î³ â¢ ( ğœ½ , ğ’™ ) = log â¡ p \scaleto â¢ ğœ½ â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ğ›¾ ğœ½ ğ’™ subscript ğ‘ \scaleto ğœ½ 4 ğ‘ ğ‘¡ ğ’™ \gamma(\bm{\theta},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Î³ ( bold_italic_Î¸ , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_Î¸ 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . 15 15 15 Results using different metrics are reported in App. D . Report issue for preceding element Figure 2: Memorisation profiles ( Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT ). We only show statistically significant entries. Report issue for preceding element Statistical Inference. Report issue for preceding element To compute statistical significance, we use the Simple Multiplier Bootstrap procedure of Callaway and Santâ€™Anna ( 2021 ) which returns simultaneous confidence intervals for all memorisation estimates, accounting for dependencies across macro-batches and checkpoint steps and thus avoiding multiple-testing issues. Report issue for preceding element 7 Results Report issue for preceding element We report the memorisation profiles of all Pythia sizes in Fig. 2 . Below, we use these memorisation profiles to describe different types of memorisation. Report issue for preceding element Figure 3: Instantaneous memorisation ( Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT for g = c ğ‘” ğ‘ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}={\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g = italic_c ). We only show statistically significant estimates. Report issue for preceding element Instantaneous Memorisation. Report issue for preceding element Instantaneous memorisation estimates (defined in Â§ 3 as Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g â¢ = c ğ‘” ğ‘ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{%
=}{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g = italic_c ) are depicted as the diagonal entries in the memorisation profiles in Fig. 2 , and are also presented in Fig. 3 .
From these estimates, we can clearly observe the effect of the treatment step on memorisation: instantaneous memorisation is stronger earlier in training than later.
Interestingly (but perhaps unsurprisingly), instantaneous memorisation correlates with the cosine learning rate schedule: it is stronger after the warm-up period (around timestep \qty [mode=math]1.5) than before it.
Furthermore, and as expected, instantaneous memorisation increases with model size. 16 16 16 Notably, we expect instantaneous memorisation to always be present in normally-trained LMs (albeit with a potentially small value). It could thus be used for power analysis (Cohen, 1992 ) : choosing the number of instances to sample per macro-batch which provides sufficient statistical power to correctly detect memorisation. Report issue for preceding element Persistent Memorisation. Report issue for preceding element Persistent memorisation estimates (defined in Â§ 3 as Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when g > c ğ‘” ğ‘ {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_g > italic_c ) are depicted as the off-diagonal entries in the memorisation profiles in Fig. 2 . Fig. 4 shows the average persistent memorisation at a specific number of timesteps after treatment; in this figure, Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT were averaged across macro-batches for each c â¢ - g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{-}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c - italic_g . 17 17 17 By averaging across macro-batches, variance is lower and more estimates become statistically significant. This way of aggregating the memorisation profile allows us to summarise the general memorisation patterns of a model.
Smaller models have lower persistent memorisation, with \qty [mode=math]70 having no persistent memorisation.
Interestingly, persistent memorisation plateaus after \qty [mode=math]25 timesteps.
This result has implications for data ordering during training.
For example, if there are particular instances that we do not want the model to memorise, but which we still want to use during training, they should be included in earlier batches. 18 18 18 We note that our results differ from Biderman etÂ al. â€™s ( 2023b ), who find no differences in memorisation due to an instanceâ€™s treatment step. We hypothesise that this discrepancy stems from the differences in metrics used to quantify memorisation and the statistical approaches adopted. Report issue for preceding element Figure 4: Average persistent memorisation ( Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT averaged per timestep after treatment, i.e., c â¢ - g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{-}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c - italic_g ). We only show statistically significant estimates. Report issue for preceding element Figure 5: Residual memorisation ( Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT for c = T = \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 95 ğ‘ ğ‘‡ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}=%
\qty[mode=math]{95}{} italic_c = italic_T = [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 ). Stronger colour intensity indicates statistical significance. Report issue for preceding element Figure 6: Pearson correlation between the memorisation profile of different models. Report issue for preceding element Residual Memorisation. Report issue for preceding element Residual memorisation estimates (defined in Â§ 3 as Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT when c = T ğ‘ ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}={%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_c = italic_T ) are depicted as the final-column entries in Fig. 2 , and are also presented in Fig. 5 (we consider T ğ‘‡ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T} italic_T to be the end of the first epoch here, i.e., timestep \qty [mode=math]95).
Interestingly, while all macro-batches undergo some degree of instantaneous memorisation, it appears that many are then forgotten by the end of the first epoch, as shown by the statistically insignificant residual memorisation estimates.
Furthermore, in line with our persistent memorisation results, residual memorisation shows a recency effect: the final macro-batches are the most memorised.
We hypothesise that this recency effect can be explained by the learning rate schedule.
Specifically, when the learning rate is high, the optimisation process moves model parameters further in the locally optimal direction, thus â€œoverwritingâ€ previous information with new information; this results in higher instantaneous and lower residual memorisation.
On the contrary, towards the end of the training process, when the learning rate is lower, previous information is â€œforgottenâ€ less as the updates are smaller (in expectation), resulting in higher residual and lower instantaneous memorisation. Report issue for preceding element Memorisation Across Scales. Report issue for preceding element Due to the cost of training large LMs, it is highly desirable to be able to make predictions about a trained modelâ€™s characteristics before undertaking training. One strategy is to derive insights from smaller models to inform the design of larger ones (Rae etÂ al., 2021 ; Black etÂ al., 2022 ; LeÂ Scao etÂ al., 2022 ) . 19 19 19 Scaling laws for other notions of memorisation ( Â§ 5 ) have been studied in Biderman etÂ al. ( 2023a ) . Predictability across scales is visually apparent in Fig. 3 and 4 where there are similar trends across model sizes.
We formalise this intuition in Fig. 6 , where we report the Pearson correlation between the memorisation profiles of different models.
Interestingly, memorisation for larger models (e.g., \qty [mode=math]12) is predictable from smaller ones (e.g., \qty [mode=math]410).
We note that \qty [mode=math]70 and \qty [mode=math]160 are less predictive of the memorisation in \qty [mode=math]12.
However, prior work has shown that both these models suffer from training instability (Godey etÂ al., 2024 ) ; the reduction in predictability with these smaller models might thus be specific to the Pythia suite. Report issue for preceding element 8 Conclusions Report issue for preceding element The memorisation of training data by neural networks has critical implications for privacy, copyright, and security. Thus, well-founded quantifications of memorisation, and corresponding accurate and efficient methods for their estimation are of great importance.
This work presents one such quantification and builds on the econometrics literature to derive an unbiased and efficient estimator of memorisation based on the difference-in-differences design.
We use this estimator to study the memorisation profiles of the Pythia model suite and find that memorisation is stronger and more persistent in larger models, determined by data order and learning rate, and stable across model sizes. Report issue for preceding element Limitations Report issue for preceding element This work estimates counterfactual memorisation in pretrained LMs.
Unfortunately, due to the costs associated with running large pretrained LMsâ€”even in inference modeâ€”we experimented with a limited number of models (the Pythia suite) trained in a single language (English).
Investigating whether other model architectures, training procedures, and natural languages result in similar memorisation profiles would be important to strengthen our conclusions.
Furthermore, when collecting the panel data needed to estimate memorisation, we subsampled the number of evaluated instances; this can significantly increase our estimatorsâ€™ variance. Report issue for preceding element Acknowledgements Report issue for preceding element Report issue for preceding element Pietro and Andreas received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 Research and Innovation programme grant AVeriTeC (Grant agreement No. 865958).
Clara is funded by a Google PhD Fellowship.
We thank the anonymous reviewers, for their helpful questions and comments that helped us improve the paper.
We also thank Machel Reid for early discussions on the topic;
Sidak Pal Singh, Gregor Bachmann, and Ornella Darova for their feedback on earlier versions of this paper; and
Davide Lesci and Marco Lesci for proofreading the final version of the paper. Report issue for preceding element References Report issue for preceding element Anagnostidis etÂ al. (2023) â†‘ Sotiris Anagnostidis, Gregor Bachmann, Lorenzo Noci, and Thomas Hofmann. 2023. The curious case of benign memorization . In The Eleventh International Conference on Learning Representations . Angrist and Pischke (2015) â†‘ Joshua Angrist and JÃ¶rn-Steffen Pischke. 2015. Mastering â€™Metrics: The Path from Cause to Effect . Princeton University Press. Arpit etÂ al. (2017) â†‘ Devansh Arpit, StanisÅ‚aw Jastrzundefinedbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, MaxinderÂ S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017. A closer look at memorization in deep networks . In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICMLâ€™17, page 233â€“242. JMLR.org. Bachmann etÂ al. (2022) â†‘ Gregor Bachmann, Thomas Hofmann, and Aurelien Lucchi. 2022. Generalization through the lens of leave-one-out error . In International Conference on Learning Representations . Bae etÂ al. (2022) â†‘ Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and RogerÂ B. Grosse. 2022. If influence functions are the answer, then what is the question? Advances in Neural Information Processing Systems , 35:17953â€“17967. Basu etÂ al. (2020) â†‘ Samyadeep Basu, Phil Pope, and Soheil Feizi. 2020. Influence functions in deep learning are fragile . In International Conference on Learning Representations . Biderman etÂ al. (2022) â†‘ Stella Biderman, Kieran Bicheno, and Leo Gao. 2022. Datasheet for the Pile . arXiv preprint 2201.07311 . Biderman etÂ al. (2023a) â†‘ Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023a. Emergent and predictable memorization in large language models . Advances in Neural Information Processing Systems , 36:28072â€“28090. Biderman etÂ al. (2023b) â†‘ Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, MohammadÂ Aflah Khan, Shivanshu Purohit, USVSNÂ Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van DerÂ Wal. 2023b. Pythia: A suite for analyzing large language models across training and scaling . In Proceedings of the 40th International Conference on Machine Learning , ICMLâ€™23. Black etÂ al. (2022) â†‘ Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, UsvsnÂ Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open-source autoregressive language model . In Proceedings of BigScience Episode #5 â€“ Workshop on Challenges & Perspectives in Creating Large Language Models , pages 95â€“136, virtual+Dublin. Association for Computational Linguistics. Callaway and Santâ€™Anna (2021) â†‘ Brantly Callaway and Pedro H.Â C. Santâ€™Anna. 2021. Difference-in-Differences with multiple time periods . Journal of Econometrics , 225(2):200â€“230. Cao etÂ al. (2022) â†‘ Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. 2022. Benign overfitting in two-layer convolutional neural networks . Advances in Neural Information Processing Systems , 35:25237â€“25250. Carlini etÂ al. (2023) â†‘ Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models . In The Eleventh International Conference on Learning Representations . Carlini etÂ al. (2019) â†‘ Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The Secret Sharer: Evaluating and testing unintended memorization in neural networks . In Proceedings of the 28th USENIX Conference on Security Symposium , SECâ€™19, pages 267â€“â€“284, USA. USENIX Association. Carlini etÂ al. (2021) â†‘ Nicholas Carlini, Florian TramÃ¨r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ãšlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models . In 30th USENIX Security Symposium (USENIX Security 21) , pages 2633â€“2650. USENIX Association. Chang and Bergen (2024) â†‘ TylerÂ A. Chang and BenjaminÂ K. Bergen. 2024. Language model behavior: A comprehensive survey . Computational Linguistics , pages 1â€“58. Cohen (1992) â†‘ Jacob Cohen. 1992. Statistical power analysis . Current Directions in Psychological Science , 1(3):98â€“101. Cole and Frangakis (2009) â†‘ StephenÂ R. Cole and ConstantineÂ E. Frangakis. 2009. The consistency statement in causal inference: A definition or an assumption? Epidemiology , 20(1). Cook and Weisberg (1980) â†‘ R.Â Dennis Cook and Sanford Weisberg. 1980. Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression . Technometrics , 22(4):495â€“508. Dey etÂ al. (2023) â†‘ Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open compute-optimal language models trained on the cerebras wafer-scale cluster . arXiv preprint 2304.03208 . Duchi etÂ al. (2011) â†‘ John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization . Journal of Machine Learning Research , 12(61):2121â€“2159. Feldman (2020) â†‘ Vitaly Feldman. 2020. Does learning require memorization? A short tale about a long tail . In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , STOC 2020, pages 954â€“959, New York, NY, USA. Association for Computing Machinery. Feldman and Zhang (2020) â†‘ Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation . In Advances in Neural Information Processing Systems , volumeÂ 33, pages 2881â€“2891. Curran Associates, Inc. Gao etÂ al. (2020) â†‘ Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling . arXiv preprint 2101.00027 . Godey etÂ al. (2024) â†‘ Nathan Godey, Ã‰ric de la Clergerie, and BenoÃ®t Sagot. 2024. Why do small language models underperform? Studying language model saturation via the softmax bottleneck . arXiv preprint 2404.07647 . Hampel (1974) â†‘ FrankÂ R. Hampel. 1974. The influence curve and its role in robust estimation . Journal of the American Statistical Association , 69(346):383â€“393. Hartmann etÂ al. (2023) â†‘ Valentin Hartmann, Anshuman Suri, Vincent Bindschaedler, David Evans, Shruti Tople, and Robert West. 2023. SoK: Memorization in general-purpose large language models . arXiv preprint 2310.18362 . Holland (1986) â†‘ PaulÂ W. Holland. 1986. Statistics and causal inference . Journal of the American Statistical Association , 81(396):945â€“960. Hu etÂ al. (2022) â†‘ Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, PhilipÂ S. Yu, and Xuyun Zhang. 2022. Membership inference attacks on machine learning: A survey . ACM Comput. Surv. , 54(11s). Ibeling and Icard (2023) â†‘ Duligur Ibeling and Thomas Icard. 2023. Comparing causal frameworks: Potential outcomes, structural models, graphs, and abstractions . Advances in Neural Information Processing Systems , 36:80130â€“80141. Ilyas etÂ al. (2022) â†‘ Andrew Ilyas, SungÂ Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. 2022. Datamodels: Understanding predictions with data and data with predictions . In Proceedings of the 39th International Conference on Machine Learning , pages 9525â€“9587. PMLR. Imbens and Rubin (2015) â†‘ GuidoÂ W. Imbens and DonaldÂ B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction . Cambridge University Press. Ishihara (2023) â†‘ Shotaro Ishihara. 2023. Training data extraction from pre-trained language models: A survey . In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023) , pages 260â€“275, Toronto, Canada. Association for Computational Linguistics. Jiang etÂ al. (2023) â†‘ AlbertÂ Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lioÂ Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, TevenÂ Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and WilliamÂ El Sayed. 2023. Mistral 7B . arXiv preprint 2310.06825 . Kandpal etÂ al. (2022) â†‘ Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates privacy risks in language models . In International Conference on Machine Learning , pages 10697â€“10707. PMLR. Kiela etÂ al. (2021) â†‘ Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4110â€“4124, Online. Association for Computational Linguistics. Kingma and Ba (2015) â†‘ DiederikÂ P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization . In 3rd International Conference on Learning Representations . Koh and Liang (2017) â†‘ PangÂ Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions . In Proceedings of the 34th International Conference on Machine Learning , pages 1885â€“1894. PMLR. LeÂ Scao etÂ al. (2022) â†‘ Teven LeÂ Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, MÂ Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, ZhengÂ Xin Yong, Julien Launay, and IzÂ Beltagy. 2022. What language model to train if you have one million GPU hours? In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 765â€“782, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Lee etÂ al. (2023) â†‘ Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. 2023. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023 , WWW â€™23, page 3637â€“3647, New York, NY, USA. Association for Computing Machinery. Lee etÂ al. (2022) â†‘ Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8424â€“8445, Dublin, Ireland. Association for Computational Linguistics. Lin etÂ al. (2022) â†‘ Jinkun Lin, Anqi Zhang, Mathias LÃ©cuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. 2022. Measuring the effect of training data on deep learning predictions via randomized experiments . In Proceedings of the 39th International Conference on Machine Learning , pages 13468â€“13504. PMLR. Lu etÂ al. (2018) â†‘ Min Lu, Saad Sadiq, DanielÂ J Feaster, and Hemant Ishwaran. 2018. Estimating individual treatment effect in observational data using random forest methods . Journal of Computational and Graphical Statistics , 27(1):209â€“219. Lukasik etÂ al. (2024) â†‘ Michal Lukasik, Vaishnavh Nagarajan, AnkitÂ Singh Rawat, AdityaÂ Krishna Menon, and Sanjiv Kumar. 2024. What do larger image classifiers memorise? Transactions on Machine Learning Research . Maini etÂ al. (2023) â†‘ Pratyush Maini, MichaelÂ C. Mozer, Hanie Sedghi, ZacharyÂ C. Lipton, J.Â Zico Kolter, and Chiyuan Zhang. 2023. Can neural network memorization be localized? In Proceedings of the 40th International Conference on Machine Learning , ICMLâ€™23. JMLR.org. McCloskey (2016) â†‘ DonaldÂ N. McCloskey. 2016. Counterfactuals , pages 1â€“5. Palgrave Macmillan UK, London. Park etÂ al. (2023) â†‘ SungÂ Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. TRAK: Attributing model behavior at scale . In Proceedings of the 40th International Conference on Machine Learning , pages 27074â€“27113. PMLR. Paszke etÂ al. (2019) â†‘ Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, LuÂ Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library . In Advances in Neural Information Processing Systems , volumeÂ 32. Curran Associates, Inc. Pearl (2009) â†‘ Judea Pearl. 2009. Causality: Models, Reasoning and Inference , 2nd edition. Cambridge University Press, USA. Penedo etÂ al. (2023) â†‘ Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web data, and web data only . arXiv preprint 2306.01116 . Rae etÂ al. (2021) â†‘ JackÂ W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, etÂ al. 2021. Scaling language models: Methods, analysis & insights from training gopher . arXiv preprint 2112.11446 . Raffel etÂ al. (2020) â†‘ Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer . Journal of Machine Learning Research , 21(140):1â€“67. Roth etÂ al. (2023) â†‘ Jonathan Roth, Pedro H.Â C. Santâ€™Anna, Alyssa Bilinski, and John Poe. 2023. Whatâ€™s trending in difference-in-differences? A synthesis of the recent econometrics literature . Journal of Econometrics , 235(2):2218â€“2244. Rubin (1974) â†‘ DonaldÂ B. Rubin. 1974. Estimating causal effects of treatments in randomized and nonrandomized studies . Journal of Educational Psychology , 66(5):688â€“701. Rubin (2005) â†‘ DonaldÂ B. Rubin. 2005. Causal inference using potential outcomes . Journal of the American Statistical Association , 100(469):322â€“331. Schioppa etÂ al. (2023) â†‘ Andrea Schioppa, Katja Filippova, Ivan Titov, and Polina Zablotskaia. 2023. Theoretical and practical perspectives on what influence functions do . Advances in Neural Information Processing Systems , 36:27560â€“27581. Splawa-Neyman (1923) â†‘ Jerzy Splawa-Neyman. 1923. On the application of probability theory to agricultural experiments. Essay on principles. Annals of Agricultural Sciences , pages 1â€“51. Stoehr etÂ al. (2024) â†‘ Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. 2024. Localizing paragraph memorization in language models . arxiv preprint 2403.19851 . Tirumala etÂ al. (2022) â†‘ Kushal Tirumala, AramÂ H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization without overfitting: Analyzing the training dynamics of large language models . In Advances in Neural Information Processing Systems . Touvron etÂ al. (2023) â†‘ Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models . arXiv preprint 2307.09288 . Vyas etÂ al. (2023) â†‘ Nikhil Vyas, ShamÂ M. Kakade, and Boaz Barak. 2023. On provable copyright protection for generative models . In International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 35277â€“35299. PMLR. Wolf etÂ al. (2020) â†‘ Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven LeÂ Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38â€“45, Online. Association for Computational Linguistics. Zhang etÂ al. (2023) â†‘ Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian TramÃ¨r, and Nicholas Carlini. 2023. Counterfactual memorization in neural language models . In Thirty-seventh Conference on Neural Information Processing Systems . Zheng and Jiang (2022) â†‘ Xiaosen Zheng and Jing Jiang. 2022. An empirical study of memorization in NLP . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6265â€“6278, Dublin, Ireland. Association for Computational Linguistics. Appendix A Proofs Report issue for preceding element A.1 Difference Estimand and Estimator Report issue for preceding element Lemma 1 (Identification of the Difference Estimand) . Report issue for preceding element The difference estimand, defined in eq. 6 , identifies expected counterfactual memorisation (i.e., the causal estimand in eq. 3 ) under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element For this proof, we start with the definition of the difference estimand and, via algebraic manipulation, show its equality to expected counterfactual memorisation: Report issue for preceding element Ï„ g , c ğšğš’ğšğš superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğšğš \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (16a) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] absent subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] By Assump. 1 (16b) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] absent subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] By linearity of expectations (16c) = Ï„ g , c absent subscript ğœ ğ‘” ğ‘ \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}} = italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT (16d) This completes the proof.
âˆ Report issue for preceding element Lemma 2 (Unbiasedness of the Difference Estimator) . Report issue for preceding element The difference estimator, defined in Estimator 1 , is an unbiased estimator of expected counterfactual memorisation Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assump. 1 . Report issue for preceding element Proof. Report issue for preceding element To prove this estimator is unbiased, let us first define the probability of sampling a batch â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT : Report issue for preceding element p â¢ ( â„¬ g ) = âˆ ğ’™ âˆˆ â„¬ g p â¢ ( ğ’™ âˆ£ G â¢ ( ğ’™ ) = g ) ğ‘ subscript â„¬ ğ‘” subscript product ğ’™ subscript â„¬ ğ‘” ğ‘ conditional ğ’™ ğº ğ’™ ğ‘” \displaystyle p(\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})=\prod_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}p({\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) italic_p ( caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) = âˆ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_x âˆ£ italic_G ( bold_italic_x ) = italic_g ) (17) Taking the expectation of our estimator with respect to the batches used for its estimation we see that: Report issue for preceding element ğ”¼ â„¬ g , â„¬ âˆ [ Ï„ ^ g , c ğšğš’ğšğš ] subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğšğš \displaystyle\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\widehat{%
\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^%
{\mathtt{diff}}\right] blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT ] = ğ”¼ â„¬ g , â„¬ âˆ [ Y Â¯ c â¢ ( g ) âˆ’ Y Â¯ c â¢ ( âˆ ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘ \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\overline%
{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25%
}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-%
\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) ] (18a) = ğ”¼ â„¬ g , â„¬ âˆ [ 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g Y c â¢ ( ğ’™ ) âˆ’ 1 | â„¬ âˆ | â¢ âˆ‘ ğ’™ âˆˆ â„¬ âˆ Y c â¢ ( ğ’™ ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ 1 subscript â„¬ subscript ğ’™ subscript â„¬ subscript ğ‘Œ ğ‘ ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{%
|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in%
\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] (18b) = ğ”¼ â„¬ g [ 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g Y c â¢ ( ğ’™ ) ] âˆ’ ğ”¼ â„¬ âˆ [ 1 | â„¬ âˆ | â¢ âˆ‘ ğ’™ âˆˆ â„¬ âˆ Y c â¢ ( ğ’™ ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ subscript ğ”¼ subscript â„¬ 1 subscript â„¬ subscript ğ’™ subscript â„¬ subscript ğ‘Œ ğ‘ ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\left[\frac{1}{|\mathcal{B}%
_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_%
{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right]-\operatorname*{%
\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{|\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] - blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ] (18c) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (18d) = Ï„ g , c ğšğš’ğšğš absent superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğšğš \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}^{\mathtt{diff}} = italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT (18e) where eq. 18c follows because the sampling of â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and â„¬ âˆ subscript â„¬ \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty} caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT are independent; eq. 18d holds due to eq. 17 and the unbiasedness of Monte Carlo estimators.
We can now invoke Lemma 1 , which states that Ï„ g , c ğšğš’ğšğš superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğšğš \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{%
\mathtt{diff}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT identifies Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under the i.i.d.Â assumption ( Assump. 1 ).
Thus, we have that the expected value of our estimator is equal to Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT , finishing the proof.
âˆ Report issue for preceding element A.2 Difference-in-Differences Estimand and Estimator Report issue for preceding element Lemma 3 (Identification of the Difference-in-Differences Estimand) . Report issue for preceding element The DiD estimand, defined in eq. 10 , identifies expected counterfactual memorisation (i.e., the causal estimand in eq. 3 ) under Assumps. 2 and 3 for all c â‰¥ g ğ‘ ğ‘” {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\geq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g} italic_c â‰¥ italic_g . Report issue for preceding element Proof. Report issue for preceding element To prove this lemma, we first note that by the no anticipations assumption ( Assump. 3 ): Report issue for preceding element ğ”¼ ğ’™ [ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] = ğ”¼ ğ’™ [ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” ğº ğ’™ ğ‘” \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\left[Y_{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\right]=\operatorname*{%
\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\left[Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\right] blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (19) Furthermore, by the parallel trends assumption ( Assump. 2 ) and linearity of expectations: Report issue for preceding element ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y c â€² â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y c â€² â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ superscript ğ‘ â€² ğ’™ ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ superscript ğ‘ â€² ğ’™ ğº ğ’™ \displaystyle\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]}=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}%
{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\big{]} blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (20) âŸ¹ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ] = ğ”¼ ğ’™ [ Y c â€² â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = g ] âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y c â€² â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ] absent subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ âˆ£ ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ subscript ğ‘Œ superscript ğ‘ â€² ğ’™ âˆ£ ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ superscript ğ‘ â€² ğ’™ âˆ£ ğº ğ’™ \displaystyle\,\,\implies\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y%
_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}=\operatorname%
*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{%
\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{\mathbb{E}}_{{\color[rgb]%
{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c^{\prime}}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{=}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} âŸ¹ blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] As in Lemma 1 , we now start with the definition of the DiD estimand and, via algebraic manipulation, show its equivalence to expected counterfactual memorisation: Report issue for preceding element Ï„ g , c ğšğš’ğš superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğš \displaystyle\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT = ğ”¼ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent ğ”¼ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” ğº ğ’™ ğ‘” ğ”¼ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor%
}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (21a) = ğ”¼ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ [ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âŸ no anticipation âˆ’ ğ”¼ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent ğ”¼ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript âŸ ğ”¼ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” ğº ğ’™ ğ‘” no anticipation ğ”¼ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\underbrace{\operatorname*{\mathbb{E}}%
\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}}_{\text{{\color[rgb]%
{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}%
\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5} no anticipation}}}-%
\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]%
{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - underâŸ start_ARG blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] end_ARG start_POSTSUBSCRIPT no anticipation end_POSTSUBSCRIPT - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (21b) = ğ”¼ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ [ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] âŸ parallel trends absent ğ”¼ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript âŸ ğ”¼ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ ğ‘” ğ”¼ subscript ğ‘Œ ğ‘ ğ’™ conditional subscript ğ‘Œ ğ‘” 1 ğ’™ ğº ğ’™ parallel trends \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\!\mid\!G({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\underbrace{\operatorname*{\mathbb{E}}%
\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0%
}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0%
}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-\operatorname*{%
\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor%
}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\!\mid\!G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]}}_{\text{{\color%
[rgb]{.5,.5,.5}\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}%
\pgfsys@color@gray@stroke{.5}\pgfsys@color@gray@fill{.5} parallel trends}}} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - underâŸ start_ARG blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] end_ARG start_POSTSUBSCRIPT parallel trends end_POSTSUBSCRIPT (21c) = ğ”¼ [ Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] absent ğ”¼ conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” ğ”¼ conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\mid G({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}\big{]}-\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (21d) = ğ”¼ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y c â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = g ] absent ğ”¼ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” conditional subscript ğ‘Œ ğ‘ ğ’™ ğº ğ’™ ğ‘” \displaystyle\quad=\operatorname*{\mathbb{E}}\big{[}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]} = blackboard_E [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] (21e) = Ï„ g , c absent subscript ğœ ğ‘” ğ‘ \displaystyle\quad=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}} = italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT (21f) where we replace the terms in eqs. 21b and 21c using their equivalences given in eq. 19 and eq. 20 , respectively.
This completes the proof.
We note that a similar proof is available in Lemma A.1 in Callaway and Santâ€™Anna ( 2021 ) .
âˆ Report issue for preceding element Lemma 4 (Unbiasedness of the Difference-in-Differences Estimator) . Report issue for preceding element The difference-in-differences estimator, defined in Estimator 2 , is an unbiased estimator of expected counterfactual memorisation Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 . Report issue for preceding element Proof. Report issue for preceding element We can follow the same logic as in Lemma 2 because the same properties hold (i.e., the sampling of â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and â„¬ âˆ subscript â„¬ \mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty} caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT are independent,
the joint probability of a set is the product of the probability of sampling individual instances, and the unbiasedness of Monte Carlo estimators). We then arrive at the following equivalence: Report issue for preceding element ğ”¼ â„¬ g , â„¬ âˆ [ Ï„ ^ g , c ğšğš’ğš ] subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğš \displaystyle\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\!\!\!\!\left[%
\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}}\right] blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ] = ğ”¼ â„¬ g , â„¬ âˆ [ Y Â¯ c â¢ ( g ) âˆ’ Y Â¯ g âˆ’ 1 â¢ ( g ) âˆ’ Y Â¯ c â¢ ( âˆ ) + Y Â¯ g âˆ’ 1 â¢ ( âˆ ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘” 1 ğ‘” subscript Â¯ ğ‘Œ ğ‘ subscript Â¯ ğ‘Œ ğ‘” 1 \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\bigg{[}%
\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g})-\overline{Y}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})+\overline{Y}_{{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\bigg{]} = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) + overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( âˆ ) ] (22a) = ğ”¼ â„¬ g , â„¬ âˆ [ 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g ( Y c â¢ ( ğ’™ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ) ) âˆ’ 1 | â„¬ âˆ | â¢ âˆ‘ ğ’™ âˆˆ â„¬ âˆ ( Y c â¢ ( ğ’™ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ) ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” subscript â„¬ 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ ğ‘” 1 ğ’™ 1 subscript â„¬ subscript ğ’™ subscript â„¬ subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ ğ‘” 1 ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\mathcal{B}_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\left[\frac{1}{%
|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]%
{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\Big{(}Y_{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\Big{)}-\frac{1}{|\mathcal%
{B}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty%
}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\Big{(}Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\Big{)}\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) - divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] (22b) = ğ”¼ â„¬ g [ 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g ( Y c â¢ ( ğ’™ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ) ) ] âˆ’ ğ”¼ â„¬ âˆ [ 1 | â„¬ âˆ | â¢ âˆ‘ ğ’™ âˆˆ â„¬ âˆ ( Y c â¢ ( ğ’™ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ) ) ] absent subscript ğ”¼ subscript â„¬ ğ‘” 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ ğ‘” 1 ğ’™ subscript ğ”¼ subscript â„¬ 1 subscript â„¬ subscript ğ’™ subscript â„¬ subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ ğ‘” 1 ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\mathcal{B}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\left[\frac{1}{|\mathcal{B}%
_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}|}\sum_%
{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\Big{)}\right]-\operatorname*{\mathbb{E}}_{\mathcal{B}_{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}%
\left[\frac{1}{|\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}|}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}\Big{(}Y_%
{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}%
({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\Big{)}\right] = blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] - blackboard_E start_POSTSUBSCRIPT caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ) ) ] (22c) = ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; g ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) â¢ âˆ£ G â¢ ( ğ’™ ) = g ] âˆ’ ğ”¼ ğ’™ [ Y c â¢ ( ğ’™ ; âˆ ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ ğ‘” subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” âˆ£ ğº ğ’™ ğ‘” subscript ğ”¼ ğ’™ subscript ğ‘Œ ğ‘ ğ’™ subscript ğ‘Œ ğ‘” 1 ğ’™ âˆ£ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb%
]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb%
]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{%
rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\!=\!{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\big{]}-%
\operatorname*{\mathbb{E}}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}}\big{[}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})-Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\!=\!{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty}\big{]} = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ] - blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (22d) = Ï„ g , c ğšğš’ğš absent superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğš \displaystyle=\tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}%
{.75,0,.25}c}}^{\mathtt{did}} = italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT (22e) Finally, we invoke Lemma 3 which proves that Ï„ g , c ğšğš’ğš superscript subscript ğœ ğ‘” ğ‘ ğšğš’ğš \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{%
\mathtt{did}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT identifies Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT under Assumps. 2 and 3 .
âˆ Report issue for preceding element A.3 Variances of Estimators Report issue for preceding element We assume that all potential outcomes Y c â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘ ğ’™ ğ‘” Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) have the same variance Ïƒ 2 superscript ğœ 2 \sigma^{2} italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .
We now first look at the variance of the difference estimator.
To this end, letâ€™s consider the variance of Y Â¯ c â¢ ( g ) subscript Â¯ ğ‘Œ ğ‘ ğ‘” \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) : Report issue for preceding element Var â¢ ( Y Â¯ c â¢ ( g ) ) Var subscript Â¯ ğ‘Œ ğ‘ ğ‘” \displaystyle\mathrm{Var}\Big{(}\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)} roman_Var ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) ) = Var â¢ ( 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g Y c â¢ ( ğ’™ ) ) = 1 | â„¬ g | 2 â¢ âˆ‘ ğ’™ âˆˆ â„¬ g Var â¢ ( Y c â¢ ( ğ’™ ; g ) âˆ£ G â¢ ( ğ’™ ) = g ) = | â„¬ g | â¢ Ïƒ 2 | â„¬ g | 2 = Ïƒ 2 | â„¬ g | absent Var 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ 1 superscript subscript â„¬ ğ‘” 2 subscript ğ’™ subscript â„¬ ğ‘” Var conditional subscript ğ‘Œ ğ‘ ğ’™ ğ‘” ğº ğ’™ ğ‘” subscript â„¬ ğ‘” superscript ğœ 2 superscript subscript â„¬ ğ‘” 2 superscript ğœ 2 subscript â„¬ ğ‘” \displaystyle=\mathrm{Var}\left(\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right)=\frac{1}{|\mathcal%
{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|^%
{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\mathrm{Var}\Big{(}Y_{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\Big{)}=\frac{|\mathcal{B}_{%
{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|\,%
\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}=\frac{\sigma^{2}}{|\mathcal{B}_{{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|} = roman_Var ( divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) ) = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Var ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) âˆ£ italic_G ( bold_italic_x ) = italic_g ) = divide start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = divide start_ARG italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG (23) This is simply the variance of estimating an expectation using the mean of | â„¬ g | subscript â„¬ ğ‘” |\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}| | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | i.i.d. random variables, each with variance Ïƒ 2 superscript ğœ 2 \sigma^{2} italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We can similarly derive the variance of Y Â¯ c â¢ ( âˆ ) subscript Â¯ ğ‘Œ ğ‘ \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) .
The variance of Ï„ ^ g , c ğšğš’ğšğš superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğšğš \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT is then: Report issue for preceding element Var â¢ ( Ï„ ^ g , c ğšğš’ğšğš ) = Ïƒ 2 | â„¬ g | + Ïƒ 2 | â„¬ âˆ | âˆ’ 2 â¢ Cov â¢ ( Y Â¯ c â¢ ( g ) , Y Â¯ c â¢ ( âˆ ) ) Var superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğšğš superscript ğœ 2 subscript â„¬ ğ‘” superscript ğœ 2 subscript â„¬ 2 Cov subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘ \mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{diff}})=\frac{\sigma^{2}}{|%
\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}|}+\frac{\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}-2\,\mathrm{Cov}(\overline{Y}_{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}),%
\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty})) roman_Var ( over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT ) = divide start_ARG italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG + divide start_ARG italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG - 2 roman_Cov ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) , overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) ) (24) Assuming batches â„¬ g subscript â„¬ ğ‘” \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}} caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and â„¬ âˆ subscript â„¬ \mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}} caligraphic_B start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT were drawn independently,
then the estimators Y Â¯ c â¢ ( g ) subscript Â¯ ğ‘Œ ğ‘ ğ‘” \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) and Y Â¯ c â¢ ( âˆ ) subscript Â¯ ğ‘Œ ğ‘ \overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) should also be independent.
Thus, Cov â¢ ( Y Â¯ c â¢ ( g ) , Y Â¯ c â¢ ( âˆ ) ) = 0 Cov subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘ 0 \mathrm{Cov}(\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}),\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}))=0 roman_Cov ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) , overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) ) = 0 . Report issue for preceding element We now look at the variance of the difference-in-differences estimator.
Let the correlation between Y c â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘ ğ’™ ğ‘” Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) and Y g âˆ’ 1 â¢ ( ğ’™ ; g ) subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}) italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) be Ï ğœŒ \rho italic_Ï .
These are, respectively, the potential outcomes of our model on a specific instance ğ’™ ğ’™ {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x before and after training on it.
For shorthand, let Î” â¢ Y Â¯ g = Y Â¯ c â¢ ( g ) âˆ’ Y Â¯ g âˆ’ 1 â¢ ( g ) Î” subscript Â¯ ğ‘Œ ğ‘” subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘” 1 ğ‘” \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g}=\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) and Î” â¢ Y Â¯ âˆ = Y Â¯ c â¢ ( âˆ ) âˆ’ Y Â¯ g âˆ’ 1 â¢ ( âˆ ) Î” subscript Â¯ ğ‘Œ subscript Â¯ ğ‘Œ ğ‘ subscript Â¯ ğ‘Œ ğ‘” 1 \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty}=\overline{Y}_{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})-\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT = overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( âˆ ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( âˆ ) .
We can show that: Report issue for preceding element Var â¢ ( Î” â¢ Y Â¯ g ) Var Î” subscript Â¯ ğ‘Œ ğ‘” \displaystyle\mathrm{Var}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g}) roman_Var ( roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) = Var â¢ ( Y Â¯ c â¢ ( g ) âˆ’ Y Â¯ g âˆ’ 1 â¢ ( g ) ) absent Var subscript Â¯ ğ‘Œ ğ‘ ğ‘” subscript Â¯ ğ‘Œ ğ‘” 1 ğ‘” \displaystyle=\mathrm{Var}\Big{(}\overline{Y}_{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}({\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-\overline{Y}_{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}({\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)} = roman_Var ( overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_g ) - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( italic_g ) ) (25a) = Var â¢ ( 1 | â„¬ g | â¢ âˆ‘ ğ’™ âˆˆ â„¬ g ( Y c â¢ ( ğ’™ ; g ) âˆ’ Y g âˆ’ 1 â¢ ( ğ’™ ; g ) ) ) absent Var 1 subscript â„¬ ğ‘” subscript ğ’™ subscript â„¬ ğ‘” subscript ğ‘Œ ğ‘ ğ’™ ğ‘” subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” \displaystyle=\mathrm{Var}\left(\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}{\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})-Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}}\right) = roman_Var ( divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) - italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ) ) (25b) = 1 | â„¬ g | 2 â¢ âˆ‘ ğ’™ âˆˆ â„¬ g ( Ïƒ 2 + Ïƒ 2 âˆ’ 2 â¢ Cov â¢ ( Y c â¢ ( ğ’™ ; g ) , Y g âˆ’ 1 â¢ ( ğ’™ ; g ) ) ) absent 1 superscript subscript â„¬ ğ‘” 2 subscript ğ’™ subscript â„¬ ğ‘” superscript ğœ 2 superscript ğœ 2 2 Cov subscript ğ‘Œ ğ‘ ğ’™ ğ‘” subscript ğ‘Œ ğ‘” 1 ğ’™ ğ‘” \displaystyle=\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}\bigg{(}\sigma%
^{2}+\sigma^{2}-2\,\mathrm{Cov}\Big{(}Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}),Y_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}-1}}({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[%
rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g})\Big{)}\bigg{)} = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 2 roman_Cov ( italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) , italic_Y start_POSTSUBSCRIPT italic_g - 1 end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) ) ) (25c) = 1 | â„¬ g | 2 â¢ âˆ‘ ğ’™ âˆˆ â„¬ g ( 2 â¢ Ïƒ 2 âˆ’ 2 â¢ Ï â¢ Ïƒ 2 ) = 2 â¢ Ïƒ 2 | â„¬ g | â¢ ( 1 âˆ’ Ï ) absent 1 superscript subscript â„¬ ğ‘” 2 subscript ğ’™ subscript â„¬ ğ‘” 2 superscript ğœ 2 2 ğœŒ superscript ğœ 2 2 superscript ğœ 2 subscript â„¬ ğ‘” 1 ğœŒ \displaystyle=\frac{1}{|\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|^{2}}\sum_{{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}\in\mathcal{B}_{\color%
[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}{(2\sigma^{2}-%
2\rho\sigma^{2})}=\frac{2\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}(1-\rho) = divide start_ARG 1 end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_x âˆˆ caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( 2 italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 2 italic_Ï italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = divide start_ARG 2 italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_Ï ) (25d) We can derive the variance for Î” â¢ Y Â¯ âˆ Î” subscript Â¯ ğ‘Œ \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty} roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT in the exact same manner.
We thus have that: Report issue for preceding element Var â¢ ( Ï„ ^ g , c ğšğš’ğš ) = Var â¢ ( Î” â¢ Y Â¯ g ) + Var â¢ ( Î” â¢ Y Â¯ âˆ ) âˆ’ 2 â¢ C â¢ o â¢ v â¢ ( Î” â¢ Y Â¯ g , Î” â¢ Y Â¯ âˆ ) Var superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğš Var Î” subscript Â¯ ğ‘Œ ğ‘” Var Î” subscript Â¯ ğ‘Œ 2 C o v Î” subscript Â¯ ğ‘Œ ğ‘” Î” subscript Â¯ ğ‘Œ \displaystyle\mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}})=\mathrm{Var}(\Delta%
\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g})+\mathrm{Var}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})-2\mathrm{Cov}(\Delta\overline{Y}_{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},\Delta%
\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}\infty}) roman_Var ( over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ) = roman_Var ( roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) + roman_Var ( roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT ) - 2 roman_C roman_o roman_v ( roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT ) (26) Note that Î” â¢ Y Â¯ g Î” subscript Â¯ ğ‘Œ ğ‘” \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}g} roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Î” â¢ Y Â¯ âˆ Î” subscript Â¯ ğ‘Œ \Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb%
}{1,.5,0}\infty} roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT are estimated with independent samples, and thus, Cov â¢ ( Î” â¢ Y Â¯ g , Î” â¢ Y Â¯ âˆ ) = 0 Cov Î” subscript Â¯ ğ‘Œ ğ‘” Î” subscript Â¯ ğ‘Œ 0 \mathrm{Cov}(\Delta\overline{Y}_{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g},\Delta\overline{Y}_{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=0 roman_Cov ( roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Î” overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT ) = 0 .
We can thus rewrite this estimatorâ€™s variance as: Report issue for preceding element Var â¢ ( Ï„ ^ g , c ğšğš’ğš ) = 2 â¢ Ïƒ 2 | â„¬ g | â¢ ( 1 âˆ’ Ï g ) + 2 â¢ Ïƒ 2 | â„¬ g | â¢ ( 1 âˆ’ Ï âˆ ) Var superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğš 2 superscript ğœ 2 subscript â„¬ ğ‘” 1 subscript ğœŒ ğ‘” 2 superscript ğœ 2 subscript â„¬ ğ‘” 1 subscript ğœŒ \displaystyle\mathrm{Var}(\widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{did}})=\frac{2\sigma^{2}}{|%
\mathcal{B}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g}}|}(1-\rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g})+\frac{2\sigma^{2}}{|\mathcal{B}_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}(1-\rho_{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) roman_Var ( over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT ) = divide start_ARG 2 italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_Ï start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) + divide start_ARG 2 italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_B start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG ( 1 - italic_Ï start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT ) (27) If we have Ï g > 0.5 subscript ğœŒ ğ‘” 0.5 \rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}>0.5 italic_Ï start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT > 0.5 and Ï âˆ > 0.5 subscript ğœŒ 0.5 \rho_{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}>0.5 italic_Ï start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT > 0.5 , then the variance of Ï„ ^ g , c ğšğš’ğš superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğš \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{did}} over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_did end_POSTSUPERSCRIPT should be lower than that of the Ï„ ^ g , c ğšğš’ğšğš superscript subscript ^ ğœ ğ‘” ğ‘ ğšğš’ğšğš \widehat{\tau}_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
1,.5,0}g},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}^{\mathtt{diff}} over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_diff end_POSTSUPERSCRIPT .
This is a reasonable assumption sinceâ€”for fixed timesteps g â¢ - 1 ğ‘” 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{-%
}1} italic_g - 1 and c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c â€”there should be a strong relationship between a modelâ€™s performance on an instance before ( g â¢ - 1 ğ‘” 1 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\mathop{-%
}1} italic_g - 1 ) and after ( c ğ‘ {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c} italic_c ) it has been trained on due to factors such as vocabulary richness and grammatical structure. Report issue for preceding element Appendix B Statistical Estimands and Estimators in Prior Work Report issue for preceding element In this section, we formalise prior worksâ€™ estimators of memorisation using our formalisation of counterfactual memorisation. Report issue for preceding element B.1 Architectural Counterfactual Memorisation Report issue for preceding element In this section, we describe one potential estimator for architectural counterfactual memorisation Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT (in Defn. 4 ).
First, we need the following assumption in order to identify the causal estimand for this quantity. Report issue for preceding element Assumption 4 (Negligible training effect) . Report issue for preceding element In expectation, the effect of having a specific instance in the training set is negligible on any validation instance. That is, for any two instances ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x and ğ± â€² superscript ğ± â€² {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime} bold_italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT : Report issue for preceding element ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ â€² ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ = âˆ ] = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ â€² ; âˆ ) â¢ âˆ£ G â¢ ( ğ’™ ) â¢ â‰  âˆ ] subscript ğ”¼ ğ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ superscript ğ’™ â€² âˆ£ ğº ğ’™ subscript ğ”¼ ğ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ superscript ğ’™ â€² âˆ£ ğº ğ’™ \displaystyle\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{%
=}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}%
\right]=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color%
[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}^{\prime};{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}\infty})\mathop{\mid}G({\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\mathop{%
\neq}{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}\right] blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ; âˆ ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] (28) Given this assumption, we can identify the following statistical estimand for Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT : Report issue for preceding element Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ superscript subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] âˆ’ ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ ğº ğ’™ subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (29) We now define the architectural estimator, associated with this statistical estimand. Report issue for preceding element Estimator 3 . Report issue for preceding element The architectural estimator , defined as: 20 20 20 We note that prior work has proposed more efficient estimators of the above ( Bachmann etÂ al. , 2022 ; Lin etÂ al. , 2022 ; Ilyas etÂ al. , 2022 ; Park etÂ al. , 2023 ) . However, these estimators remain computationally expensive for large LMs. Report issue for preceding element Ï„ ^ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ = 1 | Î˜ g | â¢ âˆ‘ ğœ½ âˆˆ Î˜ g Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) âˆ’ 1 | Î˜ âˆ | â¢ âˆ‘ ğœ½ âˆˆ Î˜ âˆ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) superscript subscript ^ ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ 1 subscript Î˜ ğ‘” subscript ğœ½ subscript Î˜ ğ‘” subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ 1 subscript Î˜ subscript ğœ½ subscript Î˜ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}}=\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}}Y_{\scaleto{{\color%
[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]%
{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_{\scaleto{%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{%
4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (30) is an unbiased estimator of Ï„ ğ± , \scaleto â¢ p â¢ ( ğ›™ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ± \scaleto ğ‘ ğ›™ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT under Assump. 4 .
In this equation, Î˜ g subscript Î˜ ğ‘” \Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}} roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Î˜ âˆ subscript Î˜ \Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
\infty}} roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT are sets of model parameters trained independently with or without ğ± ğ± {\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}} bold_italic_x in the training set. Report issue for preceding element Proof. Report issue for preceding element First, we prove that the statistical estimand Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ superscript subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}}^{\mathtt{arch}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT identifies Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT : Report issue for preceding element Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ superscript subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ \displaystyle\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] âˆ’ ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ ğº ğ’™ subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (31a) = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] âˆ’ ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] absent subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ ğº ğ’™ subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] By Assump. 4 (31b) = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) âˆ’ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] absent subscript ğ”¼ ğ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))-Y_{\scaleto{{\color[rgb]%
{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})\mid G({\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) - italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] Linearity of expectations (31c) = Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t absent subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ \displaystyle=\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt}} = italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT (31d) We now prove the estimator above is unbiased: Report issue for preceding element Ï„ ^ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ superscript subscript ^ ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT = ğ”¼ Î˜ g , Î˜ âˆ [ 1 | Î˜ g | â¢ âˆ‘ ğœ½ âˆˆ Î˜ g Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) âˆ’ 1 | Î˜ âˆ | â¢ âˆ‘ ğœ½ âˆˆ Î˜ âˆ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ] absent subscript ğ”¼ subscript Î˜ ğ‘” subscript Î˜ 1 subscript Î˜ ğ‘” subscript ğœ½ subscript Î˜ ğ‘” subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ 1 subscript Î˜ subscript ğœ½ subscript Î˜ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}},\Theta_{{\color[rgb]{%
1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}\left[\frac{1}%
{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}%
g}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{1,.5,0}g}}}Y_{\scaleto{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})-\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]%
{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{\color[rgb%
]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_{\scaleto{%
{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{%
4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] (32a) = ğ”¼ Î˜ g [ 1 | Î˜ g | â¢ âˆ‘ ğœ½ âˆˆ Î˜ g Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ] âˆ’ ğ”¼ Î˜ âˆ [ 1 | Î˜ âˆ | â¢ âˆ‘ ğœ½ âˆˆ Î˜ âˆ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ] absent subscript ğ”¼ subscript Î˜ ğ‘” 1 subscript Î˜ ğ‘” subscript ğœ½ subscript Î˜ ğ‘” subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ subscript ğ”¼ subscript Î˜ 1 subscript Î˜ subscript ğœ½ subscript Î˜ subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}}\left[\frac{1}{|\Theta_{{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}}|}\sum_{%
\bm{\theta}\in\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}g}}}Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right]-\operatorname%
*{\mathbb{E}}_{\Theta_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{1,.5,0}\infty}}}\left[\frac{1}{|\Theta_{{\color[rgb]{1,.5,0}\definecolor%
[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}|}\sum_{\bm{\theta}\in\Theta_{{%
\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}}}Y_%
{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})\right] = blackboard_E start_POSTSUBSCRIPT roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] - blackboard_E start_POSTSUBSCRIPT roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT bold_italic_Î¸ âˆˆ roman_Î˜ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ] (32b) = ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; G â¢ ( ğ’™ ) ) âˆ£ G â¢ ( ğ’™ ) â‰  âˆ ] âˆ’ ğ”¼ ğ [ Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) âˆ£ G â¢ ( ğ’™ ) = âˆ ] absent subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ ğº ğ’™ subscript ğ”¼ ğ conditional subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğº ğ’™ \displaystyle=\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{\scaleto{{\color[%
rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};G({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}))\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})\neq{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{1,.5,0}\infty}\right]-\operatorname*{\mathbb{E}}_{\bm{\psi}}\left[Y_{%
\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})\mid G({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}})={\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}\right] = blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; italic_G ( bold_italic_x ) ) âˆ£ italic_G ( bold_italic_x ) â‰  âˆ ] - blackboard_E start_POSTSUBSCRIPT bold_italic_Ïˆ end_POSTSUBSCRIPT [ italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) âˆ£ italic_G ( bold_italic_x ) = âˆ ] (32c) = Ï„ ğ’™ , \scaleto â¢ p â¢ ( ğ ) â¢ 6 â¢ p â¢ t ğšŠğš›ğšŒğš‘ absent superscript subscript ğœ ğ’™ \scaleto ğ‘ ğ 6 ğ‘ ğ‘¡ ğšŠğš›ğšŒğš‘ \displaystyle=\tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{p(\bm{\psi})}{6pt%
}}^{\mathtt{arch}} = italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_p ( bold_italic_Ïˆ ) 6 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_arch end_POSTSUPERSCRIPT (32d) This completes the proof.
âˆ Report issue for preceding element B.2 Influence Functions Report issue for preceding element As mentioned in Â§ 5.2 , influence functions approximate ğœ½ âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT using a first-order Taylor expansion of the training objective around ğœ½ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
This should lead to small errors under the following assumptions:
(i) the loss function is strictly convex in ğœ½ ğœ½ \bm{\theta} bold_italic_Î¸ , (ii) H ğœ½ subscript H ğœ½ \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT is a positive-definite matrix, and (iii) the model has converged (Koh and Liang, 2017 ) .
We make these assumptions explicit now. Report issue for preceding element Assumption 5 (Strict Convexity) . Report issue for preceding element The loss function â„’ â„’ \mathcal{L} caligraphic_L is strictly convex with respect to the parameters ğ›‰ ğ›‰ \bm{\theta} bold_italic_Î¸ . Report issue for preceding element Assumption 6 (Local Optimality) . Report issue for preceding element The parameters ğ›‰ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğ›‰ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT locally minimise the loss function â„’ â„’ \mathcal{L} caligraphic_L , meaning that the H ğ›‰ subscript H ğ›‰ \mathrm{H}_{\bm{\theta}} roman_H start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT is a positive-definite matrix and that gradient of the loss with respect to the parameters ğ›‰ \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğ›‰ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT is zero. Report issue for preceding element Given these assumptions, we can estimate the counterfactual term in Ï„ ğ’™ , c subscript ğœ ğ’™ ğ‘ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT (in eq. 2 ) by computing the performance using the updated parameters ğœ½ âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ½ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \bm{\theta}_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} bold_italic_Î¸ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT .
As mentioned in the main text, we thus define Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) = Î³ â¢ ( ğœ½ âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t ; ğ’™ ) subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ ğ›¾ subscript ğœ½ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\gamma(\bm{\theta}_{%
-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}};{\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) = italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ; bold_italic_x ) and equate Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) = Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty})=Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) . Report issue for preceding element Estimator 4 . Report issue for preceding element The influence function estimator , defined as: Report issue for preceding element Ï„ ^ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t ğš’ğš—ğšğš• = Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) âˆ’ Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) superscript subscript ^ ğœ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğš’ğš—ğšğš• subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ \displaystyle\hat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{%
.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}^{%
\mathtt{infl}}=Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})-Y_{-{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_infl end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) - italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) (33) is an unbiased estimator of Ï„ ğ± , \scaleto â¢ T â¢ 4 â¢ p â¢ t subscript ğœ ğ± \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT under Assumps. 5 and 6 . Report issue for preceding element Proof. Report issue for preceding element See Cook and Weisberg ( 1980 ) or Koh and Liang ( 2017 ) for derivations of how Y âˆ’ ğ’™ , \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) subscript ğ‘Œ ğ’™ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{-{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Y start_POSTSUBSCRIPT - bold_italic_x , italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) approximates the counterfactual Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) under the assumptions above. The estimator then follows trivially from replacing Y \scaleto â¢ T â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ \scaleto ğ‘‡ 4 ğ‘ ğ‘¡ ğ’™ Y_{\scaleto{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}T}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_T 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) in eq. 2 .
âˆ Report issue for preceding element B.3 Extractable Memorisation Report issue for preceding element As mentioned in Â§ 5.3 , extractable memorisation assumes zero-valued counterfactual performances Y c â¢ ( ğ’™ ; âˆ ) = 0 subscript ğ‘Œ ğ‘ ğ’™ 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})=0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = 0 .
We formalise this assumption, and the associated statistical estimand and estimator in this section. Report issue for preceding element Assumption 7 (Negligible counterfactual) . Report issue for preceding element In the absence of training, performance on a string should be zero: Y c â¢ ( ğ± ; âˆ ) = 0 subscript ğ‘Œ ğ‘ ğ± 0 Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty})=0 italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) = 0 . Report issue for preceding element Given this assumption, we can trivially identify counterfactual memorisation as being equivalent to the statistical estimand: Ï„ ğ’™ , c ğšğš¡ğšğš› = Y c â¢ ( ğ’™ ; g ) superscript subscript ğœ ğ’™ ğ‘ ğšğš¡ğšğš› subscript ğ‘Œ ğ‘ ğ’™ ğ‘” \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{\color[rgb]{.75,0,.25}\definecolor[%
named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}) italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; italic_g ) .
We can now define the ( k , â„“ ) ğ‘˜ â„“ (k,\!\ell) ( italic_k , roman_â„“ ) -extractable memorisation estimator under our framework. Report issue for preceding element Estimator 5 . Report issue for preceding element The ( ğ¤ , â„“ ) ğ¤ bold-â„“ \bm{(k,\ell)} bold_( bold_italic_k bold_, bold_â„“ bold_) -extractable memorisation estimator, defined as: Report issue for preceding element Ï„ ^ ğ’™ , c ğšğš¡ğšğš› = Y c â¢ ( ğ’™ ) superscript subscript ^ ğœ ğ’™ ğ‘ ğšğš¡ğšğš› subscript ğ‘Œ ğ‘ ğ’™ \displaystyle\widehat{\tau}_{{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}^{\mathtt{extr}}=Y_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}({%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}) over^ start_ARG italic_Ï„ end_ARG start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_extr end_POSTSUPERSCRIPT = italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ) (34) is an unbiased estimator of Ï„ ğ± , c subscript ğœ ğ± ğ‘ \tau_{{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}},{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT bold_italic_x , italic_c end_POSTSUBSCRIPT under Assump. 7 . Report issue for preceding element Proof. Report issue for preceding element This follows trivially from replacing Y c â¢ ( ğ’™ ; âˆ ) subscript ğ‘Œ ğ‘ ğ’™ Y_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c%
}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}};{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{1,.5,0}\infty}) italic_Y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_italic_x ; âˆ ) with 0 in eq. 2 .
âˆ Report issue for preceding element Appendix C Implementation Details Report issue for preceding element We implement all experiments using the PyTorch framework (Paszke etÂ al., 2019 ) .
We use the Pythia models as available through the transformers library (Wolf etÂ al., 2020 ) .
For a consistent evaluation between scales, we load every model using bfloat16 precision, which is needed for the larger versions. We control randomness using CUDA deterministic operations and seeding the pseudo-random number generators at every level of the stack and for each multi-processing worker.
We use the implementation of the Callaway and Santâ€™Anna ( 2021 ) estimator as available in the differences library 21 21 21 github.com/bernardodionisi/differences . . Report issue for preceding element C.1 The Pythia Suite Report issue for preceding element We use the publicly available Pythia model suite (Biderman etÂ al., 2023b ) , which was trained on the Pile (Gao etÂ al., 2020 ; Biderman etÂ al., 2022 ) . Both the preprocessed training data and intermediate checkpoints are publicly available. 22 22 22 github.com/EleutherAI/pythia . Report issue for preceding element Data. Report issue for preceding element The Pile is a \qty [mode=math]300-token curated collection of English documents.
The deduplicated version of the dataset is obtained by applying a near-deduplication method based on MinHashLSH and has \qty [mode=math]207 tokens.
Before being used for training, the dataset is shuffled, tokenised, and â€œpackedâ€ into sequences of 2 , 049 2049 2,049 2 , 049 tokens with no end-of-document token. 23 23 23 github.com/EleutherAI/pythia/issues/123 . By design, each sequence can pack multiple documents and tokens can attend across document boundaries.
Noticeably, the packing process implies that the second half-epoch of deduplicated data contains the same documents but not necessarily the same sequences.
There does not exist an official validation set for Pythia models. However, we confirmed with the authors that the original Pile validation set has not been used for training. Report issue for preceding element Models. Report issue for preceding element The Pythia model suite is composed of 16 models: transformers of 8 8 8 8 different sizes trained on the Pile as-is or deduplicated.
All model sizes were trained using a cosine learning rate schedule with warm-up, the same data order, and a batch size of 1 , 024 1024 1,024 1 , 024 sequences, resulting in exactly \qty [mode=math]143 optimisation steps.
The final \qty [mode=math]48 optimisation steps correspond to the second half-epoch.
Thus, we focus on model checkpoints at initialisation (step 0 0 ), and after every \qty [mode=math]1 iterations (steps \qty [mode=math]1- \qty [mode=math]95) resulting in 96 96 96 96 checkpoints evenly spaced throughout training. For completeness, we report the second half-epoch (steps \qty [mode=math]96- \qty [mode=math]143) analysis in App. D . Additionally, log-spaced checkpoints are available for timesteps early in training (timesteps c âˆˆ { 2 i } i = 0 9 ğ‘ superscript subscript superscript 2 ğ‘– ğ‘– 0 9 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{2^{i}\}_{i=0}^{9} italic_c âˆˆ { 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT ).
We do not consider them to obtain evaluations at evenly spaced intervals.
We use all available model sizes, that is, \qty [mode=math]70, \qty [mode=math]160, \qty [mode=math]410, \qty [mode=math]1.4, \qty [mode=math]6.9, and \qty [mode=math]12, except \qty [mode=math]2.8. We exclude \qty [mode=math]2.8 from the results since we found a potential mismatch between the available checkpoints and the data order used during training. Report issue for preceding element C.2 Hardware Details Report issue for preceding element We use a server with one NVIDIA A100 80GB PCIe , 32 32 32 32 CPUs, and 32 32 32 32 GB of RAM for all experiments. Below, we report a subset of the output of the lscpu command: Report issue for preceding element Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Address sizes:       46 bits physical,
                     48 bits virtual
Byte Order:          Little Endian
CPU(s):              32
On-line CPU(s) list: 0-31
Vendor ID:           GenuineIntel
Model name:          Intel(R) Xeon(R)
                     Silver 4210R CPU
                     @ 2.40GHz
CPU family:          6
Model:               85
Thread(s) per core:  1
Core(s) per socket:  1
Socket(s):           8
Stepping:            7
BogoMIPS:            4800.11 Report issue for preceding element Appendix D Additional Results Report issue for preceding element On the next page in Fig. 7 , we report the memorisation profiles obtained using other metrics besides sequence-level log-likelihood. Specifically, the average token-level accuracy given the true context and the average rank assigned by the model to the correct next token given the true context.
We report the results for the entire training processâ€”i.e., using all the available checkpoints: c âˆˆ { 0 , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 , â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 143 } ğ‘ 0 \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 143 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{$0$,\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_c âˆˆ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } and g âˆˆ { \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 , â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 143 } ğ‘” \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 143 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_g âˆˆ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } .
We present the metrics from most coarseâ€”i.e., average token accuracy ( Fig. 7a )â€”to most fine-grainedâ€”i.e., sequence log-likelihood ( Fig. 7c ). Report issue for preceding element As shown in Fig. 7 , different performance metrics result in distinct memorisation estimates. Specifically, finer-grained metricsâ€”like sequence log-likelihoodâ€”allow us to detect smaller memorisation effects, and vice versa.
For example, average token accuracy, which is the most coarse-grained metric, mostly does not capture instantaneous memorisation for Pythia \qty [mode=math]410. Instead, a finer-grained metricâ€”like average token rank or sequence log-likelihoodâ€”detects additional effects.
Depending on the use-case different metrics might be appropriate.
For example, analogously to extractable memorisation (Carlini etÂ al., 2021 ) , average token accuracy could be used to measure memorisation as it matches the specific use-case: detecting whether a model would generate a specific sequence when prompted with some of its tokens.
We chose sequence log-likelihood because it allows us to capture more fine-grained memorisation effects beyond the capability of the model to generate a specific sequence.
In particular, accuracy captures â€œhardâ€ transitions in the modelâ€™s output by determining whether a token is the most likely in the modelâ€™s output distribution.
Log-likelihood, on the other hand, captures more nuanced impacts of training on an instance by assessing whether a token is more likely to be generated than it would be otherwise. Report issue for preceding element (a) Average Token Accuracy: , Î³ â¢ ( ğœ½ c , ğ’™ ) = 1 | ğ’™ | â¢ âˆ‘ i = 1 | ğ’™ | ğŸ™ â¢ ( x ^ i = x i ) ğ›¾ subscript ğœ½ ğ‘ ğ’™ 1 ğ’™ superscript subscript ğ‘– 1 ğ’™ 1 subscript ^ ğ‘¥ ğ‘– subscript ğ‘¥ ğ‘– \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\frac{1}{{|{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\sum_{i=1}^{{|{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}|}}\mathbbm{1}(\hat{x}_{i}=%
{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}) italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG | bold_italic_x | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT blackboard_1 ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , where x ^ i = argmax x âˆˆ ğ’± p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( x âˆ£ ğ’™ < i ) subscript ^ ğ‘¥ ğ‘– subscript argmax ğ‘¥ ğ’± subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ conditional ğ‘¥ subscript ğ’™ absent ğ‘– \hat{x}_{i}=\operatorname*{argmax}_{x\in\mathcal{V}}p_{\scaleto{\bm{\theta}_{{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}}{%
4pt}}(x\mid{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{%
0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill%
{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_argmax start_POSTSUBSCRIPT italic_x âˆˆ caligraphic_V end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x âˆ£ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) is the predicted token at position i ğ‘– i italic_i computed using the correct previous tokens as context and | ğ’™ | ğ’™ {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element (b) Average Rank of the True Token: Î³ â¢ ( ğœ½ c , ğ’™ ) = 1 | ğ’™ | â¢ âˆ‘ i = 1 | ğ’™ | rank â¢ ( x i ) ğ›¾ subscript ğœ½ ğ‘ ğ’™ 1 ğ’™ superscript subscript ğ‘– 1 ğ’™ rank subscript ğ‘¥ ğ‘– \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\frac{1}{{|{\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\sum_{i=1}^{{|{\color[rgb]{0,0.88,0}\definecolor[named]{%
pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}|}}\mathrm{rank}({\color[%
rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}) italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG | bold_italic_x | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT roman_rank ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , where the function rank â¢ ( â‹… ) rank â‹… \mathrm{rank}(\cdot) roman_rank ( â‹… ) returns the rank of the true token at position i ğ‘– i italic_i computed from the probabilities assigned by the model using the correct previous tokens as context, i.e. p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( x âˆ£ ğ’™ < i ) subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ conditional ğ‘¥ subscript ğ’™ absent ğ‘– p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}(x\mid{\color[rgb]{0,0.88,0}%
\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.%
91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x âˆ£ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) , and | ğ’™ | ğ’™ {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element (c) Sequence Log-Likelihood: Î³ â¢ ( ğœ½ c , ğ’™ ) = log â¡ p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) ğ›¾ subscript ğœ½ ğ‘ ğ’™ subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ ğ’™ \gamma(\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}%
{rgb}{.75,0,.25}c}},{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{%
rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\log p_{\scaleto{\bm{%
\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{%
.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}%
{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}) italic_Î³ ( bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , bold_italic_x ) = roman_log italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) , where log â¡ p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( ğ’™ ) = âˆ‘ i = 1 | ğ’™ | log â¡ p \scaleto â¢ ğœ½ c â¢ 4 â¢ p â¢ t â¢ ( x i âˆ£ ğ’™ < i ) subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ ğ’™ superscript subscript ğ‘– 1 ğ’™ subscript ğ‘ \scaleto subscript ğœ½ ğ‘ 4 ğ‘ ğ‘¡ conditional subscript ğ‘¥ ğ‘– subscript ğ’™ absent ğ‘– \log p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}\definecolor[named]{%
pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}({\color[rgb]{0,0.88,0}\definecolor[%
named]{pgfstrokecolor}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{%
0.12}\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}})=\sum_{i=1}^{{|{%
\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|}}\log p_{\scaleto{\bm{\theta}_{{\color[rgb]{.75,0,.25}%
\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}}}{4pt}}({\color[rgb]{%
0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}x}_{i}\mid{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor%
}{rgb}{0,0.88,0}\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}%
\pgfsys@color@cmyk@fill{0.91}{0}{0.88}{0.12}\bm{x}}_{<i}) roman_log italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( bold_italic_x ) = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_italic_x | end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_Î¸ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT 4 italic_p italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ£ bold_italic_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) and | ğ’™ | ğ’™ {|{\color[rgb]{0,0.88,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.88,0}%
\pgfsys@color@cmyk@stroke{0.91}{0}{0.88}{0.12}\pgfsys@color@cmyk@fill{0.91}{0}%
{0.88}{0.12}\bm{x}}|} | bold_italic_x | is the number of tokens in the sequence. Report issue for preceding element Figure 7: Memorisation profiles ( Ï„ g , c subscript ğœ ğ‘” ğ‘ \tau_{{\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g},{%
\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}} italic_Ï„ start_POSTSUBSCRIPT italic_g , italic_c end_POSTSUBSCRIPT ) computed using different performance metrics Î³ ğ›¾ \gamma italic_Î³ using all the available checkpointsâ€”i.e., c âˆˆ { 0 , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 , â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 143 } ğ‘ 0 \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 143 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\in\{$0$,\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_c âˆˆ { 0 , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } and g âˆˆ { \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 1 , â€¦ , \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 143 } ğ‘” \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 1 â€¦ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 143 {\color[rgb]{1,.5,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,.5,0}g}\in\{%
\qty[mode=math]{1}{},...,\qty[mode=math]{143}{}\} italic_g âˆˆ { [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 1 , â€¦ , [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 143 } . The dashed vertical line indicates the end of the first epoch ( c â¢ = \qty â¢ [ m â¢ o â¢ d â¢ e = m â¢ a â¢ t â¢ h ] â¢ 95 ğ‘ \qty delimited-[] ğ‘š ğ‘œ ğ‘‘ ğ‘’ ğ‘š ğ‘ ğ‘¡ â„ 95 {\color[rgb]{.75,0,.25}\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}c}%
\mathop{=}\qty[mode=math]{95}{} italic_c = [ italic_m italic_o italic_d italic_e = italic_m italic_a italic_t italic_h ] 95 ). We only show statistically significant entries. Report issue for preceding element