{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab83692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-based rendering (IBR) methods rely on a set of two-\n",
      "dimensional images of a scene to generate a representation\n",
      "of the scene and render novel views. The very first novel-\n",
      "view synthesis approaches were based on light fields ,\n",
      "and developed the concept of volume rendering for novel\n",
      "views. Their work emphasized the importance of efficiently\n",
      "traversing volumetric data to produce realistic images.\n",
      "\n",
      "Various scene representations have been proposed since,\n",
      "such as triangle meshes, point clouds, voxel grids, multi-\n",
      "plane images, or neural implicit functions.\n",
      "\n",
      "Traditional mesh-based IBR methods. Structure-from-\n",
      "motion (SfM)\n",
      " and subsequent multi-view stereo\n",
      "(MVS)  allow for 3D reconstruction of surfaces, lead-\n",
      "ing to the development of several view synthesis algorithms\n",
      "relying on triangle meshes as the primary 3D representa-\n",
      "tion of scenes. Such algorithms consider textured triangles\n",
      "or warp and blend captured images on the mesh surface to\n",
      "generate novel views .\n",
      " consider deep\n",
      "learning-based mesh representations for better view synthe-\n",
      "sis, bridging the gap between traditional graphics and mod-\n",
      "ern machine learning techniques. While these mesh-based\n",
      "methods take advantage of existing graphics hardware and\n",
      "software for efficient rendering, they struggle with the cap-\n",
      "ture of accurate geometry and appearance in complex re-\n",
      "gions.\n",
      "\n",
      "Volumetric IBR methods. Volumetric methods use voxel\n",
      "grids, multiplane images, or neural networks to represent\n",
      "scenes as continuous volumetric functions of density and\n",
      "color. Recently, Neural Radiance Fields (NeRF)  intro-\n",
      "duced a novel scene representation based on a continuous\n",
      "volumetric function parameterized by a multilayer percep-\n",
      "tron (MLP). NeRF produces photorealistic renderings with\n",
      "fine details and view-dependent effects, achieved through\n",
      "volumetric ray tracing. However, the original NeRF is com-\n",
      "putationally expensive and memory intensive.\n",
      "\n",
      "To address these challenges, several works have im-\n",
      "proved NeRF’s performance and scalability. These meth-\n",
      "ods leverage discretized or sparse volumetric representa-\n",
      "tions like voxel grids and hash tables as ways to store\n",
      "learnable features acting as positional encodings for 3D\n",
      "points , hierarchical sampling strate-\n",
      "gies , or low-rank approximations . How-\n",
      "ever, they still rely on volumetric ray marching, which\n",
      "is incompatible with standard graphics hardware and soft-\n",
      "ware designed for rendering polygonal surfaces. Recent\n",
      "works have proposed modifying the NeRF’s representation\n",
      "of geometry and emitted radiance to allow for better recon-\n",
      "struction of specular materials  or relighting the scene\n",
      "through an explicit decomposition into material and lighting\n",
      "properties .\n",
      "\n",
      "Hybrid IBR methods. Some methods build on differen-\n",
      "tiable rendering to combine the advantages of mesh-based\n",
      "and volumetric methods, and allow for surface reconstruc-\n",
      "tion as well as better editability. They use a hybrid volume-\n",
      "surface representation, which enables high-quality meshes\n",
      "suitable for downstream graphics applications while effi-\n",
      "ciently modeling view-dependent appearance.\n",
      "In partic-\n",
      "ular, some works optimize neural signed distance func-\n",
      "tions (SDF) by training neural radiance fields in which the\n",
      "density is derived as a differentiable transformation of the\n",
      "SDF . A triangle mesh can finally\n",
      "be reconstructed from the SDF by applying the Marching\n",
      "Cubes algorithm . However, most of these methods do\n",
      "not target real-time rendering.\n",
      "\n",
      "Alternatively, other approaches “bake” the rendering ca-\n",
      "pacity of an optimized NeRF or neural SDF into a much ef-\n",
      "ficient structure relying on an underlying triangle mesh \n",
      "that could benefit from the traditional triangle rasteriza-\n",
      "tion pipeline. In particular, the recent BakedSDF  re-\n",
      "constructs high quality meshes by optimizing a full neural\n",
      "SDF model, baking it into a high-resolution triangle mesh\n",
      "that combines mesh rendering for interpolating features and\n",
      "deep learning to translate these features into images, and\n",
      "finally optimizes a view-dependent appearance model.\n",
      "\n",
      "However, even though it achieves real-time rendering\n",
      "and produces impressive meshes of the surface of the scene,\n",
      "this model demands training a full neural SDF with an ar-\n",
      "chitecture identical to Mip-NeRF360 , which necessi-\n",
      "tates 48 hours of training.\n",
      "\n",
      "Similarly, the recent method NeRFMeshing  pro-\n",
      "poses to also bake any NeRF model into a mesh structure,\n",
      "achieving real-time rendering. However, the meshing per-\n",
      "formed in this method lowers the quality of the rendering\n",
      "and results in a PSNR much lower than our method. Ad-\n",
      "ditionally, this method still requires training a full NeRF\n",
      "model beforehand, and needs approximately an hour of\n",
      "training on 8 V100 NVIDIA GPUs to allow for mesh train-\n",
      "ing and extraction.\n",
      "\n",
      "Our method is much faster at retrieveing a 3D mesh from\n",
      "3D Gaussian Splatting, which is itself much faster than\n",
      "NeRFs. As our experiments show, our rendering done by\n",
      "bounding Gaussians to the mesh results in higher quality\n",
      "than previous solutions based on meshes.\n",
      "\n",
      "Point-based IBR methods. Alternatively, point-based\n",
      "representations for radiance field excel at modeling thin ge-\n",
      "ometry and leverage fast point rasterization pipelines to ren-\n",
      "der images using α-blending rather than ray-marching .\n",
      "In particular, the very recent 3D Gaussian Splatting.\n",
      "model  allows for optimizing and rendering scenes with\n",
      "speed and quality never seen before.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(r'data\\output\\processed_full_citation\\no_citations\\sugar.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9997b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': [['Marc Levoy and Pat Hanrahan. Light Field Rendering. In ACM SIGGRAPH, 1996. 3'], ['Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo Tourism: Exploring Photo Collections in 3D. In ACM SIG- GRAPH, 2006. 3, 4'], ['Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven Seitz. Multi-View Stereo for Community Photo Collections. In International Conference on Computer Vision, 2007. 3'], ['Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, and Michael Cohen. Unstructured Lumigraph Ren- dering. In ACM SIGGRAPH, 2001. 3', 'Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep Blending for Free-Viewpoint Image-Based Rendering. In ACM SIG- GRAPH, 2018. 3, 7, 2, 4', 'Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Cur- less, Tom Duchamp, David H. Salesin, and Werner Stuet- zle. Surface Light Fields for 3D Photography. In ACM SIG- GRAPH, 2000. 3'], ['Gernot Riegler and Vladlen Koltun. Free View Synthesis. In European Conference on Computer Vision, 2020. 3', 'Gernot Riegler and Vladlen Koltun. Stable View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3'], ['Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View In European Conference on Computer Vision, Synthesis. 2020. 1, 3'], ['Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3', 'Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. ReLU Fields: The Little Non-Linearity That Could. In ACM SIGGRAPH, 2022. 3', 'Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant Neural Graphics Primitives with a Mul- tiresolution Hash Encoding. In ACM SIGGRAPH, 2022. 3, 7, 8, 2', 'Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance In Conference on Computer Vision Fields Reconstruction. and Pattern Recognition, 2022. 3', 'Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: In Conference Radiance Fields Without Neural Networks. on Computer Vision and Pattern Recognition, 2022. 3, 7, 8'], ['Jonathan T. Barron. Mip-NeRF 360: Unbounded Anti- Aliased Neural Radiance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3, 7, 8, 2, 4', 'Peter Hedman and Pratul P. Srinivasan. Baking Neural Radi- ance Fields for Real-Time View Synthesis. In International Conference on Computer Vision, 2021. 3', 'Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. KiloNeRF: Speeding Up Neural Radiance Fields with Thousands of Tiny MLPs. In International Conference on Computer Vision, 2021. 3', 'Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees For Real-Time Rendering of Neural Radiance Fields. In International Conference on Computer Vision, 2021. 3'], ['Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision, 2022. 3'], ['SDF'], '[3, 18, 33, 43]', '[7, 8, 20, 24, 36, 38]', ['Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar- ron, Ce Liu, and Hendrik P. A. Lensch. NeRD: Neural Re- flectance Decomposition from Image Collections. In Inter- national Conference on Computer Vision, 2021. 3', 'Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. NeROIC: Neural Rendering of Objects from Online Image Collections. In ACM SIGGRAPH, 2022. 3', 'Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T. Barron. NeRV: Neural Reflectance and Visibility Fields for Relight- ing and View Synthesis. In Conference on Computer Vision and Pattern Recognition, 2021. 3', 'Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse Rendering with Spherical Gaussians for Physics-Based Material Editing and Relight- ing. In Conference on Computer Vision and Pattern Recog- nition, 2021. 3'], ['Chong Bao and Bangbang Yang, Zeng Junyi, Bao Hu- jun, Zhang Yinda, Cui Zhaopeng, and Zhang Guofeng. NeuMesh: Learning Disentangled Neural Mesh-Based Im- plicit Field for Geometry and Texture Editing. In European Conference on Computer Vision, 2022. 4', 'Franc¸ois Darmon, B´en´edicte Bascle, Jean-Cl´ement Devaux, Pascal Monasse, and Mathieu Aubry. Improving Neural Im- plicit Surfaces Geometry with Patch Warping. In Conference on Computer Vision and Pattern Recognition, 2022. 4', 'Zhaoshuo Li, Thomas M¨uller, Alex Evans, Russell H. Tay- lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-Fidelity Neural Surface Reconstruction. In Conference on Computer Vision and Pattern Recognition, 2023. 2, 4', 'Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction. In International Con- ference on Computer Vision, 2021. 4', 'Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning Neural Im- plicit Surfaces by Volume Rendering for Multi-View Recon- In Advances in Neural Information Processing struction. Systems, 2021. 2, 4', 'Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol- ume Rendering of Neural Implicit Surfaces. In Advances in Neural Information Processing Systems, 2021. 2, 4'], ['William E. Lorensen and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. In ACM SIGGRAPH, 1987. 2, 4, 8 9 Structured View-Dependent Appearance for Neural Radi- ance Fields. In Conference on Computer Vision and Pattern Recognition, 2022. 3'], ['Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An- drea Tagliasacchi. MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. In Conference on Computer Vision and Pattern Recognition, 2023. 3, 4, 7, 8'], ['Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, and Jonathan T. Bar- ron. BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis. In ACM SIGGRAPH, 2023. 2, 3, 4, 7, 8'], ['Jonathan T. Barron. Mip-NeRF: A Multiscale Representa- tion for Anti-Aliasing Neural Radiance Fields. In Interna- tional Conference on Computer Vision, 2021. 4, 7'], ['Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and Federico Tombari. NeRFMeshing: Distilling Neural Radiance Fields into Geometrically-Accurate 3D Meshes. In DV, 2023. 2, 3, 4, 7, 8'], ['Georgios Kopanas, Julien Philip, Thomas Leimk¨uhler, and George Drettakis. Point-Based Neural Rendering with Per- View Optimization. In Computer Graphics Forum, 2021. 4', 'Darius R¨uckert, Linus Franke, and Marc Stamminger. ADOP: Approximate Differentiable One-Pixel Point Ren- dering. In ACM SIGGRAPH, 2022. 4'], ['Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¨uhler, and George Drettakis. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. In ACM SIGGRAPH, 2023. 1, 2, 4, 7, 8']], 'positions': [229, 628, 668, 971, 973, 1562, 2190, 2227, 2256, 2580, 1851, 2433, 2640, 3223, 3324, 3565, 3676, 4178, 4265, 5182, 5244]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r'data\\output\\processed_full_citation\\citations\\sugar.json', 'r') as f:\n",
    "    citations = json.load(f)\n",
    "print(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5be00d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "and developed the \n",
      "--------------------------------------------------\n",
      " and subsequent mult\n",
      "--------------------------------------------------\n",
      " allow for 3D recons\n",
      "--------------------------------------------------\n",
      ".\n",
      " consider deep\n",
      "lea\n",
      "--------------------------------------------------\n",
      " consider deep\n",
      "learn\n",
      "--------------------------------------------------\n",
      " intro-\n",
      "duced a nove\n",
      "--------------------------------------------------\n",
      ", hierarchical sampl\n",
      "--------------------------------------------------\n",
      ", or low-rank approx\n",
      "--------------------------------------------------\n",
      ". How-\n",
      "ever, they st\n",
      "--------------------------------------------------\n",
      " or relighting the s\n",
      "--------------------------------------------------\n",
      "is com-\n",
      "putationally\n",
      "--------------------------------------------------\n",
      "orks have proposed m\n",
      "--------------------------------------------------\n",
      "nto material and lig\n",
      "--------------------------------------------------\n",
      "erentiable transform\n",
      "--------------------------------------------------\n",
      "pplying the Marching\n",
      "--------------------------------------------------\n",
      "lying on an underlyi\n",
      "--------------------------------------------------\n",
      ". In particular, the\n",
      "--------------------------------------------------\n",
      "\n",
      "chitecture identica\n",
      "--------------------------------------------------\n",
      "larly, the recent me\n",
      "--------------------------------------------------\n",
      " α-blending rather t\n",
      "--------------------------------------------------\n",
      " recent 3D Gaussian \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "positions = citations['positions']\n",
    "for p in positions:\n",
    "    print(text[p:p+20])\n",
    "    print('-----'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79913754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_citation_info_from_html(html_path) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts citation information from a given HTML file.\n",
    "    Args:\n",
    "        html_path (str): Path to the HTML file.    \n",
    "    Returns:\n",
    "        dict: A dictionary containing citation information, where the keys are entry IDs and the values are dictionaries with citation details.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    biblist = soup.find(\"ul\", class_=\"ltx_biblist\")\n",
    "    bib_entries = {}\n",
    "\n",
    "    for li in biblist.find_all(\"li\", class_=\"ltx_bibitem\"):\n",
    "        entry_id = li.get(\"id\", \"\")\n",
    "        \n",
    "        tag_span = li.find(\"span\", class_=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\")\n",
    "        tag = tag_span.get_text(strip=True) if tag_span else \"\"\n",
    "        if tag[-1] == '↑': # Remove button text\n",
    "            tag = tag[:-1]\n",
    "\n",
    "        bibblocks = li.find_all(\"span\", class_=\"ltx_bibblock\")\n",
    "        authors = bibblocks[0].get_text(strip=True) if bibblocks else \"\"\n",
    "        \n",
    "        title = bibblocks[1].get_text(strip=True) if bibblocks and len(bibblocks) > 1 else \"\"\n",
    "        if title == \"\":\n",
    "            a_tag = li.find(\"a\", class_=\"ltx_ref\")\n",
    "            title = a_tag.get_text(strip=True) if a_tag else \"\"\n",
    "\n",
    "        journal = bibblocks[2].get_text(strip=True) if bibblocks and len(bibblocks) > 2 else \"\"\n",
    "\n",
    "        bib_entries[entry_id] = {\n",
    "            \"tag\": tag,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            'journal': journal\n",
    "        }\n",
    "    return bib_entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb983fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bib.bib1\n",
      "tag: Achiam et al. (2023)\n",
      "title: Gpt-4 technical report.\n",
      "authors: Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\n",
      "journal: arXiv preprint arXiv:2303.08774, 2023.\n",
      "--------------------------------------------------\n",
      "bib.bib2\n",
      "tag: Ascenso et al. (2023)\n",
      "title: The jpeg ai standard: Providing efficient human and machine visual data consumption.\n",
      "authors: Joao Ascenso, Elena Alshina, and Touradj Ebrahimi.\n",
      "journal: Ieee Multimedia, 30(1):100–111, 2023.\n",
      "--------------------------------------------------\n",
      "bib.bib3\n",
      "tag: Ballé et al. (2018)\n",
      "title: Variational image compression with a scale hyperprior.\n",
      "authors: Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston.\n",
      "journal: arXiv preprint arXiv:1802.01436, 2018.\n",
      "--------------------------------------------------\n",
      "bib.bib4\n",
      "tag: Bross et al. (2021)\n",
      "title: Overview of the versatile video coding (vvc) standard and its applications.\n",
      "authors: Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J Sullivan, and Jens-Rainer Ohm.\n",
      "journal: IEEE Transactions on Circuits and Systems for Video Technology, 31(10):3736–3764, 2021.\n",
      "--------------------------------------------------\n",
      "bib.bib5\n",
      "tag: Cha et al. (2024)\n",
      "title: Honeybee: Locality-enhanced projector for multimodal llm.\n",
      "authors: Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib6\n",
      "tag: Chamain et al. (2021)\n",
      "title: End-to-end optimized image compression for machines, a study.\n",
      "authors: Lahiru D Chamain, Fabien Racapé, Jean Bégaint, Akshay Pushparaja, and Simon Feltman.\n",
      "journal: In2021 Data Compression Conference (DCC), pp.  163–172. IEEE, 2021.\n",
      "--------------------------------------------------\n",
      "bib.bib7\n",
      "tag: Chen et al. (2023a)\n",
      "title: Shikra: Unleashing multimodal llm’s referential dialogue magic.\n",
      "authors: Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.\n",
      "journal: arXiv preprint arXiv:2306.15195, 2023a.\n",
      "--------------------------------------------------\n",
      "bib.bib8\n",
      "tag: Chen et al. (2023b)\n",
      "title: Transtic: Transferring transformer-based image compression from human perception to machine perception.\n",
      "authors: Yi-Hsin Chen, Ying-Chieh Weng, Chia-Hao Kao, Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng.\n",
      "journal: InProceedings of the IEEE/CVF International Conference on Computer Vision, pp.  23297–23307, 2023b.\n",
      "--------------------------------------------------\n",
      "bib.bib9\n",
      "tag: Chiang et al. (2023)\n",
      "title: Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n",
      "authors: Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n",
      "journal: See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.\n",
      "--------------------------------------------------\n",
      "bib.bib10\n",
      "tag: Deng et al. (2009)\n",
      "title: Imagenet: A large-scale hierarchical image database.\n",
      "authors: Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n",
      "journal: In2009 IEEE conference on computer vision and pattern recognition, pp.  248–255. Ieee, 2009.\n",
      "--------------------------------------------------\n",
      "bib.bib11\n",
      "tag: Ding et al. (2024)\n",
      "title: Hierarchical image feature compression for machines via feature sparsity learning.\n",
      "authors: Ding Ding, Zhenzhong Chen, Zizheng Liu, Xiaozhong Xu, and Shan Liu.\n",
      "journal: IEEE Signal Processing Letters, 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib12\n",
      "tag: Feng et al. (2022)\n",
      "title: Image coding for machines with omnipotent feature learning.\n",
      "authors: Ruoyu Feng, Xin Jin, Zongyu Guo, Runsen Feng, Yixin Gao, Tianyu He, Zhizheng Zhang, Simeng Sun, and Zhibo Chen.\n",
      "journal: InComputer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII, pp.  510–528. Springer, 2022.\n",
      "--------------------------------------------------\n",
      "bib.bib13\n",
      "tag: He et al. (2022)\n",
      "title: Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding.\n",
      "authors: Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  5718–5727, 2022.\n",
      "--------------------------------------------------\n",
      "bib.bib14\n",
      "tag: Jiang et al. (2023)\n",
      "title: Mistral 7b.\n",
      "authors: Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.\n",
      "journal: arXiv preprint arXiv:2310.06825, 2023.\n",
      "--------------------------------------------------\n",
      "bib.bib15\n",
      "tag: Karpathy & Fei-Fei (2015)\n",
      "title: Deep visual-semantic alignments for generating image descriptions.\n",
      "authors: Andrej Karpathy and Li Fei-Fei.\n",
      "journal: InProceedings of the IEEE conference on computer vision and pattern recognition, pp.  3128–3137, 2015.\n",
      "--------------------------------------------------\n",
      "bib.bib16\n",
      "tag: Kazemzadeh et al. (2014)\n",
      "title: Referitgame: Referring to objects in photographs of natural scenes.\n",
      "authors: Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.\n",
      "journal: InProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp.  787–798, 2014.\n",
      "--------------------------------------------------\n",
      "bib.bib17\n",
      "tag: (17)\n",
      "title: Kodak lossless true color image suite (PhotoCD PCD0992).\n",
      "authors: Eastman Kodak.\n",
      "journal: URLhttp://r0k.us/graphics/kodak.\n",
      "--------------------------------------------------\n",
      "bib.bib18\n",
      "tag: Le et al. (2021a)\n",
      "title: Image coding for machines: an end-to-end learned approach.\n",
      "authors: Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, and Esa Rahtu.\n",
      "journal: InICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.  1590–1594. IEEE, 2021a.\n",
      "--------------------------------------------------\n",
      "bib.bib19\n",
      "tag: Le et al. (2021b)\n",
      "title: Learned image coding for machines: A content-adaptive approach.\n",
      "authors: Nam Le, Honglei Zhang, Francesco Cricri, Ramin Ghaznavi-Youvalari, Hamed Rezazadegan Tavakoli, and Esa Rahtu.\n",
      "journal: In2021 IEEE International Conference on Multimedia and Expo (ICME), pp.  1–6. IEEE, 2021b.\n",
      "--------------------------------------------------\n",
      "bib.bib20\n",
      "tag: Li et al. (2023a)\n",
      "title: Seed-bench: Benchmarking multimodal llms with generative comprehension.\n",
      "authors: Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.\n",
      "journal: arXiv preprint arXiv:2307.16125, 2023a.\n",
      "--------------------------------------------------\n",
      "bib.bib21\n",
      "tag: Li et al. (2023b)\n",
      "title: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\n",
      "authors: Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n",
      "journal: InInternational conference on machine learning, pp.  19730–19742. PMLR, 2023b.\n",
      "--------------------------------------------------\n",
      "bib.bib22\n",
      "tag: Li et al. (2024)\n",
      "title: Tokenpacker: Efficient visual projector for multimodal llm.\n",
      "authors: Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang.\n",
      "journal: arXiv preprint arXiv:2407.02392, 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib23\n",
      "tag: Li et al. (2023c)\n",
      "title: Evaluating object hallucination in large vision-language models.\n",
      "authors: Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen.\n",
      "journal: InThe 2023 Conference on Empirical Methods in Natural Language Processing, 2023c.\n",
      "--------------------------------------------------\n",
      "bib.bib24\n",
      "tag: Lin et al. (2024)\n",
      "title: Vila: On pre-training for visual language models.\n",
      "authors: Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib25\n",
      "tag: Lin et al. (2014)\n",
      "title: Microsoft coco: Common objects in context.\n",
      "authors: Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.\n",
      "journal: InComputer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp.  740–755. Springer, 2014.\n",
      "--------------------------------------------------\n",
      "bib.bib26\n",
      "tag: Liu et al. (2023a)\n",
      "title: Visual instruction tuning.\n",
      "authors: Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\n",
      "journal: InConference on Neural Information Processing Systems (NeurIPS), 2023a.\n",
      "--------------------------------------------------\n",
      "bib.bib27\n",
      "tag: Liu et al. (2021)\n",
      "title: Learning in compressed domain for faster machine vision tasks.\n",
      "authors: Jinming Liu, Heming Sun, and Jiro Katto.\n",
      "journal: In2021 International Conference on Visual Communications and Image Processing (VCIP), pp.  01–05, 2021.\n",
      "--------------------------------------------------\n",
      "bib.bib28\n",
      "tag: Liu et al. (2022a)\n",
      "title: Improving multiple machine vision tasks in the compressed domain.\n",
      "authors: Jinming Liu, Heming Sun, and Jiro Katto.\n",
      "journal: In2022 26th International Conference on Pattern Recognition (ICPR), pp.  331–337. IEEE, 2022a.\n",
      "--------------------------------------------------\n",
      "bib.bib29\n",
      "tag: Liu et al. (2022b)\n",
      "title: Improving multiple machine vision tasks in the compressed domain.\n",
      "authors: Jinming Liu, Heming Sun, and Jiro Katto.\n",
      "journal: In2022 26th International Conference on Pattern Recognition (ICPR), pp.  331–337. IEEE, 2022b.\n",
      "--------------------------------------------------\n",
      "bib.bib30\n",
      "tag: Liu et al. (2023b)\n",
      "title: Learned image compression with mixed transformer-cnn architectures.\n",
      "authors: Jinming Liu, Heming Sun, and Jiro Katto.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  14388–14397, 2023b.\n",
      "--------------------------------------------------\n",
      "bib.bib31\n",
      "tag: Lu et al. (2022a)\n",
      "title: High-efficiency lossy image coding through adaptive neighborhood information aggregation.\n",
      "authors: Ming Lu, Fangdong Chen, Shiliang Pu, and Zhan Ma.\n",
      "journal: arXiv preprint arXiv:2204.11448, 2022a.\n",
      "--------------------------------------------------\n",
      "bib.bib32\n",
      "tag: Lu et al. (2022b)\n",
      "title: Transformer-based image compression.\n",
      "authors: Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma.\n",
      "journal: InData Compression Conference, 2022b.\n",
      "--------------------------------------------------\n",
      "bib.bib33\n",
      "tag: Matsubara et al. (2022)\n",
      "title: Supervised compression for resource-constrained edge computing systems.\n",
      "authors: Yoshitomo Matsubara, Ruihan Yang, Marco Levorato, and Stephan Mandt.\n",
      "journal: InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.  2685–2695, 2022.\n",
      "--------------------------------------------------\n",
      "bib.bib34\n",
      "tag: Mei et al. (2021)\n",
      "title: Learn a compression for objection detection - vae with a bridge.\n",
      "authors: Yixin Mei, Fan Li, Li Li, and Zhu Li.\n",
      "journal: In2021 International Conference on Visual Communications and Image Processing (VCIP), pp.  1–5, 2021.\n",
      "--------------------------------------------------\n",
      "bib.bib35\n",
      "tag: Mittal et al. (2024)\n",
      "title: Can’t make an omelette without breaking some eggs: Plausible action.\n",
      "authors: Himangi Mittal, Nakul Agarwal, Shao-Yuan Lo, and Kwonjoon Lee.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib36\n",
      "tag: Peng et al. (2024)\n",
      "title: Kosmos-2: Grounding multimodal large language models to the world.\n",
      "authors: Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.\n",
      "journal: InInternational Conference on Learning Representations (ICLR), 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib37\n",
      "tag: Radford et al. (2021)\n",
      "title: Learning transferable visual models from natural language supervision.\n",
      "authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n",
      "journal: InInternational conference on machine learning, pp.  8748–8763. PMLR, 2021.\n",
      "--------------------------------------------------\n",
      "bib.bib38\n",
      "tag: Ronneberger et al. (2015)\n",
      "title: U-net: Convolutional networks for biomedical image segmentation.\n",
      "authors: Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\n",
      "journal: InMedical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp.  234–241. Springer, 2015.\n",
      "--------------------------------------------------\n",
      "bib.bib39\n",
      "tag: Shi et al. (2024)\n",
      "title: Crossget: Cross-guided ensemble of tokens for accelerating vision-language transformers.\n",
      "authors: Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang.\n",
      "journal: InInternational Conference on Machine Learning (ICML), 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib40\n",
      "tag: Singh et al. (2020)\n",
      "title: End-to-end learning of compressible features.\n",
      "authors: Saurabh Singh, Sami Abu-El-Haija, Nick Johnston, Johannes Ballé, Abhinav Shrivastava, and George Toderici.\n",
      "journal: In2020 IEEE International Conference on Image Processing (ICIP), pp.  3349–3353. IEEE, 2020.\n",
      "--------------------------------------------------\n",
      "bib.bib41\n",
      "tag: Touvron et al. (2023a)\n",
      "title: Llama: Open and efficient foundation language models.\n",
      "authors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n",
      "journal: arXiv preprint arXiv:2302.13971, 2023a.\n",
      "--------------------------------------------------\n",
      "bib.bib42\n",
      "tag: Touvron et al. (2023b)\n",
      "title: Llama 2: Open foundation and fine-tuned chat models.\n",
      "authors: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n",
      "journal: arXiv preprint arXiv:2307.09288, 2023b.\n",
      "--------------------------------------------------\n",
      "bib.bib43\n",
      "tag: Wang et al. (2022)\n",
      "title: Deep image compression towards machine vision: A unified optimization framework.\n",
      "authors: Shurun Wang, Zhao Wang, Shiqi Wang, and Yan Ye.\n",
      "journal: IEEE Transactions on Circuits and Systems for Video Technology, 2022.\n",
      "--------------------------------------------------\n",
      "bib.bib44\n",
      "tag: Ye et al. (2024)\n",
      "title: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.\n",
      "authors: Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  13040–13051, 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib45\n",
      "tag: Yu et al. (2024)\n",
      "title: Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms.\n",
      "authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al.\n",
      "journal: Advances in Neural Information Processing Systems, 36, 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib46\n",
      "tag: Yuan et al. (2024)\n",
      "title: Osprey: Pixel understanding with visual instruction tuning.\n",
      "authors: Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  28202–28211, 2024.\n",
      "--------------------------------------------------\n",
      "bib.bib47\n",
      "tag: Zhang et al. (2024a)\n",
      "title: Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\n",
      "authors: Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao.\n",
      "journal: InInternational Conference on Learning Representations (ICLR), 2024a.\n",
      "--------------------------------------------------\n",
      "bib.bib48\n",
      "tag: Zhang et al. (2024b)\n",
      "title: Groundhog: Grounding large language models to holistic segmentation.\n",
      "authors: Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b.\n",
      "--------------------------------------------------\n",
      "bib.bib49\n",
      "tag: Zhu et al. (2024a)\n",
      "title: Minigpt-4: Enhancing vision-language understanding with advanced large language models.\n",
      "authors: Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\n",
      "journal: InInternational Conference on Learning Representations (ICLR), 2024a.\n",
      "--------------------------------------------------\n",
      "bib.bib50\n",
      "tag: Zhu et al. (2024b)\n",
      "title: Beyond text: Frozen large language models in visual signal comprehension.\n",
      "authors: Lei Zhu, Fangyun Wei, and Yanye Lu.\n",
      "journal: InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024b.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bib_entries = extract_citation_info_from_html(r\"data\\input\\ACL_papers\\html\\other\\Bridging Compressed Image Latents and Multimodal Large Language Models.html\")\n",
    "for k in bib_entries.keys():\n",
    "    print(k)\n",
    "    for sub_k, sub_v in bib_entries[k].items():\n",
    "        print(f\"{sub_k}: {sub_v}\")\n",
    "    print('-----'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a0bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_masked_text_and_citations(para, bib_entries):\n",
    "    # masked_parts = []\n",
    "    # citation_keys = set()\n",
    "\n",
    "    # for elem in para.descendants:\n",
    "    #     if isinstance(elem, str):\n",
    "    #         masked_parts.append(elem)\n",
    "    #     elif elem.name == \"cite\":\n",
    "    #         print(elem)\n",
    "    #         refs = elem.find_all(\"a\", class_=\"ltx_ref\")\n",
    "    #         for ref in refs:\n",
    "    #             href = ref.get(\"href\", \"\")\n",
    "    #             match = re.search(r\"#(bib\\.bib\\d+)\", href)\n",
    "    #             if match:\n",
    "    #                 key = match.group(1)\n",
    "    #                 if key in bib_entries:\n",
    "    #                     citation_keys.add(key)\n",
    "    #         masked_parts.append(\" [CITATION] \")\n",
    "    #     else:\n",
    "    #         # Append other tags as plain text (with fallback)\n",
    "    #         text = elem.get_text(\" \", strip=True)\n",
    "    #         masked_parts.append(f\"{text} \")\n",
    "\n",
    "    # masked_text = re.sub(r\"\\s+\", \" \", \"\".join(masked_parts)).strip()\n",
    "    # return masked_text, citation_keys\n",
    "    masked_parts = []\n",
    "    citation_keys = set()\n",
    "    \n",
    "    para_copy = BeautifulSoup(str(para), features=\"html.parser\")\n",
    "    \n",
    "    for cite in para_copy.find_all(\"cite\"):\n",
    "        refs = cite.find_all(\"a\", class_=\"ltx_ref\")\n",
    "        for ref in refs:\n",
    "            href = ref.get(\"href\", \"\")\n",
    "            match = re.search(r\"#(bib\\.bib\\d+)\", href)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                if key in bib_entries:\n",
    "                    citation_keys.add(key)\n",
    "        cite.replace_with(\" [CITATION]\")\n",
    "    \n",
    "    for elem in para_copy.descendants:\n",
    "        if isinstance(elem, str):\n",
    "            masked_parts.append(elem)\n",
    "    \n",
    "    masked_text = re.sub(r\"\\s+\", \" \", \"\".join(masked_parts)).strip()\n",
    "    return masked_text, citation_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f5256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Why are Sensitive Functions Hard for Transformers?', 'text': 'Given dramatic advances in machine learning applications powered by transformer models, there has been substantial interest in understanding which functions are easier or harder to learn and represent using transformers.\\nEmpirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking.\\nFor instance, Abbe et\\xa0al. ( 2023 ) experimentally argued that heldout generalization is biased towards low-degree polynomials and Bhattamishra et\\xa0al. ( 2023 ) provided empirical evidence that transformers prefer to represent functions of low sensitivity , that is, functions that do not strongly depend on many input bits.\\nPerhaps the most prominent example of such learning biases is a consistent difficulty in learning the PARITY function, mapping bitstrings to their parity.\\nThis function is extremely sensitive, in the sense that flipping any bit flips the string’s parity.\\nEmpirical studies have consistently found that training transformers to compute parities is difficult, and that solutions for shorter inputs do not generalize to longer inputs (e.g. Bhattamishra et\\xa0al., 2020 ; Chiang and Cholak, 2022 ; Delétang et\\xa0al., 2023 ; Ruoss et\\xa0al., 2023 ) .\\nThis stands in stark contrast to previously-popular reccurent models which easily fit PARITY with correct length generalization (Bhattamishra et\\xa0al., 2020 ) . Report issue for preceding element\\nWhile a substantial amount of theoetical work has considered both the learnability (e.g. Edelman et\\xa0al., 2022 ; Ahn et\\xa0al., 2023 ) and the expressiveness of transformers (e.g. Yun et\\xa0al., 2019 ; Hahn, 2020 ; Yao et\\xa0al., 2021 ; Hao et\\xa0al., 2022 ; Merrill et\\xa0al., 2022 ; Merrill and Sabharwal, 2023b ; Chiang et\\xa0al., 2023 ; Strobl et\\xa0al., 2023 ; Strobl, 2023 ; Angluin et\\xa0al., 2023 ) , existing theoretical studies do not consistently explain such learning biases. Hahn ( 2020 ) proved that, under two formal models of self-attention, no transformer can express PARITY at all input lengths.\\nHowever, various other formal results showed that slightly relaxed assumptions about the transformer architecture resolved such expressiveness limitations.\\nMost notably, Chiang and Cholak ( 2022 ) found that layer norm, by breaking the Lipschitz assumption used in Hahn ( 2020 ) ’s Theorem 2, allows expressing PARITY in principle.\\nSimultaneously, they empirically confirmed that such a solution could not be practically found via (S)GD training.\\nVarious other formal models of transformers (e.g. Weiss et\\xa0al., 2021 ; Merrill and Sabharwal, 2023b , c ; Strobl, 2023 ) can also express PARITY despite its empirical difficulty.\\nAs already concluded by Chiang and Cholak ( 2022 ) , these findings highlight a disconnect between expressive capacity and learnability: not all functions which transformers may express in principle are also learnt efficiently. Evidently, existing expressiveness theory for transformers is not able to consistently account for the practical learnability of problems under gradient descent. Report issue for preceding element\\nSome prior work has studied the learnability of problems for transformers.\\nFor example, Edelman et\\xa0al. ( 2022 ) bound the statistical capacity of the transformer architecture, showing that on those functions that transformers prefer to represent, they can generalize with good sample efficiency.\\nNotably, they found that sparse parities could indeed be learned well by transformers.\\nHowever, this result does not prove that PARITY, or other highly sensitive functions, are hard to learn, as that technique does not provide a direct characterization of which functions transformers prefer to represent.\\nOther work has studied simplified setups such as linear attention (e.g. Ahn et\\xa0al., 2023 ) or individual attention layers (e.g. Sanford et\\xa0al., 2023 ) . Report issue for preceding element\\nHere, we provide results that have direct bearing on the learnability of PARITY and other sensitive functions, characterizing the loss landscape of transformers in terms of input-space sensitivity.\\nWe formally prove that, for the transformer architecture, parameter settings achieving high sensitivity in input space are necessarily brittle, so that close neighbors in parameter space will usually define different (typically much less sensitive) functions when inputs are long.\\nAs a consequence, transformers fitting high-sensitivity functions must inhabit very steep minima.\\nWe argue that this explains both difficulty in training and length generalization for PARITY (observed by Bhattamishra et\\xa0al. ( 2020 ); Delétang et\\xa0al. ( 2023 ); Ruoss et\\xa0al. ( 2023 ) ), and a low-sensitivity and low-degree bias in random initialization and generalization (observed by Abbe et\\xa0al. ( 2023 ); Bhattamishra et\\xa0al. ( 2023 ) ). Report issue for preceding element\\nWhile unique hard attention provably cannot represent PARITY (Hahn, 2020 ; Hao et\\xa0al., 2022 ; Angluin et\\xa0al., 2023 ) , more realistic upper bounds accounting for soft attention (Weiss et\\xa0al., 2021 ; Merrill and Sabharwal, 2023b , c ; Strobl, 2023 ; Chiang and Cholak, 2022 ) leave the hardness of sensitive functions unexplained.\\nNot only does PARITY have transformers (Chiang and Cholak, 2022 ) , but it can also be easily represented in formalisms that have been suggested to meaningfully upper-bound the abilities of various formal models of soft-attention: 1 1 1 Zhou et\\xa0al. ( 2023 ) suggest that PARITY may not be representable in the RASP-L model, though the expressiveness of RASP-L is not well understood. Report issue for preceding element\\nSimple representations for PARITY, valid across all input lengths, exist in RASP (Weiss et\\xa0al., 2021 ) , uniform circuits with majority gates (Merrill and Sabharwal, 2023c ; Strobl, 2023 ) , and FO[M] (Merrill and Sabharwal, 2023b ) . Report issue for preceding element\\nWe prove this in Appendix A .\\nThus, existing expressiveness bounds do not account for the difficulty that transformers encounter in learning sensitive functions, in particular given that previously-popular recurrent models do not encounter this difficulty.\\nAnother family of results consists of Lipschitzness bounds (Hahn, 2020 ; Li et\\xa0al., 2023 ) , which bound the influence that any individual input bit has on the output of a transformer.\\nThese turn out to underpredict the abilities of transformers: Report issue for preceding element\\nBy results of Hahn ( 2020 ); Li et\\xa0al. ( 2023 ) , the following holds:\\nConsider a transformer without layer norm.\\nIf x , x ′ ∈ { ± 1 } n 𝑥 superscript 𝑥 ′ superscript plus-or-minus 1 𝑛 x,x^{\\\\prime}\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x , italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT differ only in the i 𝑖 i italic_i -th bit, then at any other position j ≠ i 𝑗 𝑖 j\\\\neq i italic_j ≠ italic_i , the output of a transformer differs only by 𝒪 \\u2062 ( 1 n ) 𝒪 1 𝑛 \\\\mathcal{O}(\\\\frac{1}{n}) caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ) . Report issue for preceding element\\nThis accounts for the difficulty of learning PARITY.\\nBut the bound suggests even simple sparse functions, such as FIRST (the language 1 \\u2062 ( 0 | 1 ) ∗ 1 superscript conditional 0 1 1(0|1)^{*} 1 ( 0 | 1 ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) to be difficult, but transformers learn these well (Bhattamishra et\\xa0al., 2023 ; Edelman et\\xa0al., 2022 ) . Indeed, Chiang and Cholak ( 2022 ) note that the bound is overcome by layer norm or input-length-dependent scaling of attention logits, which enable modeling of sparse functions. Report issue for preceding element\\nWe will show that the observed low-sensitivity bias can be understood in terms of the loss landscape : while transformers can express highly sensitive functions, such transformers are isolated in parameter space, and minima interpolating a sensitive function are very sharp.\\nIndeed, we prove that tiny perturbations of a highly sensitive transformer tend to define, when inputs are sufficiently long, very different functions with much lower sensitivity. Report issue for preceding element\\nWe will focus on boolean functions.\\nFollowing the conventions in the Analysis of Boolean Functions literature (O’Donnell, 2014 ) in modeling bitstrings as elements of { − 1 , 1 } n superscript 1 1 𝑛 \\\\{-1,1\\\\}^{n} { - 1 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , we assume the alphabet Σ = { − 1 , 1 } Σ 1 1 \\\\Sigma=\\\\{-1,1\\\\} roman_Σ = { - 1 , 1 } , with word embeddings e \\u2062 ( − 1 ) , e \\u2062 ( + 1 ) ∈ ℝ d 𝑒 1 𝑒 1 superscript ℝ 𝑑 e(-1),e(+1)\\\\in\\\\mathbb{R}^{d} italic_e ( - 1 ) , italic_e ( + 1 ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\\nThere further are positional encodings p 1 , p 2 , p 3 , ⋯ ∈ ℝ d subscript 𝑝 1 subscript 𝑝 2 subscript 𝑝 3 ⋯ superscript ℝ 𝑑 p_{1},p_{2},p_{3},\\\\dots\\\\in\\\\mathbb{R}^{d} italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , ⋯ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\\nAt the zero-th layer, token and positional encodings are added: y i ( 0 ) := e \\u2062 ( x i ) + p i assign superscript subscript 𝑦 𝑖 0 𝑒 subscript 𝑥 𝑖 subscript 𝑝 𝑖 y_{i}^{(0)}:=e(x_{i})+p_{i} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT := italic_e ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( i = 1 , … , n 𝑖 1 … 𝑛 i=1,\\\\dots,n italic_i = 1 , … , italic_n ), where x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the input string. Report issue for preceding element\\nA transformer has a fixed number L 𝐿 L italic_L of layers ; the activations y i ( k ) ∈ ℝ d superscript subscript 𝑦 𝑖 𝑘 superscript ℝ 𝑑 y_{i}^{(k)}\\\\in\\\\mathbb{R}^{d} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT at position i 𝑖 i italic_i of the k 𝑘 k italic_k -th layer ( k = 1 , … , L 𝑘 1 … 𝐿 k=1,\\\\dots,L italic_k = 1 , … , italic_L ) are defined as follows.\\nEach layer has a set of H 𝐻 H italic_H attention heads ; we first compute attention scores for the h ℎ h italic_h -th head: Report issue for preceding element a i , j ( k , h ) = superscript subscript 𝑎 𝑖 𝑗 𝑘 ℎ absent \\\\displaystyle a_{i,j}^{(k,h)}= italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = ( K k , h \\u2062 y j ( k − 1 ) ) T \\u2062 Q k , h \\u2062 y i ( k − 1 ) superscript subscript 𝐾 𝑘 ℎ superscript subscript 𝑦 𝑗 𝑘 1 𝑇 subscript 𝑄 𝑘 ℎ superscript subscript 𝑦 𝑖 𝑘 1 \\\\displaystyle(K_{k,h}y_{j}^{(k-1)})^{T}Q_{k,h}y_{i}^{(k-1)} ( italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT a ^ i , j ( k , h ) = superscript subscript ^ 𝑎 𝑖 𝑗 𝑘 ℎ absent \\\\displaystyle\\\\widehat{a}_{i,j}^{(k,h)}= over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = exp \\u2061 ( a i , j ( k , h ) ) ∑ s a i , s ( k , h ) superscript subscript 𝑎 𝑖 𝑗 𝑘 ℎ subscript 𝑠 superscript subscript 𝑎 𝑖 𝑠 𝑘 ℎ \\\\displaystyle\\\\frac{\\\\exp(a_{i,j}^{(k,h)})}{\\\\sum_{s}a_{i,s}^{(k,h)}} divide start_ARG roman_exp ( italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT end_ARG where K k , h subscript 𝐾 𝑘 ℎ K_{k,h} italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (“key”), Q k , h subscript 𝑄 𝑘 ℎ Q_{k,h} italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (“query”) are ∈ ℝ d × d absent superscript ℝ 𝑑 𝑑 \\\\in\\\\mathbb{R}^{d\\\\times d} ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT .\\nThe activation of the head is computed by weighting according to attention weights a ^ i , j ( k , h ) superscript subscript ^ 𝑎 𝑖 𝑗 𝑘 ℎ \\\\hat{a}_{i,j}^{(k,h)} over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT , and applying a linear transformation V 𝑉 V italic_V (“value”): Report issue for preceding element b i , h ( k ) = ∑ j = 1 n a ^ i , j ( k , h ) \\u2062 V k , h \\u2062 y j ( k − 1 ) superscript subscript 𝑏 𝑖 ℎ 𝑘 superscript subscript 𝑗 1 𝑛 superscript subscript ^ 𝑎 𝑖 𝑗 𝑘 ℎ subscript 𝑉 𝑘 ℎ superscript subscript 𝑦 𝑗 𝑘 1 b_{i,h}^{(k)}=\\\\sum_{j=1}^{n}\\\\hat{a}_{i,j}^{(k,h)}V_{k,h}y_{j}^{(k-1)} italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT (1) The per-position activations are then computed as Report issue for preceding element Y i ( k ) := f M \\u2062 L \\u2062 P \\u2062 ( y i ( k − 1 ) + ∑ h = 1 H b i , h ( k ) ) assign superscript subscript 𝑌 𝑖 𝑘 superscript 𝑓 𝑀 𝐿 𝑃 superscript subscript 𝑦 𝑖 𝑘 1 superscript subscript ℎ 1 𝐻 superscript subscript 𝑏 𝑖 ℎ 𝑘 Y_{i}^{(k)}:=f^{MLP}\\\\left(y_{i}^{(k-1)}+\\\\sum_{h=1}^{H}b_{i,h}^{(k)}\\\\right) italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (2) where f M \\u2062 L \\u2062 P superscript 𝑓 𝑀 𝐿 𝑃 f^{MLP} italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT is a one-layer MLP with a skip-connection.\\nTransformers additionally implement layer norm (Ba et\\xa0al., 2016 ) : Report issue for preceding element L \\u2062 a \\u2062 y \\u2062 e \\u2062 r \\u2062 N \\u2062 o \\u2062 r \\u2062 m \\u2062 ( y ) := y − m \\u2062 e \\u2062 a \\u2062 n \\u2062 ( y ) σ 2 \\u2062 ( y ) + ϵ assign 𝐿 𝑎 𝑦 𝑒 𝑟 𝑁 𝑜 𝑟 𝑚 𝑦 𝑦 𝑚 𝑒 𝑎 𝑛 𝑦 superscript 𝜎 2 𝑦 italic-ϵ LayerNorm(y):=\\\\frac{y-mean(y)}{\\\\sqrt{\\\\sigma^{2}(y)+\\\\epsilon}} italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) := divide start_ARG italic_y - italic_m italic_e italic_a italic_n ( italic_y ) end_ARG start_ARG square-root start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_y ) + italic_ϵ end_ARG end_ARG (3) where ϵ ≥ 0 italic-ϵ 0 \\\\epsilon\\\\geq 0 italic_ϵ ≥ 0 is a hyperparameter ensuring numerical stability, and σ 2 \\u2062 ( ⋅ ) superscript 𝜎 2 ⋅ \\\\sigma^{2}(\\\\cdot) italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( ⋅ ) denotes the variance.\\nBy design, ‖ L \\u2062 a \\u2062 y \\u2062 e \\u2062 r \\u2062 N \\u2062 o \\u2062 r \\u2062 m \\u2062 ( y ) ‖ 2 ≤ d subscript norm 𝐿 𝑎 𝑦 𝑒 𝑟 𝑁 𝑜 𝑟 𝑚 𝑦 2 𝑑 \\\\|LayerNorm(y)\\\\|_{2}\\\\leq\\\\sqrt{d} ∥ italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ square-root start_ARG italic_d end_ARG , with equality at ϵ = 0 italic-ϵ 0 \\\\epsilon=0 italic_ϵ = 0 .\\nTransformer variants differ in where exactly layer norm is applied (e.g. Takase et\\xa0al., 2022 ) ; we here assume for notational simplicity that layer norm applies after the MLP, but the details are irrelevant to our results, provided layer norm applies at least once. We thus set: Report issue for preceding element y i ( k ) := L \\u2062 a \\u2062 y \\u2062 e \\u2062 r \\u2062 N \\u2062 o \\u2062 r \\u2062 m \\u2062 ( Y i ( k ) ) assign superscript subscript 𝑦 𝑖 𝑘 𝐿 𝑎 𝑦 𝑒 𝑟 𝑁 𝑜 𝑟 𝑚 superscript subscript 𝑌 𝑖 𝑘 y_{i}^{(k)}:=LayerNorm\\\\left(Y_{i}^{(k)}\\\\right) italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (4) Of key importance will be the the normalization factor: Report issue for preceding element N i ( k ) := 1 σ 2 \\u2062 ( Y i ( k ) ) + ϵ assign superscript subscript 𝑁 𝑖 𝑘 1 superscript 𝜎 2 superscript subscript 𝑌 𝑖 𝑘 italic-ϵ N_{i}^{(k)}:=\\\\frac{1}{\\\\sqrt{\\\\sigma^{2}(Y_{i}^{(k)})+\\\\epsilon}} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) + italic_ϵ end_ARG end_ARG (5) Our theoretical results will link N i ( k ) superscript subscript 𝑁 𝑖 𝑘 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT both to input-space sensitivity and parameter-space sharpness:\\nWe will find that large values of N i ( k ) superscript subscript 𝑁 𝑖 𝑘 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT can increase expressive capacity, but at the price of increased brittleness. Report issue for preceding element\\nFinally, we assume that predictions are made by T := v o \\u2062 u \\u2062 t T ⋅ y n ( L ) assign 𝑇 ⋅ superscript subscript 𝑣 𝑜 𝑢 𝑡 𝑇 superscript subscript 𝑦 𝑛 𝐿 T:=v_{out}^{T}\\\\cdot y_{n}^{(L)} italic_T := italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ⋅ italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT for some parameter v o \\u2062 u \\u2062 t ∈ ℝ d subscript 𝑣 𝑜 𝑢 𝑡 superscript ℝ 𝑑 v_{out}\\\\in\\\\mathbb{R}^{d} italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT .\\nThroughout, we will add the input string x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT as an argument when needed for disambiguation, e.g., writing T \\u2062 ( x ) 𝑇 𝑥 T(x) italic_T ( italic_x ) for the overall prediction made on x 𝑥 x italic_x . Report issue for preceding element\\nOur results are centered around average sensitivity , a simple but foundational complexity metric for functions on the Boolean cube (e.g. Kahn et\\xa0al., 1988 ; De\\xa0Wolf, 2008 ; O’Donnell, 2014 ) : Report issue for preceding element\\nFor a bitstring x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and a function f : { ± 1 } n → ℝ : 𝑓 → superscript plus-or-minus 1 𝑛 ℝ f:\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R} italic_f : { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R , we write Report issue for preceding element s \\u2062 ( x , f ) := 1 4 \\u2062 ∑ i = 1 n | f \\u2062 ( x ) − f \\u2062 ( x ⊕ i ) | 2 assign 𝑠 𝑥 𝑓 1 4 superscript subscript 𝑖 1 𝑛 superscript 𝑓 𝑥 𝑓 superscript 𝑥 direct-sum 𝑖 2 s(x,f):=\\\\frac{1}{4}\\\\sum_{i=1}^{n}|f(x)-f(x^{{}^{\\\\oplus i}})|^{2} italic_s ( italic_x , italic_f ) := divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_f ( italic_x ) - italic_f ( italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ⊕ italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (6) where x ⊕ i superscript 𝑥 direct-sum 𝑖 x^{{}^{\\\\oplus i}} italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ⊕ italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT denotes the bitstring x 𝑥 x italic_x with the i 𝑖 i italic_i -th bit flipped.\\nThe average sensitivity for inputs of length n 𝑛 n italic_n is Report issue for preceding element a \\u2062 s n \\u2062 ( f ) := 1 2 n \\u2062 ∑ x ∈ { ± 1 } n s \\u2062 ( x , f ) assign 𝑎 subscript 𝑠 𝑛 𝑓 1 superscript 2 𝑛 subscript 𝑥 superscript plus-or-minus 1 𝑛 𝑠 𝑥 𝑓 as_{n}(f):=\\\\frac{1}{2^{n}}\\\\sum_{x\\\\in\\\\{\\\\pm 1\\\\}^{n}}s(x,f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) := divide start_ARG 1 end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_s ( italic_x , italic_f ) (7)\\nIf f 𝑓 f italic_f maps to { ± 1 } plus-or-minus 1 \\\\{\\\\pm 1\\\\} { ± 1 } , then s \\u2062 ( x , f ) 𝑠 𝑥 𝑓 s(x,f) italic_s ( italic_x , italic_f ) is the number of Hamming neighbors of x 𝑥 x italic_x on which f 𝑓 f italic_f flips.\\nThis definition of a \\u2062 s n \\u2062 ( f ) 𝑎 subscript 𝑠 𝑛 𝑓 as_{n}(f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) corresponds to the “total influence” from O’Donnell ( 2014 , Def. 2.27) .\\nWe explicitly define average sensitivity relative to input length n 𝑛 n italic_n , as we will investigate the behavior of transformers performing a single function f 𝑓 f italic_f across varying input lengths.\\nThe use of squared distances, rather than simple absolute distances, ensures that results about a \\u2062 s n \\u2062 ( f ) 𝑎 subscript 𝑠 𝑛 𝑓 as_{n}(f) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) transfer to results about degree profiles (Abbe et\\xa0al., 2023 ) , which we will later investigate (Eq. 17 ). Report issue for preceding element\\nAverage sensitivity is a general complexity metric with wide-ranging applications in theoretical computer science (e.g. Jukna, 2012 ) .\\nIt is closely linked to the Fourier analysis on the Boolean cube (De\\xa0Wolf, 2008 ) , and is an average-case version of a family of sensitivity measures, closely related to other natural metrics such as decision tree depth and polynomial degree (Hatami et\\xa0al., 2010 ) .\\nBoth average sensitivity itself (Bhattamishra et\\xa0al., 2023 ) and the Fourier structure (Abbe et\\xa0al., 2023 ) have been empirically linked to transformers’ generalization behavior. We will ground these empirical findings by relating average sensitivity to loss landscapes for the transformer architecture. Report issue for preceding element\\nOur theoretical results will apply to general functions on the Boolean cube.\\nIn order to ground these, we will illustrate transformers’ low-sensitivity bias at the example of a few natural functions which have played a role in the theoretical literature of transformers or are otherwise illustrative of variability in average sensitivity. Report issue for preceding element\\nPARITY indicates whether the number of ones in a bitstring is even (output 1) or odd (output -1); over the input space { ± 1 } n superscript plus-or-minus 1 𝑛 \\\\{\\\\pm 1\\\\}^{n} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and output space { ± 1 } plus-or-minus 1 \\\\{\\\\pm 1\\\\} { ± 1 } , it can be formally defined as the map x ↦ ∏ i = 1 n x i maps-to 𝑥 superscript subscript product 𝑖 1 𝑛 subscript 𝑥 𝑖 x\\\\mapsto\\\\prod_{i=1}^{n}x_{i} italic_x ↦ ∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .\\nAs flipping any input bit flips the output, a \\u2062 s n \\u2062 ( f ) = n 𝑎 subscript 𝑠 𝑛 𝑓 𝑛 as_{n}(f)=n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n .\\nAs described above, this function can in principle be represented by transformers, but has empirically been found to be very hard to learn. Report issue for preceding element\\nMAJORITY maps x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to 1 if # \\u2062 { i : x i = 1 } > n / 2 # conditional-set 𝑖 subscript 𝑥 𝑖 1 𝑛 2 \\\\#\\\\{i:x_{i}=1\\\\}>n/2 # { italic_i : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 } > italic_n / 2 , and − 1 1 -1 - 1 else.\\nTransformers show good length generalization (Merrill et\\xa0al., 2022 ; Zhou et\\xa0al., 2023 ) .\\nIt is known that a \\u2062 s n \\u2062 ( f ) = Θ \\u2062 ( n ) 𝑎 subscript 𝑠 𝑛 𝑓 Θ 𝑛 as_{n}(f)=\\\\Theta(\\\\sqrt{n}) italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = roman_Θ ( square-root start_ARG italic_n end_ARG ) (O’Donnell, 2014 , Ex. 2.22) .\\nHowever, s \\u2062 ( f , x ) = n 𝑠 𝑓 𝑥 𝑛 s(f,x)=n italic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x 𝑥 x italic_x are almost fully balanced. Report issue for preceding element\\nFIRST maps x 𝑥 x italic_x to its first bit, x 1 subscript 𝑥 1 x_{1} italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\\nAs only the first bit matters, a \\u2062 s n \\u2062 ( f ) = 1 𝑎 subscript 𝑠 𝑛 𝑓 1 as_{n}(f)=1 italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1 .\\nIt is a simple example of a sparse function; more generally, a k 𝑘 k italic_k -PARITY is a restriction of the PARITY function to only k 𝑘 k italic_k inputs, where k 𝑘 k italic_k is a constant.\\nTransformers learn such sparse functions well (Bhattamishra et\\xa0al., 2023 ; Edelman et\\xa0al., 2022 ) . Report issue for preceding element\\nMEAN maps x ↦ 1 n \\u2062 ∑ i = 1 n x i ∈ [ − 1 , 1 ] maps-to 𝑥 1 𝑛 superscript subscript 𝑖 1 𝑛 subscript 𝑥 𝑖 1 1 x\\\\mapsto\\\\frac{1}{n}\\\\sum_{i=1}^{n}x_{i}\\\\in[-1,1] italic_x ↦ divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ [ - 1 , 1 ] . We have a \\u2062 s n \\u2062 ( f ) = 1 n 𝑎 subscript 𝑠 𝑛 𝑓 1 𝑛 as_{n}(f)=\\\\frac{1}{n} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG . Report issue for preceding element\\nThe PARITY function, when varying the number of bits considered, is a universal basis for Boolean functions, in the sense that any function { ± 1 } n → ℝ → superscript plus-or-minus 1 𝑛 ℝ \\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R can be represented as a linear combination of parities applying to different subsets of { x 1 , … , x n } subscript 𝑥 1 … subscript 𝑥 𝑛 \\\\{x_{1},\\\\dots,x_{n}\\\\} { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } .\\nFunctions are more sensitive when parities applying to larger subsets appear in this representation.\\nWe will investigate this connection further below. Report issue for preceding element\\nWe first prove that representing sensitive functions with transformers requires large parameter norms and, when inputs get longer and longer, highly unbounded normalization factors N i ( k ) superscript subscript 𝑁 𝑖 𝑘 N_{i}^{(k)} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT in layer norm ( 5 ).\\nWe start from the global Lipschitzness bounds developed by Hahn ( 2020 ); Edelman et\\xa0al. ( 2022 ); Li et\\xa0al. ( 2023 ) , but obtain more fine-grained average-case and high-probability bounds. These will then form the basis of our characterization of loss landscape sharpness around sensitive transformers.\\nOur bounds will include a constant C 𝐶 C italic_C that is the product of Report issue for preceding element exp \\u2061 ( 4 \\u2062 d \\u2062 max h \\u2062 ∑ i = 2 L ‖ K i , h T \\u2062 Q i , h ‖ 2 ) 4 𝑑 subscript ℎ superscript subscript 𝑖 2 𝐿 subscript norm superscript subscript 𝐾 𝑖 ℎ 𝑇 subscript 𝑄 𝑖 ℎ 2 \\\\exp\\\\left(4d\\\\max_{h}\\\\sum_{i=2}^{L}\\\\|K_{i,h}^{T}Q_{i,h}\\\\|_{2}\\\\right) roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∥ italic_K start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (8) and a term polynomial in H 𝐻 H italic_H , d 𝑑 d italic_d , the spectral norms of all parameter matrices appearing in the transformer, and the maximum norm of any positional or word embedding.\\nSee ( 32 ) in the Appendix for formal definition. Report issue for preceding element\\nExisting Lipschitzness bounds (Fact 2 ) imply a bound along the lines of 2 2 2 Lipschitzness bounds by Edelman et\\xa0al. ( 2022 ) are not directly applicable here, as these authors consider Lipschitzness in the parameter space , not the effect of changes in the input space as Theorem 3. See also Appendix, Remark 10 . Report issue for preceding element s \\u2062 ( f , x ) ≤ C \\u2062 exp \\u2061 ( 4 \\u2062 d \\u2062 max h \\u2061 ‖ K 1 , h T \\u2062 Q 1 , h ‖ 2 ) ϵ L / 2 𝑠 𝑓 𝑥 𝐶 4 𝑑 subscript ℎ subscript norm superscript subscript 𝐾 1 ℎ 𝑇 subscript 𝑄 1 ℎ 2 superscript italic-ϵ 𝐿 2 s(f,x)\\\\leq\\\\frac{C\\\\exp(4d\\\\max_{h}\\\\|K_{1,h}^{T}Q_{1,h}\\\\|_{2})}{\\\\epsilon^{L/2}} italic_s ( italic_f , italic_x ) ≤ divide start_ARG italic_C roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∥ italic_K start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (9) uniformly for each x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . Li et\\xa0al. ( 2023 ) noted that the exponential dependency on the spectral norm of the key-query matrix is unavoidable for a global Lipschitzness bound.\\nOur first result is that this dependency can be eliminated at the input layer for the vast majority of inputs, at the cost of a logarithmic factor, leading to a bound of the form (assuming ϵ > 0 italic-ϵ 0 \\\\epsilon>0 italic_ϵ > 0 in ( 3 )) Report issue for preceding element s \\u2062 ( f , x ) ≤ C \\u2062 log \\u2061 n ϵ L / 2 𝑠 𝑓 𝑥 𝐶 𝑛 superscript italic-ϵ 𝐿 2 s(f,x)\\\\leq C\\\\frac{\\\\log{n}}{\\\\epsilon^{L/2}} italic_s ( italic_f , italic_x ) ≤ italic_C divide start_ARG roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (10) for 1 − H \\u2062 n − 2 1 𝐻 superscript 𝑛 2 1-Hn^{-2} 1 - italic_H italic_n start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT of inputs.\\nWe show this using a concentration bound argument, combining a Chernoff bound applying to each attention weight individually, with a union bound over all attention weights (Appendix, Lemma 11 ).\\nNext, we address the role of layer norm. Chiang and Cholak ( 2022 ) showed that, at ϵ → 0 → italic-ϵ 0 \\\\epsilon\\\\rightarrow 0 italic_ϵ → 0 in ( 3 ), layer norm enables transformers to represent PARITY. Lipschitzness bounds in terms of ϵ italic-ϵ \\\\epsilon italic_ϵ ( 10 ) cease being meaningful in this limit.\\nWe thus study the layer-norm induced blowup in more detail.\\nIn each layer, we consider the maximum blowup, given an input string x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : Report issue for preceding element τ ( k ) \\u2062 ( x ) := max w = 1 , … , n \\u2061 { 1 + N w ( k ) \\u2062 ( x ) } assign superscript 𝜏 𝑘 𝑥 subscript 𝑤 1 … 𝑛 1 superscript subscript 𝑁 𝑤 𝑘 𝑥 \\\\tau^{(k)}(x):=\\\\max_{w=1,\\\\dots,n}\\\\{1+N_{w}^{(k)}(x)\\\\} italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) := roman_max start_POSTSUBSCRIPT italic_w = 1 , … , italic_n end_POSTSUBSCRIPT { 1 + italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) } (11) The addition of 1 is for technical reasons (Appendix, Lemma 16 ); it has little impact in the cases relevant to our results, which will be when τ ( k ) \\u2062 ( x ) = ω \\u2062 ( 1 ) superscript 𝜏 𝑘 𝑥 𝜔 1 \\\\tau^{(k)}(x)=\\\\omega(1) italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) = italic_ω ( 1 ) .\\nWe then write Blowup \\u2061 ( x ) := ∏ k = 1 L τ ( k ) \\u2062 ( x ) assign Blowup 𝑥 superscript subscript product 𝑘 1 𝐿 superscript 𝜏 𝑘 𝑥 \\\\operatorname{Blowup}(x):=\\\\prod_{k=1}^{L}\\\\tau^{(k)}(x) roman_Blowup ( italic_x ) := ∏ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) , an upper bound on the product of the successive layer-norm-induced blowups when going from the input to the output.\\nWhen ϵ = 0 italic-ϵ 0 \\\\epsilon=0 italic_ϵ = 0 in ( 3 ), Blowup \\u2061 ( x ) Blowup 𝑥 \\\\operatorname{Blowup}(x) roman_Blowup ( italic_x ) can be arbitrarily large.\\nForeshadowing Theorem 6 , we will find that large values of Blowup \\u2061 ( x ) Blowup 𝑥 \\\\operatorname{Blowup}(x) roman_Blowup ( italic_x ) create very sharp minima. Report issue for preceding element\\nOur first theorem localizes the layer norm blowup to the Hamming neighborhoods of sensitive inputs: Report issue for preceding element\\nConsider a transformer with layer norm at arbitrary ϵ ≥ 0 italic-ϵ 0 \\\\epsilon\\\\geq 0 italic_ϵ ≥ 0 .\\nWith probability 1 − H n 2 1 𝐻 superscript 𝑛 2 1-\\\\frac{H}{n^{2}} 1 - divide start_ARG italic_H end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG over the choice of x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , we have Report issue for preceding element s \\u2062 ( f , x ) C \\u2062 n \\u2062 log \\u2061 n ≤ Blowup ( x ) 2 + 1 n ∑ i = 1 n Blowup ( x ⊕ i ) 2 \\\\frac{s(f,x)}{C\\\\sqrt{n\\\\log n}}\\\\leq\\\\operatorname{Blowup}(x)^{2}+\\\\frac{1}{n}\\\\sum%\\n_{i=1}^{n}\\\\operatorname{Blowup}(x^{\\\\oplus i})^{2} divide start_ARG italic_s ( italic_f , italic_x ) end_ARG start_ARG italic_C square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG ≤ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_Blowup ( italic_x start_POSTSUPERSCRIPT ⊕ italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (12)\\nThe proof is in Appendix B.5 .\\nThis permits us to state a bound on average sensitivity, in terms of the average layer norm blowup: Report issue for preceding element\\nConsider a transformer with layer norm at arbitrary ϵ ≥ 0 italic-ϵ 0 \\\\epsilon\\\\geq 0 italic_ϵ ≥ 0 .\\nThen Report issue for preceding element C ⋅ 𝔼 [ Blowup ( x ) 2 ] ≥ a \\u2062 s n \\u2062 ( f ) n \\u2062 log \\u2061 n − H n C\\\\cdot\\\\mathbb{E}[\\\\operatorname{Blowup}(x)^{2}]\\\\geq\\\\frac{as_{n}(f)}{\\\\sqrt{n\\\\log%\\n{n}}}-\\\\frac{H}{n} italic_C ⋅ blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≥ divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG - divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG (13) where the expectation is over the uniform distribution over x ∈ { ± 1 } n 𝑥 superscript plus-or-minus 1 𝑛 x\\\\in\\\\{\\\\pm 1\\\\}^{n} italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . Report issue for preceding element\\nThe proof is in Appendix B.6 .\\nNote that H n 𝐻 𝑛 \\\\frac{H}{n} divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG is small when n 𝑛 n italic_n is large, and the bound is dominated by a \\u2062 s n \\u2062 ( f ) n \\u2062 log \\u2061 n 𝑎 subscript 𝑠 𝑛 𝑓 𝑛 𝑛 \\\\frac{as_{n}(f)}{\\\\sqrt{n\\\\log{n}}} divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG .\\nThis result thus shows a tradeoff between parameters and LN blowup: at least one of them needs to be large to represent a sensitive function. When the sensitivity depends on the input length and grows faster than n \\u2062 log \\u2061 n 𝑛 𝑛 \\\\sqrt{n\\\\log n} square-root start_ARG italic_n roman_log italic_n end_ARG , this tradeoff changes with the input length, requiring larger parameters or larger layer norm blowup as the input length increases. Report issue for preceding element\\nLet us investigate the implications for the functions introduced in Section above.\\nFor PARITY, a \\u2062 s n \\u2062 ( f ) = n 𝑎 subscript 𝑠 𝑛 𝑓 𝑛 as_{n}(f)=n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n , and\\n( 13 ) predicts Report issue for preceding element C ⋅ 𝔼 [ Blowup ( x ) 2 ] = Ω ( n log \\u2061 n ) = ω ( 1 ) C\\\\cdot\\\\mathbb{E}[\\\\operatorname{Blowup}(x)^{2}]=\\\\Omega\\\\left(\\\\frac{\\\\sqrt{n}}{%\\n\\\\log{n}}\\\\right)=\\\\omega(1) italic_C ⋅ blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = roman_Ω ( divide start_ARG square-root start_ARG italic_n end_ARG end_ARG start_ARG roman_log italic_n end_ARG ) = italic_ω ( 1 ) (14) showing that, for fixed parameters, the layer norm blowup needs to increase as the input length increases.\\nFor the other functions, the bound is O \\u2062 ( 1 ) 𝑂 1 O(1) italic_O ( 1 ) :\\nFor FIRST, s \\u2062 ( f , x ) = a \\u2062 s n \\u2062 ( f ) = 1 𝑠 𝑓 𝑥 𝑎 subscript 𝑠 𝑛 𝑓 1 s(f,x)=as_{n}(f)=1 italic_s ( italic_f , italic_x ) = italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1 , and the RHS of ( 13 ) is O \\u2062 ( 1 ) 𝑂 1 O(1) italic_O ( 1 ) .\\nIndeed, sparse functions can be modeled well by a family of transformers where the logits in the input layer scale with log \\u2061 n 𝑛 \\\\log{n} roman_log italic_n (Edelman et\\xa0al., 2022 ; Chiang and Cholak, 2022 ) .\\nUnlike the prior Lipschitzness bounds ( 9 ), these scaled logits do not contribute to C 𝐶 C italic_C – our new bound is thus consistent with the ease with which transformers learn sparse functions.\\nFor MEAN, s \\u2062 ( f , x ) ∼ 1 n 2 similar-to 𝑠 𝑓 𝑥 1 superscript 𝑛 2 s(f,x)\\\\sim\\\\frac{1}{n^{2}} italic_s ( italic_f , italic_x ) ∼ divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ; again no blowup is predicted.\\nFor MAJORITY, as a \\u2062 s n \\u2062 ( f ) ∼ n similar-to 𝑎 subscript 𝑠 𝑛 𝑓 𝑛 as_{n}(f)\\\\sim\\\\sqrt{n} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) ∼ square-root start_ARG italic_n end_ARG , no nontrivial average blowup is predicted.\\nHowever, s \\u2062 ( f , x ) = n 𝑠 𝑓 𝑥 𝑛 s(f,x)=n italic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x 𝑥 x italic_x are almost fully balanced; on such strings or their Hamming neighbors, ( 12 ) predicts B \\u2062 l \\u2062 o \\u2062 w \\u2062 u \\u2062 p = Ω \\u2062 ( n 1 / 4 ) 𝐵 𝑙 𝑜 𝑤 𝑢 𝑝 Ω superscript 𝑛 1 4 Blowup=\\\\Omega(n^{1/4}) italic_B italic_l italic_o italic_w italic_u italic_p = roman_Ω ( italic_n start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ) . Report issue for preceding element\\nLeveraging Corollary 5 , we now show that transformers expressing sensitive functions must be very sensitive to small perturbations of model parameters.\\nThat is, sensitivity in input space entails sensitivity in parameter space .\\nAn important consequence is that, for any highly sensitive function, any interpolating minima will be very sharp.\\nThis property is nontrivial, as seen from the fact that it disappears when adding a scratchpad (see below). Report issue for preceding element\\nGiven a parameter vector θ 𝜃 \\\\theta italic_θ defining the transformer T θ subscript 𝑇 𝜃 T_{\\\\theta} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , the average direction sharpness is (e.g. Wen et\\xa0al., 2022 ; Jiang et\\xa0al., 2020 ; Foret et\\xa0al., 2020 ) Report issue for preceding element L ρ , n \\u2062 ( T ) := 𝔼 x ∈ { ± 1 } n \\u2062 𝔼 ‖ Δ ‖ 2 = ρ \\u2062 ( T θ + Δ \\u2062 ( x ) − T θ \\u2062 ( x ) ) 2 assign subscript 𝐿 𝜌 𝑛 𝑇 subscript 𝔼 𝑥 superscript plus-or-minus 1 𝑛 subscript 𝔼 subscript norm Δ 2 𝜌 superscript subscript 𝑇 𝜃 Δ 𝑥 subscript 𝑇 𝜃 𝑥 2 L_{\\\\rho,n}(T):=\\\\mathbb{E}_{x\\\\in\\\\{\\\\pm 1\\\\}^{n}}\\\\mathbb{E}_{\\\\|\\\\Delta\\\\|_{2}=\\\\rho}(%\\nT_{\\\\theta+\\\\Delta}(x)-T_{\\\\theta}(x))^{2} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ) := blackboard_E start_POSTSUBSCRIPT italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ∥ roman_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_ρ end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ + roman_Δ end_POSTSUBSCRIPT ( italic_x ) - italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (15) where the second expectation is over the radius- ρ 𝜌 \\\\rho italic_ρ sphere in the space of parameter vectors. L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT quantifies the change in T θ subscript 𝑇 𝜃 T_{\\\\theta} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT when perturbing θ 𝜃 \\\\theta italic_θ by a size- ρ 𝜌 \\\\rho italic_ρ vector in a random direction.\\nLower bounds on L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT (Theorem 6 ) immediately entail lower bounds on other common sharpness measures, such as worst-case sharpness, and (in the limit ρ → 0 → 𝜌 0 \\\\rho\\\\rightarrow 0 italic_ρ → 0 ) the trace of the loss function’s Hessian at θ 𝜃 \\\\theta italic_θ . Report issue for preceding element\\nIn defining the parameter vector θ 𝜃 \\\\theta italic_θ in ( 15 ), we exclude positional encodings, both because they are often frozen, and because their number depends on n 𝑛 n italic_n , hindering fair comparison across n 𝑛 n italic_n . Report issue for preceding element\\nOur next result lower-bounds L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in terms of the average sensitivity, provided the transformer is sufficiently wide in relation to its depth: Report issue for preceding element\\nLet T θ subscript 𝑇 𝜃 T_{\\\\theta} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT be a transformer where d > 12 \\u2062 L 𝑑 12 𝐿 d>12L italic_d > 12 italic_L .\\nAssume T θ \\u2062 ( x ) ∈ [ − 1 , 1 ] subscript 𝑇 𝜃 𝑥 1 1 T_{\\\\theta}(x)\\\\in[-1,1] italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ∈ [ - 1 , 1 ] for each x 𝑥 x italic_x .\\nThen: Report issue for preceding element lim ρ → 0 lim inf n → ∞ L ρ , n \\u2062 ( T ) ≥ lim inf n → ∞ a \\u2062 s n \\u2062 ( T θ ) 2 \\u2062 n − L \\u2062 exp \\u2061 ( − Ω \\u2062 ( d ) ) subscript → 𝜌 0 subscript limit-infimum → 𝑛 subscript 𝐿 𝜌 𝑛 𝑇 subscript limit-infimum → 𝑛 𝑎 subscript 𝑠 𝑛 subscript 𝑇 𝜃 2 𝑛 𝐿 Ω 𝑑 \\\\lim_{\\\\rho\\\\rightarrow 0}\\\\liminf_{n\\\\rightarrow\\\\infty}L_{\\\\rho,n}(T)\\\\geq\\\\liminf_{%\\nn\\\\rightarrow\\\\infty}\\\\frac{as_{n}(T_{\\\\theta})}{2n}-L\\\\exp(-\\\\Omega(d)) roman_lim start_POSTSUBSCRIPT italic_ρ → 0 end_POSTSUBSCRIPT lim inf start_POSTSUBSCRIPT italic_n → ∞ end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ) ≥ lim inf start_POSTSUBSCRIPT italic_n → ∞ end_POSTSUBSCRIPT divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_n end_ARG - italic_L roman_exp ( - roman_Ω ( italic_d ) ) (16) Here, “ Ω \\u2062 ( d ) Ω 𝑑 \\\\Omega(d) roman_Ω ( italic_d ) ” scales positively with d 𝑑 d italic_d .\\nIf T θ subscript 𝑇 𝜃 T_{\\\\theta} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT has Boolean outputs ( T θ \\u2062 ( x ) ∈ { ± 1 } subscript 𝑇 𝜃 𝑥 plus-or-minus 1 T_{\\\\theta}(x)\\\\in\\\\{\\\\pm 1\\\\} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ∈ { ± 1 } ),\\nthen the factor “2” can be eliminated from ( 16 ). Report issue for preceding element\\nInformally, this theorem says that, when a \\u2062 s n \\u2062 ( T θ ) ∼ n similar-to 𝑎 subscript 𝑠 𝑛 subscript 𝑇 𝜃 𝑛 as_{n}(T_{\\\\theta})\\\\sim n italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ∼ italic_n , then even tiny perturbations to the parameters will, in expectation, lead to a substantial change in predictions on long inputs.\\nThis means that, as the input gets longer, the Hessian of the mean-squared loss at the minimizer fitting a sensitive function has unboundedly large entries. Report issue for preceding element\\nSee Appendix C for the proof.\\nThe key idea of the proof is that, if T θ subscript 𝑇 𝜃 T_{\\\\theta} italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT is very sensitive in input space, small perturbations to the parameters usually lead to a large drop in sensitivity when n 𝑛 n italic_n is large, because they lead to large changes in the layer-norm induced blowup that is needed to represent high-sensitivity functions.\\nAs a consequence, transformers computing sensitive functions are isolated in parameter space. Report issue for preceding element\\nThe theorem applies when d 𝑑 d italic_d is substantially larger than L 𝐿 L italic_L , as is indeed true of typical transformers (e.g., LLaMa 7B has d = 4096 𝑑 4096 d=4096 italic_d = 4096 and L = 32 𝐿 32 L=32 italic_L = 32 ).\\nThe convergence of the limits is slower when the parameter-norms, as summarized by C 𝐶 C italic_C , are larger, as larger parameters can increase sensitivity.\\nHowever, remarkably, for any fixed transformer, C 𝐶 C italic_C becomes irrelevant for L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the limit where n → ∞ → 𝑛 n\\\\rightarrow\\\\infty italic_n → ∞ ( 16 ). Report issue for preceding element\\nWhile we stated Theorem 6 for an individual transformer, the same statement holds for families of transformers where weights may depend on n 𝑛 n italic_n , as long as C 𝐶 C italic_C remains bounded.\\nA consequence is that scaling attention logits with log \\u2061 n 𝑛 \\\\log n roman_log italic_n (used to represent sparse functions by Edelman et\\xa0al. ( 2022 ); Chiang and Cholak ( 2022 ) ) will, at least in the input layer, not mitigate the difficulty of sensitive functions. Report issue for preceding element\\nWe discuss how Theorem 6 unifies a range of diverse empirical findings about the behavior of transformers. Report issue for preceding element\\nFor PARITY, L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT converges to 1 1 1 1 for large d 𝑑 d italic_d , showing that arbitrarily small perturbations to a transformer computing PARITY will lead to a high loss for sufficiently long inputs.\\nPreviously, Chiang and Cholak ( 2022 ) noted that, for their hand-constructed transformer representing PARITY, small changes to a specific parameter led to a large increase in loss, suggesting that this made the solution impossible to reach with SGD.\\nTheorem 6 shows that this phenomenon is unavoidable for any transformer representing a high-sensitivity function.\\nFor functions with sublinear average sensitivity, Theorem 6 entails no nontrivial lower bound on sharpness, and no such phenomenon is predicted. Report issue for preceding element\\nA key step in Theorem 6 is to show that s \\u2062 ( T θ + Δ , x ) 𝑠 subscript 𝑇 𝜃 Δ 𝑥 s(T_{\\\\theta+\\\\Delta},x) italic_s ( italic_T start_POSTSUBSCRIPT italic_θ + roman_Δ end_POSTSUBSCRIPT , italic_x ) is bounded with very high probability over the choice of Δ Δ \\\\Delta roman_Δ (Appendix, Eq. ( 70 )); this immediately entails that high-sensitivity transformers can only inhabit a small volume in parameter space.\\nThis explains why randomly initialized transformers empirically show low average sensitivity, more so than recurrent networks (Bhattamishra et\\xa0al., 2023 ) . Report issue for preceding element\\nAn important corollary is that, for a sensitive function, length generalization requires exact match to the minimum: the slightest deviation from the exact minimum will, in expectation, lead to failure when inputs get sufficient long, even if the minimum itself represents a length-generalizing solution.\\nThis provides, for the first time, a rigorous explanation why, despite the in-principle existence of length-generalizing transformers, transformers struggle with length generalization for PARITY (e.g. Bhattamishra et\\xa0al., 2020 ) . Report issue for preceding element\\nAnother corollary is that, in expectation, the training loss landscape around an interpolating minimum places a constraint on a function’s overall sensitivity, even if not a single pair of Hamming neighbors are in the training set.\\nThis is because ( 16 ) remains true, on average across randomly selected training sets, if replacing the expectation over the full input space with an expectation over the training set in ( 15 ).\\nThis means that, for long inputs, flat minima of the training loss generalize with bounded sensitivity.\\nTo the extent that gradient-based training tends to find flatter minima (e.g. Damian et\\xa0al., 2023 ) , this provides a theoretical justification for the empirical result that transformers’ generalization behavior, when trained on a Boolean function on some training subset of { ± 1 } n superscript plus-or-minus 1 𝑛 \\\\{\\\\pm 1\\\\}^{n} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , shows a strong bias towards low average sensitivity (Bhattamishra et\\xa0al., 2023 ) . Report issue for preceding element\\nAbbe et\\xa0al. ( 2023 ) proposed a min-degree bias, that is, a generalization bias towards functions that are linear combinations of functions that each depend on only a few inputs. Any function f : { ± 1 } n → ℝ : 𝑓 → superscript plus-or-minus 1 𝑛 ℝ f:\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R} italic_f : { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R can be uniquely written as a linear combination of the multilinear monomials χ P \\u2062 ( x ) := ∏ i ∈ P x i assign subscript 𝜒 𝑃 𝑥 subscript product 𝑖 𝑃 subscript 𝑥 𝑖 \\\\chi_{P}(x):=\\\\prod_{i\\\\in P}x_{i} italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) := ∏ start_POSTSUBSCRIPT italic_i ∈ italic_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where P ⊆ { 1 , … , n } 𝑃 1 … 𝑛 P\\\\subseteq\\\\{1,\\\\dots,n\\\\} italic_P ⊆ { 1 , … , italic_n } : f \\u2062 ( x ) = ∑ P λ P \\u2062 χ P \\u2062 ( x ) 𝑓 𝑥 subscript 𝑃 subscript 𝜆 𝑃 subscript 𝜒 𝑃 𝑥 f(x)=\\\\sum_{P}\\\\lambda_{P}\\\\chi_{P}(x) italic_f ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) .\\nThe coefficients λ P subscript 𝜆 𝑃 \\\\lambda_{P} italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT define the Fourier-Walsh transform of f 𝑓 f italic_f .\\nFor χ P subscript 𝜒 𝑃 \\\\chi_{P} italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , both its degree as a polynomial, and its average sensitivity, are | P | 𝑃 |P| | italic_P | .\\nThe degree profile, as defined by Abbe et\\xa0al. ( 2023 ) , is the tuple ( d 1 , … , d n ) subscript 𝑑 1 … subscript 𝑑 𝑛 (d_{1},...,d_{n}) ( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) where d i = ∑ P : | P | = i | λ P | 2 subscript 𝑑 𝑖 subscript : 𝑃 𝑃 𝑖 superscript subscript 𝜆 𝑃 2 d_{i}=\\\\sum_{P:|P|=i}|\\\\lambda_{P}|^{2} italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_P : | italic_P | = italic_i end_POSTSUBSCRIPT | italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Minimization of degree profile then refers to setting as many of the later entries to zero, and minimizing the size of the last nonzero entry. Abbe et\\xa0al. ( 2023 ) proved inductive biases towards functions with a low degree profile for a random features model and diagonal networks. Evidence in the case of transformers was limited to empirical data on three example functions.\\nOur results entail prove an bound on degree profiles for the full transformer architecture, because\\naverage sensitivity is a summary statistic of the degree profile (O’Donnell, 2014 ) : Report issue for preceding element a \\u2062 s n \\u2062 ( f ) = ∑ P ⊆ { 1 , … , n } λ P 2 \\u2062 | P | = ∑ i = 0 n i ⋅ d i 𝑎 subscript 𝑠 𝑛 𝑓 subscript 𝑃 1 … 𝑛 superscript subscript 𝜆 𝑃 2 𝑃 superscript subscript 𝑖 0 𝑛 ⋅ 𝑖 subscript 𝑑 𝑖 as_{n}(f)=\\\\sum_{P\\\\subseteq\\\\{1,\\\\dots,n\\\\}}\\\\lambda_{P}^{2}|P|=\\\\sum_{i=0}^{n}i%\\n\\\\cdot d_{i} italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = ∑ start_POSTSUBSCRIPT italic_P ⊆ { 1 , … , italic_n } end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_P | = ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_i ⋅ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (17) Hence, functions with degree profile assigning substantial weight to degrees of order ∼ n similar-to absent 𝑛 \\\\sim n ∼ italic_n are brittle in parameter space, corresponding to very sharp minima. Report issue for preceding element\\nPARITY can be solved well with a scratchpad (Anil et\\xa0al., 2022 ; Liu et\\xa0al., 2023 ) .\\nExisting theoretical accounts of the benefit of intermediate steps for transformers’ expressive capacity (e.g. Merrill and Sabharwal, 2023a ; Feng et\\xa0al., 2023 ) do not account for the benefit of intermediate steps for PARITY-like problems: While the theoretical models of transformer expressivenes used in these studies do predict versions with intermediate steps to be easy, they do not predict that computing PARITY in a single step would be hard, due to Fact 1.\\nThe concept of average sensitivity provides a simple explanation for the benefit of intermediate steps.\\nFormally, we can consider the problem of simulating a finite automaton with state set 𝒳 𝒳 \\\\mathcal{X} caligraphic_X either translating to the final state t n subscript 𝑡 𝑛 {t}_{n} italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT in one go (standard), or to autoregressively translate it into a sequence of states t 1 , … , t n subscript 𝑡 1 … subscript 𝑡 𝑛 {t}_{1},...,{t}_{n} italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (scratchpad). Then (proof in Appendix D ): Report issue for preceding element\\nSimulating an automaton with scratchpad has sensitivity 𝒪 \\u2062 ( 1 ) 𝒪 1 \\\\mathcal{O}(1) caligraphic_O ( 1 ) for each autoregressive step. Report issue for preceding element\\nWe conducted experiments to test the predictions made by our theory, specifically assessing predictions regarding loss landscapes and sharpness of the minima.\\nIn all experiments, we use the transformer encoder architecture, using the default implementation in PyTorch (Paszke et\\xa0al., 2019 ) . Each model is trained to fit a function f 𝑓 f italic_f on a specific sequence length n 𝑛 n italic_n . Each training input x 𝑥 x italic_x is generated uniformly from { ± 1 } n superscript plus-or-minus 1 𝑛 \\\\{\\\\pm 1\\\\}^{n} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , and each input bit, treated as a separate token, is embedded using learned token and positional encodings. 3 3 3 Code is available at https://github.com/lacoco-lab/sensitivity-hardness . Report issue for preceding element\\nThe representation of the last token is passed to a linear layer to generate the prediction T θ \\u2062 ( x ) subscript 𝑇 𝜃 𝑥 T_{\\\\theta}(x) italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) . Parameters θ 𝜃 \\\\theta italic_θ of the transformer are optimized for MSE loss between T θ \\u2062 ( x ) subscript 𝑇 𝜃 𝑥 T_{\\\\theta}(x) italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) and f \\u2062 ( x ) 𝑓 𝑥 f(x) italic_f ( italic_x ) using AdamW (Loshchilov and Hutter, 2017 ) . For full details on hyperparameters and training setup, refer to Appendix E.1 . Report issue for preceding element\\nIn implementation, we assumed versions of the functions outputting to { 0 , 1 } 0 1 \\\\{0,1\\\\} { 0 , 1 } ; we rescaled sharpness values accordingly for comparability with the theory. Report issue for preceding element\\nWe analyzed models using the following metrics: Report issue for preceding element 1. Parameter Norm. We compute the L2 norm of the entire model’s parameter vector, excluding positional encoding matrices.\\nWe discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable. Report issue for preceding element 2. LayerNorm Blowup. This metric is computed by computing the maximum normalization factor ( 5 ) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to Blowup Blowup \\\\operatorname{Blowup} roman_Blowup . 4 4 4 In the theory, Blowup has an additional 1+… in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B \\u2062 l \\u2062 o \\u2062 w \\u2062 u \\u2062 p = ω \\u2062 ( 1 ) 𝐵 𝑙 𝑜 𝑤 𝑢 𝑝 𝜔 1 Blowup=\\\\omega(1) italic_B italic_l italic_o italic_w italic_u italic_p = italic_ω ( 1 ) . Report issue for preceding element 3. Sharpness. In order to avoid committing to any specific ρ 𝜌 \\\\rho italic_ρ , we sample Δ Δ \\\\Delta roman_Δ in ( 15 ) not from the radius- ρ 𝜌 \\\\rho italic_ρ -sphere, but from a mean-zero Gaussian with STD ρ = 0.02 𝜌 0.02 \\\\rho=0.02 italic_ρ = 0.02 .\\nWe estimate using N s , p subscript 𝑁 𝑠 𝑝 N_{s,p} italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and N s , b subscript 𝑁 𝑠 𝑏 N_{s,b} italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x 𝑥 x italic_x . This provides results equivalent to L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the ρ → 0 → 𝜌 0 \\\\rho\\\\rightarrow 0 italic_ρ → 0 asymptotic, while avoiding committing to any specific ρ 𝜌 \\\\rho italic_ρ . Report issue for preceding element\\nParameter Norm. We compute the L2 norm of the entire model’s parameter vector, excluding positional encoding matrices.\\nWe discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable. Report issue for preceding element\\nLayerNorm Blowup. This metric is computed by computing the maximum normalization factor ( 5 ) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to Blowup Blowup \\\\operatorname{Blowup} roman_Blowup . 4 4 4 In the theory, Blowup has an additional 1+… in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B \\u2062 l \\u2062 o \\u2062 w \\u2062 u \\u2062 p = ω \\u2062 ( 1 ) 𝐵 𝑙 𝑜 𝑤 𝑢 𝑝 𝜔 1 Blowup=\\\\omega(1) italic_B italic_l italic_o italic_w italic_u italic_p = italic_ω ( 1 ) . Report issue for preceding element\\nSharpness. In order to avoid committing to any specific ρ 𝜌 \\\\rho italic_ρ , we sample Δ Δ \\\\Delta roman_Δ in ( 15 ) not from the radius- ρ 𝜌 \\\\rho italic_ρ -sphere, but from a mean-zero Gaussian with STD ρ = 0.02 𝜌 0.02 \\\\rho=0.02 italic_ρ = 0.02 .\\nWe estimate using N s , p subscript 𝑁 𝑠 𝑝 N_{s,p} italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and N s , b subscript 𝑁 𝑠 𝑏 N_{s,b} italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x 𝑥 x italic_x . This provides results equivalent to L ρ , n subscript 𝐿 𝜌 𝑛 L_{\\\\rho,n} italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the ρ → 0 → 𝜌 0 \\\\rho\\\\rightarrow 0 italic_ρ → 0 asymptotic, while avoiding committing to any specific ρ 𝜌 \\\\rho italic_ρ . Report issue for preceding element\\nAs we are interested in properties of models that compute given functions, runs that did not converge (evaluation MSE higher than 10 − 3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT ) were discarded. Report issue for preceding element\\nIn this experiment, we train transformers to fit f ∈ { PARITY , MAJORITY , FIRST , MEAN } 𝑓 PARITY MAJORITY FIRST MEAN f\\\\in\\\\{\\\\text{PARITY},\\\\text{MAJORITY},\\\\text{FIRST},\\\\text{MEAN}\\\\} italic_f ∈ { PARITY , MAJORITY , FIRST , MEAN } on sequence lengths from 4 to 30. For each function and sequence length, we retrain the model 10 times from different random initializations. Report issue for preceding element\\nFor PARITY, sharpness stably increases with the input length (Figure 1 ).\\nFor the other functions, whose sensitivity grows more slowly with n 𝑛 n italic_n , (a) the absolute value of sharpness is orders of magnitude lower than for PARITY; (b) there is little increase with n 𝑛 n italic_n .\\nMore results are shown in Appendix E.2 . Report issue for preceding element\\nAt any fixed input length n 𝑛 n italic_n , high sensitivity can be achieved by a combination of large weights and a large LayerNorm blowup.\\nBy Theorem 5 , the product of C 𝐶 C italic_C and squared blowup is bounded from below with some value B n \\u2062 ( f ) subscript 𝐵 𝑛 𝑓 B_{n}(f) italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) . Hence, the product of C 𝐶 \\\\sqrt{C} square-root start_ARG italic_C end_ARG and blowup is bounded with B n \\u2062 ( f ) subscript 𝐵 𝑛 𝑓 \\\\sqrt{B_{n}(f)} square-root start_ARG italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG , and the sum of 1 2 \\u2062 log \\u2061 C 1 2 𝐶 \\\\frac{1}{2}\\\\log C divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_C and log \\u2061 Blowup Blowup \\\\log\\\\mathrm{Blowup} roman_log roman_Blowup is bounded with 1 2 \\u2062 log \\u2061 B n \\u2062 ( f ) 1 2 subscript 𝐵 𝑛 𝑓 \\\\frac{1}{2}\\\\log{B_{n}(f)} divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) .\\nHowever, C 𝐶 C italic_C depends exponentially on parameters, and thus we expect the parameter norm to trade off with the logarithm of the layer norm blowup.\\nMoreover, for PARITY the value of B n \\u2062 ( f ) subscript 𝐵 𝑛 𝑓 B_{n}(f) italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) increases with n 𝑛 n italic_n , and therefore sum of parameter norms and the logarithm of the blowup should also increase with n 𝑛 n italic_n . Report issue for preceding element\\nTo test this prediction, for each function f 𝑓 f italic_f we train a set of models with varying sequence lengths n 𝑛 n italic_n , weight decay and learning rate parameters. It allows us to obtain datapoints with diverse values of LN Blowup and parameter norm. Report issue for preceding element\\nResults for PARITY can be seen in Figure 2 , and for other functions in Figure 9 . For all functions, there is a clear log Blowup-Parameter Norm tradeoff.\\nFor PARITY, the shape of the tradeoff indeed depends on n 𝑛 n italic_n , with transformers trained for high n 𝑛 n italic_n located above others in the log Blowup-Parameter Norm coordinates. For other functions, dependency on n 𝑛 n italic_n is not visible, at least at this range of n 𝑛 n italic_n . Report issue for preceding element\\nAs discussed above, Theorem 6 predicts that transformers will generalize with low sensitivity, as low-sensitivity functions will tend to have flatter minima. Report issue for preceding element\\nTo test this prediction, we created random functions f := { ± 1 } n → { ± 1 } assign 𝑓 superscript plus-or-minus 1 𝑛 → plus-or-minus 1 f:=\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\{\\\\pm 1\\\\} italic_f := { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → { ± 1 } , and sampled random training sets from { ± 1 } n superscript plus-or-minus 1 𝑛 \\\\{\\\\pm 1\\\\}^{n} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT of size 128/256/512. We fixed n = 10 𝑛 10 n=10 italic_n = 10 .\\nFor each f 𝑓 f italic_f , we train a transformer T 1 subscript 𝑇 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on the training set and use it to label the whole input space of sequences of length n 𝑛 n italic_n (1024 objects in our case).\\nReplicating Bhattamishra et\\xa0al. ( 2023 ) , these extrapolated functions T 1 subscript 𝑇 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT have lower average sensitivity than the original functions f 𝑓 f italic_f , indicating a low-sensitivity bias in generalization (Figure 4 ).\\nNow, in order to directly compare the sharpness of minima corresponding to the true function f 𝑓 f italic_f and the extrapolated function T 1 subscript 𝑇 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , we trained new transformers to fit both functions on the entire dataset { ± 1 } n superscript plus-or-minus 1 𝑛 \\\\{\\\\pm 1\\\\}^{n} { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , and measured the sharpness for these two new transformers. The results were averaged over 10 random functions f 𝑓 f italic_f , 10 training sets per f 𝑓 f italic_f and training set size, and 5 new transformers per each T 1 subscript 𝑇 1 T_{1} italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\\nSharpness was indeed lower for the transformer fitting the extrapolated functions than for the transformer matching the original random functions f 𝑓 f italic_f .\\nThis also held when measuring sharpness only on the training set (Appendix, Figure 15 ). Report issue for preceding element\\nBy Theorem 7 , sensitivity of each autoregressive step when computing PARITY with scratchpad is O \\u2062 ( 1 ) 𝑂 1 O(1) italic_O ( 1 ) . Hence, Theorem 6 provides no nontrivial lower bound for L ρ , n \\u2062 ( T ) subscript 𝐿 𝜌 𝑛 𝑇 L_{\\\\rho,n}(T) italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ) .\\nWe trained an Encoder-Decoder Transformer, predicting PARITY of i 𝑖 i italic_i -th substring on i 𝑖 i italic_i -th autoregressive step: t i = PARITY \\u2062 ( x 1 : i ) = x i ⊕ t i − 1 subscript 𝑡 𝑖 PARITY subscript 𝑥 : 1 𝑖 direct-sum subscript 𝑥 𝑖 subscript 𝑡 𝑖 1 {t}_{i}=\\\\mathrm{PARITY}(x_{1:i})=x_{i}\\\\oplus{t}_{i-1} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_PARITY ( italic_x start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊕ italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ( t 0 = 0 subscript 𝑡 0 0 {t}_{0}=0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 ).\\nThe visual dependency between sharpness and length of input for PARITY with a scratchpad is shown in Figure 11 . Even for length around 300, sharpness is low and there is little increase with input length.\\nThus, decrease in sensitivity due to the scratchpad can explain why prior work (Anil et\\xa0al., 2022 ) found that PARITY is easy for Transformers with scratchpad. Report issue for preceding element\\nFigure 3 represents the evolution of loss and sharpness of Transformer models trained for PARITY with input length 25. The results are averaged across 39 converged runs. Similar curves for parameter norm and LayerNorm blowup are presented in Appendix 12 . Report issue for preceding element\\nA dramatic increase in sharpness occurs at exactly the time when loss falls to 0.\\nThis suggests the presence of a steep minimum in the loss landscape, and fitting PARITY requires the optimization procedure to find this minimum.\\nFigure 12 (Appendix) shows further details of this process. During learning, there is a small but sharp increase in the blowup which makes the high sensitivity of the model possible, increasing the left-hand side of the inequality in Corollary 5 . Following that, non-zero weight decay drives the parameter norm down, which – by the theoretically predicted tradeoff between blowup and parameter norm – is accompanied by an exponential increase in blowup. Report issue for preceding element\\nWe have provided a rigorous theoretical explanation of the inductive bias towards low sensitivity observed in empirical research on transformers.\\nTheorem 6 describes a fundamental inductive bias of the transformer architecture: for long inputs, fitting sensitive functions is only possible in sharp minima of the loss.\\nThis holds without assumptions often made in previous work about expressive capacity, such as non-infinite precision (e.g. Merrill and Sabharwal, 2023b ; Angluin et\\xa0al., 2023 ) , hard attention (e.g. Hahn, 2020 ) , or Lipschitz-continuous variants of layer norm (Edelman et\\xa0al., 2022 ) .\\nWe speculate that Theorem 6 reflects a fundamental limitation of parallelized differentiable computing with bounded depth.\\nOur results show that it is overcome by scaling the number of computation steps with the input length.\\nWhile we focused on functions outputting a single label; an interesting direction for future research is the extension of this theory to sequence-to-sequence transductions, for which some empirical data has been reported (e.g. Zhou et\\xa0al., 2023 ) , but theoretical understanding remains wide open. Report issue for preceding element\\nDue to the relation between average sensitivity and Fourier analysis on the Boolean cube (Equation 17 ), a low-sensitivity bias can be viewed as a sequence modeling analogue of a spectral bias towards low frequencies observed in other neural architectures (e.g. Rahaman et\\xa0al., 2019 ; Fridovich-Keil et\\xa0al., 2022 ) . Report issue for preceding element\\nWe note that, while our results show that sensitive transformers are very brittle, these results do not by themselves have implications for real-world generalization , as a low-sensitivity bias need not always be beneficial in real-world setups. Indeed, the relationship between sharpness and real-world generalization is not straightforward (e.g. Andriushchenko et\\xa0al., 2023 ; Kaur et\\xa0al., 2023 ) . Our theory suggests that transformers generalize well to the extent that real-world data has bounded sensitivity (e.g. Hahn et\\xa0al., 2021 ) . Report issue for preceding element\\nWe have proven that, under the transformer architecture, high sensitivity in input space can only be achieved in very sharp minima.\\nEmpirical results confirm the predictions of the theory.\\nTaken together, our results explain a diverse set of empirical observations about transformers not explained by previous theoretical work.\\nThey suggest shifting theoretical research from in-principle expressiveness considerations to studying quantitative bounds and the shape of the loss landscape in order to understand the abilities of transformers. Report issue for preceding element\\nA limitation of our results is that the theoretical results are asymptotic, providing statements about the limit of very long input sequences. Providing more quantitative bounds that tightly characterize finite-length behavior is an interesting problem for future research. Report issue for preceding element\\nA second limitation is that we only target functions outputting a single value. Operationalizing sensitivity-like metrics for sequence-to-sequence functions may be required in order to expand the theory to such functions. Report issue for preceding element\\nThird, our results apply to transformer encoders. It remains open if transformer decoders, with causal attention masking, face a different set of limitations than we have shown here in the absence of masking. Report issue for preceding element\\nFourth, our theoretical results concern the loss landscape, not the training dynamics itself. Further technical advances may be needed to directly prove corresponding results for training dynamics. Report issue for preceding element\\nWe thank Lena Strobl, Dana Angluin, and David Chiang for useful discussion.\\nWe thank David Chiang, Paul Lintilhac, and Yuval Pinter for spotting various errors in a previous version, and the anonymous ARR reviewers for useful feedback. Report issue for preceding element', 'masked_text': 'Given dramatic advances in machine learning applications powered by transformer models, there has been substantial interest in understanding which functions are easier or harder to learn and represent using transformers. Empirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking. For instance, [CITATION] experimentally argued that heldout generalization is biased towards low-degree polynomials and [CITATION] provided empirical evidence that transformers prefer to represent functions of low sensitivity, that is, functions that do not strongly depend on many input bits. Perhaps the most prominent example of such learning biases is a consistent difficulty in learning the PARITY function, mapping bitstrings to their parity. This function is extremely sensitive, in the sense that flipping any bit flips the string’s parity. Empirical studies have consistently found that training transformers to compute parities is difficult, and that solutions for shorter inputs do not generalize to longer inputs [CITATION]. This stands in stark contrast to previously-popular reccurent models which easily fit PARITY with correct length generalization [CITATION].Report issue for preceding element\\nWhile a substantial amount of theoetical work has considered both the learnability [CITATION] and the expressiveness of transformers [CITATION], existing theoretical studies do not consistently explain such learning biases. [CITATION] proved that, under two formal models of self-attention, no transformer can express PARITY at all input lengths. However, various other formal results showed that slightly relaxed assumptions about the transformer architecture resolved such expressiveness limitations. Most notably, [CITATION] found that layer norm, by breaking the Lipschitz assumption used in [CITATION]’s Theorem 2, allows expressing PARITY in principle. Simultaneously, they empirically confirmed that such a solution could not be practically found via (S)GD training. Various other formal models of transformers [CITATION] can also express PARITY despite its empirical difficulty. As already concluded by [CITATION], these findings highlight a disconnect between expressive capacity and learnability: not all functions which transformers may express in principle are also learnt efficiently. Evidently, existing expressiveness theory for transformers is not able to consistently account for the practical learnability of problems under gradient descent.Report issue for preceding element\\nSome prior work has studied the learnability of problems for transformers. For example, [CITATION] bound the statistical capacity of the transformer architecture, showing that on those functions that transformers prefer to represent, they can generalize with good sample efficiency. Notably, they found that sparse parities could indeed be learned well by transformers. However, this result does not prove that PARITY, or other highly sensitive functions, are hard to learn, as that technique does not provide a direct characterization of which functions transformers prefer to represent. Other work has studied simplified setups such as linear attention [CITATION] or individual attention layers [CITATION].Report issue for preceding element\\nHere, we provide results that have direct bearing on the learnability of PARITY and other sensitive functions, characterizing the loss landscape of transformers in terms of input-space sensitivity. We formally prove that, for the transformer architecture, parameter settings achieving high sensitivity in input space are necessarily brittle, so that close neighbors in parameter space will usually define different (typically much less sensitive) functions when inputs are long. As a consequence, transformers fitting high-sensitivity functions must inhabit very steep minima. We argue that this explains both difficulty in training and length generalization for PARITY (observed by [CITATION]), and a low-sensitivity and low-degree bias in random initialization and generalization (observed by [CITATION]).Report issue for preceding element\\nWhile unique hard attention provably cannot represent PARITY [CITATION], more realistic upper bounds accounting for soft attention [CITATION] leave the hardness of sensitive functions unexplained. Not only does PARITY have transformers [CITATION], but it can also be easily represented in formalisms that have been suggested to meaningfully upper-bound the abilities of various formal models of soft-attention:111 [CITATION] suggest that PARITY may not be representable in the RASP-L model, though the expressiveness of RASP-L is not well understood.Report issue for preceding element\\nSimple representations for PARITY, valid across all input lengths, exist in RASP [CITATION], uniform circuits with majority gates [CITATION], and FO[M] [CITATION].Report issue for preceding element\\nWe prove this in Appendix A. Thus, existing expressiveness bounds do not account for the difficulty that transformers encounter in learning sensitive functions, in particular given that previously-popular recurrent models do not encounter this difficulty. Another family of results consists of Lipschitzness bounds [CITATION], which bound the influence that any individual input bit has on the output of a transformer. These turn out to underpredict the abilities of transformers:Report issue for preceding element\\nBy results of [CITATION], the following holds: Consider a transformer without layer norm. If x,x′∈{±1}n𝑥superscript𝑥′superscriptplus-or-minus1𝑛x,x^{\\\\prime}\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x , italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT differ only in the i𝑖iitalic_i-th bit, then at any other position j≠i𝑗𝑖j\\\\neq iitalic_j ≠ italic_i, the output of a transformer differs only by 𝒪\\u2062(1n)𝒪1𝑛\\\\mathcal{O}(\\\\frac{1}{n})caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ).Report issue for preceding element\\nThis accounts for the difficulty of learning PARITY. But the bound suggests even simple sparse functions, such as FIRST (the language 1\\u2062(0|1)∗1superscriptconditional011(0|1)^{*}1 ( 0 | 1 ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT) to be difficult, but transformers learn these well [CITATION]. Indeed, [CITATION] note that the bound is overcome by layer norm or input-length-dependent scaling of attention logits, which enable modeling of sparse functions.Report issue for preceding element\\nWe will show that the observed low-sensitivity bias can be understood in terms of the loss landscape: while transformers can express highly sensitive functions, such transformers are isolated in parameter space, and minima interpolating a sensitive function are very sharp. Indeed, we prove that tiny perturbations of a highly sensitive transformer tend to define, when inputs are sufficiently long, very different functions with much lower sensitivity.Report issue for preceding element\\nWe will focus on boolean functions. Following the conventions in the Analysis of Boolean Functions literature [CITATION] in modeling bitstrings as elements of {−1,1}nsuperscript11𝑛\\\\{-1,1\\\\}^{n}{ - 1 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we assume the alphabet Σ={−1,1}Σ11\\\\Sigma=\\\\{-1,1\\\\}roman_Σ = { - 1 , 1 }, with word embeddings e\\u2062(−1),e\\u2062(+1)∈ℝd𝑒1𝑒1superscriptℝ𝑑e(-1),e(+1)\\\\in\\\\mathbb{R}^{d}italic_e ( - 1 ) , italic_e ( + 1 ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. There further are positional encodings p1,p2,p3,⋯∈ℝdsubscript𝑝1subscript𝑝2subscript𝑝3⋯superscriptℝ𝑑p_{1},p_{2},p_{3},\\\\dots\\\\in\\\\mathbb{R}^{d}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , ⋯ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. At the zero-th layer, token and positional encodings are added: yi(0):=e\\u2062(xi)+piassignsuperscriptsubscript𝑦𝑖0𝑒subscript𝑥𝑖subscript𝑝𝑖y_{i}^{(0)}:=e(x_{i})+p_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT := italic_e ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (i=1,…,n𝑖1…𝑛i=1,\\\\dots,nitalic_i = 1 , … , italic_n), where x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the input string.Report issue for preceding element\\nA transformer has a fixed number L𝐿Litalic_L of layers; the activations yi(k)∈ℝdsuperscriptsubscript𝑦𝑖𝑘superscriptℝ𝑑y_{i}^{(k)}\\\\in\\\\mathbb{R}^{d}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT at position i𝑖iitalic_i of the k𝑘kitalic_k-th layer (k=1,…,L𝑘1…𝐿k=1,\\\\dots,Litalic_k = 1 , … , italic_L) are defined as follows. Each layer has a set of H𝐻Hitalic_H attention heads; we first compute attention scores for the hℎhitalic_h-th head:Report issue for preceding element ai,j(k,h)=superscriptsubscript𝑎𝑖𝑗𝑘ℎabsent\\\\displaystyle a_{i,j}^{(k,h)}=italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = (Kk,h\\u2062yj(k−1))T\\u2062Qk,h\\u2062yi(k−1)superscriptsubscript𝐾𝑘ℎsuperscriptsubscript𝑦𝑗𝑘1𝑇subscript𝑄𝑘ℎsuperscriptsubscript𝑦𝑖𝑘1\\\\displaystyle(K_{k,h}y_{j}^{(k-1)})^{T}Q_{k,h}y_{i}^{(k-1)}( italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT a^i,j(k,h)=superscriptsubscript^𝑎𝑖𝑗𝑘ℎabsent\\\\displaystyle\\\\widehat{a}_{i,j}^{(k,h)}=over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT = exp\\u2061(ai,j(k,h))∑sai,s(k,h)superscriptsubscript𝑎𝑖𝑗𝑘ℎsubscript𝑠superscriptsubscript𝑎𝑖𝑠𝑘ℎ\\\\displaystyle\\\\frac{\\\\exp(a_{i,j}^{(k,h)})}{\\\\sum_{s}a_{i,s}^{(k,h)}}divide start_ARG roman_exp ( italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT end_ARG where Kk,hsubscript𝐾𝑘ℎK_{k,h}italic_K start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (“key”), Qk,hsubscript𝑄𝑘ℎQ_{k,h}italic_Q start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT (“query”) are ∈ℝd×dabsentsuperscriptℝ𝑑𝑑\\\\in\\\\mathbb{R}^{d\\\\times d}∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT. The activation of the head is computed by weighting according to attention weights a^i,j(k,h)superscriptsubscript^𝑎𝑖𝑗𝑘ℎ\\\\hat{a}_{i,j}^{(k,h)}over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT, and applying a linear transformation V𝑉Vitalic_V (“value”):Report issue for preceding element bi,h(k)=∑j=1na^i,j(k,h)\\u2062Vk,h\\u2062yj(k−1)superscriptsubscript𝑏𝑖ℎ𝑘superscriptsubscript𝑗1𝑛superscriptsubscript^𝑎𝑖𝑗𝑘ℎsubscript𝑉𝑘ℎsuperscriptsubscript𝑦𝑗𝑘1b_{i,h}^{(k)}=\\\\sum_{j=1}^{n}\\\\hat{a}_{i,j}^{(k,h)}V_{k,h}y_{j}^{(k-1)}italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k , italic_h ) end_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_k , italic_h end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT (1) The per-position activations are then computed asReport issue for preceding element Yi(k):=fM\\u2062L\\u2062P\\u2062(yi(k−1)+∑h=1Hbi,h(k))assignsuperscriptsubscript𝑌𝑖𝑘superscript𝑓𝑀𝐿𝑃superscriptsubscript𝑦𝑖𝑘1superscriptsubscriptℎ1𝐻superscriptsubscript𝑏𝑖ℎ𝑘Y_{i}^{(k)}:=f^{MLP}\\\\left(y_{i}^{(k-1)}+\\\\sum_{h=1}^{H}b_{i,h}^{(k)}\\\\right)italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (2) where fM\\u2062L\\u2062Psuperscript𝑓𝑀𝐿𝑃f^{MLP}italic_f start_POSTSUPERSCRIPT italic_M italic_L italic_P end_POSTSUPERSCRIPT is a one-layer MLP with a skip-connection. Transformers additionally implement layer norm [CITATION]:Report issue for preceding element L\\u2062a\\u2062y\\u2062e\\u2062r\\u2062N\\u2062o\\u2062r\\u2062m\\u2062(y):=y−m\\u2062e\\u2062a\\u2062n\\u2062(y)σ2\\u2062(y)+ϵassign𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚𝑦𝑦𝑚𝑒𝑎𝑛𝑦superscript𝜎2𝑦italic-ϵLayerNorm(y):=\\\\frac{y-mean(y)}{\\\\sqrt{\\\\sigma^{2}(y)+\\\\epsilon}}italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) := divide start_ARG italic_y - italic_m italic_e italic_a italic_n ( italic_y ) end_ARG start_ARG square-root start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_y ) + italic_ϵ end_ARG end_ARG (3) where ϵ≥0italic-ϵ0\\\\epsilon\\\\geq 0italic_ϵ ≥ 0 is a hyperparameter ensuring numerical stability, and σ2\\u2062(⋅)superscript𝜎2⋅\\\\sigma^{2}(\\\\cdot)italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( ⋅ ) denotes the variance. By design, ‖L\\u2062a\\u2062y\\u2062e\\u2062r\\u2062N\\u2062o\\u2062r\\u2062m\\u2062(y)‖2≤dsubscriptnorm𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚𝑦2𝑑\\\\|LayerNorm(y)\\\\|_{2}\\\\leq\\\\sqrt{d}∥ italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_y ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ square-root start_ARG italic_d end_ARG, with equality at ϵ=0italic-ϵ0\\\\epsilon=0italic_ϵ = 0. Transformer variants differ in where exactly layer norm is applied [CITATION]; we here assume for notational simplicity that layer norm applies after the MLP, but the details are irrelevant to our results, provided layer norm applies at least once. We thus set:Report issue for preceding element yi(k):=L\\u2062a\\u2062y\\u2062e\\u2062r\\u2062N\\u2062o\\u2062r\\u2062m\\u2062(Yi(k))assignsuperscriptsubscript𝑦𝑖𝑘𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚superscriptsubscript𝑌𝑖𝑘y_{i}^{(k)}:=LayerNorm\\\\left(Y_{i}^{(k)}\\\\right)italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) (4) Of key importance will be the the normalization factor:Report issue for preceding element Ni(k):=1σ2\\u2062(Yi(k))+ϵassignsuperscriptsubscript𝑁𝑖𝑘1superscript𝜎2superscriptsubscript𝑌𝑖𝑘italic-ϵN_{i}^{(k)}:=\\\\frac{1}{\\\\sqrt{\\\\sigma^{2}(Y_{i}^{(k)})+\\\\epsilon}}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) + italic_ϵ end_ARG end_ARG (5) Our theoretical results will link Ni(k)superscriptsubscript𝑁𝑖𝑘N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT both to input-space sensitivity and parameter-space sharpness: We will find that large values of Ni(k)superscriptsubscript𝑁𝑖𝑘N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT can increase expressive capacity, but at the price of increased brittleness.Report issue for preceding element\\nFinally, we assume that predictions are made by T:=vo\\u2062u\\u2062tT⋅yn(L)assign𝑇⋅superscriptsubscript𝑣𝑜𝑢𝑡𝑇superscriptsubscript𝑦𝑛𝐿T:=v_{out}^{T}\\\\cdot y_{n}^{(L)}italic_T := italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ⋅ italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT for some parameter vo\\u2062u\\u2062t∈ℝdsubscript𝑣𝑜𝑢𝑡superscriptℝ𝑑v_{out}\\\\in\\\\mathbb{R}^{d}italic_v start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Throughout, we will add the input string x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT as an argument when needed for disambiguation, e.g., writing T\\u2062(x)𝑇𝑥T(x)italic_T ( italic_x ) for the overall prediction made on x𝑥xitalic_x.Report issue for preceding element\\nOur results are centered around average sensitivity, a simple but foundational complexity metric for functions on the Boolean cube [CITATION]:Report issue for preceding element\\nFor a bitstring x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and a function f:{±1}n→ℝ:𝑓→superscriptplus-or-minus1𝑛ℝf:\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R}italic_f : { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R, we writeReport issue for preceding element s\\u2062(x,f):=14\\u2062∑i=1n|f\\u2062(x)−f\\u2062(x⊕i)|2assign𝑠𝑥𝑓14superscriptsubscript𝑖1𝑛superscript𝑓𝑥𝑓superscript𝑥direct-sum𝑖2s(x,f):=\\\\frac{1}{4}\\\\sum_{i=1}^{n}|f(x)-f(x^{{}^{\\\\oplus i}})|^{2}italic_s ( italic_x , italic_f ) := divide start_ARG 1 end_ARG start_ARG 4 end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_f ( italic_x ) - italic_f ( italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ⊕ italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (6) where x⊕isuperscript𝑥direct-sum𝑖x^{{}^{\\\\oplus i}}italic_x start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ⊕ italic_i end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT denotes the bitstring x𝑥xitalic_x with the i𝑖iitalic_i-th bit flipped. The average sensitivity for inputs of length n𝑛nitalic_n isReport issue for preceding element a\\u2062sn\\u2062(f):=12n\\u2062∑x∈{±1}ns\\u2062(x,f)assign𝑎subscript𝑠𝑛𝑓1superscript2𝑛subscript𝑥superscriptplus-or-minus1𝑛𝑠𝑥𝑓as_{n}(f):=\\\\frac{1}{2^{n}}\\\\sum_{x\\\\in\\\\{\\\\pm 1\\\\}^{n}}s(x,f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) := divide start_ARG 1 end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_s ( italic_x , italic_f ) (7)\\nIf f𝑓fitalic_f maps to {±1}plus-or-minus1\\\\{\\\\pm 1\\\\}{ ± 1 }, then s\\u2062(x,f)𝑠𝑥𝑓s(x,f)italic_s ( italic_x , italic_f ) is the number of Hamming neighbors of x𝑥xitalic_x on which f𝑓fitalic_f flips. This definition of a\\u2062sn\\u2062(f)𝑎subscript𝑠𝑛𝑓as_{n}(f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) corresponds to the “total influence” from [CITATION]. We explicitly define average sensitivity relative to input length n𝑛nitalic_n, as we will investigate the behavior of transformers performing a single function f𝑓fitalic_f across varying input lengths. The use of squared distances, rather than simple absolute distances, ensures that results about a\\u2062sn\\u2062(f)𝑎subscript𝑠𝑛𝑓as_{n}(f)italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) transfer to results about degree profiles [CITATION], which we will later investigate (Eq. 17).Report issue for preceding element\\nAverage sensitivity is a general complexity metric with wide-ranging applications in theoretical computer science [CITATION]. It is closely linked to the Fourier analysis on the Boolean cube [CITATION], and is an average-case version of a family of sensitivity measures, closely related to other natural metrics such as decision tree depth and polynomial degree [CITATION]. Both average sensitivity itself [CITATION] and the Fourier structure [CITATION] have been empirically linked to transformers’ generalization behavior. We will ground these empirical findings by relating average sensitivity to loss landscapes for the transformer architecture.Report issue for preceding element\\nOur theoretical results will apply to general functions on the Boolean cube. In order to ground these, we will illustrate transformers’ low-sensitivity bias at the example of a few natural functions which have played a role in the theoretical literature of transformers or are otherwise illustrative of variability in average sensitivity.Report issue for preceding element\\nPARITY indicates whether the number of ones in a bitstring is even (output 1) or odd (output -1); over the input space {±1}nsuperscriptplus-or-minus1𝑛\\\\{\\\\pm 1\\\\}^{n}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and output space {±1}plus-or-minus1\\\\{\\\\pm 1\\\\}{ ± 1 }, it can be formally defined as the map x↦∏i=1nximaps-to𝑥superscriptsubscriptproduct𝑖1𝑛subscript𝑥𝑖x\\\\mapsto\\\\prod_{i=1}^{n}x_{i}italic_x ↦ ∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. As flipping any input bit flips the output, a\\u2062sn\\u2062(f)=n𝑎subscript𝑠𝑛𝑓𝑛as_{n}(f)=nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n. As described above, this function can in principle be represented by transformers, but has empirically been found to be very hard to learn.Report issue for preceding element\\nMAJORITY maps x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT to 1 if #\\u2062{i:xi=1}>n/2#conditional-set𝑖subscript𝑥𝑖1𝑛2\\\\#\\\\{i:x_{i}=1\\\\}>n/2# { italic_i : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 } > italic_n / 2, and −11-1- 1 else. Transformers show good length generalization [CITATION]. It is known that a\\u2062sn\\u2062(f)=Θ\\u2062(n)𝑎subscript𝑠𝑛𝑓Θ𝑛as_{n}(f)=\\\\Theta(\\\\sqrt{n})italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = roman_Θ ( square-root start_ARG italic_n end_ARG ) [CITATION]. However, s\\u2062(f,x)=n𝑠𝑓𝑥𝑛s(f,x)=nitalic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x𝑥xitalic_x are almost fully balanced.Report issue for preceding element\\nFIRST maps x𝑥xitalic_x to its first bit, x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. As only the first bit matters, a\\u2062sn\\u2062(f)=1𝑎subscript𝑠𝑛𝑓1as_{n}(f)=1italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1. It is a simple example of a sparse function; more generally, a k𝑘kitalic_k-PARITY is a restriction of the PARITY function to only k𝑘kitalic_k inputs, where k𝑘kitalic_k is a constant. Transformers learn such sparse functions well [CITATION].Report issue for preceding element\\nMEAN maps x↦1n\\u2062∑i=1nxi∈[−1,1]maps-to𝑥1𝑛superscriptsubscript𝑖1𝑛subscript𝑥𝑖11x\\\\mapsto\\\\frac{1}{n}\\\\sum_{i=1}^{n}x_{i}\\\\in[-1,1]italic_x ↦ divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ [ - 1 , 1 ]. We have a\\u2062sn\\u2062(f)=1n𝑎subscript𝑠𝑛𝑓1𝑛as_{n}(f)=\\\\frac{1}{n}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG.Report issue for preceding element\\nThe PARITY function, when varying the number of bits considered, is a universal basis for Boolean functions, in the sense that any function {±1}n→ℝ→superscriptplus-or-minus1𝑛ℝ\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R can be represented as a linear combination of parities applying to different subsets of {x1,…,xn}subscript𝑥1…subscript𝑥𝑛\\\\{x_{1},\\\\dots,x_{n}\\\\}{ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. Functions are more sensitive when parities applying to larger subsets appear in this representation. We will investigate this connection further below.Report issue for preceding element\\nWe first prove that representing sensitive functions with transformers requires large parameter norms and, when inputs get longer and longer, highly unbounded normalization factors Ni(k)superscriptsubscript𝑁𝑖𝑘N_{i}^{(k)}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT in layer norm (5). We start from the global Lipschitzness bounds developed by [CITATION], but obtain more fine-grained average-case and high-probability bounds. These will then form the basis of our characterization of loss landscape sharpness around sensitive transformers. Our bounds will include a constant C𝐶Citalic_C that is the product ofReport issue for preceding element exp\\u2061(4\\u2062d\\u2062maxh\\u2062∑i=2L‖Ki,hT\\u2062Qi,h‖2)4𝑑subscriptℎsuperscriptsubscript𝑖2𝐿subscriptnormsuperscriptsubscript𝐾𝑖ℎ𝑇subscript𝑄𝑖ℎ2\\\\exp\\\\left(4d\\\\max_{h}\\\\sum_{i=2}^{L}\\\\|K_{i,h}^{T}Q_{i,h}\\\\|_{2}\\\\right)roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ∥ italic_K start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT italic_i , italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (8) and a term polynomial in H𝐻Hitalic_H, d𝑑ditalic_d, the spectral norms of all parameter matrices appearing in the transformer, and the maximum norm of any positional or word embedding. See (32) in the Appendix for formal definition.Report issue for preceding element\\nExisting Lipschitzness bounds (Fact 2) imply a bound along the lines of222Lipschitzness bounds by [CITATION] are not directly applicable here, as these authors consider Lipschitzness in the parameter space, not the effect of changes in the input space as Theorem 3. See also Appendix, Remark 10. Report issue for preceding element s\\u2062(f,x)≤C\\u2062exp\\u2061(4\\u2062d\\u2062maxh\\u2061‖K1,hT\\u2062Q1,h‖2)ϵL/2𝑠𝑓𝑥𝐶4𝑑subscriptℎsubscriptnormsuperscriptsubscript𝐾1ℎ𝑇subscript𝑄1ℎ2superscriptitalic-ϵ𝐿2s(f,x)\\\\leq\\\\frac{C\\\\exp(4d\\\\max_{h}\\\\|K_{1,h}^{T}Q_{1,h}\\\\|_{2})}{\\\\epsilon^{L/2}}italic_s ( italic_f , italic_x ) ≤ divide start_ARG italic_C roman_exp ( 4 italic_d roman_max start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∥ italic_K start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_Q start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (9) uniformly for each x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. [CITATION] noted that the exponential dependency on the spectral norm of the key-query matrix is unavoidable for a global Lipschitzness bound. Our first result is that this dependency can be eliminated at the input layer for the vast majority of inputs, at the cost of a logarithmic factor, leading to a bound of the form (assuming ϵ>0italic-ϵ0\\\\epsilon>0italic_ϵ > 0 in (3))Report issue for preceding element s\\u2062(f,x)≤C\\u2062log\\u2061nϵL/2𝑠𝑓𝑥𝐶𝑛superscriptitalic-ϵ𝐿2s(f,x)\\\\leq C\\\\frac{\\\\log{n}}{\\\\epsilon^{L/2}}italic_s ( italic_f , italic_x ) ≤ italic_C divide start_ARG roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT italic_L / 2 end_POSTSUPERSCRIPT end_ARG (10) for 1−H\\u2062n−21𝐻superscript𝑛21-Hn^{-2}1 - italic_H italic_n start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT of inputs. We show this using a concentration bound argument, combining a Chernoff bound applying to each attention weight individually, with a union bound over all attention weights (Appendix, Lemma 11). Next, we address the role of layer norm. [CITATION] showed that, at ϵ→0→italic-ϵ0\\\\epsilon\\\\rightarrow 0italic_ϵ → 0 in (3), layer norm enables transformers to represent PARITY. Lipschitzness bounds in terms of ϵitalic-ϵ\\\\epsilonitalic_ϵ (10) cease being meaningful in this limit. We thus study the layer-norm induced blowup in more detail. In each layer, we consider the maximum blowup, given an input string x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT:Report issue for preceding element τ(k)\\u2062(x):=maxw=1,…,n\\u2061{1+Nw(k)\\u2062(x)}assignsuperscript𝜏𝑘𝑥subscript𝑤1…𝑛1superscriptsubscript𝑁𝑤𝑘𝑥\\\\tau^{(k)}(x):=\\\\max_{w=1,\\\\dots,n}\\\\{1+N_{w}^{(k)}(x)\\\\}italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) := roman_max start_POSTSUBSCRIPT italic_w = 1 , … , italic_n end_POSTSUBSCRIPT { 1 + italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) } (11) The addition of 1 is for technical reasons (Appendix, Lemma 16); it has little impact in the cases relevant to our results, which will be when τ(k)\\u2062(x)=ω\\u2062(1)superscript𝜏𝑘𝑥𝜔1\\\\tau^{(k)}(x)=\\\\omega(1)italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ) = italic_ω ( 1 ). We then write Blowup\\u2061(x):=∏k=1Lτ(k)\\u2062(x)assignBlowup𝑥superscriptsubscriptproduct𝑘1𝐿superscript𝜏𝑘𝑥\\\\operatorname{Blowup}(x):=\\\\prod_{k=1}^{L}\\\\tau^{(k)}(x)roman_Blowup ( italic_x ) := ∏ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_τ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_x ), an upper bound on the product of the successive layer-norm-induced blowups when going from the input to the output. When ϵ=0italic-ϵ0\\\\epsilon=0italic_ϵ = 0 in (3), Blowup\\u2061(x)Blowup𝑥\\\\operatorname{Blowup}(x)roman_Blowup ( italic_x ) can be arbitrarily large. Foreshadowing Theorem 6, we will find that large values of Blowup\\u2061(x)Blowup𝑥\\\\operatorname{Blowup}(x)roman_Blowup ( italic_x ) create very sharp minima.Report issue for preceding element\\nOur first theorem localizes the layer norm blowup to the Hamming neighborhoods of sensitive inputs:Report issue for preceding element\\nConsider a transformer with layer norm at arbitrary ϵ≥0italic-ϵ0\\\\epsilon\\\\geq 0italic_ϵ ≥ 0. With probability 1−Hn21𝐻superscript𝑛21-\\\\frac{H}{n^{2}}1 - divide start_ARG italic_H end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG over the choice of x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we haveReport issue for preceding element s\\u2062(f,x)C\\u2062n\\u2062log\\u2061n≤Blowup(x)2+1n∑i=1nBlowup(x⊕i)2\\\\frac{s(f,x)}{C\\\\sqrt{n\\\\log n}}\\\\leq\\\\operatorname{Blowup}(x)^{2}+\\\\frac{1}{n}\\\\sum% _{i=1}^{n}\\\\operatorname{Blowup}(x^{\\\\oplus i})^{2}divide start_ARG italic_s ( italic_f , italic_x ) end_ARG start_ARG italic_C square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG ≤ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_Blowup ( italic_x start_POSTSUPERSCRIPT ⊕ italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (12)\\nThe proof is in Appendix B.5. This permits us to state a bound on average sensitivity, in terms of the average layer norm blowup:Report issue for preceding element\\nConsider a transformer with layer norm at arbitrary ϵ≥0italic-ϵ0\\\\epsilon\\\\geq 0italic_ϵ ≥ 0. ThenReport issue for preceding element C⋅𝔼[Blowup(x)2]≥a\\u2062sn\\u2062(f)n\\u2062log\\u2061n−HnC\\\\cdot\\\\mathbb{E}[\\\\operatorname{Blowup}(x)^{2}]\\\\geq\\\\frac{as_{n}(f)}{\\\\sqrt{n\\\\log% {n}}}-\\\\frac{H}{n}italic_C ⋅ blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ≥ divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG - divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG (13) where the expectation is over the uniform distribution over x∈{±1}n𝑥superscriptplus-or-minus1𝑛x\\\\in\\\\{\\\\pm 1\\\\}^{n}italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.Report issue for preceding element\\nThe proof is in Appendix B.6. Note that Hn𝐻𝑛\\\\frac{H}{n}divide start_ARG italic_H end_ARG start_ARG italic_n end_ARG is small when n𝑛nitalic_n is large, and the bound is dominated by a\\u2062sn\\u2062(f)n\\u2062log\\u2061n𝑎subscript𝑠𝑛𝑓𝑛𝑛\\\\frac{as_{n}(f)}{\\\\sqrt{n\\\\log{n}}}divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG start_ARG square-root start_ARG italic_n roman_log italic_n end_ARG end_ARG. This result thus shows a tradeoff between parameters and LN blowup: at least one of them needs to be large to represent a sensitive function. When the sensitivity depends on the input length and grows faster than n\\u2062log\\u2061n𝑛𝑛\\\\sqrt{n\\\\log n}square-root start_ARG italic_n roman_log italic_n end_ARG, this tradeoff changes with the input length, requiring larger parameters or larger layer norm blowup as the input length increases.Report issue for preceding element\\nLet us investigate the implications for the functions introduced in Section above. For PARITY, a\\u2062sn\\u2062(f)=n𝑎subscript𝑠𝑛𝑓𝑛as_{n}(f)=nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = italic_n, and (13) predictsReport issue for preceding element C⋅𝔼[Blowup(x)2]=Ω(nlog\\u2061n)=ω(1)C\\\\cdot\\\\mathbb{E}[\\\\operatorname{Blowup}(x)^{2}]=\\\\Omega\\\\left(\\\\frac{\\\\sqrt{n}}{% \\\\log{n}}\\\\right)=\\\\omega(1)italic_C ⋅ blackboard_E [ roman_Blowup ( italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = roman_Ω ( divide start_ARG square-root start_ARG italic_n end_ARG end_ARG start_ARG roman_log italic_n end_ARG ) = italic_ω ( 1 ) (14) showing that, for fixed parameters, the layer norm blowup needs to increase as the input length increases. For the other functions, the bound is O\\u2062(1)𝑂1O(1)italic_O ( 1 ): For FIRST, s\\u2062(f,x)=a\\u2062sn\\u2062(f)=1𝑠𝑓𝑥𝑎subscript𝑠𝑛𝑓1s(f,x)=as_{n}(f)=1italic_s ( italic_f , italic_x ) = italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = 1, and the RHS of (13) is O\\u2062(1)𝑂1O(1)italic_O ( 1 ). Indeed, sparse functions can be modeled well by a family of transformers where the logits in the input layer scale with log\\u2061n𝑛\\\\log{n}roman_log italic_n [CITATION]. Unlike the prior Lipschitzness bounds (9), these scaled logits do not contribute to C𝐶Citalic_C – our new bound is thus consistent with the ease with which transformers learn sparse functions. For MEAN, s\\u2062(f,x)∼1n2similar-to𝑠𝑓𝑥1superscript𝑛2s(f,x)\\\\sim\\\\frac{1}{n^{2}}italic_s ( italic_f , italic_x ) ∼ divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG; again no blowup is predicted. For MAJORITY, as a\\u2062sn\\u2062(f)∼nsimilar-to𝑎subscript𝑠𝑛𝑓𝑛as_{n}(f)\\\\sim\\\\sqrt{n}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) ∼ square-root start_ARG italic_n end_ARG, no nontrivial average blowup is predicted. However, s\\u2062(f,x)=n𝑠𝑓𝑥𝑛s(f,x)=nitalic_s ( italic_f , italic_x ) = italic_n whenever the ones and zeros in x𝑥xitalic_x are almost fully balanced; on such strings or their Hamming neighbors, (12) predicts B\\u2062l\\u2062o\\u2062w\\u2062u\\u2062p=Ω\\u2062(n1/4)𝐵𝑙𝑜𝑤𝑢𝑝Ωsuperscript𝑛14Blowup=\\\\Omega(n^{1/4})italic_B italic_l italic_o italic_w italic_u italic_p = roman_Ω ( italic_n start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ).Report issue for preceding element\\nLeveraging Corollary 5, we now show that transformers expressing sensitive functions must be very sensitive to small perturbations of model parameters. That is, sensitivity in input space entails sensitivity in parameter space. An important consequence is that, for any highly sensitive function, any interpolating minima will be very sharp. This property is nontrivial, as seen from the fact that it disappears when adding a scratchpad (see below).Report issue for preceding element\\nGiven a parameter vector θ𝜃\\\\thetaitalic_θ defining the transformer Tθsubscript𝑇𝜃T_{\\\\theta}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, the average direction sharpness is [CITATION]Report issue for preceding element Lρ,n\\u2062(T):=𝔼x∈{±1}n\\u2062𝔼‖Δ‖2=ρ\\u2062(Tθ+Δ\\u2062(x)−Tθ\\u2062(x))2assignsubscript𝐿𝜌𝑛𝑇subscript𝔼𝑥superscriptplus-or-minus1𝑛subscript𝔼subscriptnormΔ2𝜌superscriptsubscript𝑇𝜃Δ𝑥subscript𝑇𝜃𝑥2L_{\\\\rho,n}(T):=\\\\mathbb{E}_{x\\\\in\\\\{\\\\pm 1\\\\}^{n}}\\\\mathbb{E}_{\\\\|\\\\Delta\\\\|_{2}=\\\\rho}(% T_{\\\\theta+\\\\Delta}(x)-T_{\\\\theta}(x))^{2}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ) := blackboard_E start_POSTSUBSCRIPT italic_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ∥ roman_Δ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_ρ end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ + roman_Δ end_POSTSUBSCRIPT ( italic_x ) - italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (15) where the second expectation is over the radius-ρ𝜌\\\\rhoitalic_ρ sphere in the space of parameter vectors. Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT quantifies the change in Tθsubscript𝑇𝜃T_{\\\\theta}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT when perturbing θ𝜃\\\\thetaitalic_θ by a size-ρ𝜌\\\\rhoitalic_ρ vector in a random direction. Lower bounds on Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT (Theorem 6) immediately entail lower bounds on other common sharpness measures, such as worst-case sharpness, and (in the limit ρ→0→𝜌0\\\\rho\\\\rightarrow 0italic_ρ → 0) the trace of the loss function’s Hessian at θ𝜃\\\\thetaitalic_θ.Report issue for preceding element\\nIn defining the parameter vector θ𝜃\\\\thetaitalic_θ in (15), we exclude positional encodings, both because they are often frozen, and because their number depends on n𝑛nitalic_n, hindering fair comparison across n𝑛nitalic_n.Report issue for preceding element\\nOur next result lower-bounds Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in terms of the average sensitivity, provided the transformer is sufficiently wide in relation to its depth:Report issue for preceding element\\nLet Tθsubscript𝑇𝜃T_{\\\\theta}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT be a transformer where d>12\\u2062L𝑑12𝐿d>12Litalic_d > 12 italic_L. Assume Tθ\\u2062(x)∈[−1,1]subscript𝑇𝜃𝑥11T_{\\\\theta}(x)\\\\in[-1,1]italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ∈ [ - 1 , 1 ] for each x𝑥xitalic_x. Then:Report issue for preceding element limρ→0lim infn→∞Lρ,n\\u2062(T)≥lim infn→∞a\\u2062sn\\u2062(Tθ)2\\u2062n−L\\u2062exp\\u2061(−Ω\\u2062(d))subscript→𝜌0subscriptlimit-infimum→𝑛subscript𝐿𝜌𝑛𝑇subscriptlimit-infimum→𝑛𝑎subscript𝑠𝑛subscript𝑇𝜃2𝑛𝐿Ω𝑑\\\\lim_{\\\\rho\\\\rightarrow 0}\\\\liminf_{n\\\\rightarrow\\\\infty}L_{\\\\rho,n}(T)\\\\geq\\\\liminf_{% n\\\\rightarrow\\\\infty}\\\\frac{as_{n}(T_{\\\\theta})}{2n}-L\\\\exp(-\\\\Omega(d))roman_lim start_POSTSUBSCRIPT italic_ρ → 0 end_POSTSUBSCRIPT lim inf start_POSTSUBSCRIPT italic_n → ∞ end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ) ≥ lim inf start_POSTSUBSCRIPT italic_n → ∞ end_POSTSUBSCRIPT divide start_ARG italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_n end_ARG - italic_L roman_exp ( - roman_Ω ( italic_d ) ) (16) Here, “Ω\\u2062(d)Ω𝑑\\\\Omega(d)roman_Ω ( italic_d )” scales positively with d𝑑ditalic_d. If Tθsubscript𝑇𝜃T_{\\\\theta}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT has Boolean outputs (Tθ\\u2062(x)∈{±1}subscript𝑇𝜃𝑥plus-or-minus1T_{\\\\theta}(x)\\\\in\\\\{\\\\pm 1\\\\}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ∈ { ± 1 }), then the factor “2” can be eliminated from (16).Report issue for preceding element\\nInformally, this theorem says that, when a\\u2062sn\\u2062(Tθ)∼nsimilar-to𝑎subscript𝑠𝑛subscript𝑇𝜃𝑛as_{n}(T_{\\\\theta})\\\\sim nitalic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ) ∼ italic_n, then even tiny perturbations to the parameters will, in expectation, lead to a substantial change in predictions on long inputs. This means that, as the input gets longer, the Hessian of the mean-squared loss at the minimizer fitting a sensitive function has unboundedly large entries.Report issue for preceding element\\nSee Appendix C for the proof. The key idea of the proof is that, if Tθsubscript𝑇𝜃T_{\\\\theta}italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT is very sensitive in input space, small perturbations to the parameters usually lead to a large drop in sensitivity when n𝑛nitalic_n is large, because they lead to large changes in the layer-norm induced blowup that is needed to represent high-sensitivity functions. As a consequence, transformers computing sensitive functions are isolated in parameter space.Report issue for preceding element\\nThe theorem applies when d𝑑ditalic_d is substantially larger than L𝐿Litalic_L, as is indeed true of typical transformers (e.g., LLaMa 7B has d=4096𝑑4096d=4096italic_d = 4096 and L=32𝐿32L=32italic_L = 32). The convergence of the limits is slower when the parameter-norms, as summarized by C𝐶Citalic_C, are larger, as larger parameters can increase sensitivity. However, remarkably, for any fixed transformer, C𝐶Citalic_C becomes irrelevant for Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the limit where n→∞→𝑛n\\\\rightarrow\\\\inftyitalic_n → ∞ (16).Report issue for preceding element\\nWhile we stated Theorem 6 for an individual transformer, the same statement holds for families of transformers where weights may depend on n𝑛nitalic_n, as long as C𝐶Citalic_C remains bounded. A consequence is that scaling attention logits with log\\u2061n𝑛\\\\log nroman_log italic_n (used to represent sparse functions by [CITATION]) will, at least in the input layer, not mitigate the difficulty of sensitive functions.Report issue for preceding element\\nWe discuss how Theorem 6 unifies a range of diverse empirical findings about the behavior of transformers.Report issue for preceding element\\nFor PARITY, Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT converges to 1111 for large d𝑑ditalic_d, showing that arbitrarily small perturbations to a transformer computing PARITY will lead to a high loss for sufficiently long inputs. Previously, [CITATION] noted that, for their hand-constructed transformer representing PARITY, small changes to a specific parameter led to a large increase in loss, suggesting that this made the solution impossible to reach with SGD. Theorem 6 shows that this phenomenon is unavoidable for any transformer representing a high-sensitivity function. For functions with sublinear average sensitivity, Theorem 6 entails no nontrivial lower bound on sharpness, and no such phenomenon is predicted.Report issue for preceding element\\nA key step in Theorem 6 is to show that s\\u2062(Tθ+Δ,x)𝑠subscript𝑇𝜃Δ𝑥s(T_{\\\\theta+\\\\Delta},x)italic_s ( italic_T start_POSTSUBSCRIPT italic_θ + roman_Δ end_POSTSUBSCRIPT , italic_x ) is bounded with very high probability over the choice of ΔΔ\\\\Deltaroman_Δ (Appendix, Eq. (70)); this immediately entails that high-sensitivity transformers can only inhabit a small volume in parameter space. This explains why randomly initialized transformers empirically show low average sensitivity, more so than recurrent networks [CITATION].Report issue for preceding element\\nAn important corollary is that, for a sensitive function, length generalization requires exact match to the minimum: the slightest deviation from the exact minimum will, in expectation, lead to failure when inputs get sufficient long, even if the minimum itself represents a length-generalizing solution. This provides, for the first time, a rigorous explanation why, despite the in-principle existence of length-generalizing transformers, transformers struggle with length generalization for PARITY [CITATION].Report issue for preceding element\\nAnother corollary is that, in expectation, the training loss landscape around an interpolating minimum places a constraint on a function’s overall sensitivity, even if not a single pair of Hamming neighbors are in the training set. This is because (16) remains true, on average across randomly selected training sets, if replacing the expectation over the full input space with an expectation over the training set in (15). This means that, for long inputs, flat minima of the training loss generalize with bounded sensitivity. To the extent that gradient-based training tends to find flatter minima [CITATION], this provides a theoretical justification for the empirical result that transformers’ generalization behavior, when trained on a Boolean function on some training subset of {±1}nsuperscriptplus-or-minus1𝑛\\\\{\\\\pm 1\\\\}^{n}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, shows a strong bias towards low average sensitivity [CITATION].Report issue for preceding element\\n[CITATION] proposed a min-degree bias, that is, a generalization bias towards functions that are linear combinations of functions that each depend on only a few inputs. Any function f:{±1}n→ℝ:𝑓→superscriptplus-or-minus1𝑛ℝf:\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\mathbb{R}italic_f : { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R can be uniquely written as a linear combination of the multilinear monomials χP\\u2062(x):=∏i∈Pxiassignsubscript𝜒𝑃𝑥subscriptproduct𝑖𝑃subscript𝑥𝑖\\\\chi_{P}(x):=\\\\prod_{i\\\\in P}x_{i}italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ) := ∏ start_POSTSUBSCRIPT italic_i ∈ italic_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where P⊆{1,…,n}𝑃1…𝑛P\\\\subseteq\\\\{1,\\\\dots,n\\\\}italic_P ⊆ { 1 , … , italic_n }: f\\u2062(x)=∑PλP\\u2062χP\\u2062(x)𝑓𝑥subscript𝑃subscript𝜆𝑃subscript𝜒𝑃𝑥f(x)=\\\\sum_{P}\\\\lambda_{P}\\\\chi_{P}(x)italic_f ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_x ). The coefficients λPsubscript𝜆𝑃\\\\lambda_{P}italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT define the Fourier-Walsh transform of f𝑓fitalic_f. For χPsubscript𝜒𝑃\\\\chi_{P}italic_χ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT, both its degree as a polynomial, and its average sensitivity, are |P|𝑃|P|| italic_P |. The degree profile, as defined by [CITATION], is the tuple (d1,…,dn)subscript𝑑1…subscript𝑑𝑛(d_{1},...,d_{n})( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) where di=∑P:|P|=i|λP|2subscript𝑑𝑖subscript:𝑃𝑃𝑖superscriptsubscript𝜆𝑃2d_{i}=\\\\sum_{P:|P|=i}|\\\\lambda_{P}|^{2}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_P : | italic_P | = italic_i end_POSTSUBSCRIPT | italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Minimization of degree profile then refers to setting as many of the later entries to zero, and minimizing the size of the last nonzero entry. [CITATION] proved inductive biases towards functions with a low degree profile for a random features model and diagonal networks. Evidence in the case of transformers was limited to empirical data on three example functions. Our results entail prove an bound on degree profiles for the full transformer architecture, because average sensitivity is a summary statistic of the degree profile [CITATION]:Report issue for preceding element a\\u2062sn\\u2062(f)=∑P⊆{1,…,n}λP2\\u2062|P|=∑i=0ni⋅di𝑎subscript𝑠𝑛𝑓subscript𝑃1…𝑛superscriptsubscript𝜆𝑃2𝑃superscriptsubscript𝑖0𝑛⋅𝑖subscript𝑑𝑖as_{n}(f)=\\\\sum_{P\\\\subseteq\\\\{1,\\\\dots,n\\\\}}\\\\lambda_{P}^{2}|P|=\\\\sum_{i=0}^{n}i% \\\\cdot d_{i}italic_a italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) = ∑ start_POSTSUBSCRIPT italic_P ⊆ { 1 , … , italic_n } end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_P | = ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_i ⋅ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (17) Hence, functions with degree profile assigning substantial weight to degrees of order ∼nsimilar-toabsent𝑛\\\\sim n∼ italic_n are brittle in parameter space, corresponding to very sharp minima.Report issue for preceding element\\nPARITY can be solved well with a scratchpad [CITATION]. Existing theoretical accounts of the benefit of intermediate steps for transformers’ expressive capacity [CITATION] do not account for the benefit of intermediate steps for PARITY-like problems: While the theoretical models of transformer expressivenes used in these studies do predict versions with intermediate steps to be easy, they do not predict that computing PARITY in a single step would be hard, due to Fact 1. The concept of average sensitivity provides a simple explanation for the benefit of intermediate steps. Formally, we can consider the problem of simulating a finite automaton with state set 𝒳𝒳\\\\mathcal{X}caligraphic_X either translating to the final state tnsubscript𝑡𝑛{t}_{n}italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT in one go (standard), or to autoregressively translate it into a sequence of states t1,…,tnsubscript𝑡1…subscript𝑡𝑛{t}_{1},...,{t}_{n}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (scratchpad). Then (proof in Appendix D):Report issue for preceding element\\nSimulating an automaton with scratchpad has sensitivity 𝒪\\u2062(1)𝒪1\\\\mathcal{O}(1)caligraphic_O ( 1 ) for each autoregressive step.Report issue for preceding element\\nWe conducted experiments to test the predictions made by our theory, specifically assessing predictions regarding loss landscapes and sharpness of the minima. In all experiments, we use the transformer encoder architecture, using the default implementation in PyTorch [CITATION]. Each model is trained to fit a function f𝑓fitalic_f on a specific sequence length n𝑛nitalic_n. Each training input x𝑥xitalic_x is generated uniformly from {±1}nsuperscriptplus-or-minus1𝑛\\\\{\\\\pm 1\\\\}^{n}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and each input bit, treated as a separate token, is embedded using learned token and positional encodings.333Code is available at https://github.com/lacoco-lab/sensitivity-hardness.Report issue for preceding element\\nThe representation of the last token is passed to a linear layer to generate the prediction Tθ\\u2062(x)subscript𝑇𝜃𝑥T_{\\\\theta}(x)italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ). Parameters θ𝜃\\\\thetaitalic_θ of the transformer are optimized for MSE loss between Tθ\\u2062(x)subscript𝑇𝜃𝑥T_{\\\\theta}(x)italic_T start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) and f\\u2062(x)𝑓𝑥f(x)italic_f ( italic_x ) using AdamW [CITATION]. For full details on hyperparameters and training setup, refer to Appendix E.1.Report issue for preceding element\\nIn implementation, we assumed versions of the functions outputting to {0,1}01\\\\{0,1\\\\}{ 0 , 1 }; we rescaled sharpness values accordingly for comparability with the theory.Report issue for preceding element\\nWe analyzed models using the following metrics:Report issue for preceding element 1. Parameter Norm. We compute the L2 norm of the entire model’s parameter vector, excluding positional encoding matrices. We discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable.Report issue for preceding element 2. LayerNorm Blowup. This metric is computed by computing the maximum normalization factor (5) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to BlowupBlowup\\\\operatorname{Blowup}roman_Blowup.444In the theory, Blowup has an additional 1+… in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B\\u2062l\\u2062o\\u2062w\\u2062u\\u2062p=ω\\u2062(1)𝐵𝑙𝑜𝑤𝑢𝑝𝜔1Blowup=\\\\omega(1)italic_B italic_l italic_o italic_w italic_u italic_p = italic_ω ( 1 ).Report issue for preceding element 3. Sharpness. In order to avoid committing to any specific ρ𝜌\\\\rhoitalic_ρ, we sample ΔΔ\\\\Deltaroman_Δ in (15) not from the radius-ρ𝜌\\\\rhoitalic_ρ-sphere, but from a mean-zero Gaussian with STD ρ=0.02𝜌0.02\\\\rho=0.02italic_ρ = 0.02. We estimate using Ns,psubscript𝑁𝑠𝑝N_{s,p}italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and Ns,bsubscript𝑁𝑠𝑏N_{s,b}italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x𝑥xitalic_x. This provides results equivalent to Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the ρ→0→𝜌0\\\\rho\\\\rightarrow 0italic_ρ → 0 asymptotic, while avoiding committing to any specific ρ𝜌\\\\rhoitalic_ρ.Report issue for preceding element\\nParameter Norm. We compute the L2 norm of the entire model’s parameter vector, excluding positional encoding matrices. We discard the norm of positional encodings, so that the norms of the models trained for different sequence lengths are comparable.Report issue for preceding element\\nLayerNorm Blowup. This metric is computed by computing the maximum normalization factor (5) across the entire layer in each application of layer norm, and take the product over all applications of layer norm. This essentially corresponds to BlowupBlowup\\\\operatorname{Blowup}roman_Blowup.444In the theory, Blowup has an additional 1+… in each factor for technical reasons. This difference is immaterial, as we are interested in situations where B\\u2062l\\u2062o\\u2062w\\u2062u\\u2062p=ω\\u2062(1)𝐵𝑙𝑜𝑤𝑢𝑝𝜔1Blowup=\\\\omega(1)italic_B italic_l italic_o italic_w italic_u italic_p = italic_ω ( 1 ).Report issue for preceding element\\nSharpness. In order to avoid committing to any specific ρ𝜌\\\\rhoitalic_ρ, we sample ΔΔ\\\\Deltaroman_Δ in (15) not from the radius-ρ𝜌\\\\rhoitalic_ρ-sphere, but from a mean-zero Gaussian with STD ρ=0.02𝜌0.02\\\\rho=0.02italic_ρ = 0.02. We estimate using Ns,psubscript𝑁𝑠𝑝N_{s,p}italic_N start_POSTSUBSCRIPT italic_s , italic_p end_POSTSUBSCRIPT perturbations and Ns,bsubscript𝑁𝑠𝑏N_{s,b}italic_N start_POSTSUBSCRIPT italic_s , italic_b end_POSTSUBSCRIPT input strings x𝑥xitalic_x. This provides results equivalent to Lρ,nsubscript𝐿𝜌𝑛L_{\\\\rho,n}italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT in the ρ→0→𝜌0\\\\rho\\\\rightarrow 0italic_ρ → 0 asymptotic, while avoiding committing to any specific ρ𝜌\\\\rhoitalic_ρ.Report issue for preceding element\\nAs we are interested in properties of models that compute given functions, runs that did not converge (evaluation MSE higher than 10−3superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT) were discarded.Report issue for preceding element\\nIn this experiment, we train transformers to fit f∈{PARITY,MAJORITY,FIRST,MEAN}𝑓PARITYMAJORITYFIRSTMEANf\\\\in\\\\{\\\\text{PARITY},\\\\text{MAJORITY},\\\\text{FIRST},\\\\text{MEAN}\\\\}italic_f ∈ { PARITY , MAJORITY , FIRST , MEAN } on sequence lengths from 4 to 30. For each function and sequence length, we retrain the model 10 times from different random initializations.Report issue for preceding element\\nFor PARITY, sharpness stably increases with the input length (Figure 1). For the other functions, whose sensitivity grows more slowly with n𝑛nitalic_n, (a) the absolute value of sharpness is orders of magnitude lower than for PARITY; (b) there is little increase with n𝑛nitalic_n. More results are shown in Appendix E.2.Report issue for preceding element\\nAt any fixed input length n𝑛nitalic_n, high sensitivity can be achieved by a combination of large weights and a large LayerNorm blowup. By Theorem 5, the product of C𝐶Citalic_C and squared blowup is bounded from below with some value Bn\\u2062(f)subscript𝐵𝑛𝑓B_{n}(f)italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ). Hence, the product of C𝐶\\\\sqrt{C}square-root start_ARG italic_C end_ARG and blowup is bounded with Bn\\u2062(f)subscript𝐵𝑛𝑓\\\\sqrt{B_{n}(f)}square-root start_ARG italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) end_ARG, and the sum of 12\\u2062log\\u2061C12𝐶\\\\frac{1}{2}\\\\log Cdivide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_C and log\\u2061BlowupBlowup\\\\log\\\\mathrm{Blowup}roman_log roman_Blowup is bounded with 12\\u2062log\\u2061Bn\\u2062(f)12subscript𝐵𝑛𝑓\\\\frac{1}{2}\\\\log{B_{n}(f)}divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ). However, C𝐶Citalic_C depends exponentially on parameters, and thus we expect the parameter norm to trade off with the logarithm of the layer norm blowup. Moreover, for PARITY the value of Bn\\u2062(f)subscript𝐵𝑛𝑓B_{n}(f)italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f ) increases with n𝑛nitalic_n, and therefore sum of parameter norms and the logarithm of the blowup should also increase with n𝑛nitalic_n.Report issue for preceding element\\nTo test this prediction, for each function f𝑓fitalic_f we train a set of models with varying sequence lengths n𝑛nitalic_n, weight decay and learning rate parameters. It allows us to obtain datapoints with diverse values of LN Blowup and parameter norm.Report issue for preceding element\\nResults for PARITY can be seen in Figure 2, and for other functions in Figure 9. For all functions, there is a clear log Blowup-Parameter Norm tradeoff. For PARITY, the shape of the tradeoff indeed depends on n𝑛nitalic_n, with transformers trained for high n𝑛nitalic_n located above others in the log Blowup-Parameter Norm coordinates. For other functions, dependency on n𝑛nitalic_n is not visible, at least at this range of n𝑛nitalic_n.Report issue for preceding element\\nAs discussed above, Theorem 6 predicts that transformers will generalize with low sensitivity, as low-sensitivity functions will tend to have flatter minima.Report issue for preceding element\\nTo test this prediction, we created random functions f:={±1}n→{±1}assign𝑓superscriptplus-or-minus1𝑛→plus-or-minus1f:=\\\\{\\\\pm 1\\\\}^{n}\\\\rightarrow\\\\{\\\\pm 1\\\\}italic_f := { ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → { ± 1 }, and sampled random training sets from {±1}nsuperscriptplus-or-minus1𝑛\\\\{\\\\pm 1\\\\}^{n}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT of size 128/256/512. We fixed n=10𝑛10n=10italic_n = 10. For each f𝑓fitalic_f, we train a transformer T1subscript𝑇1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on the training set and use it to label the whole input space of sequences of length n𝑛nitalic_n (1024 objects in our case). Replicating [CITATION], these extrapolated functions T1subscript𝑇1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT have lower average sensitivity than the original functions f𝑓fitalic_f, indicating a low-sensitivity bias in generalization (Figure 4). Now, in order to directly compare the sharpness of minima corresponding to the true function f𝑓fitalic_f and the extrapolated function T1subscript𝑇1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we trained new transformers to fit both functions on the entire dataset {±1}nsuperscriptplus-or-minus1𝑛\\\\{\\\\pm 1\\\\}^{n}{ ± 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and measured the sharpness for these two new transformers. The results were averaged over 10 random functions f𝑓fitalic_f, 10 training sets per f𝑓fitalic_f and training set size, and 5 new transformers per each T1subscript𝑇1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Sharpness was indeed lower for the transformer fitting the extrapolated functions than for the transformer matching the original random functions f𝑓fitalic_f. This also held when measuring sharpness only on the training set (Appendix, Figure 15).Report issue for preceding element\\nBy Theorem 7, sensitivity of each autoregressive step when computing PARITY with scratchpad is O\\u2062(1)𝑂1O(1)italic_O ( 1 ). Hence, Theorem 6 provides no nontrivial lower bound for Lρ,n\\u2062(T)subscript𝐿𝜌𝑛𝑇L_{\\\\rho,n}(T)italic_L start_POSTSUBSCRIPT italic_ρ , italic_n end_POSTSUBSCRIPT ( italic_T ). We trained an Encoder-Decoder Transformer, predicting PARITY of i𝑖iitalic_i-th substring on i𝑖iitalic_i-th autoregressive step: ti=PARITY\\u2062(x1:i)=xi⊕ti−1subscript𝑡𝑖PARITYsubscript𝑥:1𝑖direct-sumsubscript𝑥𝑖subscript𝑡𝑖1{t}_{i}=\\\\mathrm{PARITY}(x_{1:i})=x_{i}\\\\oplus{t}_{i-1}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_PARITY ( italic_x start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊕ italic_t start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT (t0=0subscript𝑡00{t}_{0}=0italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0). The visual dependency between sharpness and length of input for PARITY with a scratchpad is shown in Figure 11. Even for length around 300, sharpness is low and there is little increase with input length. Thus, decrease in sensitivity due to the scratchpad can explain why prior work [CITATION] found that PARITY is easy for Transformers with scratchpad.Report issue for preceding element\\nFigure 3 represents the evolution of loss and sharpness of Transformer models trained for PARITY with input length 25. The results are averaged across 39 converged runs. Similar curves for parameter norm and LayerNorm blowup are presented in Appendix 12.Report issue for preceding element\\nA dramatic increase in sharpness occurs at exactly the time when loss falls to 0. This suggests the presence of a steep minimum in the loss landscape, and fitting PARITY requires the optimization procedure to find this minimum. Figure 12 (Appendix) shows further details of this process. During learning, there is a small but sharp increase in the blowup which makes the high sensitivity of the model possible, increasing the left-hand side of the inequality in Corollary 5. Following that, non-zero weight decay drives the parameter norm down, which – by the theoretically predicted tradeoff between blowup and parameter norm – is accompanied by an exponential increase in blowup.Report issue for preceding element\\nWe have provided a rigorous theoretical explanation of the inductive bias towards low sensitivity observed in empirical research on transformers. Theorem 6 describes a fundamental inductive bias of the transformer architecture: for long inputs, fitting sensitive functions is only possible in sharp minima of the loss. This holds without assumptions often made in previous work about expressive capacity, such as non-infinite precision [CITATION], hard attention [CITATION], or Lipschitz-continuous variants of layer norm [CITATION]. We speculate that Theorem 6 reflects a fundamental limitation of parallelized differentiable computing with bounded depth. Our results show that it is overcome by scaling the number of computation steps with the input length. While we focused on functions outputting a single label; an interesting direction for future research is the extension of this theory to sequence-to-sequence transductions, for which some empirical data has been reported [CITATION], but theoretical understanding remains wide open.Report issue for preceding element\\nDue to the relation between average sensitivity and Fourier analysis on the Boolean cube (Equation 17), a low-sensitivity bias can be viewed as a sequence modeling analogue of a spectral bias towards low frequencies observed in other neural architectures [CITATION].Report issue for preceding element\\nWe note that, while our results show that sensitive transformers are very brittle, these results do not by themselves have implications for real-world generalization, as a low-sensitivity bias need not always be beneficial in real-world setups. Indeed, the relationship between sharpness and real-world generalization is not straightforward [CITATION]. Our theory suggests that transformers generalize well to the extent that real-world data has bounded sensitivity [CITATION].Report issue for preceding element\\nWe have proven that, under the transformer architecture, high sensitivity in input space can only be achieved in very sharp minima. Empirical results confirm the predictions of the theory. Taken together, our results explain a diverse set of empirical observations about transformers not explained by previous theoretical work. They suggest shifting theoretical research from in-principle expressiveness considerations to studying quantitative bounds and the shape of the loss landscape in order to understand the abilities of transformers.Report issue for preceding element\\nA limitation of our results is that the theoretical results are asymptotic, providing statements about the limit of very long input sequences. Providing more quantitative bounds that tightly characterize finite-length behavior is an interesting problem for future research.Report issue for preceding element\\nA second limitation is that we only target functions outputting a single value. Operationalizing sensitivity-like metrics for sequence-to-sequence functions may be required in order to expand the theory to such functions.Report issue for preceding element\\nThird, our results apply to transformer encoders. It remains open if transformer decoders, with causal attention masking, face a different set of limitations than we have shown here in the absence of masking.Report issue for preceding element\\nFourth, our theoretical results concern the loss landscape, not the training dynamics itself. Further technical advances may be needed to directly prove corresponding results for training dynamics.Report issue for preceding element\\nWe thank Lena Strobl, Dana Angluin, and David Chiang for useful discussion. We thank David Chiang, Paul Lintilhac, and Yuval Pinter for spotting various errors in a previous version, and the anonymous ARR reviewers for useful feedback.Report issue for preceding element', 'citations': [{'tag': 'Li et\\xa0al. (2023)', 'title': 'Transformers\\nas algorithms: Generalization and stability in in-context learning.', 'authors': 'Yingcong Li, Muhammed\\xa0Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.\\n2023.', 'journal': 'InProceedings of the 40th International Conference on Machine\\nLearning, volume 202 ofProceedings of Machine Learning Research,\\npages 19565–19594. PMLR.'}, {'tag': 'Jiang et\\xa0al. (2020)', 'title': 'Fantastic\\ngeneralization measures and where to find them.', 'authors': 'Yiding Jiang, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy\\nBengio. 2020.', 'journal': 'InInternational Conference on Learning Representations.'}, {'tag': 'Hatami et\\xa0al. (2010)', 'title': 'Variations on the sensitivity conjecture.', 'authors': 'Pooya Hatami, Raghav Kulkarni, and Denis Pankratov. 2010.', 'journal': 'Theory of Computing, 4:1–27.'}, {'tag': 'De\\xa0Wolf (2008)', 'title': 'A brief introduction to fourier analysis on the boolean cube.', 'authors': 'Ronald De\\xa0Wolf. 2008.', 'journal': 'Theory of Computing, pages 1–20.'}, {'tag': 'Merrill and Sabharwal (2023b)', 'title': 'A logic for expressing log-precision transformers.', 'authors': 'William Merrill and Ashish Sabharwal. 2023b.', 'journal': 'InThirty-seventh Conference on Neural Information Processing\\nSystems.'}, {'tag': 'Damian et\\xa0al. (2023)', 'title': 'Self-stabilization:\\nThe implicit bias of gradient descent at the edge of stability.', 'authors': 'Alex Damian, Eshaan Nichani, and Jason\\xa0D. Lee. 2023.', 'journal': 'InThe Eleventh International Conference on Learning\\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.'}, {'tag': 'Paszke et\\xa0al. (2019)', 'title': 'Pytorch: An imperative style, high-performance deep learning library.', 'authors': 'Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\xa0Fang, Junjie Bai, and Soumith\\nChintala. 2019.', 'journal': 'InAdvances in Neural Information Processing Systems,\\nvolume\\xa032. Curran Associates, Inc.'}, {'tag': 'Yun et\\xa0al. (2019)', 'title': 'Are transformers universal\\napproximators of sequence-to-sequence functions?', 'authors': 'Chulhee Yun, Srinadh Bhojanapalli, Ankit\\xa0Singh Rawat, Sashank\\xa0J. Reddi, and\\nSanjiv Kumar. 2019.', 'journal': ''}, {'tag': 'Chiang and Cholak (2022)', 'title': 'Overcoming a\\ntheoretical limitation of self-attention.', 'authors': 'David Chiang and Peter Cholak. 2022.', 'journal': 'InProceedings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\\nIreland, May 22-27, 2022, pages 7654–7664. Association for Computational\\nLinguistics.'}, {'tag': 'Weiss et\\xa0al. (2021)', 'title': 'Thinking like transformers.', 'authors': 'Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.', 'journal': 'InInternational Conference on Machine Learning, pages\\n11080–11090. PMLR.'}, {'tag': 'Strobl et\\xa0al. (2023)', 'title': 'Transformers as\\nrecognizers of formal languages: A survey on expressivity.', 'authors': 'Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin. 2023.', 'journal': 'CoRR, abs/2311.00208.'}, {'tag': 'Kahn et\\xa0al. (1988)', 'title': 'The influence of variables on boolean functions.', 'authors': 'J.\\xa0Kahn, G.\\xa0Kalai, and N.\\xa0Linial. 1988.', 'journal': 'In[Proceedings 1988] 29th Annual Symposium on Foundations of\\nComputer Science, pages 68–80.'}, {'tag': 'Anil et\\xa0al. (2022)', 'title': 'Exploring length generalization in large language models.', 'authors': 'Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay\\nRamasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022.', 'journal': 'Advances in Neural Information Processing Systems,\\n35:38546–38556.'}, {'tag': 'Sanford et\\xa0al. (2023)', 'title': 'Representational\\nstrengths and limitations of transformers.', 'authors': 'Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2023.', 'journal': 'CoRR, abs/2306.02896.'}, {'tag': 'Yao et\\xa0al. (2021)', 'title': 'Self-attention\\nnetworks can process bounded hierarchical languages.', 'authors': 'Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. 2021.', 'journal': 'InProceedings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th International Joint Conference on\\nNatural Language Processing (Volume 1: Long Papers). Association for\\nComputational Linguistics.'}, {'tag': 'Ahn et\\xa0al. (2023)', 'title': 'Linear attention is (maybe)\\nall you need (to understand transformer optimization).', 'authors': 'Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit\\nSra. 2023.', 'journal': ''}, {'tag': 'Hahn et\\xa0al. (2021)', 'title': 'Sensitivity as a complexity measure for sequence classification tasks.', 'authors': 'Michael Hahn, Dan Jurafsky, and Richard Futrell. 2021.', 'journal': 'Transactions of the Association for Computational Linguistics,\\n9:891–908.'}, {'tag': 'Ruoss et\\xa0al. (2023)', 'title': 'Randomized positional\\nencodings boost length generalization of transformers.', 'authors': 'Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert\\nCsordás, Mehdi Bennani, Shane Legg, and Joel Veness. 2023.', 'journal': ''}, {'tag': 'Jukna (2012)', 'title': 'Boolean Function Complexity: Advances and Frontiers.', 'authors': 'Stasys Jukna. 2012.', 'journal': ''}, {'tag': 'Kaur et\\xa0al. (2023)', 'title': 'On the maximum hessian eigenvalue and generalization.', 'authors': 'Simran Kaur, Jeremy Cohen, and Zachary\\xa0Chase Lipton. 2023.', 'journal': 'InProceedings on, pages 51–65. PMLR.'}, {'tag': 'Bhattamishra et\\xa0al. (2023)', 'title': 'Simplicity\\nbias in transformers and their ability to learn sparse boolean functions.', 'authors': 'Satwik Bhattamishra, Arkil Patel, Varun Kanade, and Phil Blunsom. 2023.', 'journal': 'InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\\nCanada, July 9-14, 2023, pages 5767–5791. Association for Computational\\nLinguistics.'}, {'tag': 'Angluin et\\xa0al. (2023)', 'title': 'Masked hard-attention transformers and boolean rasp recognize exactly\\nthe star-free languages.', 'authors': 'Dana Angluin, David Chiang, and Andy Yang. 2023.', 'journal': 'arXiv preprint arXiv:2310.13897.'}, {'tag': 'Chiang et\\xa0al. (2023)', 'title': 'Tighter\\nbounds on the expressivity of transformer encoders.', 'authors': 'David Chiang, Peter Cholak, and Anand Pillay. 2023.', 'journal': 'InInternational Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\\nMachine Learning Research, pages 5544–5562. PMLR.'}, {'tag': 'Feng et\\xa0al. (2023)', 'title': 'Towards revealing\\nthe mystery behind chain of thought: A theoretical perspective.', 'authors': 'Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di\\xa0He, and Liwei Wang. 2023.', 'journal': 'InThirty-seventh Conference on Neural Information Processing\\nSystems.'}, {'tag': 'Rahaman et\\xa0al. (2019)', 'title': 'On the\\nspectral bias of neural networks.', 'authors': 'Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred\\nHamprecht, Yoshua Bengio, and Aaron Courville. 2019.', 'journal': 'InProceedings of the 36th International Conference on Machine\\nLearning, volume\\xa097 ofProceedings of Machine Learning Research,\\npages 5301–5310. PMLR.'}, {'tag': 'Ba et\\xa0al. (2016)', 'title': 'Layer normalization.', 'authors': 'Lei\\xa0Jimmy Ba, Jamie\\xa0Ryan Kiros, and Geoffrey\\xa0E. Hinton. 2016.', 'journal': 'CoRR, abs/1607.06450.'}, {'tag': 'Delétang et\\xa0al. (2023)', 'title': 'Neural networks\\nand the chomsky hierarchy.', 'authors': 'Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein,\\nLi\\xa0Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel\\nVeness, and Pedro\\xa0A. Ortega. 2023.', 'journal': ''}, {'tag': 'Merrill et\\xa0al. (2022)', 'title': 'Saturated transformers are constant-depth threshold circuits.', 'authors': 'William Merrill, Ashish Sabharwal, and Noah\\xa0A. Smith. 2022.', 'journal': 'Trans. Assoc. Comput. Linguistics, 10:843–856.'}, {'tag': 'Foret et\\xa0al. (2020)', 'title': 'Sharpness-aware minimization for efficiently improving\\ngeneralization.', 'authors': 'Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2020.', 'journal': 'arXiv preprint arXiv:2010.01412.'}, {'tag': 'Hahn (2020)', 'title': 'Theoretical limitations of self-attention in neural sequence models.', 'authors': 'Michael Hahn. 2020.', 'journal': 'Transactions of the Association for Computational Linguistics,\\n8:156–171.'}, {'tag': 'O’Donnell (2014)', 'title': 'Analysis of Boolean Functions.', 'authors': 'Ryan O’Donnell. 2014.', 'journal': 'Cambridge University Press.'}, {'tag': 'Andriushchenko et\\xa0al. (2023)', 'title': 'A\\nmodern look at the relationship between sharpness and generalization.', 'authors': 'Maksym Andriushchenko, Francesco Croce, Maximilian Müller, Matthias Hein,\\nand Nicolas Flammarion. 2023.', 'journal': 'InInternational Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\\nMachine Learning Research, pages 840–902. PMLR.'}, {'tag': 'Bhattamishra et\\xa0al. (2020)', 'title': 'On the\\nability and limitations of transformers to recognize formal languages.', 'authors': 'Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020.', 'journal': 'InProceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing, EMNLP 2020, Online, November 16-20, 2020,\\npages 7096–7116. Association for Computational Linguistics.'}, {'tag': 'Takase et\\xa0al. (2022)', 'title': 'On layer normalizations and residual connections in transformers.', 'authors': 'Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. 2022.', 'journal': 'arXiv preprint arXiv:2206.00330.'}, {'tag': 'Liu et\\xa0al. (2023)', 'title': 'Transformers\\nlearn shortcuts to automata.', 'authors': 'Bingbin Liu, Jordan\\xa0T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.\\n2023.', 'journal': 'InThe Eleventh International Conference on Learning\\nRepresentations.'}, {'tag': 'Wen et\\xa0al. (2022)', 'title': 'How does\\nsharpness-aware minimization minimize sharpness?', 'authors': 'Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. 2022.', 'journal': 'CoRR, abs/2211.05729.'}, {'tag': 'Fridovich-Keil et\\xa0al. (2022)', 'title': 'Spectral bias in practice: The role of function frequency in\\ngeneralization.', 'authors': 'Sara Fridovich-Keil, Raphael Gontijo\\xa0Lopes, and Rebecca Roelofs. 2022.', 'journal': 'InAdvances in Neural Information Processing Systems,\\nvolume\\xa035, pages 7368–7382. Curran Associates, Inc.'}, {'tag': 'Abbe et\\xa0al. (2023)', 'title': 'Generalization on the unseen, logic reasoning and degree curriculum.', 'authors': 'Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. 2023.', 'journal': 'InInternational Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of\\nMachine Learning Research, pages 31–60. PMLR.'}, {'tag': 'Merrill and Sabharwal (2023c)', 'title': 'The parallelism tradeoff: Limitations of log-precision transformers.', 'authors': 'William Merrill and Ashish Sabharwal. 2023c.', 'journal': 'Transactions of the Association for Computational Linguistics,\\n11:531–545.'}, {'tag': 'Zhou et\\xa0al. (2023)', 'title': 'What algorithms can transformers learn? a study in length\\ngeneralization.', 'authors': 'Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh\\nSusskind, Samy Bengio, and Preetum Nakkiran. 2023.', 'journal': 'arXiv preprint arXiv:2310.16028.'}, {'tag': 'Edelman et\\xa0al. (2022)', 'title': 'Inductive biases and variable creation in self-attention mechanisms.', 'authors': 'Benjamin\\xa0L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. 2022.', 'journal': 'InInternational Conference on Machine Learning, pages\\n5793–5831. PMLR.'}, {'tag': 'Merrill and Sabharwal (2023a)', 'title': 'The expressive\\npower of transformers with chain of thought.', 'authors': 'William Merrill and Ashish Sabharwal. 2023a.', 'journal': 'InNeurIPS 2023 Workshop on Mathematics of Modern Machine\\nLearning.'}, {'tag': 'Hao et\\xa0al. (2022)', 'title': 'Formal language recognition by hard attention transformers:\\nPerspectives from circuit complexity.', 'authors': 'Yiding Hao, Dana Angluin, and Robert Frank. 2022.', 'journal': 'Transactions of the Association for Computational Linguistics,\\n10:800–810.'}, {'tag': 'Loshchilov and Hutter (2017)', 'title': 'Decoupled\\nweight decay regularization.', 'authors': 'Ilya Loshchilov and Frank Hutter. 2017.', 'journal': 'InInternational Conference on Learning Representations.'}, {'tag': 'Strobl (2023)', 'title': 'Average-hard\\nattention transformers are constant-depth uniform threshold circuits.', 'authors': 'Lena Strobl. 2023.', 'journal': 'CoRR, abs/2308.03212.'}]}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "html_path = r'data\\input\\ACL_papers\\html\\best_and_best_social_impact\\Why are Sensitive Functions Hard for Transformers_.html'\n",
    "\n",
    "bib_entries = extract_citation_info_from_html(html_path)\n",
    "\n",
    "with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "full_text = []\n",
    "full_masked_text = []\n",
    "full_citation_keys = set()\n",
    "sections_dict = {}\n",
    "paragraphs_dict = {}\n",
    "\n",
    "\n",
    "for section in soup.find_all(\"section\", class_=\"ltx_section\"):\n",
    "    section_id = section.get(\"id\", \"\")\n",
    "    section_title_tag = section.find(\"h2\", class_=\"ltx_title\")\n",
    "    section_title = section_title_tag.get_text(strip=True) if section_title_tag else section_id\n",
    "\n",
    "    section_citations_keys = set()\n",
    "    section_masked_text = []\n",
    "    section_text = []\n",
    "\n",
    "    for para in section.find_all(\"div\", class_=\"ltx_para\"):\n",
    "        para_id = para.get(\"id\", \"\")\n",
    "        para_text = para.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        masked_text, citations_keys = get_masked_text_and_citations(para, bib_entries)\n",
    "\n",
    "        citations = [bib_entries[key] for key in citations_keys if key in bib_entries]\n",
    "\n",
    "        paragraphs_dict[para_id] = {\n",
    "            \"text\": para_text,\n",
    "            \"masked_text\": masked_text,\n",
    "            \"citations\": citations\n",
    "        }\n",
    "\n",
    "        section_masked_text.append(masked_text)\n",
    "        full_masked_text.append(masked_text)\n",
    "        \n",
    "        section_text.append(para_text)\n",
    "        full_text.append(para_text)\n",
    "\n",
    "        section_citations_keys.update(citations_keys)\n",
    "\n",
    "    section_citations = [bib_entries[key] for key in section_citations_keys if key in bib_entries]\n",
    "    sections_dict[section_id] = {\n",
    "        \"title\": section_title,\n",
    "        \"text\": \"\\n\".join(section_text),\n",
    "        \"masked_text\": \"\\n\".join(section_masked_text),\n",
    "        \"citations\": section_citations\n",
    "    }\n",
    "\n",
    "    full_citation_keys.update(section_citations_keys)\n",
    "\n",
    "\n",
    "full_citations = [bib_entries[key] for key in full_citation_keys if key in bib_entries]\n",
    "\n",
    "full_dict = {\n",
    "    \"title\": soup.find(\"h1\", class_=\"ltx_title ltx_title_document\").get_text(strip=True) if soup.find(\"h1\", class_=\"ltx_title ltx_title_document\") else \"Untitled\",\n",
    "    \"text\": \"\\n\".join(full_text),\n",
    "    \"masked_text\": \"\\n\".join(full_masked_text),\n",
    "    \"citations\": full_citations\n",
    "}\n",
    "\n",
    "\n",
    "parsed_result = {\n",
    "    \"title\": full_dict[\"title\"],\n",
    "    \"full\": full_dict,\n",
    "    \"sections\": sections_dict,\n",
    "    \"paragraphs\": paragraphs_dict\n",
    "}\n",
    "\n",
    "\n",
    "# for para_id, para_info in parsed_result['paragraphs'].items():\n",
    "#     print(f\"Paragraph ID: {para_id}\")\n",
    "#     print(para_info.keys())\n",
    "#     print('-----'*10)\n",
    "#     for k, v in para_info.items():\n",
    "#         print(f\"Key: {k}; Value: {v}\")\n",
    "#     print('*'*100)\n",
    "print(parsed_result[\"full\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9444e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m output_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtests\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(testset_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     testset \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      7\u001b[0m choose_idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m11\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m choose_idx:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "testset_path = r'data\\output\\dataset_coverage\\splits\\test.json'\n",
    "output_root = r'tests'\n",
    "with open(testset_path, 'r', encoding='utf-8') as f:\n",
    "    testset = json.load(f)\n",
    "\n",
    "choose_idx = [3, 7, 11]\n",
    "for idx in choose_idx:\n",
    "    paper = testset[idx]\n",
    "    paper_id = paper['paper']\n",
    "    output_path = os.path.join(output_root, f\"{paper_id}.json\")\n",
    "\n",
    "    if idx == 3:\n",
    "        masked_text = paper['full_paper']['masked_text']\n",
    "    elif idx == 7:\n",
    "        paras = paper['paragraphs']\n",
    "        for para in paras.values():\n",
    "            if len(para['citations']) > 0:\n",
    "                masked_text = para['masked_text']\n",
    "\n",
    "    elif idx == 11:\n",
    "        sections = paper['sections']\n",
    "        for sec in sections.values():\n",
    "            if len(sec['citations']) > 0:\n",
    "                masked_text = sec['masked_text']\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(masked_text, f)\n",
    "    print(f\"save masked file to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AINuggets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
