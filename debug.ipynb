{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab83692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1. Foundational Models for Depth and Flow\n",
      "\n",
      "In the tasks of monocular depth (MDE) and flow estimation,\n",
      "well-generalizable foundation models have replaced early\n",
      "deep learning approaches  in the last years. For\n",
      "MDE, DepthAnything  uses a data engine to construct\n",
      "a large corpus of automatically annotated data to learn rel-\n",
      "ative depth estimation. Additional fine-tuning allows for\n",
      "metric depth estimates. DepthAnythingV2  finetunes\n",
      "the previous model using synthetic data for better perfor-\n",
      "mance. Metric3D  and Metric3Dv2  transform im-\n",
      "ages to canonical camera intrinsics with a fixed focal length.\n",
      "DepthPro  proposes a two-stage training curriculum with\n",
      "a second stage solely on synthetic data to sharpen bound-\n",
      "ary predictions. DepthCrafter  leverages a conditional\n",
      "diffusion model to predict temporally consistent depth maps\n",
      "\n",
      "for videos. In this work, we utilize UniDepth  for metric\n",
      "MDE, which uses a geometric invariance loss on different\n",
      "image augmentation to enforce consistency.\n",
      "\n",
      "RAFT  presented the state of the art for optical flow\n",
      "It improved previous meth-\n",
      "estimation for a long time.\n",
      "ods by introducing a recurrent look-up operator on corre-\n",
      "lation volumes to iteratively refine flow predictions with-\n",
      "out needing coarse-to-fine flow pyramids. GMFlow \n",
      "avoids correlation volumes and instead leverages the prop-\n",
      "erties of transformers for global matching on feature maps.\n",
      "This removes the need for iterative steps to improve runtime\n",
      "performance. UniMatch  extends GMFlow network by\n",
      "tasks of disparity and depth prediction to enable cross-task\n",
      "transfer learning of a single transformer network.\n",
      "\n",
      "We rely on both off-the-shelf MDE and Optical Flow\n",
      "networks to benefit from strong geometric priors during\n",
      "training and inference.\n",
      "\n",
      "2.2. SfM and SLAM\n",
      "\n",
      "For many decades, the problem of recovering camera pa-\n",
      "rameters and geometry from images has been formulated\n",
      "as the Structure-from-Motion (SfM) pipeline .\n",
      "While many different implementations of the SfM pipeline\n",
      "exist, COLMAP  has emerged as the standard due to\n",
      "its robustness and flexibility. One of the drawbacks of SfM\n",
      "methods is their high computational cost. Simultaneous Lo-\n",
      "cation and Mapping (SLAM)  approaches em-\n",
      "ploy a similar pipeline to SfM but focus on the efficient pro-\n",
      "cessing of consecutive video frames. In recent years, these\n",
      "classical optimization-based approaches were enhanced by\n",
      "learned components . However,\n",
      "relying on epipolar geometry  or photometric consis-\n",
      "tency  makes them susceptible to high error on highly\n",
      "dynamic scenes. The strong focus on self driving data pro-\n",
      "vided datasets with mostly static environments ,\n",
      "an assumption that does not hold for casual videos.\n",
      "\n",
      "2.3. Learning Based SfM and SLAM\n",
      "\n",
      "Largely learning-based methods started to replace classical\n",
      "SLAM and SfM systems due to improved robustness .\n",
      "DROID-SLAM extends the framework of RAFT  by an\n",
      "update operator on both depth and pose estimates. A final\n",
      "differentiable bundle adjustment (BA) layer produces the fi-\n",
      "nal pose estimates. ParticleSfM  utilizes dense corre-\n",
      "spondences inside a BA framework to optimize poses. The\n",
      "dense correspondences are initialized from optical flow, and\n",
      "dynamic points are filtered using trajectory-based motion\n",
      "segmentation. CasualSAM  predicts both depth and\n",
      "movement from images to get frame-to-frame motion. A\n",
      "global optimization aligns the scale of the prediction and\n",
      "refines the poses. Dust3R  is a dense multi-view stereo\n",
      "method that regresses point coordinates between an im-\n",
      "age pair. This allows it to be extended to either SfM or\n",
      "\n",
      "SLAM. FlowMap  proposes to reconstruct a scene by\n",
      "overfitting a depth network to it and aligning depth maps\n",
      "via correspondences from flow or point tracking. LEAP-\n",
      "VO  combines visual and temporal information of video\n",
      "sequences to improve the tracking accuracy of points and\n",
      "identify occluded and dynamic points. A sliding window\n",
      "bundle adjustment then optimizes the poses. The concurrent\n",
      "work of MonST3R  finetunes Dust3r on mostly syn-\n",
      "thetic data to generalize it to dynamic scenes. While these\n",
      "works achieve impressive progress, they generally obtain\n",
      "poses from aligning depth and point maps or by optimizing\n",
      "them per-scene. This makes it hard to inject prior informa-\n",
      "tion about camera motion. In contrast, our method uses a\n",
      "neural network to predict a trajectory, which can effectively\n",
      "learn priors over realistic camera motions.\n"
     ]
    }
   ],
   "source": [
    "with open(r'data\\output\\processed\\no_citations\\AnyCam.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9997b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': [['Flownet: Learning optical flow with convolutional networks', 'Unsupervised monocular depth estimation with left- In Proceedings of the IEEE conference right consistency', 'Digging into self-supervised monocular In Proceedings of the IEEE/CVF inter- depth estimation'], ['Depth anything: Unleashing the power of large-scale unlabeled data'], ['Depth any- thing v2'], ['Metric3d: Towards zero-shot metric 3d prediction from a single image'], ['Metric3d v2: A versatile monocular geomet- ric foundation model for zero-shot metric depth and surface normal estimation'], ['Depth pro: Sharp monocular metric depth in less than a second'], ['Depthcrafter: Generating consistent long depth sequences for open-world videos'], ['Unidepth: Universal monocular metric depth estimation'], ['Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow'], ['Gmflow: Learning optical flow via global matching'], ['Unifying flow, stereo and depth estimation'], ['Multiple view ge- ometry in computer vision', 'A critique of structure-from-motion algo- rithms', 'A survey of structure from motion*'], ['Structure- In Proceedings of the IEEE con- from-motion revisited'], ['Dsac-differentiable ransac for camera localization', 'Superpoint: Self-supervised interest point detection and description', 'Ground- arXiv preprint ing image matching in 3d with mast3r', 'Learning correspondence uncer- tainty via differentiable nonlinear least squares', 'Deep fundamental matrix estimation', 'In Proceedings of matching with graph neural networks', 'Back to the feature: Learning robust camera localization from pixels to pose', 'Deep virtual stereo odometry: Leveraging deep depth predic- tion for monocular direct sparse odometry'], ['In defense of the eight-point algorithm'], ['Direct sparse odometry'], ['nuscenes: A multi- In Proceedings of modal dataset for autonomous driving', 'Vision meets robotics: The kitti dataset', 'Scalability in perception for autonomous driving: Waymo open dataset'], ['Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras'], ['Raft: Recurrent all-pairs field In Computer Vision–ECCV transforms for optical flow'], ['Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild'], ['Structure and motion from casual videos'], ['Dust3r: Geometric 3d vi- sion made easy'], ['Flowmap: High-quality camera poses, in- arXiv preprint trinsics, and depth via gradient descent'], ['Leap-vo: Long-term effective any point tracking for visual In Proceedings of the IEEE/CVF Conference odometry'], ['Monst3r: A simple approach for estimat- arXiv preprint ing geometry in the presence of motion']], 'positions': [186, 229, 421, 507, 523, 610, 745, 877, 995, 1266, 1469, 1913, 1986, 2665, 2705, 2735, 2889, 3086, 3129, 3287, 3509, 3672, 3829, 3981, 4219]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r'data\\output\\processed\\citations\\AnyCam.json', 'r') as f:\n",
    "    citations = json.load(f)\n",
    "print(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5be00d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the last years. \n",
      "--------------------------------------------------\n",
      " uses a data engine \n",
      "--------------------------------------------------\n",
      " finetunes\n",
      "the previ\n",
      "--------------------------------------------------\n",
      " and Metric3Dv2  tra\n",
      "--------------------------------------------------\n",
      " transform im-\n",
      "ages \n",
      "--------------------------------------------------\n",
      " proposes a two-stag\n",
      "--------------------------------------------------\n",
      " leverages a conditi\n",
      "--------------------------------------------------\n",
      " for metric\n",
      "MDE, whi\n",
      "--------------------------------------------------\n",
      " presented the state\n",
      "--------------------------------------------------\n",
      "\n",
      "avoids correlation \n",
      "--------------------------------------------------\n",
      " extends GMFlow netw\n",
      "--------------------------------------------------\n",
      ".\n",
      "While many differe\n",
      "--------------------------------------------------\n",
      " has emerged as the \n",
      "--------------------------------------------------\n",
      " Learning Based SfM \n",
      "--------------------------------------------------\n",
      "arning-based methods\n",
      "--------------------------------------------------\n",
      "o replace classical\n",
      "\n",
      "--------------------------------------------------\n",
      "se estimates. A fina\n",
      "--------------------------------------------------\n",
      "se correspondences a\n",
      "--------------------------------------------------\n",
      "ical flow, and\n",
      "dynam\n",
      "--------------------------------------------------\n",
      "-frame motion. A\n",
      "glo\n",
      "--------------------------------------------------\n",
      "nded to either SfM o\n",
      "--------------------------------------------------\n",
      " point tracking. LEA\n",
      "--------------------------------------------------\n",
      "ynamic points. A sli\n",
      "--------------------------------------------------\n",
      "to generalize it to \n",
      "--------------------------------------------------\n",
      "ra motion. In contra\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "positions = citations['positions']\n",
    "for p in positions:\n",
    "    print(text[p:p+20])\n",
    "    print('-----'*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AINuggets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
